{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitenv4fad118203a04c659ce9a79a2e522ae8",
   "display_name": "Python 3.6.9 64-bit ('env')",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Keyword extraction using TextRank"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preprocessing & Keyword extraction "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EXTRACT KEYWORDS FROM TEXTS #### \n",
    "\n",
    "def get_text_keywords(docs: list, scores: bool) -> list, list:\n",
    "\n",
    "    \"\"\"This function gets a list of texts and returns 1) a list with nested lists, each of which corresponds to the keywords of each text; 2) a list of texts after cleaning and lemmatization\"\"\"\n",
    "\n",
    "    # Import libraries\n",
    "    import spacy\n",
    "    import re\n",
    "\n",
    "    # Import class from py module (required)\n",
    "    from textrank4keyword import TextRank4Keyword\n",
    "\n",
    "    # init spacy nlp object\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # remove punctuations\n",
    "    docs = [re.sub('[^a-zA-Z]', ' ', text) for text in docs]\n",
    "    \n",
    "    # convert to lowercase\n",
    "    # docs = [text.lower() for text in docs]\n",
    "    \n",
    "    # remove tags\n",
    "    docs = [re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text) for text in docs]\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    docs = [re.sub(\"(\\\\d|\\\\W)+\",\" \",text) for text in docs]\n",
    "\n",
    "    texts_nlp = [nlp(doc) for doc in docs]\n",
    "\n",
    "    # lemmatize text before extracting keywords\n",
    "    lemmatized = [\" \".join([token.lemma_ for token in text]) for text in texts_nlp]\n",
    "\n",
    "    ## Generate keywords with TextRank4Keyword class\n",
    "    # init textrank object\n",
    "    tr4w = TextRank4Keyword()\n",
    "\n",
    "    # apply TextRank to all texts in the dataset and store a list of keywords for each text\n",
    "    list_of_keywords_lists_w3 = []\n",
    "\n",
    "    for doc in lemmatized:\n",
    "        # We only include Nouns (common and proper nouns), a windows-size of 3 and keep words' case\n",
    "        tr4w.analyze(doc, candidate_pos = ['NOUN', 'PROPN'], window_size=3, lower=False)\n",
    "        list_of_keywords_lists_w3.append(tr4w.get_keywords(7))\n",
    "\n",
    "    # return list of lists of keywords with or without scores\n",
    "    list_of_keywords_lists = []\n",
    "\n",
    "    # decide whether you want the list with keywords' scores or not\n",
    "    if scores == True:\n",
    "        list_of_keywords_lists = list_of_keywords_lists_w3\n",
    "    else:\n",
    "        for kwlist in list_of_keywords_lists_w3:\n",
    "            kw_list = [kw for kw, value in kwlist]\n",
    "            list_of_keywords_lists.append(kw_list)\n",
    "            \n",
    "    # return the lists\n",
    "    return list_of_keywords_lists, lemmatized"
   ]
  },
  {
   "source": [
    "## Post-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### POST-PROCESSING #### \n",
    "\n",
    "\n",
    "def get_cooc_keywords(texts_keywords: list, texts: list) -> list:\n",
    "\"\"\"TextRank requires a post-processing in which we check whether single words in a list of keywords form actually a multiword. We check whether keyword pairs (co-occurrences) occur in their respective text. If so, then the keywords are collapsed into a single keyword.\"\"\"\n",
    "\n",
    "    ## Generate all possible pair keyword combinations in all orders ('AB', 'BA')\n",
    "    import itertools\n",
    "    combinations_1 = [list(itertools.combinations(kw_list,2)) for kw_list in texts_keywords]\n",
    "\n",
    "    # Reverse order of tuples elements\n",
    "    def reverse(tuples): \n",
    "        new_tup = tuples[::-1] \n",
    "        return new_tup \n",
    "    \n",
    "    combinations_2 = []\n",
    "    for comb_list in combinations_1:\n",
    "        lst = [reverse(pair) for pair in comb_list]\n",
    "        combinations_2.append(lst)\n",
    "\n",
    "    # Convert tuples into strings for combinations_1 and 2 and save them into two lists\n",
    "    def convertTuple(tup): \n",
    "        str =  ' '.join(tup) \n",
    "        return str\n",
    "\n",
    "    pairs_strings_1 = []\n",
    "    for comb_list in combinations_1:\n",
    "        lst = [convertTuple(pair) for pair in comb_list]\n",
    "        pairs_strings_1.append(lst)\n",
    "\n",
    "    pairs_strings_2 = []\n",
    "    for comb_list in combinations_2:\n",
    "        lst = [convertTuple(pair) for pair in comb_list]\n",
    "        pairs_strings_2.append(lst)\n",
    "    \n",
    "    # Check if combinations of keyword pairings appear in their respective texts. \n",
    "    multiword_keyword_list = []\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "\n",
    "        pair_list = []\n",
    "\n",
    "        for j in pairs_strings_1[i]:\n",
    "            if j in texts[i]:\n",
    "                pair_list.append(j)\n",
    "        multiword_keyword_list.append(pair_list)\n",
    "\n",
    "        for j in pairs_strings_2[i]:\n",
    "            if j in texts[i]:\n",
    "                multiword_keyword_list[i].append(j)\n",
    "    \n",
    "    ## Merge multiword and singleword keywords in one single list\n",
    "    single_words = []\n",
    "\n",
    "    for kw_text, multiword in zip(texts_keywords, multiword_keyword_list):\n",
    "        pair_list = [w for w in kw_text if w not in str(multiword)]  \n",
    "        single_words.append(pair_list)\n",
    "\n",
    "    # Map index of single_words list and multiword list.\n",
    "    # The result is a nested list that needs to be flatten\n",
    "    lst_zip = list(zip(single_words, multiword_keyword_list))\n",
    "\n",
    "\n",
    "    # Function to flatten the list\n",
    "    def flattenNestedList(nestedList):\n",
    "        ''' Converts a nested list to a flat list '''\n",
    "        flatList = []\n",
    "        # Iterate over all the elements in given list\n",
    "        for elem in nestedList:\n",
    "            # Check if type of element is list\n",
    "            if isinstance(elem, list):\n",
    "                # Extend the flat list by adding contents of this element (list)\n",
    "                flatList.extend(flattenNestedList(elem))\n",
    "            else:\n",
    "                # Append the element to the list\n",
    "                flatList.append(elem)    \n",
    "        return flatList\n",
    "\n",
    "    # Convert lst_zip into list (zip() returns a tuple)\n",
    "    fl = [list(elem) for elem in lst_zip]\n",
    "\n",
    "    # Obtain a single list of keywords with single and multiwords in a single list\n",
    "    multiword_singleword_keywords_by_text = [flattenNestedList(elem) for elem in fl]\n",
    "\n",
    "    return multiword_singleword_keywords_by_text"
   ]
  },
  {
   "source": [
    "## Word and Text embeddings, and cosine similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### WORD EMBEDDINGS AND COSINE SIMILARITY ####\n",
    "\n",
    "def flair_embed_docPool(sentence: str) -> Vector :\n",
    "    \"\"\" Embed words with Flair's WordEmbeddings and DocumentPoolEmbeddings (for multi-words)\"\"\"\n",
    "\n",
    "    from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings, FlairEmbeddings\n",
    "    from flair.data import Sentence\n",
    "    \n",
    "    # init the word embeddings\n",
    "    flair_embedding_forward = FlairEmbeddings('news-forward')\n",
    "\n",
    "    # initialize the document embeddings, mode = mean\n",
    "    document_embeddings = DocumentPoolEmbeddings([flair_embedding_forward])\n",
    "\n",
    "    # create an example sentence\n",
    "    sentence = Sentence(sentence)\n",
    "\n",
    "    # embed the sentence with our document embedding\n",
    "    document_embeddings.embed(sentence)\n",
    "\n",
    "    # now check out the embedded sentence.\n",
    "    return sentence.get_embedding()\n",
    "\n",
    "# Define function to calculate cosine similarity\n",
    "def get_cosine_similarity(vector1, vector2):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    return cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0][0]\n",
    "\n",
    "# Embed words, calculate cosine similarity and get the results (see below) \n",
    "def cosine_list(keywords_by_text: list, ai_entities: list) -> list, dict:\n",
    "    thresh_dicts = {}\n",
    "\n",
    "    keys_list = []\n",
    "    values_list = []\n",
    "    thresh_dict = {}\n",
    "\n",
    "    threshold = 0.60\n",
    "\n",
    "    for ai in ai_entities:\n",
    "        for words_list in keywords_by_text:\n",
    "            try:\n",
    "                res = get_cosine_similarity(flair_embed_docPool(words_list), flair_embed_docPool(ai))\n",
    "                #print(f\"The cosine similarity between the keywords in [{words_list}] and [{ai}] is: {res*100}\")\n",
    "                if res > threshold:\n",
    "                    keys_list.append(ai)\n",
    "                    values_list.append(round(res*100,3))\n",
    "            except IndexError:\n",
    "                pass\n",
    "        \n",
    "        for index in range(len(keywords_by_text)):\n",
    "\n",
    "            thresh_dict = {}\n",
    "            for i in range(len(keys_list)):\n",
    "                thresh_dict[keys_list[i]] = values_list[i]\n",
    "                thresh_dicts[index] = thresh_dict\n",
    "\n",
    "    list_entities = [value for value in thresh_dicts.items()]\n",
    "\n",
    "    all_dicts = [x[1] for x in list_entities]\n",
    "\n",
    "    mother_lst = []\n",
    "\n",
    "    for a_dict in all_dicts:\n",
    "        mother_lst.append(sorted(a_dict, key=a_dict.get, reverse=True)[0])\n",
    "\n",
    "    # Return a list with top1 ai_entities or industries per text, and the dictionary with cosine similarity values > than threshold    \n",
    "    return mother_lst, thresh_dicts"
   ]
  },
  {
   "source": [
    "### Run code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Import libraries ###########\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "########### Load data ###########\n",
    "\n",
    "## Main data\n",
    "df = pd.read_csv('YOUR-PATH')\n",
    "#df.drop([\"Unnamed: 0.1\", \"Unnamed: 0\"], axis = 1, inplace = True)\n",
    "\n",
    "## List of ai entities\n",
    "ai_entities = pd.read_csv('YOUR-PATH')\n",
    "ai_ent = ai_entities['Applications'].to_list()\n",
    "\n",
    "## List of industries\n",
    "industries = pd.read_csv('YOUR-PATH')\n",
    "ind = industries['industries'].to_list()\n",
    "\n",
    "\n",
    "\n",
    "########### Lemmatize industries ###########\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "# Convert to lowercase\n",
    "ind_list = [industry.lower() for industry in ind]\n",
    "    \n",
    "# Remove special characters and digits\n",
    "ind_list = [re.sub(\"(\\\\d|\\\\W)+\",\" \",industry) for industry in ind_list]\n",
    "\n",
    "industry_nlp = [nlp(industry) for industry in ind_list]\n",
    "\n",
    "# We lemmatize industry before extracting keywords\n",
    "lemmatized_industries = [\" \".join([token.lemma_ for token in industry]) for industry in industry_nlp]\n",
    "\n",
    "\n",
    "\n",
    "########### Prepare data for passing it through functions ###########\n",
    "\n",
    "## Unify title and text to extract keywords from\n",
    "df['titext'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "docs = df['titext'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### SAMPLE TO TRY CODE ### \n",
    "\n",
    "# Uncomment and execute the lines below if you want to try your code first with a small sample (change n value)\n",
    "\n",
    "# data_sample = df.sample(n = 2, replace=False, random_state = 123, axis = 0)\n",
    "# docs = data_sample['titext'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### GET KEYWORDS ###########\n",
    "\n",
    "# Get the keywords and the lemmatized text\n",
    "texts_keywords, lemmatized_text = get_text_keywords(docs, scores = False)\n",
    "\n",
    "# Get complete keywords\n",
    "complete_keywords = get_cooc_keywords(texts_keywords, lemmatized_text)\n",
    "\n",
    "# Save keywords in the dataframe\n",
    "data_sample['def_keywords'] = complete_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### GET RESULTS OF COSINE SIMILARITY ###########\n",
    "\n",
    "#Get list with keywords by text in string form (needed to proceed to embedding)\n",
    "keywords_by_text = [\" \".join(map(str, text_keywords)) for text_keywords in complete_keywords]\n",
    "\n",
    "#Execute word embeddings + cosine similarity between each keyword list and each AI usecase/entity\n",
    "top1_ai_entities, entities_above_threshold = cosine_list(keywords_by_text, ai_ent)\n",
    "#top1_ai_entities\n",
    "\n",
    "#Execute word embeddings + cosine similarity between each keyword list and each industry\n",
    "top1_industries, industries_above_threshold = cosine_list(keywords_by_text, lemmatized_industries)\n",
    "#top1_industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SAVE RESULTS IN THE DF AND IN A CSV FILE ####\n",
    "\n",
    "df[\"top1_entities\"] = top1_ai_entities\n",
    "df[\"top1_industries\"] = top1_industries\n",
    "\n",
    "# Select the columns that you need\n",
    "data_final = df[[\"title\",\"text\",\"date\",\"source\",\"top1_entities\",\"top1_industries\"]]\n",
    "data_final.to_csv('YOUR-PATH/results.csv')"
   ]
  }
 ]
}