,link,text,title,date,author,tags,keywords,summary,source
0,https://news.mit.edu/2021/language-learning-efficiency-0210,"


Human language can be inefficient. Some words are vital. Others, expendable.

Reread the first sentence of this story. Just two words, “language” and “inefficient,” convey almost the entire meaning of the sentence. The importance of key words underlies a popular new tool for natural language processing (NLP) by computers: the attention mechanism. When coded into a broader NLP algorithm, the attention mechanism homes in on key words rather than treating every word with equal importance. That yields better results in NLP tasks like detecting positive or negative sentiment or predicting which words should come next in a sentence.

The attention mechanism’s accuracy often comes at the expense of speed and computing power, however. It runs slowly on general-purpose processors like you might find in consumer-grade computers. So, MIT researchers have designed a combined software-hardware system, dubbed SpAtten, specialized to run the attention mechanism. SpAtten enables more streamlined NLP with less computing power.

“Our system is similar to how the human brain processes language,” says Hanrui Wang. “We read very fast and just focus on key words. That’s the idea with SpAtten.”

The research will be presented this month at the IEEE International Symposium on High-Performance Computer Architecture. Wang is the paper’s lead author and a PhD student in the Department of Electrical Engineering and Computer Science. Co-authors include Zhekai Zhang and their advisor, Assistant Professor Song Han.

Since its introduction in 2015, the attention mechanism has been a boon for NLP. It’s built into state-of-the-art NLP models like Google’s BERT and OpenAI’s GPT-3. The attention mechanism’s key innovation is selectivity — it can infer which words or phrases in a sentence are most important, based on comparisons with word patterns the algorithm has previously encountered in a training phase. Despite the attention mechanism’s rapid adoption into NLP models, it’s not without cost.

NLP models require a hefty load of computer power, thanks in part to the high memory demands of the attention mechanism. “This part is actually the bottleneck for NLP models,” says Wang. One challenge he points to is the lack of specialized hardware to run NLP models with the attention mechanism. General-purpose processors, like CPUs and GPUs, have trouble with the attention mechanism’s complicated sequence of data movement and arithmetic. And the problem will get worse as NLP models grow more complex, especially for long sentences. “We need algorithmic optimizations and dedicated hardware to process the ever-increasing computational demand,” says Wang.

The researchers developed a system called SpAtten to run the attention mechanism more efficiently. Their design encompasses both specialized software and hardware. One key software advance is SpAtten’s use of “cascade pruning,” or eliminating unnecessary data from the calculations. Once the attention mechanism helps pick a sentence’s key words (called tokens), SpAtten prunes away unimportant tokens and eliminates the corresponding computations and data movements. The attention mechanism also includes multiple computation branches (called heads). Similar to tokens, the unimportant heads are identified and pruned away. Once dispatched, the extraneous tokens and heads don’t factor into the algorithm’s downstream calculations, reducing both computational load and memory access.

To further trim memory use, the researchers also developed a technique called “progressive quantization.” The method allows the algorithm to wield data in smaller bitwidth chunks and fetch as few as possible from memory. Lower data precision, corresponding to smaller bitwidth, is used for simple sentences, and higher precision is used for complicated ones. Intuitively it’s like fetching the phrase “cmptr progm” as the low-precision version of “computer program.”

Alongside these software advances, the researchers also developed a hardware architecture specialized to run SpAtten and the attention mechanism while minimizing memory access. Their architecture design employs a high degree of “parallelism,” meaning multiple operations are processed simultaneously on multiple processing elements, which is useful because the attention mechanism analyzes every word of a sentence at once. The design enables SpAtten to rank the importance of tokens and heads (for potential pruning) in a small number of computer clock cycles. Overall, the software and hardware components of SpAtten combine to eliminate unnecessary or inefficient data manipulation, focusing only on the tasks needed to complete the user’s goal.

The philosophy behind the system is captured in its name. SpAtten is a portmanteau of “sparse attention,” and the researchers note in the paper that SpAtten is “homophonic with ‘spartan,’ meaning simple and frugal.” Wang says, “that’s just like our technique here: making the sentence more concise.” That concision was borne out in testing.

The researchers coded a simulation of SpAtten’s hardware design — they haven’t fabricated a physical chip yet — and tested it against competing general-purposes processors. SpAtten ran more than 100 times faster than the next best competitor (a TITAN Xp GPU). Further, SpAtten was more than 1,000 times more energy efficient than competitors, indicating that SpAtten could help trim NLP’s substantial electricity demands.

The researchers also integrated SpAtten into their previous work, to help validate their philosophy that hardware and software are best designed in tandem. They built a specialized NLP model architecture for SpAtten, using their Hardware-Aware Transformer (HAT) framework, and achieved a roughly two times speedup over a more general model.

The researchers think SpAtten could be useful to companies that employ NLP models for the majority of their artificial intelligence workloads. “Our vision for the future is that new algorithms and hardware that remove the redundancy in languages will reduce cost and save on the power budget for data center NLP workloads” says Wang.

On the opposite end of the spectrum, SpAtten could bring NLP to smaller, personal devices. “We can improve the battery life for mobile phone or IoT devices,” says Wang, referring to internet-connected “things” — televisions, smart speakers, and the like. “That’s especially important because in the future, numerous IoT devices will interact with humans by voice and natural language, so NLP will be the first application we want to employ.”

Han says SpAtten’s focus on efficiency and redundancy removal is the way forward in NLP research. “Human brains are sparsely activated [by key words]. NLP models that are sparsely activated will be promising in the future,” he says. “Not all words are equal — pay attention only to the important ones.”


",A language learning system that pays attention — more efficiently than ever before,2021-02-10,['Daniel Ackerman'],Artificial intelligence/Machine learning/Algorithms/Language/internet of things/Electrical Engineering & Computer Science (eecs)/MIT Schwarzman College of Computing/School of Engineering,"['spatten', 'data', 'key', 'language', 'mechanism', 'words', 'learning', 'attention', 'researchers', 'pays', 'models', 'efficiently', 'system', 'hardware', 'nlp']","The importance of key words underlies a popular new tool for natural language processing (NLP) by computers: the attention mechanism.
When coded into a broader NLP algorithm, the attention mechanism homes in on key words rather than treating every word with equal importance.
So, MIT researchers have designed a combined software-hardware system, dubbed SpAtten, specialized to run the attention mechanism.
NLP models require a hefty load of computer power, thanks in part to the high memory demands of the attention mechanism.
The researchers developed a system called SpAtten to run the attention mechanism more efficiently.",Mit
1,https://news.mit.edu/2021/fabricating-fully-functional-drones-0208,"


From Star Trek’s replicators to Richie Rich’s wishing machine, popular culture has a long history of parading flashy machines that can instantly output any item to a user’s delight. 
While 3D printers have now made it possible to produce a range of objects that include product models, jewelry, and novelty toys, we still lack the ability to fabricate more complex devices that are essentially ready-to-go right out of the printer. 
A group from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) recently developed a new system to print functional, custom-made devices and robots, without human intervention. Their single system uses a three-ingredient recipe that lets users create structural geometry, print traces, and assemble electronic components like sensors and actuators. 








      

            LaserFactory: Fabricating fully functional devices        

    



“LaserFactory” has two parts that work in harmony: a software toolkit that allows users to design custom devices, and a hardware platform that fabricates them. 
CSAIL PhD student Martin Nisser says that this type of “one-stop shop” could be beneficial for product developers, makers, researchers, and educators looking to rapidly prototype things like wearables, robots, and printed electronics. 
“Making fabrication inexpensive, fast, and accessible to a layman remains a challenge,” says Nisser, lead author on a paper about LaserFactory that will appear in the ACM Conference on Human Factors in Computing Systems in May. “By leveraging widely available manufacturing platforms like 3D printers and laser cutters, LaserFactory is the first system that integrates these capabilities and automates the full pipeline for making functional devices in one system.” 
Inside LaserFactory 
Let’s say a user has aspirations to create their own drone. They’d first design their device by placing components on it from a parts library, and then draw on circuit traces, which are the copper or aluminum lines on a printed circuit board that allow electricity to flow between electronic components. They’d then finalize the drone’s geometry in the 2D editor. In this case, they’d use propellers and batteries on the canvas, wire them up to make electrical connections, and draw the perimeter to define the quadcopter’s shape. 
The user can then preview their design before the software translates their custom blueprint into machine instructions. The commands are embedded into a single fabrication file for LaserFactory to make the device in one go, aided by the standard laser cutter software. On the hardware side, an add-on that prints circuit traces and assembles components is clipped onto the laser cutter. 
Similar to a chef, LaserFactory automatically cuts the geometry, dispenses silver for circuit traces, picks and places components, and finally cures the silver to make the traces conductive, securing the components in place to complete fabrication. 
The device is then fully functional, and in the case of the drone, it can immediately take off to begin a task — a feature that could in theory be used for diverse jobs such as delivery or search-and-rescue operations.
As a future avenue, the team hopes to increase the quality and resolution of the circuit traces, which would allow for denser and more complex electronics. 
As well as fine-tuning the current system, the researchers hope to build on this technology by exploring how to create a fuller range of 3D geometries, potentially through integrating traditional 3D printing into the process. 
“Beyond engineering, we’re also thinking about how this kind of one-stop shop for fabrication devices could be optimally integrated into today’s existing supply chains for manufacturing, and what challenges we may need to solve to allow for that to happen,” says Nisser. “In the future, people shouldn’t be expected to have an engineering degree to build robots, any more than they should have a computer science degree to install software.” 
This research is based upon work supported by the National Science Foundation. The work was also supported by a Microsoft Research Faculty Fellowship and The Royal Swedish Academy of Sciences.


",Fabricating fully functional drones,2021-02-08,['Rachel Gordon'],Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Research/3-D printing/Design/Manufacturing/Retail/Human-computer interaction/Invention/MIT Schwarzman College of Computing,"['users', 'devices', 'circuit', 'work', 'fully', 'drones', 'components', 'system', 'laserfactory', 'fabricating', '3d', 'functional', 'theyd', 'traces']","A group from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) recently developed a new system to print functional, custom-made devices and robots, without human intervention.
Their single system uses a three-ingredient recipe that lets users create structural geometry, print traces, and assemble electronic components like sensors and actuators.
On the hardware side, an add-on that prints circuit traces and assembles components is clipped onto the laser cutter.
As a future avenue, the team hopes to increase the quality and resolution of the circuit traces, which would allow for denser and more complex electronics.
The work was also supported by a Microsoft Research Faculty Fellowship and The Royal Swedish Academy of Sciences.",Mit
2,https://news.mit.edu/2021/examining-world-through-signals-and-systems-cathy-wu-0205,"


There’s a mesmerizing video animation on YouTube of simulated, self-driving traffic streaming through a six-lane, four-way intersection. Dozens of cars flow through the streets, pausing, turning, slowing, and speeding up to avoid colliding with their neighbors. And not a single car stopping. But what if even one of those vehicles was not autonomous? What if only one was?
In the coming decades, autonomous vehicles will play a growing role in society, whether keeping drivers safer, making deliveries, or increasing accessibility and mobility for elderly or disabled passengers.
But MIT Assistant Professor Cathy Wu argues that autonomous vehicles are just part of a complex transport system that may involve individual self-driving cars, delivery fleets, human drivers, and a range of last-mile solutions to get passengers to their doorstep — not to mention road infrastructure like highways, roundabouts, and, yes, intersections.
Transport today accounts for about one-third of U.S. energy consumption. The decisions we make today about autonomous vehicles could have a big impact on this number — ranging from a 40 percent decrease in energy use to a doubling of energy consumption.
So how can we better understand the problem of integrating autonomous vehicles into the transportation system? Equally important, how can we use this understanding to guide us toward better-functioning systems?
Wu, who joined the Laboratory for Information and Decision Systems (LIDS) and MIT in 2019, is the Gilbert W. Winslow Assistant Professor of Civil and Environmental Engineering as well as a core faculty member of the MIT Institute for Data, Systems, and Society. Growing up in a Philadelphia-area family of electrical engineers, Wu sought a field that would enable her to harness engineering skills to solve societal challenges. 
During her years as an undergraduate at MIT, she reached out to Professor Seth Teller of the Computer Science and Artificial Intelligence Laboratory to discuss her interest in self-driving cars.
Teller, who passed away in 2014, met her questions with warm advice, says Wu. “He told me, ‘If you have an idea of what your passion in life is, then you have to go after it as hard as you possibly can. Only then can you hope to find your true passion.’
“Anyone can tell you to go after your dreams, but his insight was that dreams and ambitions are not always clear from the start. It takes hard work to find and pursue your passion.” 
Chasing that passion, Wu would go on to work with Teller, as well as in Professor Daniela Rus’s Distributed Robotics Laboratory, and finally as a graduate student at the University of California at Berkeley, where she won the IEEE Intelligent Transportation Systems Society's best PhD award in 2019.
In graduate school, Wu had an epiphany: She realized that for autonomous vehicles to fulfill their promise of fewer accidents, time saved, lower emissions, and greater socioeconomic and physical accessibility, these goals must be explicitly designed-for, whether as physical infrastructure, algorithms used by vehicles and sensors, or deliberate policy decisions.
At LIDS, Wu uses a type of machine learning called reinforcement learning to study how traffic systems behave, and how autonomous vehicles in those systems ought to behave to get the best possible outcomes.
Reinforcement learning, which was most famously used by AlphaGo, DeepMind’s human-beating Go program, is a powerful class of methods that capture the idea behind trial-and-error — given an objective, a learning agent repeatedly attempts to achieve the objective, failing and learning from its mistakes in the process.
In a traffic system, the objectives might be to maximize the overall average velocity of vehicles, to minimize travel time, to minimize energy consumption, and so on.
When studying common components of traffic networks such as grid roads, bottlenecks, and on- and off-ramps, Wu and her colleagues have found that reinforcement learning can match, and in some cases exceed, the performance of current traffic control strategies. And more importantly, reinforcement learning can shed new light toward understanding complex networked systems — which have long evaded classical control techniques. For instance, if just 5 to 10 percent of vehicles on the road were autonomous and used reinforcement learning, that could eliminate congestion and boost vehicle speeds by 30 to 140 percent. And the learning from one scenario often translates well to others. These insights could one day soon help to inform public policy or business decisions.
In the course of this research, Wu and her colleagues helped improve a class of reinforcement learning methods called policy gradient methods. Their advancements turned out to be a general improvement to most existing deep reinforcement learning methods.
But reinforcement learning techniques will need to be continually improved to keep up with the scale and shifts in infrastructure and changing behavior patterns. And research findings will need to be translated into action by urban planners, auto makers and other organizations.
Today, Wu is collaborating with public agencies in Taiwan and Indonesia to use insights from her work to guide better dialogues and decisions. By changing traffic signals or using nudges to shift drivers’ behavior, are there other ways to achieve lower emissions or smoother traffic? 
“I’m surprised by this work every day,” says Wu. “We set out to answer a question about self-driving cars, and it turns out you can pull apart the insights, apply them in other ways, and then this leads to new exciting questions to answer.”
Wu is happy to have found her intellectual home at LIDS. Her experience of it is as a “very deep, intellectual, friendly, and welcoming place.” And she counts among her research inspirations MIT course 6.003 (Signals and Systems) — a class she encourages everyone to take — taught in the tradition of professors Alan Oppenheim (Research Laboratory of Electronics) and Alan Willsky (LIDS). “The course taught me that so much in this world could be fruitfully examined through the lens of signals and systems, be it electronics or institutions or society,” she says. “I am just realizing as I’m saying this, that I've been empowered by LIDS thinking all along!”
Research and teaching through a pandemic haven’t been easy, but Wu is making the best of a challenging first year as faculty. (“I’ve been working from home in Cambridge — my short walking commute is irrelevant at this point,” she says wryly.) To unwind, she enjoys running, listening to podcasts covering topics ranging from science to history, and reverse-engineering her favorite Trader Joe’s frozen foods.
She’s also been working on two Covid-related projects born at MIT: One explores how data from the environment, such as data collected by internet-of-things-connected thermometers, can help identify emerging community outbreaks. Another project asks if it’s possible to ascertain how contagious the virus is on public transport, and how different factors might decrease the transmission risk.
Both are in their early stages, Wu says. “We hope to contribute a bit to the pool of knowledge that can help decision-makers somewhere. It’s been very enlightening and rewarding to do this and see all the other efforts going on around MIT.” 


",Examining the world through signals and systems,2021-02-05,['Grace Chua'],Civil and environmental engineering/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Distributed Robotics Laboratory/Laboratory for Information and Decision Systems (LIDS)/Research Laboratory of Electronics (RLE)/IDSS/Faculty/Profile/Artificial intelligence/Machine learning/Autonomous vehicles/Traffic management/Transportation/MIT Schwarzman College of Computing/School of Engineering,"['traffic', 'systems', 'lids', 'examining', 'learning', 'signals', 'autonomous', 'work', 'world', 'vehicles', 'wu', 'reinforcement', 'mit']","So how can we better understand the problem of integrating autonomous vehicles into the transportation system?
At LIDS, Wu uses a type of machine learning called reinforcement learning to study how traffic systems behave, and how autonomous vehicles in those systems ought to behave to get the best possible outcomes.
And more importantly, reinforcement learning can shed new light toward understanding complex networked systems — which have long evaded classical control techniques.
Their advancements turned out to be a general improvement to most existing deep reinforcement learning methods.
But reinforcement learning techniques will need to be continually improved to keep up with the scale and shifts in infrastructure and changing behavior patterns.",Mit
3,https://news.mit.edu/2021/byte-sized-learning-eecs-iap-0204,"


For 50 years, MIT students have taken advantage of Independent Activities Period, a special mini-term, only four weeks long, tucked between the end of the fall and beginning of the spring semesters. This year, IAP looked a little different, as Covid-19 precautions led instructors to shift their classes online, but the term’s spirit of innovative, creative exploration remained.
In keeping with that spirit, IAP offerings from the Department of Electrical Engineering and Computer Science (EECS) ranged from the playful to the profound.  
Many MIT students take advantage of the short IAP session to delve into a topic or field outside their regular comfort zone. “Deep Learning For Art, Aesthetics, and Creativity” was designed to accommodate exactly that sort of exploration. The course’s instructor, Ali Jahanian, says his inspiration for teaching the IAP stems from his own experiences with the arts. “I have a background in design, painting, and visual arts, and always got inspired from that kind of creative work. In my research, I have been working on understanding and quantifying aesthetics and design, but after my PhD thesis, I got more involved with the intriguing notion of learning by creating,” he reports. “While many of my colleagues are working on understanding intelligence, I am specifically interested in the angle of creativity and innovation of our intelligence. For instance, how can we generalize from data to create data that is out-of-distribution of the seen datasets?”
Jahanian’s approach is specifically geared to appeal not only to computer scientists, but to anyone with curiosity about the artistic possibilities of AI. “The idea of the course is to help students understand how can we use AI for creativity, and how creativity can help us learn and develop better AI. First they need to understand what’s happening right now, in 2021, around AI and creativity — then they need to understand the boundaries of those notions and how they can push that boundary forward,” says Jahanian, who sees the use of artificial intelligence as an inherently creative act. “To me, AI is fascinating because it’s a reflection of how we are. We have the desire of replicating and recreating ourselves, and that is why artists get joy out of creating.”
The applications of a creative AI — or one that can predict the human aesthetic preference — are numerous. “We can certainly learn and quantify our taste. Those kinds of quantitative algorithms have broad applications in understanding our emotions and feeling; for instance, if you have a robot which communicates with you, in your home, you want that robot to understand what you want, what you like, what is your personal taste.” More immediately, Jahanian believes his students will benefit from the tangible nature of art as a tool for learning. “I think of this process as learning by defining a problem; if the problem is something interesting and tangible, something we can relate to, then we have a better chance of engaging ourselves,” says Jahanian, noting that the scholarly term for the satisfying feeling of solving a problem is “visceral aesthetics.” “Hopefully, the students will get motivated to learn something about AI through this cool topic. Sometimes it is hard for students to understand how a loss function works, but if I could see it visually while trying to match two images, maybe that can help me to intuitively understand loss function or the math behind it. Maybe that even helps me become motivated to learn more.”
For those who like a bit of risk with their learning, the Pokerbots competition, now going on its 10th year, offered the chance to win real prizes by designing and deploying competitive, card-playing bots in a virtual tournament. “I joined freshman year because I was interested in the intersection of math, computer science, and game theory; Pokerbots seemed like a good way to combine all three,” says Shreyas Srinivasan, now a junior in mathematics with computer science who is serving as the president of MIT Pokerbots for the second year. “The project of building a bot forces you to apply the concepts you’ve learned, dive deep into them, and increase your understanding. It leaves you with a sense of accomplishment and the unique experience of creating a bot that can demolish human players in poker.""
Srinivasan isn’t exaggerating the bots’ power. “There is a good comparison to be made to chess and other games that have been solved with computational processes,” says Stephen Otremba, a senior in Course 6-3 (Computer Science and Engineering) and head instructor of this year’s Pokerbots course. “With the progress we’ve made in the realms of machine and reinforcement learning, programs have been able to advance more quickly than human play.” Each Pokerbots attendee had the chance to test that claim; at the end of each IAP, the organizers have traditionally set aside time for the players to try their luck against their own bots. “Over the last few years, in the game variants that we set, the bots are able to beat their creators handily,” reports Srinivasan. Those variants change every year — this year, the organizers took their inspiration from popular minigame Blotto by providing each player with an individual pool of chips. The players then created a bot that can advantageously allocate the pool across three simultaneous poker games. Fittingly, this year’s competition was primarily sponsored by quantitative trading firms. Srinivasan explains why: “Both trading and poker revolve around risk management, and this year’s game variant is all about managing the resource of your starting stack of chips across three boards. It is similar to trading where you are managing a portfolio of different stocks and securities and seeking maximum returns from that allocation.”
Both Srinivasan and Otremba acknowledge the changes that Covid brought to their beloved competition, but those changes are not without a silver lining. “Something we’re considering implementing in future years is continuing to livestream our lectures, because it means that more students are able to attend,” says Srinivasan, who also reports greater ease in booking guest speakers over Zoom. This year, one of those guest speakers included famed computational poker researcher Noam Brown, now an AI researcher at Facebook, who developed Libratus, one of the first AIs capable of beating professional poker players. Whether Pokerbots students go on to reign on the competitive circuit or join a trading firm, their experience is sure to serve them well. Says Otremba: “The Pokerbots competition not only provided me an outlet in a real project environment, but taught me about putting my ideas into code, collaborating with a team effectively, learning all the stages that software development goes through; handling all the challenges like debugging and theory problems. It provided me with a great way to get some valuable experience in the software area and acted like a springboard to even more challenging projects.”
“Code For Good” was another offering for students who wanted to use their technical skills to make a positive difference in the world. The long-running workshop partnered with nonprofit organizations to tackle technical projects, giving students a chance to contribute valuable time and expertise to organizations that align with their personal areas of interest. “I really believe in the mission of Code For Good, helping nonprofits solve their challenges with technology,” says Lucy Liao, a senior majoring in 6-3 who joined Code For Good in her junior year as an organizer, a project management role that helps link a team of students with a nonprofit. “It was an interesting experience because, in addition to thinking about projects and code, there was a lot of communications as well — talking to the nonprofits, making sure everyone is aware of the deadlines, keeping the projects moving smoothly,” Liao reports.
Those projects run the gamut, from discrete and contained to sprawling and ambitious. Senior Victoria Juan explains the application process: “Throughout the school year, the nonprofits apply for help through a Google Form which asks them to include details about themselves, like what kind of project they’re looking for and what kind of resources or budget they already have. At the beginning of IAP, we make the projects known to the students, who make their choices based on the project; sometimes, people are passionate about the mission of the nonprofit as well.” Those nonprofits run the gamut, from a peace-building organization based in Afghanistan to the Cambridge, Massachusetts, YWCA.  
While the Code For Good team used to exert a degree of editorial control over the scope of the projects, they more recently experimented with allowing the students to take on as much as they feel they can accomplish — with surprising results. “There were quite a few projects which I thought were so big, and I was not really sure if they were doable within one month, but I’ve been surprised by how much progress our students have been able to make over the course of one January,” says Liao. “It’s cool to see what people have been able to accomplish in such a short time.”
Besides the resume-building software accomplishments, Code For Good students are able to add leadership skills to their portfolio. Says Juan: “Helping to run the IAP was a really valuable experience because I had the chance to experience being the teacher and the leader of a classroom: making deadlines, running weekly presentations, to feel that responsibility and to make sure that the teams and projects are running on schedule — or, if they’re not, help them find a way to resolve those issues.” And Liao points out the surprisingly important role of communications in this tech-forward club: “A big part of the experience is learning to communicate with nontechnical people. We work often with people at MIT who are very technical, and so that’s easy for us — but with people who aren’t familiar with computers and are less comfortable with technical lingo, there are challenges. I think that’s valuable communications experience to get.”
Not too bad for a byte-sized semester!


",Byte-sized learning,2021-02-04,['Jane Halpern'],School of Engineering/MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Independent Activities Period/Classes and programs/Students/Undergraduate/Artificial intelligence/Gaming,"['bytesized', 'good', 'code', 'learning', 'projects', 'iap', 'ai', 'experience', 'pokerbots', 'understand', 'students']","“Deep Learning For Art, Aesthetics, and Creativity” was designed to accommodate exactly that sort of exploration.
In my research, I have been working on understanding and quantifying aesthetics and design, but after my PhD thesis, I got more involved with the intriguing notion of learning by creating,” he reports.
“The idea of the course is to help students understand how can we use AI for creativity, and how creativity can help us learn and develop better AI.
Whether Pokerbots students go on to reign on the competitive circuit or join a trading firm, their experience is sure to serve them well.
I think that’s valuable communications experience to get.”Not too bad for a byte-sized semester!",Mit
4,https://news.mit.edu/2021/machine-learning-model-helps-determine-protein-structures-0204,"


Cryo-electron microscopy (cryo-EM) allows scientists to produce high-resolution, three-dimensional images of tiny molecules such as proteins. This technique works best for imaging proteins that exist in only one conformation, but MIT researchers have now developed a machine-learning algorithm that helps them identify multiple possible structures that a protein can take.

Unlike AI techniques that aim to predict protein structure from sequence data alone, protein structure can also be experimentally determined using cryo-EM, which produces hundreds of thousands, or even millions, of two-dimensional images of protein samples frozen in a thin layer of ice. Computer algorithms then piece together these images, taken from different angles, into a three-dimensional representation of the protein in a process termed reconstruction.

In a Nature Methods paper, the MIT researchers report a new AI-based software for reconstructing multiple structures and motions of the imaged protein — a major goal in the protein science community. Instead of using the traditional representation of protein structure as electron-scattering intensities on a 3D lattice, which is impractical for modeling multiple structures, the researchers introduced a new neural network architecture that can efficiently generate the full ensemble of structures in a single model.

“With the broad representation power of neural networks, we can extract structural information from noisy images and visualize detailed movements of macromolecular machines,” says Ellen Zhong, an MIT graduate student and the lead author of the paper.









      

            Graduate student Ellen Zhong shows how her team combines cryo-electron microscopy and machine learning to visualize molecules in 3D.        

    



With their software, they discovered protein motions from imaging datasets where only a single static 3D structure was originally identified. They also visualized large-scale flexible motions of the spliceosome — a protein complex that coordinates the splicing of the protein coding sequences of transcribed RNA.

“Our idea was to try to use machine-learning techniques to better capture the underlying structural heterogeneity, and to allow us to inspect the variety of structural states that are present in a sample,” says Joseph Davis, the Whitehead Career Development Assistant Professor in MIT’s Department of Biology.

Davis and Bonnie Berger, the Simons Professor of Mathematics at MIT and head of the Computation and Biology group at the Computer Science and Artificial Intelligence Laboratory, are the senior authors of the study, which appears today in Nature Methods. MIT postdoc Tristan Bepler is also an author of the paper.

Visualizing a multistep process

The researchers demonstrated the utility of their new approach by analyzing structures that form during the process of assembling ribosomes — the cell organelles responsible for reading messenger RNA and translating it into proteins. Davis began studying the structure of ribosomes while a postdoc at the Scripps Research Institute. Ribosomes have two major subunits, each of which contains many individual proteins that are assembled in a multistep process.

To study the steps of ribosome assembly in detail, Davis stalled the process at different points and then took electron microscope images of the resulting structures. At some points, blocking assembly resulted in accumulation of just a single structure, suggesting that there is only one way for that step to occur. However, blocking other points resulted in many different structures, suggesting that the assembly could occur in a variety of ways.

Because some of these experiments generated so many different protein structures, traditional cryo-EM reconstruction tools did not work well to determine what those structures were.

“In general, it’s an extremely challenging problem to try to figure out how many states you have when you have a mixture of particles,” Davis says.

After starting his lab at MIT in 2017, he teamed up with Berger to use machine learning to develop a model that can use the two-dimensional images produced by cryo-EM to generate all of the three-dimensional structures found in the original sample.

In the new Nature Methods study, the researchers demonstrated the power of the technique by using it to identify a new ribosomal state that hadn’t been seen before. Previous studies had suggested that as a ribosome is assembled, large structural elements, which are akin to the foundation for a building, form first. Only after this foundation is formed are the “active sites” of the ribosome, which read messenger RNA and synthesize proteins, added to the structure.

In the new study, however, the researchers found that in a very small subset of ribosomes, about 1 percent, a structure that is normally added at the end actually appears before assembly of the foundation. To account for that, Davis hypothesizes that it might be too energetically expensive for cells to ensure that every single ribosome is assembled in the correct order.

“The cells are likely evolved to find a balance between what they can tolerate, which is maybe a small percentage of these types of potentially deleterious structures, and what it would cost to completely remove them from the assembly pathway,” he says.

Viral proteins

The researchers are now using this technique to study the coronavirus spike protein, which is the viral protein that binds to receptors on human cells and allows them to enter cells. The receptor binding domain (RBD) of the spike protein has three subunits, each of which can point either up or down.

“For me, watching the pandemic unfold over the past year has emphasized how important front-line antiviral drugs will be in battling similar viruses, which are likely to emerge in the future. As we start to think about how one might develop small molecule compounds to force all of the RBDs into the ‘down’ state so that they can’t interact with human cells, understanding exactly what the ‘up’ state looks like and how much conformational flexibility there is will be informative for drug design. We hope our new technique can reveal these sorts of structural details,” Davis says.

The research was funded by the National Science Foundation Graduate Research Fellowship Program, the National Institutes of Health, and the MIT Jameel Clinic for Machine Learning and Health. This work was supported by MIT Satori computation cluster hosted at the MGHPCC.


",Machine-learning model helps determine protein structures,2021-02-04,['Anne Trafton'],Research/Biology/Mathematics/School of Science/Computer Science and Artificial Intelligence Laboratory (CSAIL)/MIT Schwarzman College of Computing/National Science Foundation (NSF)/National Institutes of Health (NIH)/Machine learning/Artificial intelligence,"['structural', 'protein', 'model', 'davis', 'structures', 'researchers', 'proteins', 'helps', 'images', 'structure', 'study', 'determine', 'machinelearning', 'mit']","In a Nature Methods paper, the MIT researchers report a new AI-based software for reconstructing multiple structures and motions of the imaged protein — a major goal in the protein science community.
Unlike AI techniques that aim to predict protein structure from sequence data alone, protein structure can also be experimentally determined using cryo-EM, which produces hundreds of thousands, or even millions, of two-dimensional images of protein samples frozen in a thin layer of ice.
This technique works best for imaging proteins that exist in only one conformation, but MIT researchers have now developed a machine-learning algorithm that helps them identify multiple possible structures that a protein can take.
They also visualized large-scale flexible motions of the spliceosome — a protein complex that coordinates the splicing of the protein coding sequences of transcribed RNA.
Because some of these experiments generated so many different protein structures, traditional cryo-EM reconstruction tools did not work well to determine what those structures were.",Mit
5,https://news.mit.edu/2021/epigenomic-map-reveals-circuitry-human-disease-regions-0203,"


Twenty years ago this month, the first draft of the human genome was publicly released. One of the major surprises that came from that project was the revelation that only 1.5 percent of the human genome consists of protein-coding genes.

Over the past two decades, it has become apparent that those noncoding stretches of DNA, originally thought to be “junk DNA,” play critical roles in development and gene regulation. In a new study published today, a team of researchers from MIT has published the most comprehensive map yet of this noncoding DNA.

This map provides in-depth annotation of epigenomic marks — modifications indicating which genes are turned on or off in different types of cells — across 833 tissues and cell types, a significant increase over what has been covered before. The researchers also identified groups of regulatory elements that control specific biological programs, and they uncovered candidate mechanisms of action for about 30,000 genetic variants linked to 540 specific traits.

“What we’re delivering is really the circuitry of the human genome. Twenty years later, we not only have the genes, we not only have the noncoding annotations, but we have the modules, the upstream regulators, the downstream targets, the disease variants, and the interpretation of these disease variants,” says Manolis Kellis, a professor of computer science, a member of MIT’s Computer Science and Artificial Intelligence Laboratory and of the Broad Institute of MIT and Harvard, and the senior author of the new study.

MIT graduate student Carles Boix is the lead author of the paper, which appears today in Nature. Other authors of the paper are MIT graduate students Benjamin James and former MIT postdocs Yongjin Park and Wouter Meuleman, who are now principal investigators at the University of British Columbia and the Altius Institute for Biomedical Sciences, respectively. The researchers have made all of their data publicly available for the broader scientific community to use.

Epigenomic control

Layered atop the human genome — the sequence of nucleotides that makes up the genetic code — is the epigenome. The epigenome consists of chemical marks that help determine which genes are expressed at different times, and in different cells. These marks include histone modifications, DNA methylation, and how accessible a given stretch of DNA is.

“Epigenomics directly reads the marks used by our cells to remember what to turn on and what to turn off in every cell type, and in every tissue of our body. They act as post-it notes, highlighters, and underlining,” Kellis says. “Epigenomics allows us to peek at what each cell marked as important in every cell type, and thus understand how the genome actually functions.”

Mapping these epigenomic annotations can reveal genetic control elements, and the cell types in which different elements are active. These control elements can be grouped into clusters or modules that function together to control specific biological functions. Some of these elements are enhancers, which are bound by proteins that activate gene expression, while others are repressors that turn genes off.

The new map, EpiMap (Epigenome Integration across Multiple Annotation Projects), builds on and combines data from several large-scale mapping consortia, including ENCODE, Roadmap Epigenomics, and Genomics of Gene Regulation.

The researchers assembled a total of 833 biosamples, representing diverse tissues and cell types, each of which was mapped with a slightly different subset of epigenomic marks, making it difficult to fully integrate data across the multiple consortia. They then filled in the missing datasets, by combining available data for similar marks and biosamples, and used the resulting compendium of 10,000 marks across 833 biosamples to study gene regulation and human disease.

The researchers annotated more than 2 million enhancer sites, covering only 0.8 percent of each biosample, and collectively 13 percent of the genome. They grouped them into 300 modules based on their activity patterns, and linked them to the biological processes they control, the regulators that control them, and the short sequence motifs that mediate this control. The researchers also predicted 3.3 million links between control elements and the genes that they target based on their coordinated activity patterns, representing the most complete circuitry of the human genome to date.

Disease links

Since the final draft of the human genome was completed in 2003, researchers have performed thousands of genome-wide association studies (GWAS), revealing common genetic variants that predispose their carriers to a particular trait or disease.

These studies have yielded about 120,000 variants, but only 7 percent of these are located within protein-coding genes, leaving 93 percent that lie in regions of noncoding DNA.

How noncoding variants act is extremely difficult to resolve, however, for many reasons. First, genetic variants are inherited in blocks, making it difficult to pinpoint causal variants among dozens of variants in each disease-associated region. Moreover, noncoding variants can act at large distances, sometimes millions of nucleotides away, making it difficult to find their target gene of action. They are also extremely dynamic, making it difficult to know which tissue they act in. Lastly, understanding their upstream regulators remains an unsolved problem.

In this study, the researchers were able to address these questions and provide candidate mechanistic insights for more than 30,000 of these noncoding GWAS variants. The researchers found that variants associated with the same trait tended to be enriched in specific tissues that are biologically relevant to the trait. For example, genetic variants linked to intelligence were found to be in noncoding regions active in the brain, while variants associated with cholesterol level are in regions active in the liver.

The researchers also showed that some traits or diseases are affected by enhancers active in many different tissue types. For example, they found that genetic variants associated with coronary heart disease (CAD) were active in adipose tissue, coronary arteries, and the liver, among many other tissues.

Kellis’ lab is now working with diverse collaborators to pursue their leads in specific diseases, guided by these genome-wide predictions. They are profiling heart tissue from patients with coronary artery disease, microglia from Alzheimer’s patients, and muscle, adipose, and blood from obesity patients, which are predicted mediators of these disease based on the current paper, and his lab’s previous work.

Many other labs are already using the EpiMap data to pursue studies of diverse diseases. “We hope that our predictions will be used broadly in industry and in academia to help elucidate genetic variants and their mechanisms of action, help target therapies to the most promising targets, and help accelerate drug development for many disorders,” Kellis says.

The research was funded by the National Institutes of Health.


","Epigenomic map reveals circuitry of 30,000 human disease regions",2021-02-03,['Anne Trafton'],Research/Genetics/DNA/Biology/Electrical Engineering & Computer Science (eecs)/Broad Institute/Computer Science and Artificial Intelligence Laboratory (CSAIL)/School of Engineering/National Institutes of  (NIH)/MIT Schwarzman College of Computing,"['genetic', 'marks', 'human', 'circuitry', 'regions', 'reveals', 'disease', 'variants', 'researchers', 'control', 'genes', 'noncoding', 'elements', 'map', '30000', 'epigenomic', 'genome']","Twenty years ago this month, the first draft of the human genome was publicly released.
“What we’re delivering is really the circuitry of the human genome.
Epigenomic controlLayered atop the human genome — the sequence of nucleotides that makes up the genetic code — is the epigenome.
These control elements can be grouped into clusters or modules that function together to control specific biological functions.
First, genetic variants are inherited in blocks, making it difficult to pinpoint causal variants among dozens of variants in each disease-associated region.",Mit
6,https://news.mit.edu/2021/robust-artificial-intelligence-tools-predict-future-cancer-0128,"


To catch cancer earlier, we need to predict who is going to get it in the future. The complex nature of forecasting risk has been bolstered by artificial intelligence (AI) tools, but the adoption of AI in medicine has been limited by poor performance on new patient populations and neglect to racial minorities. 
Two years ago, a team of scientists from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Jameel Clinic demonstrated a deep learning system to predict cancer risk using just a patient’s mammogram. The model showed significant promise and even improved inclusivity: It was equally accurate for both white and Black women, which is especially important given that Black women are 43 percent more likely to die from breast cancer. 
But to integrate image-based risk models into clinical care and make them widely available, the researchers say the models needed both algorithmic improvements and large-scale validation across several hospitals to prove their robustness. 
To that end, they tailored their new “Mirai” algorithm to capture the unique requirements of risk modeling. Mirai jointly models a patient’s risk across multiple future time points, and can optionally benefit from clinical risk factors such as age or family history, if they are available. The algorithm is also designed to produce predictions that are consistent across minor variances in clinical environments, like the choice of mammography machine.  








      

            Robust artificial intelligence tools may be used to predict future breast cancer.        

    



The team trained Mirai on the same dataset of over 200,000 exams from Massachusetts General Hospital (MGH) from their prior work, and validated it on test sets from MGH, the Karolinska Institute in Sweden, and Chang Gung Memorial Hospital in Taiwan. Mirai is now installed at MGH, and the team’s collaborators are actively working on integrating the model into care. 
Mirai was significantly more accurate than prior methods in predicting cancer risk and identifying high-risk groups across all three datasets. When comparing high-risk cohorts on the MGH test set, the team found that their model identified nearly two times more future cancer diagnoses compared the current clinical standard, the Tyrer-Cuzick model. Mirai was similarly accurate across patients of different races, age groups, and breast density categories in the MGH test set, and across different cancer subtypes in the Karolinska test set. 
“Improved breast cancer risk models enable targeted screening strategies that achieve earlier detection, and less screening harm than existing guidelines,” says Adam Yala, CSAIL PhD student and lead author on a paper about Mirai that was published this week in Science Translational Medicine. “Our goal is to make these advances part of the standard of care. We are partnering with clinicians from Novant Health in North Carolina, Emory in Georgia, Maccabi in Israel, TecSalud in Mexico, Apollo in India, and Barretos in Brazil to further validate the model on diverse populations and study how to best clinically implement it.” 
How it works 
Despite the wide adoption of breast cancer screening, the researchers say the practice is riddled with controversy: More-aggressive screening strategies aim to maximize the benefits of early detection, whereas less-frequent screenings aim to reduce false positives, anxiety, and costs for those who will never even develop breast cancer.  
Current clinical guidelines use risk models to determine which patients should be recommended for supplemental imaging and MRI. Some guidelines use risk models with just age to determine if, and how often, a woman should get screened; others combine multiple factors related to age, hormones, genetics, and breast density to determine further testing. Despite decades of effort, the accuracy of risk models used in clinical practice remains modest.  
Recently, deep learning mammography-based risk models have shown promising performance. To bring this technology to the clinic, the team identified three innovations they believe are critical for risk modeling: jointly modeling time, the optional use of non-image risk factors, and methods to ensure consistent performance across clinical settings. 
1. Time 
Inherent to risk modeling is learning from patients with different amounts of follow-up, and assessing risk at different time points: this can determine how often they get screened, whether they should have supplemental imaging, or even consider preventive treatments. 
Although it’s possible to train separate models to assess risk for each time point, this approach can result in risk assessments that don’t make sense — like predicting that a patient has a higher risk of developing cancer within two years than they do within five years. To address this, the team designed their model to predict risk at all time points simultaneously, by using a tool called an “additive-hazard layer.” 
The additive-hazard layer works as follows: Their network predicts a patient’s risk at a time point, such as five years, as an extension of their risk at the previous time point, such as four years. In doing so, their model can learn from data with variable amounts of follow-up, and then produce self-consistent risk assessments. 
2. Non-image risk factors 
While this method primarily focuses on mammograms, the team wanted to also use non-image risk factors such as age and hormonal factors if they were available — but not require them at the time of the test. One approach would be to add these factors as an input to the model with the image, but this design would prevent the majority of hospitals (such as Karolinska and CGMH), which don’t have this infrastructure, from using the model. 
For Mirai to benefit from risk factors without requiring them, the network predicts that information at training time, and if it's not there, it can use its own predictive version. Mammograms are rich sources of health information, and so many traditional risk factors such as age and menopausal status can be easily predicted from their imaging. As a result of this design, the same model could be used by any clinic globally, and if they have that additional information, they can use it. 
3. Consistent performance across clinical environments 
To incorporate deep-learning risk models into clinical guidelines, the models must perform consistently across diverse clinical environments, and its predictions cannot be affected by minor variations like which machine the mammogram was taken on. Even across a single hospital, the scientists found that standard training did not produce consistent predictions before and after a change in mammography machines, as the algorithm could learn to rely on different cues specific to the environment. To de-bias the model, the team used an adversarial scheme where the model specifically learns mammogram representations that are invariant to the source clinical environment, to produce consistent predictions. 
To further test these updates across diverse clinical settings, the scientists evaluated Mirai on new test sets from Karolinska in Sweden and Chang Gung Memorial Hospital in Taiwan, and found it obtained consistent performance. The team also analyzed the model’s performance across races, ages, and breast density categories in the MGH test set, and across cancer subtypes on the Karolinska dataset, and found it performed similarly across all subgroups. 
“African-American women continue to present with breast cancer at younger ages, and often at later stages,” says Salewai Oseni, a breast surgeon at Massachusetts General Hospital who was not involved with the work. “This, coupled with the higher instance of triple-negative breast cancer in this group, has resulted in increased breast cancer mortality. This study demonstrates the development of a risk model whose prediction has notable accuracy across race. The opportunity for its use clinically is high.” 
Here's how Mirai works: 
1. The mammogram image is put through something called an ""image encoder.""
2. Each image representation, as well as which view it came from, is aggregated with other images from other views to obtain a representation of the entire mammogram.
3. With the mammogram, a patient's traditional risk factors are predicted using a Tyrer-Cuzick model (age, weight, hormonal factors). If unavailable, predicted values are used. 
4. With this information, the additive-hazard layer predicts a patient’s risk for each year over the next five years. 
Improving Mirai 
Although the current model doesn’t look at any of the patient’s previous imaging results, changes in imaging over time contain a wealth of information. In the future the team aims to create methods that can effectively utilize a patient's full imaging history.
In a similar fashion, the team notes that the model could be further improved by utilizing “tomosynthesis,” an X-ray technique for screening asymptomatic cancer patients. Beyond improving accuracy, additional research is required to determine how to adapt image-based risk models to different mammography devices with limited data. 
“We know MRI can catch cancers earlier than mammography, and that earlier detection improves patient outcomes,” says Yala. “But for patients at low risk of cancer, the risk of false-positives can outweigh the benefits. With improved risk models, we can design more nuanced risk-screening guidelines that offer more sensitive screening, like MRI, to patients who will develop cancer, to get better outcomes while reducing unnecessary screening and over-treatment for the rest.” 
“We’re both excited and humbled to ask the question if this AI system will work for African-American populations,” says Judy Gichoya, MD, MS and assistant professor of interventional radiology and informatics at Emory University, who was not involved with the work. “We’re extensively studying this question, and how to detect failure.” 
Yala wrote the paper on Mirai alongside MIT research specialist Peter G. Mikhael, radiologist Fredrik Strand of Karolinska University Hospital, Gigin Lin of Chang Gung Memorial Hospital, Associate Professor Kevin Smith of KTH Royal Institute of Technology, Professor Yung-Liang Wan of Chang Gung University, Leslie Lamb of MGH, Kevin Hughes of MGH, senior author and Harvard Medical School Professor Constance Lehman of MGH, and senior author and MIT Professor Regina Barzilay. 
The work was supported by grants from Susan G Komen, Breast Cancer Research Foundation, Quanta Computing, and the MIT Jameel Clinic. It was also supported by Chang Gung Medical Foundation Grant, and by Stockholm Läns Landsting HMT Grant. 
 


",Robust artificial intelligence tools to predict future cancer,2021-01-28,['Rachel Gordon'],School of Engineering/MIT Schwarzman College of Computing/Biological engineering/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Institute for Medical Engineering and Science (IMES)/Health care/Medicine/Research/Women/Cancer/Policy/Disease/Artificial intelligence/Machine learning/Health sciences and technology/Collaboration/Algorithms/Technology and society,"['team', 'model', 'future', 'cancer', 'models', 'tools', 'predict', 'clinical', 'mirai', 'patients', 'factors', 'artificial', 'intelligence', 'robust', 'breast', 'risk']","Mirai was significantly more accurate than prior methods in predicting cancer risk and identifying high-risk groups across all three datasets.
“This, coupled with the higher instance of triple-negative breast cancer in this group, has resulted in increased breast cancer mortality.
With this information, the additive-hazard layer predicts a patient’s risk for each year over the next five years.
Beyond improving accuracy, additional research is required to determine how to adapt image-based risk models to different mammography devices with limited data.
The work was supported by grants from Susan G Komen, Breast Cancer Research Foundation, Quanta Computing, and the MIT Jameel Clinic.",Mit
7,https://news.mit.edu/2021/mit-convenes-influential-industry-leaders-fight-climate-change-0128,"


Launched today, the MIT Climate and Sustainability Consortium (MCSC) convenes an alliance of leaders from a broad range of industries and aims to vastly accelerate large-scale, real-world implementation of solutions to address the threat of climate change. The MCSC unites similarly motivated, highly creative and influential companies to work with MIT to build a process, market, and ambitious implementation strategy for environmental innovation. 
The work of the consortium will involve a true cross-sector collaboration to meet the urgency of climate change. The MCSC will take positive action and foster the necessary collaboration to meet this challenge, with the intention of influencing efforts across industries. Through a unifying, deeply inclusive, global effort, the MCSC will strive to drive down costs, lower barriers to adoption of best-available technology and processes, speed retirement of carbon-intensive power generating and materials-producing equipment, direct investment where it will be most effective, and rapidly translate best practices from one industry to the next in an effort to deploy social and technological solutions at a pace more rapid than the planet’s intensifying crises.







Play video






“If we hope to decarbonize the economy, we must work with the companies that make the economy run. Drawing its members from a broad range of industries, the MCSC will convene an alliance of influential corporations motivated to work with MIT, and with each other, to pilot and deploy the solutions necessary to reach their own ambitious decarbonization commitments,” says MIT President L. Rafael Reif. “By sharing solutions across companies and sectors, the consortium has the potential to vastly accelerate the implementation of large-scale, real-world solutions to help meet the global climate emergency. And as an Institute-wide effort, it will also complement MIT’s existing climate initiatives and make them more effective: Just as the Climate Grand Challenges effort is accelerating research on climate science and solutions, the consortium aims to accelerate the adoption of such solutions, at scale and across industries.”
Led by the MIT School of Engineering and engaging students, faculty, and researchers from across the entire Institute, the MIT Climate and Sustainability Consortium has called upon companies from a broad range of industries — from aviation to agriculture, consumer services to electronics, chemical production to textiles, and infrastructure to software — to roll up their sleeves and work closely with every corner of MIT.
“This new collaboration represents the incredible potential for academia and industry to work together on a shared mission to shape research, identify opportunities for innovation, and rapidly advance practical solutions with the sense of urgency needed to address our climate challenge. There are no bounds to what we can achieve together,” says Anantha P. Chandrakasan, dean of the School of Engineering, Vannevar Bush Professor of Electrical Engineering and Computer Science, and chair of the MIT Climate and Sustainability Consortium.
The inaugural members of the MCSC are companies with intricate supply chains that are among the best positioned to help lead the mission to solve the climate crisis. The inaugural member companies of the MCSC recognize the responsibility industry has in the rapid deployment of social and technology solutions. They represent the heart of global industry and have made a commitment to not only work with MIT but with one another, to tackle the climate challenge with the urgency required to realize their goals.
These industry leaders can both help inspire transformative change within their own sectors and demonstrate the value of working together, across sectors, at scale. The inaugural members of the MIT Climate and Sustainability Consortium are:

Accenture is a global professional services company that delivers on the promise of technology and human ingenuity, which includes helping clients across 40 industries reach their sustainability goals by transitioning to low-carbon energy; reducing the carbon footprint of IT, cloud, and software; and designing and delivering net-zero, circular supply chains.
	 
Apple is a global leader in technology innovation, providing seamless experiences across Apple devices and empowering people with breakthrough services.
	 
Boeing is the world’s largest aerospace company and leading provider of commercial airplanes, defense, space and security systems, and global services.
	 
Cargill is a global food manufacturer with the goal of nourishing the world in a safe, responsible, and sustainable way.
	 
Dow is a global manufacturer of innovative products that solve the materials science challenges of its customers and contribute to a more sustainable world. 
	 
IBM is a hybrid cloud platform and artificial intelligence company.
	 
Inditex is one of the world’s largest fashion retail groups with eight distinct brands focused on fitting its products to meet customer demands in a sustainable way through an integrated platform of physical and online stores.
	 
LafargeHolcim is the world's global leader in building materials and solutions at the forefront of sustainable construction.
	 
MathWorks develops mathematical computing software used to accelerate the pace of engineering and science.
	 
Nexplore (Hochtief) is an innovative company that develops technology solutions to digitize the infrastructure sector, using next-generation technologies including artificial intelligence, blockchain, computer vision, natural language processing, and internet of things. Nexplore was founded in 2018 by HOCHTIEF, one of the largest infrastructure construction groups worldwide.
	 
Rand-Whitney Containerboard (RWCB), a Kraft Group company, is a manufacturer of lightweight, high-performance recycled linerboard for corrugated containers, using the most environmentally sustainable production processes and methods.
	 
PepsiCo is a global food and beverage company that aims to use its scale, reach, and expertise to help build a more sustainable food system.
	 
Verizon is one of the world’s leading providers of technology, communications, information and entertainment products and services.

Jeffrey Grossman will serve as director of the MCSC. Grossman is the Morton and Claire Goulder and Family Professor in Environmental Systems, head of the Department of Materials Science and Engineering, and a MacVicar Faculty Fellow. Elsa Olivetti, the Esther and Harold E. Edgerton Associate Professor in Materials Science and Engineering, will serve as associate director. A steering committee comprised of faculty spanning all five of MIT’s schools and the MIT Stephen A. Schwarzman College of Computing, will help to drive the work of the consortium.


",MIT convenes influential industry leaders in the fight against climate change,2021-01-28,['Lori Loturco'],School of Architecture and Planning/School of Engineering/School of Humanities Arts and Social Sciences/School of Science/MIT Schwarzman College of Computing/Sloan School of Management/Electrical Engineering & Computer Science (eecs)/Materials Science and Engineering/President L. Rafael Reif/Climate change/Sustainability/Industry/Faculty/Collaboration/Global/Environment,"['influential', 'technology', 'leaders', 'global', 'convenes', 'change', 'sustainable', 'mcsc', 'worlds', 'work', 'solutions', 'climate', 'fight', 'industry', 'company', 'mit']","The work of the consortium will involve a true cross-sector collaboration to meet the urgency of climate change.
Launched today, the MIT Climate and Sustainability Consortium (MCSC) convenes an alliance of leaders from a broad range of industries and aims to vastly accelerate large-scale, real-world implementation of solutions to address the threat of climate change.
The MCSC unites similarly motivated, highly creative and influential companies to work with MIT to build a process, market, and ambitious implementation strategy for environmental innovation.
“By sharing solutions across companies and sectors, the consortium has the potential to vastly accelerate the implementation of large-scale, real-world solutions to help meet the global climate emergency.
The inaugural member companies of the MCSC recognize the responsibility industry has in the rapid deployment of social and technology solutions.",Mit
8,https://news.mit.edu/2021/machine-learning-adapts-0128,"


MIT researchers have developed a type of neural network that learns on the job, not just during its training phase. These flexible algorithms, dubbed “liquid” networks, change their underlying equations to continuously adapt to new data inputs. The advance could aid decision making based on data streams that change over time, including those involved in medical diagnosis and autonomous driving.

“This is a way forward for the future of robot control, natural language processing, video processing — any form of time series data processing,” says Ramin Hasani, the study’s lead author. “The potential is really significant.”

The research will be presented at February’s AAAI Conference on Artificial Intelligence. In addition to Hasani, a postdoc in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), MIT co-authors include Daniela Rus, CSAIL director and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science, and PhD student Alexander Amini. Other co-authors include Mathias Lechner of the Institute of Science and Technology Austria and Radu Grosu of the Vienna University of Technology.

Time series data are both ubiquitous and vital to our understanding the world, according to Hasani. “The real world is all about sequences. Even our perception — you’re not perceiving images, you’re perceiving sequences of images,” he says. “So, time series data actually create our reality.”

He points to video processing, financial data, and medical diagnostic applications as examples of time series that are central to society. The vicissitudes of these ever-changing data streams can be unpredictable. Yet analyzing these data in real time, and using them to anticipate future behavior, can boost the development of emerging technologies like self-driving cars. So Hasani built an algorithm fit for the task.

Hasani designed a neural network that can adapt to the variability of real-world systems. Neural networks are algorithms that recognize patterns by analyzing a set of “training” examples. They’re often said to mimic the processing pathways of the brain — Hasani drew inspiration directly from the microscopic nematode, C. elegans. “It only has 302 neurons in its nervous system,” he says, “yet it can generate unexpectedly complex dynamics.”

Hasani coded his neural network with careful attention to how C. elegans neurons activate and communicate with each other via electrical impulses. In the equations he used to structure his neural network, he allowed the parameters to change over time based on the results of a nested set of differential equations.

This flexibility is key. Most neural networks’ behavior is fixed after the training phase, which means they’re bad at adjusting to changes in the incoming data stream. Hasani says the fluidity of his “liquid” network makes it more resilient to unexpected or noisy data, like if heavy rain obscures the view of a camera on a self-driving car. “So, it’s more robust,” he says.

There’s another advantage of the network’s flexibility, he adds: “It’s more interpretable.”

Hasani says his liquid network skirts the inscrutability common to other neural networks. “Just changing the representation of a neuron,” which Hasani did with the differential equations, “you can really explore some degrees of complexity you couldn’t explore otherwise.” Thanks to Hasani’s small number of highly expressive neurons, it’s easier to peer into the “black box” of the network’s decision making and diagnose why the network made a certain characterization.

“The model itself is richer in terms of expressivity,” says Hasani. That could help engineers understand and improve the liquid network’s performance.

Hasani’s network excelled in a battery of tests. It edged out other state-of-the-art time series algorithms by a few percentage points in accurately predicting future values in datasets, ranging from atmospheric chemistry to traffic patterns. “In many applications, we see the performance is reliably high,” he says. Plus, the network’s small size meant it completed the tests without a steep computing cost. “Everyone talks about scaling up their network,” says Hasani. “We want to scale down, to have fewer but richer nodes.”

Hasani plans to keep improving the system and ready it for industrial application. “We have a provably more expressive neural network that is inspired by nature. But this is just the beginning of the process,” he says. “The obvious question is how do you extend this? We think this kind of network could be a key element of future intelligence systems.”

This research was funded, in part, by Boeing, the National Science Foundation, the Austrian Science Fund, and Electronic Components and Systems for European Leadership.


",“Liquid” machine-learning system adapts to changing conditions,2021-01-28,['Daniel Ackerman'],Technology and society/Machine learning/Algorithms/Artificial intelligence/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/School of Engineering/MIT Schwarzman College of Computing,"['data', 'changing', 'future', 'processing', 'hasani', 'series', 'conditions', 'network', 'science', 'system', 'networks', 'liquid', 'machinelearning', 'adapts', 'neural']","MIT researchers have developed a type of neural network that learns on the job, not just during its training phase.
These flexible algorithms, dubbed “liquid” networks, change their underlying equations to continuously adapt to new data inputs.
There’s another advantage of the network’s flexibility, he adds: “It’s more interpretable.”Hasani says his liquid network skirts the inscrutability common to other neural networks.
That could help engineers understand and improve the liquid network’s performance.
“We have a provably more expressive neural network that is inspired by nature.",Mit
9,https://news.mit.edu/2021/false-news-fact-check-timing-0125,"


The battle to stop false news and online misinformation is not going to end any time soon, but a new finding from MIT scholars may help ease the problem.

In an experiment, the researchers discovered that fact-checking labels, when attached to online news headlines, actually work better after people read false headlines, compared to when they precede the headline or accompany it.

“We found that whether a false claim was corrected before people read it, while they read it, or after they read it influenced the effectiveness of the correction,” says David Rand, an MIT professor and co-author of a new paper detailing the study’s results.

Specifically, the researchers found, when “true” and “false” labels were shown immediately after participants in the experiment read headlines, it reduced people’s misclassification of those headlines by 25.3 percent. By contrast, there was an 8.6 percent reduction when labels appeared along with the headlines, and a 5.7 percent decrease in misclassification when the correct label appeared beforehand.

“Timing does matter when delivering fact-checks,” says Nadia M. Brashier, a cognitive neuroscientist and postdoc at Harvard University, and lead author of the paper.

The paper, “Timing Matters When Correcting Fake News,” appears this week in Proceedings of the National Academy of Sciences. The authors are Brashier; Rand; Gordon Pennycook, an assistant professor of behavioral science at University of Regina’s Hill/Levene Schools of Business; and Adam Berinsky, the Mitsui Professor of Political Science at MIT and the director of the MIT Political Experiments Research Lab.

To conduct the study, the scholars ran experiments with a total of 2,683 people, who looked at 18 true news headlines from major media sources and 18 false headlines that have been debunked by the fact-checking website snopes.com. Treatment groups of participants saw “true” and “false” tags before, during, or after reading the 36 headlines; a control group did not. All participants rated the headlines for accuracy. One week later, everyone looked at the same headlines, without any fact-check information at all, and again rated the headlines for accuracy.

The findings confounded the researchers’ expectations.

“Going into the project, I had anticipated it would work best to give the correction beforehand, so that people already knew to disbelieve the false claim when they came into contact with it,” Rand says. “To my surprise, we actually found the opposite. Debunking the claim after they were exposed to it was the most effective.""

But why might his approach — “debunking” rather than “prebunking,” as the researchers call it — get the best results?

The scholars write that the results are consistent with a “concurrent storage hypothesis” of cognition, which proposes that people can retain both false information and corrections in their minds at the same time. It may not be possible to get people to ignore false headlines, but people are willing to update their beliefs about them.

“Allowing people to form their own impressions of news headlines, then providing ‘true’ or ‘false’ tags afterward, might act as feedback,” Brashier says. “And other research shows that feedback makes correct information ‘stick.’” Importantly, this suggests that the results might be different if participants did not explicitly rate the accuracy of the headlines when being exposed to them — for example, if they were just scrolling through their news feeds.

Overall, Berinsky suggests, the research helps inform tools that social media platforms and other content providers could use, as they look for better methods to label and limit the flow of misinformation online.

“There is no single magic bullet that can cure the problem of misinformation,” says Berinsky, who has long studied political rumors and misinformation. “Studying basic questions in a systematic way is a critical step toward a portfolio of effective solutions. Like David, I was somewhat surprised by our findings, but this finding is an important step forward in helping us combat misinformation.”

The study was made possible through support to the researchers provided by the National Science Foundation, the Ethics and Governance of Artificial Intelligence Initiative of the Miami Foundation, the William and Flora Hewlett Foundation, the Reset Project of Luminate, the Social Sciences and Humanities Research Council of Canada, and Google.


","To combat false news, correct after reading",2021-01-25,['Peter Dizikes'],Political science/Politics/School of Humanities Arts and Social Sciences/MIT Sloan School of Management,"['true', 'misinformation', 'research', 'researchers', 'correct', 'headlines', 'combat', 'false', 'participants', 'read', 'reading', 'science', 'mit']","In an experiment, the researchers discovered that fact-checking labels, when attached to online news headlines, actually work better after people read false headlines, compared to when they precede the headline or accompany it.
Specifically, the researchers found, when “true” and “false” labels were shown immediately after participants in the experiment read headlines, it reduced people’s misclassification of those headlines by 25.3 percent.
Treatment groups of participants saw “true” and “false” tags before, during, or after reading the 36 headlines; a control group did not.
It may not be possible to get people to ignore false headlines, but people are willing to update their beliefs about them.
“Allowing people to form their own impressions of news headlines, then providing ‘true’ or ‘false’ tags afterward, might act as feedback,” Brashier says.",Mit
10,https://news.mit.edu/2021/learning-and-about-ai-technology-0125,"


Between remote learning, more time spent at home, and working parents trying to keep their kids occupied, children across the United States have clocked in record-breaking hours of screen time during the pandemic. Much of it is supervised and curated by teachers or parents — but increasingly, kids of all ages are watching videos, playing games, and interacting with devices powered by artificial intelligence. As head of the Personal Robots group and AI Education at MIT, Media Lab Professor Cynthia Breazeal is on a mission to help this generation of young people to grow up understanding the AI they use.
At “AI Education: Research and Practice,” an Open Learning Talks event in December, Breazeal shared her vision for educating students not only about how AI works, but how to design and use it themselves — an initiative she calls AI Literacy for All. The AI Education project Breazeal is leading at MIT is a collaboration between MIT Open Learning and the Abdul Latif Jameel World Education Lab, the Media Lab, and the MIT Schwarzman College of Computing. Through research projects, hands-on activities, and scalable learning modules, Breazeal and her AI Education affiliates across MIT are creating a robust resource hub for educators, parents, and learners of all ages to understand how AI functions in different day-to-day roles, and how to approach both using and creating artificial intelligence with a basis in ethics, inclusion, and empathy.








      

            Open Learning Talks | AI Education: Research and Practice        

    



“It’s at this intersection of human psychology, engagement, and AI and technology, and we’re learning a lot,” Breazeal said as she explained her group’s research to the audience. “We’re not trying to build technologies to replace teachers or compete with parents. These are fluffy, pet-like robots, but they can engage children in this interaction where there are aspects like a motivating ally, like a friend ... there are aspects like this companion animal, and this nonjudgmental companion animal gives the nature of this relationship this very different flavor, where even if they’re embarrassed to make mistakes in front of their teacher or their friends, they seem not to be in front of the robot — and you can’t learn if you’re not willing to take learning risks.” 
Breazeal shared examples from her Personal Robots group’s efforts, including recent studies on personalized learning companions for early childhood education, developing comprehensive K-12 AI literacy programs, and creating tools to help kids get creative using AI technologies.
“So how do you empower kids to create things with AI? You’re not going to put a middle-schooler on Tensorflow and say ‘Good luck,’ right?” Breazeal said. “MIT is the home base for things like Scratch and App Inventor, so the team is taking these more advanced AI methods and curricula and concepts, and augmenting these platforms to empower kids to use these AI technologies, to learn about them and then design projects of their own, and port them to different kinds of platforms.”  
Host Professor Eric Klopfer, director of the Scheller Teacher Education Program and the Education Arcade at MIT and head of MIT Comparative Media Studies and Writing, engaged Breazeal in a dialogue about all aspects of AI education and fielded questions from the live audience, ranging from emotional connection with robots to screen time, data collection, and representation in research and design. 
“How does AI in education narrow the gap that we see between socioeconomic groups? How do we see AI bridging that gap rather than widening the gap?” asked Klopfer, as he and Breazeal shared insights on training teachers, providing hands-on activities and paper prototyping to expand access and inclusion on technology education. “The technology itself is not the impetus for the divide anymore; it’s the way the technologies are being used, and the way people are trained to be able to use them,” Klopfer said. “It’s so key that we don’t repeat our mistakes from past technological innovations, where we just distribute devices to schools without thinking about the training and expertise that needs to go with that.”
And in an increasingly tech-driven society, access and education are key to creating equity for, and encouraging thoughtful participation from, all users. “We want a much more diverse, inclusive group of people being able to participate in shaping this future [with AI],” said Breazeal.
Launched last fall, Open Learning Talks is a public, online event series that features conversations between leaders from MIT and around the world, sharing their research and insights on education, teaching, and the science of learning. Upcoming events include William Bonvillian and Sanjay Sarma discussing their new book, “Workforce Education,” on Feb. 23; and Professor D. Fox Harrell and Rocky Bucano, executive director of the Universal Hip Hop Museum, in mid-March. 


",Learning with — and about — AI technology,2021-01-25,[],"School of Architecture and Planning/School of Humanities Arts and Social Sciences/MIT Schwarzman College of Computing/Office of Open Learning/Media Lab/Abdul Latif Jameel World Education Lab (J-WEL)/Robots/K-12 education/Artificial intelligence/Education, teaching, academics/Special events and guest speakers/Behavior/Digital technology/Human-computer interaction/Learning/STEM education/Technology and society/Robotics","['technology', 'research', 'learning', 'parents', 'creating', 'robots', 'ai', 'breazeal', 'kids', 'mit', 'education']","At “AI Education: Research and Practice,” an Open Learning Talks event in December, Breazeal shared her vision for educating students not only about how AI works, but how to design and use it themselves — an initiative she calls AI Literacy for All.
The AI Education project Breazeal is leading at MIT is a collaboration between MIT Open Learning and the Abdul Latif Jameel World Education Lab, the Media Lab, and the MIT Schwarzman College of Computing.
As head of the Personal Robots group and AI Education at MIT, Media Lab Professor Cynthia Breazeal is on a mission to help this generation of young people to grow up understanding the AI they use.
“It’s at this intersection of human psychology, engagement, and AI and technology, and we’re learning a lot,” Breazeal said as she explained her group’s research to the audience.
“We want a much more diverse, inclusive group of people being able to participate in shaping this future [with AI],” said Breazeal.",Mit
11,https://news.mit.edu/2021/how-chess-plays-out-at-mit-0122,"


Chess has a long history at MIT that began decades before 62 million households tuned in to Netflix’s miniseries “The Queen's Gambit.” Though the show ranked as Netflix’s No. 1 in 63 countries within its first month, and sparked a global surge in the sale of chess sets and books, several members of MIT’s chess club say, with a laugh, that they haven’t seen it yet.
Tyrone Davis III, a junior computer science major, a U.S. National Chess Master, and the president of MIT’s chess club, says he plans to watch the miniseries eventually. For now, he says it’s been exciting to see growing public interest around the game he’s been playing since middle school.
“The hardest thing about chess is the beginning stages,” Davis says. “Once you learn how the pieces move, then you can have fun. But that’s only after you go through the difficult beginning time of learning how everything works. I hope the show could help motivate people through those tough beginning stages, so they can actually start having fun.”
A history of chess at MIT
“The Queen's Gambit” was released on Netflix last October, and is based on a novel of the same name written in 1983 by Walter Tevis. The novel and television show follow a rising chess star named Beth during the 1950s and '60s, as she ascends from child prodigy to international success.
During the same time period (in real life), artificial intelligence experts at MIT were shaping the future of chess.
Throughout the early 1900s, scientists and chess players around the world dreamed of a chess-playing machine. In 1951, English computer scientist Dietrich Prinz successfully created one, but it was not powerful enough to play a complete game. Then, in 1958, an IBM programmer developed a much stronger chess-playing machine, but novice players could still easily beat it.
The first computer to play chess “convincingly” was the Kotok-McCarthy computer program, developed by MIT students between 1959 and 1962. The students worked with John McCarthy, a computer scientist and cognitive scientist at MIT. In 1966, the Kotok-McCarthy program participated in and lost the first-ever chess match between two machines.
The first chess program to ever be ranked and to win against a human during a tournament was also developed at MIT. The Mac Hack, as the program came to be called, was written by computer programmer Richard Greenblatt. As a student at MIT and an avid chess player, Greenblatt published his work in a 1967 paper entitled “The Greenblatt Chess Program.”
Hubert Dreyfus, a prominent MIT professor of philosophy at the time, had previously noted the shortcomings of chess-playing machines. Then, he lost to the Greenblatt machine.
Meet MIT’s chess team
In 1996, Deep Blue, an IBM machine, became the first computer to beat a world champion in chess. Now, artificial intelligence can play better than most humans.
Online chess platforms are now extremely popular, enabling players to connect across the globe and also practice against artificial intelligence — which has been very handy amid the Covid-19 pandemic.
Several hundred members of the MIT community subscribe to the MIT Chess Club’s email list. Between 15 and 20 of those people consistently show up for weekly Friday afternoon club meetings, which now occur online and allow players to practice against one another.
“There’s a sort of latent energy at MIT of a lot of people that are vaguely interested in chess,” says William Cuozzo, a junior majoring in physics and computer science, and a member of MIT’s chess team executive board. “A lot of people are somewhat interested in chess, but they just don't play that often, or they never got over that initial hump of learning how to play.”
Aileen Ma, a junior in computer science and a member of MIT’s chess team executive board, says she hopes that the show will inspire people to take up chess as a new hobby.
“Even though I haven't seen the show, I had a lot of my friends who have and they’ve said ‘I had no idea chess was so exciting,’ she says of “The Queen's Gambit.” “And it's really great to see female representation, because there's a lot of female grandmasters out there.”
Before the pandemic brought a halt to on-campus activities, MIT’s chess club hosted one U.S. Chess Federation tournament every academic year. They also previously attended the Pan-American Collegiate, Ivy League Challenge, and World Amateur Team tournaments, and played with many universities in the Boston area.
The club has turned to websites like lichess.org to keep playing chess remotely, often with brand-new opponents.
In “The Queen's Gambit,” Beth faces many of her most formidable opponents in Moscow, Russia (then the USSR). In mid-October this semester, MIT’s chess team faced off against players from the Moscow Institute of Physics and Technology (MIPT) for a weekend-long lichess.org tournament. After 171 games played, and almost 12,000 total moves, MIPT earned a narrow victory.
“I think one of the best parts of these online matches is that everyone has a chance to participate and earn points for the team,” says Ma. “It was a lot of fun. A lot of people who don't normally come to meetings came to help us try to improve our score.”
Learning the game
Davis grew up in the Bronx, where he practiced chess in Union Square and other parks around New York City. Being able to sit down and look at a board for six hours at a time at a young age, Davis says, improved his ability to concentrate.
“Depending on when in your life you find the game, it could definitely impact you in different ways,” Davis says. “It definitely helped me be able to sit down for hours and apply my brain to certain long tasks that require a lot of attention span.”
Cuozzo learned to play chess at a very young age, but says he really got interested in playing during high school, when a friend challenged and subsequently “crushed” him. “I wanted to at least be able to play with him,” he says.
“The Queen's Gambit” has spurred public interest in learning the game of chess. And the flood of excitement comes at a good time — there are now more resources to learn chess than ever before, says Howard Zhong, a sophomore majoring in computer science and math, and a U.S. National Master. Zhong learned to play chess at age 6, and is currently a member of MIT’s chess team. “Even five to 10 years ago, there were not as many resources online,” Zhong says. “The majority was just reading books, or having a coach. But now, with so many online resources, I think it's really made it a lot easier to improve.""
Zhong and Cuozzo recommend watching chess tutorials like ChessNetwork on YouTube, and exploring practice resources on websites like chess.com or lichess.org. Davis says it’s important not to be discouraged by the initial learning curve — it may take a few days or weeks to learn the moves.
MIT’s chess club welcomes people of all levels of skill. Members of the MIT community who are interested in participating can sign up for the mailing list. The club plans to participate in more virtual tournaments and games in the coming months.
“Sometimes the public perception of chess is that if you're not really good, it's not worth playing,” Ma says. “But recently, I think there's just been a lot more effort toward trying to get people involved even at the more intermediate levels of chess.”


",How chess plays out at MIT,2021-01-22,['Alison Gold'],School of Science/School of Engineering/Electrical Engineering & Computer Science (eecs)/Physics/Gaming/Student life/History of MIT/Undergraduate/Students,"['mits', 'plays', 'club', 'team', 'computer', 'lot', 'play', 'players', 'queens', 'mit', 'chess']","Tyrone Davis III, a junior computer science major, a U.S. National Chess Master, and the president of MIT’s chess club, says he plans to watch the miniseries eventually.
The first computer to play chess “convincingly” was the Kotok-McCarthy computer program, developed by MIT students between 1959 and 1962.
Meet MIT’s chess teamIn 1996, Deep Blue, an IBM machine, became the first computer to beat a world champion in chess.
Several hundred members of the MIT community subscribe to the MIT Chess Club’s email list.
Zhong learned to play chess at age 6, and is currently a member of MIT’s chess team.",Mit
12,https://news.mit.edu/2021/3-questions-thomas-malone-daniela-rus-how-work-will-change-ai-0121,"


As part of the MIT Task Force on the Work of the Future’s series of research briefs, Professor Thomas Malone, Professor Daniela Rus, and Robert Laubacher collaborated on ""Artificial Intelligence and the Future of Work,"" a brief that provides a comprehensive overview of AI today and what lies at the AI frontier. 

The authors delve into the question of how work will change with AI and provide policy prescriptions that speak to different parts of society. Thomas Malone is director of the MIT Center for Collective Intelligence and the Patrick J. McGovern Professor of Management in the MIT Sloan School of Management. Daniela Rus is director of the Computer Science and Artificial Intelligence Laboratory, the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science, and a member of the MIT Task Force on the Work of the Future. Robert Laubacher is associate director of the MIT Center for Collective Intelligence.

Here, Malone and Rus provide an overview of their research.

Q: You argue in your brief that despite major recent advances, artificial intelligence is nowhere close to matching the breadth and depth of perception, reasoning, communication, and creativity of people. Could you explain some of the limitations of AI?

Rus: Despite recent and significant strides in the AI field, and great promise for the future, today’s AI systems are still quite limited in their ability to reason, make decisions, interact reliably with people and the physical world. Some of today’s greatest successes are due to a machine learning method called deep learning. These systems are trained using vast amounts of data that needs to be manually labeled. Their performance is dependent on the quantity and quality of data used to train them. The larger the training set for the network, the better its performance, and, in turn, the better the product that relies on the machine learning engine. But training large models has high computation cost. Also, bad training data leads to bad performance: when the data has bias, the system response propagates the same bias.

Another limitation of current AI systems is robustness. Current state-of-the-art classifiers achieve impressive performance on benchmarks, but their predictions tend to be brittle. Specifically, inputs that were initially classified correctly can become misclassified once a carefully constructed but indiscernible perturbation is added to them. An important consequence of the lack of robustness is the lack of trust. One of the worrisome factors about the use of AI is the lack of guarantee that an input will be processed and classified correctly. The complex nature of training and using neural networks leads to systems that are difficult for people to understand. The systems are not able to provide explanations for how they reached decisions.

Q: What are the ways AI is complementing, or could complement, human work?

Malone: Today’s AI programs have only specialized intelligence; they’re only capable of doing certain specialized tasks. But humans have a kind of general intelligence that lets them do a much broader range of things.

That means some of the best ways for AI systems to complement human work is to do specialized tasks that computers can do better, faster, or more cheaply than people can. For example, AI systems can be helpful by doing tasks such as interpreting medical X-rays, evaluating the risk of fraud in a credit card charge, or generating unusual new product designs.

And humans can use their social skills, common sense, and other kinds of general intelligence to do things computers can’t do well. For instance, people can provide emotional support to patients diagnosed with cancer. They can decide when to believe customer explanations for unusual credit card transactions, and they can reject new product designs that customers would probably never want.

In other words, many of the most important uses of computers in the future won’t be replacing people; they’ll be working with people in human-computer groups — “superminds” — that can do things better than either people or computers alone could do.

The possibilities here go far beyond what people usually think of when they hear a phrase like “humans in the loop,” Instead of AI technologies just being tools to augment individual humans, we believe that many of their most important uses will occur in the context of groups of humans — often connected by the internet. So we should move from thinking about humans in the loop to computers in the group.

Q: What are some of your recommendations for education, business, and government regarding policies to help smooth the transition of AI technology adoption? 

Rus: In our report, we highlight four types of actions that can reduce the pain associated with job transitions: education and training, matching jobs to job seekers, creating new jobs, and providing counseling and financial support to people as they transition from old to new jobs. Importantly, we will need partnership among a broad range of institutions to get this work done.

Malone: We expect that — as with all previous labor-saving technologies — AI will eventually lead to the creation of more new jobs than it eliminates. But we see many opportunities for different parts of society to help smooth this transition, especially for the individuals whose old jobs are disrupted and who cannot easily find new ones.

For example, we believe that businesses should focus on applying AI in ways that don’t just replace people but that create new jobs by providing novel kinds of products and services. We recommend that all schools include computer literacy and computational thinking in their curricula, and we believe that community colleges should offer more reskilling and online micro-degree programs, often including apprenticeships at local employers.

We think that current worker organizations (such as labor unions and professional associations) or new ones (perhaps called “guilds”) should expand their roles to provide benefits previously tied to formal employment (such as insurance and pensions, career development, social connections, a sense of identity, and income security).

And we believe that governments should increase their investments in education and reskilling programs to make the American workforce once again the best-educated in the world. And they should reshape the legal and regulatory framework that governs work to encourage creating more new jobs.


",3 Questions: Thomas Malone and Daniela Rus on how AI will change work,2021-01-21,[],Jobs/3 Questions/Faculty/Artificial intelligence/School of Engineering/Sloan School of Management/MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Center for Collective Intelligence/Computer Science and Artificial Intelligence Laboratory (CSAIL),"['training', 'thomas', 'provide', 'systems', 'computers', 'humans', 'questions', 'malone', 'work', 'daniela', 'ai', 'rus', 'intelligence', 'change', 'mit', 'jobs']","The authors delve into the question of how work will change with AI and provide policy prescriptions that speak to different parts of society.
Thomas Malone is director of the MIT Center for Collective Intelligence and the Patrick J. McGovern Professor of Management in the MIT Sloan School of Management.
Rus: Despite recent and significant strides in the AI field, and great promise for the future, today’s AI systems are still quite limited in their ability to reason, make decisions, interact reliably with people and the physical world.
Another limitation of current AI systems is robustness.
For example, AI systems can be helpful by doing tasks such as interpreting medical X-rays, evaluating the risk of fraud in a credit card charge, or generating unusual new product designs.",Mit
13,https://news.mit.edu/2021/fengdi-guo-awarded-first-place-lttp-data-analysis-student-contest-0121,"


Pavement deterioration takes many forms. It can manifest in almost imperceptible flaws, like surface roughness, to much more evident distresses, such as web-like alligator cracks. While the causes of these distresses are numerous, one cause, in particular, can impose an intractable burden: the weight of a vehicle.
In a prize-winning paper, Fengdi Guo, a PhD candidate at the MIT Concrete Sustainability Hub, helps clarify the layered relationship between traffic weight and pavement deterioration. The machine learning models he proposes have found that traffic weight induces specific kinds of damage in asphalt pavements, accelerating their deterioration rates. Concrete pavements, however, proved insensitive to traffic weight.
The paper, “Assessing the Influence of Overweight Vehicles on Pavement Performance,” was awarded first place in The Aramis López Challenge Category of the LTPP Analysis Student Contest, a joint effort of the Federal Highway Administration (FHWA) and the American Society of Civil Engineers' Transportation and Development Institute. Guo will present his findings at the 2021 Transportation Research Board annual meeting.
As one might expect, predicting pavement deterioration is crucial to maintaining road networks. And traffic — specifically, accumulative traffic weights during a period — can play a key role in how quickly a pavement deteriorates.
“The accumulative traffic weight is the product of two components: traffic volume, represented by the annual average daily truck traffic (AADTT), and traffic weight, represented by the approximate weight of a flatbed truck,” explains Guo. “If, for instance, AADTT on a segment were to increase by 1,000, the time between maintenances would decrease by five months, on average.”
When one considers the latest transportation trends, truck traffic weight is likely to become especially problematic; according to the U.S. Energy Information Administration, heavy- and medium-duty vehicle traffic is expected to grow by nearly 40 percent by 2050, far outstripping growth in passenger vehicle traffic.
Accommodating such heavy truck traffic will require more sophisticated tools — particularly because the relationship between traffic weight and pavement deterioration has remained uncharted.
“Though greater traffic weight indisputably deteriorates asphalt pavements,” says Guo, “the types of deterioration it causes are much more unclear. Numerous factors, from precipitation rates to the thickness of a single layer of a pavement, can alter how a pavement responds to the weight of a vehicle.”
To account for these many factors, researchers and engineers have tended to use either complex mechanistic models or data-driven models. The former focus narrowly on the mechanical properties of pavements, require large computational resources, and are not suitable for analyzing a pavement network. The latter can be applied to a pavement network, yet they cannot incorporate a pavement’s unique maintenance and deterioration history.
In his paper, Guo sought to expand the scope of data-driven models. Instead of simply estimating a pavement’s key historical factors, he incorporated them directly into his calculations.
His approach relies on what is known as a recurrent neural network (RNN). A technique of artificial intelligence, neural networks loosely mimic neurons of the mind to solve complex problems. He developed three RNN models for the prediction of roughness, rut, and alligator crack for asphalt pavements — performance metrics that he found to be sensitive to traffic weights in his paper.
To create his neural network, Guo developed a matrix of input layers that supply relevant data (such as pavement structures and freeze index), hidden layers that process and relate that data, and output layers that present the final calculations. Different from conventional feed-forward neural networks in pavement engineering, the hidden layers in RNN models can store key historical information for pavement deterioration.
Once Guo developed these models, he inputted road quality data from the FWHA’s Long Term Pavement Performance (LTTP) database. What he found was a clear relationship between traffic weight and certain forms of damage.
“My models show that increased traffic weights on asphalt pavements accelerate deterioration rates for roughness by 1.3 percent, rut by 7 percent, and alligator crack by 3.7 percent, given a representative asphalt pavement,” Guo explains.
Since he had a limited dataset, Guo’s model could not determine the role of traffic weight on other forms of damage for asphalt pavements. In the future, he will use more robust datasets to understand these other potential repercussions. He also hopes to explore the nationwide economic influence caused by overweight vehicles.  
Up until now, the specific impacts of traffic weight on road quality have been a loaded issue in the transportation community. Though some questions remain, Guo’s models have helped clarify a pernicious problem and helped advance an avenue for further, fruitful research.
The research was supported through the MIT Concrete Sustainability Hub by the Portland Cement Association and the Ready Mixed Concrete Research and Education Foundation.


",Fengdi Guo awarded first place in LTTP Data Analysis Student Contest,2021-01-21,['Andrew Logan'],"School of Engineering/Civil and environmental engineering/Concrete Sustainability Hub/Sustainability/Concrete/Infrastructure/Transportation/Awards, honors and fellowships/Students/Graduate, postdoctoral/Construction/Artificial intelligence/Computer science and technology","['lttp', 'data', 'traffic', 'awarded', 'student', 'place', 'fengdi', 'guo', 'models', 'analysis', 'weight', 'contest', 'pavement', 'truck', 'transportation', 'asphalt', 'deterioration', 'pavements']","In a prize-winning paper, Fengdi Guo, a PhD candidate at the MIT Concrete Sustainability Hub, helps clarify the layered relationship between traffic weight and pavement deterioration.
As one might expect, predicting pavement deterioration is crucial to maintaining road networks.
“The accumulative traffic weight is the product of two components: traffic volume, represented by the annual average daily truck traffic (AADTT), and traffic weight, represented by the approximate weight of a flatbed truck,” explains Guo.
Accommodating such heavy truck traffic will require more sophisticated tools — particularly because the relationship between traffic weight and pavement deterioration has remained uncharted.
“Though greater traffic weight indisputably deteriorates asphalt pavements,” says Guo, “the types of deterioration it causes are much more unclear.",Mit
14,https://news.mit.edu/2021/robot-customized-hardware-0121,"


Contemporary robots can move quickly. “The motors are fast, and they’re powerful,” says Sabrina Neuman.

Yet in complex situations, like interactions with people, robots often don’t move quickly. “The hang up is what’s going on in the robot’s head,” she adds.

Perceiving stimuli and calculating a response takes a “boatload of computation,” which limits reaction time, says Neuman, who recently graduated with a PhD from the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). Neuman has found a way to fight this mismatch between a robot’s “mind” and body. The method, called robomorphic computing, uses a robot’s physical layout and intended applications to generate a customized computer chip that minimizes the robot’s response time.

The advance could fuel a variety of robotics applications, including, potentially, frontline medical care of contagious patients. “It would be fantastic if we could have robots that could help reduce risk for patients and hospital workers,” says Neuman.

Neuman will present the research at this April’s International Conference on Architectural Support for Programming Languages and Operating Systems. MIT co-authors include graduate student Thomas Bourgeat and Srini Devadas, the Edwin Sibley Webster Professor of Electrical Engineering and Neuman’s PhD advisor. Other co-authors include Brian Plancher, Thierry Tambe, and Vijay Janapa Reddi, all of Harvard University. Neuman is now a postdoctoral NSF Computing Innovation Fellow at Harvard’s School of Engineering and Applied Sciences.

There are three main steps in a robot’s operation, according to Neuman. The first is perception, which includes gathering data using sensors or cameras. The second is mapping and localization: “Based on what they’ve seen, they have to construct a map of the world around them and then localize themselves within that map,” says Neuman. The third step is motion planning and control — in other words, plotting a course of action.

These steps can take time and an awful lot of computing power. “For robots to be deployed into the field and safely operate in dynamic environments around humans, they need to be able to think and react very quickly,” says Plancher. “Current algorithms cannot be run on current CPU hardware fast enough.”

Neuman adds that researchers have been investigating better algorithms, but she thinks software improvements alone aren’t the answer. “What’s relatively new is the idea that you might also explore better hardware.” That means moving beyond a standard-issue CPU processing chip that comprises a robot’s brain — with the help of hardware acceleration.

Hardware acceleration refers to the use of a specialized hardware unit to perform certain computing tasks more efficiently. A commonly used hardware accelerator is the graphics processing unit (GPU), a chip specialized for parallel processing. These devices are handy for graphics because their parallel structure allows them to simultaneously process thousands of pixels. “A GPU is not the best at everything, but it’s the best at what it’s built for,” says Neuman. “You get higher performance for a particular application.” Most robots are designed with an intended set of applications and could therefore benefit from hardware acceleration. That’s why Neuman’s team developed robomorphic computing.

The system creates a customized hardware design to best serve a particular robot’s computing needs. The user inputs the parameters of a robot, like its limb layout and how its various joints can move. Neuman’s system translates these physical properties into mathematical matrices. These matrices are “sparse,” meaning they contain many zero values that roughly correspond to movements that are impossible given a robot’s particular anatomy. (Similarly, your arm’s movements are limited because it can only bend at certain joints — it’s not an infinitely pliable spaghetti noodle.)

The system then designs a hardware architecture specialized to run calculations only on the non-zero values in the matrices. The resulting chip design is therefore tailored to maximize efficiency for the robot’s computing needs. And that customization paid off in testing.

Hardware architecture designed using this method for a particular application outperformed off-the-shelf CPU and GPU units. While Neuman’s team didn’t fabricate a specialized chip from scratch, they programmed a customizable field-programmable gate array (FPGA) chip according to their system’s suggestions. Despite operating at a slower clock rate, that chip performed eight times faster than the CPU and 86 times faster than the GPU.

“I was thrilled with those results,” says Neuman. “Even though we were hamstrung by the lower clock speed, we made up for it by just being more efficient.”

Plancher sees widespread potential for robomorphic computing. “Ideally we can eventually fabricate a custom motion-planning chip for every robot, allowing them to quickly compute safe and efficient motions,” he says. “I wouldn't be surprised if 20 years from now every robot had a handful of custom computer chips powering it, and this could be one of them.” Neuman adds that robomorphic computing might allow robots to relieve humans of risk in a range of settings, such as caring for covid-19 patients or manipulating heavy objects.

“This work is exciting because it shows how specialized circuit designs can be used to accelerate a core component of robot control,” says Robin Deits, a robotics engineer at Boston Dynamics who was not involved in the research. “Software performance is crucial for robotics because the real world never waits around for the robot to finish thinking.” He adds that Neuman’s advance could enable robots to think faster, “unlocking exciting behaviors that previously would be too computationally difficult.”

Neuman next plans to automate the entire system of robomorphic computing. Users will simply drag and drop their robot’s parameters, and “out the other end comes the hardware description. I think that’s the thing that’ll push it over the edge and make it really useful.”

This research was funded by the National Science Foundation, the Computing Research Agency, the CIFellows Project, and the Defense Advanced Research Projects Agency.


",Designing customized “brains” for robots,2021-01-21,['Daniel Ackerman'],Robots/Robotics/Technology and society/Artificial intelligence/Machine learning/Algorithms/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/MIT Schwarzman College of Computing/School of Engineering/National Science Foundation (NSF),"['specialized', 'designing', 'research', 'brains', 'chip', 'customized', 'neuman', 'computing', 'robots', 'hardware', 'robot', 'robomorphic', 'neumans']","The method, called robomorphic computing, uses a robot’s physical layout and intended applications to generate a customized computer chip that minimizes the robot’s response time.
Hardware acceleration refers to the use of a specialized hardware unit to perform certain computing tasks more efficiently.
The system creates a customized hardware design to best serve a particular robot’s computing needs.
The resulting chip design is therefore tailored to maximize efficiency for the robot’s computing needs.
“Even though we were hamstrung by the lower clock speed, we made up for it by just being more efficient.”Plancher sees widespread potential for robomorphic computing.",Mit
15,https://news.mit.edu/2021/three-mit-faculty-elected-2020-acm-fellows-0120,"


Three MIT computer science faculty members have been elected as fellows of the Association for Computing Machinery (ACM).
The new fellows are among 95 ACM members recognized as the top 1 percent for their outstanding accomplishments in computing and information technology and/or outstanding service to ACM and the larger computing community. Fellows are nominated by their peers, with nominations reviewed by a distinguished selection committee.
Anantha Chandrakasan is dean of the School of Engineering and the Vannevar Bush Professor of Electrical Engineering and Computer Science. He leads the MIT Energy-Efficient Circuits and Systems Group, which works on a variety of projects such as ultra-low-power internet-of-things devices, energy-efficient processors, machine learning processors, hardware security for computing devices, and wireless systems. He was recognized as a 2020 ACM fellow for energy-efficient design methodologies and circuits that enable ultra-low-power wireless sensors and computing devices. 
Alan Edelman is an applied mathematics professor for the Department of Mathematics, the Applied Computing Group leader for the Computer Science and Artificial Intelligence Laboratory, and co-founder of the Julia programming language. His research includes high-performance computing, numerical computation, linear algebra, random matrix theory, and scientific machine learning. He was recognized as a 2020 ACM fellow for contributions to algorithms and languages for numerical and scientific computing.
Samuel Madden is the MIT Schwarzman College of Computing Distinguished Professor of Computing. Madden’s research is in the area of database systems, focusing on database analytics and query processing, ranging from clouds to sensors to modern high-performance server architectures. He co-directs the Data Systems for AI Lab initiative and the Data Systems Group, investigating issues related to systems and algorithms for data focusing on applying new methodologies for processing data, including applying machine learning methods to data systems and engineering data systems for applying machine learning at scale. He was recognized as a 2020 ACM fellow for contributions to data management and sensor computing systems.


",Three MIT faculty elected 2020 ACM Fellows,2021-01-20,[],"School of Engineering/School of Science/MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Mathematics/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Faculty/Awards, honors and fellowships/Artificial intelligence/Computer science and technology","['fellows', 'data', 'systems', 'faculty', '2020', 'recognized', 'learning', 'computing', 'elected', 'professor', 'machine', 'acm', 'mit', 'science']","Three MIT computer science faculty members have been elected as fellows of the Association for Computing Machinery (ACM).
The new fellows are among 95 ACM members recognized as the top 1 percent for their outstanding accomplishments in computing and information technology and/or outstanding service to ACM and the larger computing community.
He was recognized as a 2020 ACM fellow for contributions to algorithms and languages for numerical and scientific computing.
He co-directs the Data Systems for AI Lab initiative and the Data Systems Group, investigating issues related to systems and algorithms for data focusing on applying new methodologies for processing data, including applying machine learning methods to data systems and engineering data systems for applying machine learning at scale.
He was recognized as a 2020 ACM fellow for contributions to data management and sensor computing systems.",Mit
16,https://news.mit.edu/2021/intro-fast-paced-world-artificial-intelligence-0119,"


The field of artificial intelligence is moving at a staggering clip, with breakthroughs emerging in labs across MIT. Through the Undergraduate Research Opportunities Program (UROP), undergraduates get to join in. In two years, the MIT Quest for Intelligence has placed 329 students in research projects aimed at pushing the frontiers of computing and artificial intelligence, and using these tools to revolutionize how we study the brain, diagnose and treat disease, and search for new materials with mind-boggling properties.
Rafael Gomez-Bombarelli, an assistant professor in the MIT Department of Materials Science and Engineering, has enlisted several Quest-funded undergraduates in his mission to discover new molecules and materials with the help of AI. “They bring a blue-sky open mind and a lot of energy,” he says. “Through the Quest, we had the chance to connect with students from other majors who probably wouldn’t have thought to reach out.”
Some students stay in a lab for just one semester. Others never leave. Nick Bonaker is now in his third year working with Tamara Broderick, an associate professor in the Department of Electrical Engineering and Computer Science, to develop assistive technology tools for people with severe motor impairments.
“Nick has continually impressed me and our collaborators by picking up tools and ideas so quickly,” she says. “I particularly appreciate his focus on engaging so carefully and thoughtfully with the needs of the motor-impaired community. He has very carefully incorporated feedback from motor-impaired users, our charity collaborators, and other academics.”
This fall, MIT Quest celebrated two years of sponsoring UROP students. We highlight four of our favorite projects from last semester below.
Squeezing more energy from the sun
The price of solar energy is dropping as technology for converting sunlight into energy steadily improves. Solar cells are now close to hitting 50 percent efficiency in lab experiments, but there’s no reason to stop there, says Sean Mann, a sophomore majoring in computer science.
In a UROP project with Giuseppe Romano, a researcher at MIT’s Institute for Soldier Nanotechnologies, Mann is developing a solar cell simulator that would allow deep learning algorithms to systematically find better solar cell designs. Efficiency gains in the past have been made by evaluating new materials and geometries with hundreds of variables. “Traditional ways of exploring new designs is expensive, because simulations only measure the efficiency of that one design,” says Mann. “It doesn’t tell you how to improve it, which means you need either expert knowledge or lots more experiments to improve on it.”
The goal of Mann’s project is to develop a so-called differentiable solar cell simulator that computes the efficiency of a cell and describes how tweaking certain parameters will improve efficiency. Armed with this information, AI can predict which adjustments from among a dizzying array of combinations will boost cell performance the most. “Coupling this simulator with a neural network designed to maximize cell efficiency will eventually lead to some really good designs,” he says.
Mann is currently building an interface between AI models and traditional simulators. The biggest challenge so far, he says, has been debugging the simulator, which solves differential equations. He pulled several all-nighters double-checking his equations and code until he found the bug: an array of numbers off by one, skewing his results. With that obstacle down, Mann is now looking for algorithms to help the solver converge more quickly, a crucial step toward efficient optimization.
Teaching neural networks physics to identify stress fractures 
Sensors deep within the modern jet engine sound an alarm when something goes wrong. But diagnosing the precise failure is often impossible without tinkering with the engine itself. To get a clearer picture faster, engineers are experimenting with physics-informed deep learning algorithms to translate these sensor distress signals.
“It would be way easier to find the part that has something wrong with it, rather than take the whole engine apart,” says Julia Gaubatz, a senior majoring in aerospace engineering. “It could really save people time and money in industry.”
Gaubatz spent the fall programming physical constraints into a deep learning model in a UROP project with Raul Radovitzky, a professor in MIT’s Department of Aeronautics and Astronautics, graduate student Grégoire Chomette, and third-year student Parker Mayhew. Their goal is to analyze the high-frequency signals coming from, say, a jet engine shaft, to pinpoint where a part may be stressed and about to crack. They hope to identify the particular points of failure by training neural networks on numerical simulations of how materials break to understand the underlying physics.
Working from her off-campus apartment in Cambridge, Massachusetts, Gaubatz built a smaller, simplified version of their physics-informed model to make sure their assumptions were correct. “It’s easier to look at the weights the neural network is coming up with to understand its predictions,” she says. “It’s like a test to check that the model is doing what it should according to theory.”
She picked the project to try applying what she had learned in a course on machine learning to solid mechanics, which focuses on how materials deform and break under force. Engineers are just starting to incorporate deep learning into the field, she says, and “it’s exciting to see how a new mathematical concept may change how we do things.”
Training an AI to reason its way through visual problems
An artificial intelligence model that can play chess at superhuman levels may be hopeless at Sudoku. Humans, by contrast, pick up new games easily by adapting old knowledge to new environments. To give AI more of this flexibility, researchers created the ARC visual-reasoning dataset to motivate the field to create new techniques for solving problems involving abstraction and reasoning.
“If an AI does well on the test, it signals a more human-like intelligence,” says first-year student Subhash Kantamneni, who joined a UROP project this fall in the lab of Department of Brain and Cognitive Sciences (BSC) Professor Tomaso Poggio, which is part of the Center for Minds, Brains and Machines.
Poggio’s lab hopes to crack the ARC challenge by merging deep learning and automated program-writing to train an agent to solve ARC’s 400 tasks by writing its own programs. Much of their work takes place in DreamCoder, a tool developed at MIT that learns new concepts while solving specialized tasks. Using DreamCoder, the lab has so far solved 70 ARC tasks, and Kantamneni this fall worked with master of engineering student Simon Alford to tackle the rest.
To try and solve ARC’s 20 or so pattern-completion tasks, Kantamneni created a script to generate similar examples to train the deep learning model. He also wrote several mini programs, or primitives, to solve a separate class of tasks that involve performing logical operations on pixels. With the help of these new primitives, he says, DreamCoder learned to combine the old and new programs to solve ARC’s 10 or so pixelwise tasks.
The coding and debugging was hard work, he says, but the other lab members made him feel at home and appreciated. “I don’t think they even knew I was a freshman,” he says. “They listened to what I had to say and valued my input.”
Putting language comprehension under a microscope
Language is more than a system of symbols: It allows us to express concepts and ideas, think and reason, and communicate and coordinate with others. To understand how the brain does it, psychologists have developed methods for tracking how quickly people grasp what they read and hear. Longer reading times can indicate when a word has been improperly used, offering insight into how the brain incrementally finds meaning in a string of words.
In a UROP project this fall in Roger Levy’s lab in BCS, sophomore Pranali Vani ran a set of sentence-processing experiments online that were developed by an earlier UROP student. In each sentence, one word is placed in such a way that it creates an impression of ambiguity or implausibility. The weirder the sentence, the longer it takes a human subject to decipher its meaning. For example, placing a verb like “tripped” at the end of a sentence, as in “The woman brought the sandwich from the kitchen tripped,” tends to throw off native English speakers. Though grammatically correct, the wording implies that bringing rather than tripping is the main action of the sentence, creating confusion for the reader.
In three sets of experiments, Vani found that the biggest slowdowns came when the verb was positioned in a way that sounded ungrammatical. Vani and her advisor, Ethan Wilcox, a PhD student at Harvard University, got similar results when they ran the experiments on a deep learning model.
“The model was ‘surprised’ when the grammatical interpretation is unlikely,” says Wilcox. Though the model isn’t explicitly trained on English grammar, he says, the results suggest that a neural network trained on reams of text effectively learns the rules anyway.
Vani says she enjoyed learning how to program in R and shell scripts. She also gained an appreciation for the persistence needed to conduct original research. “It takes a long time,” she says. “There’s a lot of thought that goes into each detail and each decision made during the course of an experiment.”
Funding for MIT Quest UROP projects this fall was provided, in part, by the MIT-IBM Watson AI Lab.


",An intro to the fast-paced world of artificial intelligence,2021-01-19,['Kim Martineau'],"School of Engineering/School of Science/MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Materials Science and Engineering/Aeronautical and astronautical engineering/Brain and cognitive sciences/Quest for Intelligence/Undergraduate Research Opportunities Program (UROP)/MIT-IBM Watson AI Lab/Artificial intelligence/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Machine learning/Students/Undergraduate/Education, teaching, academics/Algorithms","['model', 'student', 'materials', 'learning', 'world', 'fastpaced', 'ai', 'lab', 'deep', 'artificial', 'urop', 'intelligence', 'intro', 'project', 'mit']","The field of artificial intelligence is moving at a staggering clip, with breakthroughs emerging in labs across MIT.
To get a clearer picture faster, engineers are experimenting with physics-informed deep learning algorithms to translate these sensor distress signals.
To try and solve ARC’s 20 or so pattern-completion tasks, Kantamneni created a script to generate similar examples to train the deep learning model.
In a UROP project this fall in Roger Levy’s lab in BCS, sophomore Pranali Vani ran a set of sentence-processing experiments online that were developed by an earlier UROP student.
Vani and her advisor, Ethan Wilcox, a PhD student at Harvard University, got similar results when they ran the experiments on a deep learning model.",Mit
17,https://news.mit.edu/2021/james-dicarlo-named-director-mit-quest-intelligence-0114,"


James DiCarlo, the Peter de Florez Professor of Neuroscience, has been appointed to the role of director of the MIT Quest for Intelligence. MIT Quest was launched in 2018 to discover the basis of natural intelligence, create new foundations for machine intelligence, and deliver new tools and technologies for humanity.
As director, DiCarlo will forge new collaborations with researchers within MIT and beyond to accelerate progress in understanding intelligence and developing the next generation of intelligence tools.
“We have discovered and developed surprising new connections between natural and artificial intelligence,” says DiCarlo, currently head of the Department of Brain and Cognitive Sciences (BCS). “The scientific understanding of natural intelligence, and advances in building artificial intelligence with positive real-world impact, are interlocked aspects of a unified, collaborative grand challenge, and MIT must continue to lead the way.” 
Aude Oliva, senior research scientist at the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the MIT director of the MIT-IBM Watson AI Lab, will lead industry engagements as director of MIT Quest Corporate. Nicholas Roy, professor of aeronautics and astronautics and a member of CSAIL, will lead the development of systems to deliver on the mission as director of MIT Quest Systems Engineering. Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing, will serve as chair of MIT Quest.
“The MIT Quest’s leadership team has positioned this initiative to spearhead our understanding of natural and artificial intelligence, and I am delighted that Jim is taking on this role,” says Huttenlocher, the Henry Ellis Warren (1894) Professor of Electrical Engineering and Computer Science.
DiCarlo will step down from his current role as head of BCS, a position he has held for nearly nine years, and will continue as faculty in BCS and as an investigator in the McGovern Institute for Brain Research.
“Jim has been a highly productive leader for his department, the School of Science, and the Institute at large. I’m excited to see the impact he will make in this new role,” says Nergis Mavalvala, dean of the School of Science and the Curtis and Kathleen Marble Professor of Astrophysics.
As department head, DiCarlo oversaw significant progress in the department’s scientific and educational endeavors. Roughly a quarter of current BCS faculty were hired on his watch, strengthening the department’s foundations in cognitive, systems, and cellular and molecular brain science. In addition, DiCarlo developed a new departmental emphasis in computation, deepening BCS’s ties with the MIT Schwarzman College of Computing and other MIT units such as the Center for Brains, Minds and Machines. He also developed and leads an NIH-funded graduate training program in computationally-enabled integrative neuroscience. As a result, BCS is one of the few departments in the world that is attempting to decipher, in engineering terms, how the human mind emerges from the biological components of the brain.
To prepare students for this future, DiCarlo collaborated with BCS Associate Department Head Michale Fee to design and execute a total overhaul of the Course 9 curriculum. In addition, partnering with the Department of Electrical Engineering and Computer Science, BCS developed a new major, Course 6-9 (Computation and Cognition), to fill the rapidly growing interest in this interdisciplinary topic. In only its second year, Course 6-9 already has more than 100 undergraduate majors.
DiCarlo has also worked tirelessly to build a more open, connected, and supportive culture across the entire BCS community in Building 46. In this work, as in everything, DiCarlo sought to bring people together to address challenges collaboratively. He attributes progress to strong partnerships with Li-Huei Tsai, the Picower Professor of Neuroscience in BCS and director of the Picower Institute for Learning and Memory; Robert Desimone, the Doris and Don Berkey Professor in BCS and director of the McGovern Institute for Brain Research; and to the work of dozens of faculty and staff. For example, in collaboration with associate department head Professor Rebecca Saxe, the department has focused on faculty mentorship of graduate students, and, in collaboration with postdoc officer Professor Mark Bear, the department developed postdoc salary and benefit standards. Both initiatives have become models for the Institute. In recent months, DiCarlo partnered with new associate department head Professor Laura Schulz to constructively focus renewed energy and resources on initiatives to address systemic racism and promote diversity, equity, inclusion, and social justice.
“Looking ahead, I share Jim’s vision for the research and educational programs of the department, and for enhancing its cohesiveness as a community, especially with regard to issues of diversity, equity, inclusion, and justice,” says Mavalvala. “I am deeply committed to supporting his successor in furthering these goals while maintaining the great intellectual strength of BCS.”
In his own research, DiCarlo uses a combination of large-scale neurophysiology, brain imaging, optogenetic methods, and high-throughput computational simulations to understand the neuronal mechanisms and cortical computations that underlie human visual intelligence. Working in animal models, he and his research collaborators have established precise connections between the internal workings of the visual system and the internal workings of particular computer vision systems. And they have demonstrated that these science-to-engineering connections lead to new ways to modulate neurons deep in the brain as well as to improved machine vision systems. His lab’s goals are to help develop more human-like machine vision, new neural prosthetics to restore or augment lost senses, new learning strategies, and an understanding of how visual cognition is impaired in agnosia, autism, and dyslexia. 
DiCarlo earned both a PhD in biomedical engineering and an MD from The Johns Hopkins University in 1998, and completed his postdoc training in primate visual neurophysiology at Baylor College of Medicine. He joined the MIT faculty in 2002.
A search committee will convene early this year to recommend candidates for the next department head of BCS. DiCarlo will continue to lead the department until that new head is selected.


",James DiCarlo named director of the MIT Quest for Intelligence,2021-01-14,[],Brain and cognitive sciences/Aeronautical and astronautical engineering/Electrical Engineering & Computer Science (eecs)/Quest for Intelligence/McGovern Institute/Computer Science and Artificial Intelligence Laboratory (CSAIL)/MIT-IBM Watson AI Lab/Center for Brains Minds and Machines/Picower Institute/Faculty/Neuroscience/Artificial intelligence/MIT Schwarzman College of Computing/School of Science/School of Engineering,"['named', 'systems', 'director', 'james', 'dicarlo', 'department', 'bcs', 'professor', 'quest', 'brain', 'head', 'intelligence', 'mit']","James DiCarlo, the Peter de Florez Professor of Neuroscience, has been appointed to the role of director of the MIT Quest for Intelligence.
MIT Quest was launched in 2018 to discover the basis of natural intelligence, create new foundations for machine intelligence, and deliver new tools and technologies for humanity.
As director, DiCarlo will forge new collaborations with researchers within MIT and beyond to accelerate progress in understanding intelligence and developing the next generation of intelligence tools.
As department head, DiCarlo oversaw significant progress in the department’s scientific and educational endeavors.
A search committee will convene early this year to recommend candidates for the next department head of BCS.",Mit
18,https://news.mit.edu/2021/model-viruses-escape-immune-0114,"


One reason it’s so difficult to produce effective vaccines against some viruses, including influenza and HIV, is that these viruses mutate very rapidly. This allows them to evade the antibodies generated by a particular vaccine, through a process known as “viral escape.”

MIT researchers have now devised a new way to computationally model viral escape, based on models that were originally developed to analyze language. The model can predict which sections of viral surface proteins are more likely to mutate in a way that enables viral escape, and it can also identify sections that are less likely to mutate, making them good targets for new vaccines.

“Viral escape is a big problem,” says Bonnie Berger, the Simons Professor of Mathematics and head of the Computation and Biology group in MIT’s Computer Science and Artificial Intelligence Laboratory. “Viral escape of the surface protein of influenza and the envelope surface protein of HIV are both highly responsible for the fact that we don’t have a universal flu vaccine, nor do we have a vaccine for HIV, both of which cause hundreds of thousands of deaths a year.”

In a study appearing today in Science, Berger and her colleagues identified possible targets for vaccines against influenza, HIV, and SARS-CoV-2. Since that paper was accepted for publication, the researchers have also applied their model to the new variants of SARS-CoV-2 that recently emerged in the United Kingdom and South Africa. That analysis, which has not yet been peer-reviewed, flagged viral genetic sequences that should be further investigated for their potential to escape the existing vaccines, the researchers say.

Berger and Bryan Bryson, an assistant professor of biological engineering at MIT and a member of the Ragon Institute of MGH, MIT, and Harvard, are the senior authors of the paper, and the lead author is MIT graduate student Brian Hie.

The language of proteins

Different types of viruses acquire genetic mutations at different rates, and HIV and influenza are among those that mutate the fastest. For these mutations to promote viral escape, they must help the virus change the shape of its surface proteins so that antibodies can no longer bind to them. However, the protein can’t change in a way that makes it nonfunctional. 

The MIT team decided to model these criteria using a type of computational model known as a language model, from the field of natural language processing (NLP). These models were originally designed to analyze patterns in language, specifically, the frequency which with certain words occur together. The models can then make predictions of which words could be used to complete a sentence such as “Sally ate eggs for …” The chosen word must be both grammatically correct and have the right meaning. In this example, an NLP model might predict “breakfast,” or “lunch.”

The researchers’ key insight was that this kind of model could also be applied to biological information such as genetic sequences. In that case, grammar is analogous to the rules that determine whether the protein encoded by a particular sequence is functional or not, and semantic meaning is analogous to whether the protein can take on a new shape that helps it evade antibodies. Therefore, a mutation that enables viral escape must maintain the grammaticality of the sequence but change the protein’s structure in a useful way.

“If a virus wants to escape the human immune system, it doesn’t want to mutate itself so that it dies or can’t replicate,” Hie says. “It wants to preserve fitness but disguise itself enough so that it’s undetectable by the human immune system.”

To model this process, the researchers trained an NLP model to analyze patterns found in genetic sequences, which allows it to predict new sequences that have new functions but still follow the biological rules of protein structure. One significant advantage of this kind of modeling is that it requires only sequence information, which is much easier to obtain than protein structures. The model can be trained on a relatively small amount of information — in this study, the researchers used 60,000 HIV sequences, 45,000 influenza sequences, and 4,000 coronavirus sequences.

“Language models are very powerful because they can learn this complex distributional structure and gain some insight into function just from sequence variation,” Hie says. “We have this big corpus of viral sequence data for each amino acid position, and the model learns these properties of amino acid co-occurrence and co-variation across the training data.”

Blocking escape

Once the model was trained, the researchers used it to predict sequences of the coronavirus spike protein, HIV envelope protein, and influenza hemagglutinin (HA) protein that would be more or less likely to generate escape mutations.

For influenza, the model revealed that the sequences least likely to mutate and produce viral escape were in the stalk of the HA protein. This is consistent with recent studies showing that antibodies that target the HA stalk (which most people infected with the flu or vaccinated against it do not develop) can offer near-universal protection against any flu strain.

The model’s analysis of coronaviruses suggested that a part of the spike protein called the S2 subunit is least likely to generate escape mutations. The question still remains as to how rapidly the SARS-CoV-2 virus mutates, so it is unknown how long the vaccines now being deployed to combat the Covid-19 pandemic will remain effective. Initial evidence suggests that the virus does not mutate as rapidly as influenza or HIV. However, the researchers recently identified new mutations that have appeared in Singapore, South Africa, and Malaysia, that they believe should be investigated for potential viral escape (these new data are not yet peer-reviewed).

In their studies of HIV, the researchers found that the V1-V2 hypervariable region of the protein has many possible escape mutations, which is consistent with previous findings, and they also found sequences that would have a lower probability of escape.

The researchers are now working with others to use their model to identify possible targets for cancer vaccines that stimulate the body’s own immune system to destroy tumors. They say it could also be used to design small-molecule drugs that might be less likely to provoke resistance, for diseases such as tuberculosis.

“There are so many opportunities, and the beautiful thing is all we need is sequence data, which is easy to produce,” Bryson says.

The research was funded by a National Defense Science and Engineering Graduate Fellowship from the Department of Defense and a National Science Foundation Graduate Research Fellowship.


",Model analyzes how viruses escape the immune system,2021-01-14,['Anne Trafton'],Research/Microbes/Vaccines/HIV/AIDS/Covid-19/Viruses/Genetics/Biological engineering/Computer science and technology/Artificial intelligence/Electrical Engineering & Computer Science (eecs)/Mathematics/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Broad Institute/School of Engineering/School of Science/MIT Schwarzman College of Computing/National Science Foundation (NSF),"['protein', 'hiv', 'escape', 'model', 'mutate', 'researchers', 'sequence', 'analyzes', 'influenza', 'viruses', 'system', 'viral', 'sequences', 'immune']","One reason it’s so difficult to produce effective vaccines against some viruses, including influenza and HIV, is that these viruses mutate very rapidly.
“Viral escape is a big problem,” says Bonnie Berger, the Simons Professor of Mathematics and head of the Computation and Biology group in MIT’s Computer Science and Artificial Intelligence Laboratory.
For these mutations to promote viral escape, they must help the virus change the shape of its surface proteins so that antibodies can no longer bind to them.
Therefore, a mutation that enables viral escape must maintain the grammaticality of the sequence but change the protein’s structure in a useful way.
For influenza, the model revealed that the sequences least likely to mutate and produce viral escape were in the stalk of the HA protein.",Mit
19,https://news.mit.edu/2021/center-constructive-communication-0113,"


Today MIT announced the launch of the interdisciplinary Center for Constructive Communication, which will leverage data-driven analytics to better understand current social and mass media ecosystems and design new tools and communication networks capable of bridging social, cultural, and political divides.
An important aspect of the new center is its commitment to reach beyond academia to work closely with experienced, locally based organizations and trusted influencers in underserved, marginalized communities across the country. These collaborations will be critical for launching pilot programs to evaluate which tools offer the greatest potential to create more trusted communication within our deeply fragmented society.
Based at the MIT Media Lab, and fostering collaborations across the MIT campus and beyond, the center is being established with over $10 million in commitments from foundations, corporations, government programs, and philanthropists. It will bring together researchers in artificial intelligence, computational social science, digital interactive design, and learning technologies to collaborate with software engineers, journalists, artists, public health experts, and community organizers.
The center will be directed by Deb Roy, professor of media arts and sciences, whose work on machine learning, human-machine interaction, analysis of large-scale media ecosystems, and advancing constructive dialogue will be integrated into the new center’s broad research program. For the past year, Roy also served as the executive director of the Media Lab. As an entrepreneur, Roy was co-founder and CEO of Bluefin Labs, a media analytics company acquired by Twitter in 2013, and is co-founder and chairperson of Cortico, a nonprofit collaborator with the center that builds systems for bringing underheard community voices into a healthier public sphere.
“Social media technologies promised to open up our worlds to new people and perspectives, but too often have ended up limiting and distorting our understanding of others,” says Roy. “Last week’s violence in Washington, D.C. — carried out by a mob mobilized largely in online bubbles — laid bare the challenge at hand. We now live in a fragmented society dominated by the loudest, most extreme voices, stifling avenues of communication that might lead to more constructive dialogues.”
“What our new center hopes to achieve is the creation of new ‘spaces’ where diverse voices and nuanced perspectives of so many who have been marginalized can be heard, and the design of tools and methods that enable influencers and organizations to play new and even more beneficial roles in society,” says Roy.
“A societal imperative”
In addressing these highly polarizing divides with the aid of new human-machine systems, center researchers will remain firmly committed to ensuring that the AI technologies key to this effort will enhance rather than replace human capabilities, and will move quickly to identify and guard against misuse of any of the work coming out of the center.
Marking the center’s launch, MIT President L. Rafael Reif says, “A signature strength of MIT is developing technological solutions to address humanity’s great challenges. Yet those who design and benefit from advanced technologies have a special responsibility to protect society from their unintended harms. Few forces in our society today are more powerful — and therefore more dangerous — than social media. To the challenge of protecting society against social media’s potential harms and helping it fulfill its highest purposes MIT offers a unique depth and breadth of expertise. The establishment of the Center for Constructive Communication, with Deb Roy at the helm, is a significant step in this important work.”
The center’s research program, focused on creating new human-machine communication capabilities, will be advanced by center fellows from BIPOC (Black, Indigenous, and people of color) and other communities, who work in media and communications, health disparities, immigration, racial justice, AI, and data analytics. These fellows will be important collaborators in designing, testing, and deploying strategies that build on ongoing research that uses AI-powered tools for listening, mapping, and shaping how information spreads.
“Involving trusted local influencers and organizers in the creation of these new capabilities is critical to addressing the harms of misinformation on societal issues such as Covid-19, immigration, and poverty,” says Ceasar McDowell, professor of the practice of civic design and associate head of the Department of Urban Studies and Planning, who will oversee the center’s fellowship program. “This requires building local capacity to use sophisticated communication analytics, to effectively identify, understand, and counter misinformation that threatens the health, safety, and security of marginalized communities.”
Martha Minow, the 300th Anniversary University Professor at Harvard University, who serves as a senior advisor to the center, stresses the importance of its work in maintaining a healthy pluralistic democracy. “In recent years, we’ve seen just how quickly an epidemic of misinformation and broken communication networks can expose the frailties of our democracy,” says Minow. “Opening trusted communication channels, encouraging dialogue across the full political spectrum, and engaging our most marginalized communities is not merely a wish list — it is a societal imperative.”
Building on the Laboratory for Social Machines
The Center for Constructive Communication represents the evolution and scaling of the Laboratory for Social Machines (LSM), which was established by Roy at the MIT Media Lab in 2014 and today includes a research team of approximately 30. LSM’s work built on Roy’s earlier Media Lab research on language analysis, and expanded it into social media analytics often presented through data visualization that detailed social media and media ecosystems.
More recently, LSM has expanded into the design of social technologies to support communication and learning across divides, establishing a track record that includes:

more than 160 peer-reviewed publications in human-machine communication and learning;
a study, in collaboration with Sinan Aral, the David Austin Professor of Management, on the spread of false news that was the cover story of Science magazine;
a tech-assisted coaching system, Learning Loops, for supporting kids’ narrative development, already successfully piloted with hundreds of participants in collaboration with community organizations; and
Beat the Virus, a coalition, created in close collaboration with global health expert, visiting professor, and former U.S. Assistant Surgeon General Susan Blumenthal, created in response to Covid-19, to deliver science-grounded public health guidance via social media influencers and to serve as a resource hub for trusted information about the pandemic. In addition, LSM social media analytics guided the generation of over 650 million media impressions and 5.5 million engagements with no paid media.

The new center will incorporate all work currently underway in LSM, including:

PULSE (Public Understanding, Listening, and Sense-Making Engine), a  collection of tools and methods for combining human listening with machine learning to make sense of public expressions of opinion and lived experience;
HealthPULSE, a public health communication system that leverages the PULSE toolkit to navigate our fragmented media landscape to provide reliable and relevant public guidance;
Clover, a pro-social media network designed to promote positive identity development, a sense of belonging, and exploration for tweens and early teens; and
StoryLine, media and content analytics for understanding how the content of stories connects with the audiences of stories.

Through a cooperation agreement with Cortico, the center incorporates Local Voices Network (LVN) data and methods in many projects. LVN aims to build infrastructure for a stronger democracy through technology-powered insights from small group facilitated conversations connected in a network of community engagement.
In addition to current collaborations with Cortico, New America, 826 Boston, and Frontline/PBS, the center is seeking to expand its relationship with numerous additional organizations across fields — organizations best suited to building the kind of trust needed to achieve meaningful change.


",MIT launches Center for Constructive Communication,2021-01-13,[],Media Lab/School of Architecture and Planning/Faculty/Administration/L. Rafael Reif/Technology and society/Social media,"['roy', 'public', 'media', 'work', 'launches', 'analytics', 'health', 'communication', 'constructive', 'center', 'social', 'mit']","Today MIT announced the launch of the interdisciplinary Center for Constructive Communication, which will leverage data-driven analytics to better understand current social and mass media ecosystems and design new tools and communication networks capable of bridging social, cultural, and political divides.
Based at the MIT Media Lab, and fostering collaborations across the MIT campus and beyond, the center is being established with over $10 million in commitments from foundations, corporations, government programs, and philanthropists.
Few forces in our society today are more powerful — and therefore more dangerous — than social media.
LSM’s work built on Roy’s earlier Media Lab research on language analysis, and expanded it into social media analytics often presented through data visualization that detailed social media and media ecosystems.
In addition, LSM social media analytics guided the generation of over 650 million media impressions and 5.5 million engagements with no paid media.",Mit
20,https://news.mit.edu/2021/antonio-torralba-elected-aaai-fellow-0112,"


Antonio Torralba, faculty head of Artificial Intelligence and Decision Making within the Department of Electrical Engineering and Computer Science (EECS) and the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science, has been selected as a 2021 Fellow by the Association for the Advancement of Artificial Intelligence (AAAI). AAAI Fellows are selected in recognition of their significant and extended contributions to the field (contributions which typically span a decade or more), including technical results, publications, patent awards, and contributions to group efforts.
Torralba received a degree in telecommunications engineering from Telecom BCN in Spain in 1994 and a PhD in signal, image, and speech processing from the Institut National Polytechnique de Grenoble, France in 2000. From 2000 to 2005, he received postdoc training at both the Department of Brain and Cognitive Sciences and the Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT. From 2017 to 2020, he was the MIT director of the MIT-IBM Watson AI Lab, and the inaugural director of the MIT Quest for Intelligence from 2018 to 2020. He is currently a member of both CSAIL and the Center for Brains, Minds and Machines.
Torralba’s research primarily focuses upon computer vision, machine learning, and the challenge of building computer systems that mimic human visual perception; additionally, Torralba is interested in neural networks, common-sense reasoning, computational photography, image databases, the intersections between visual art and computation, and the development of systems that can perceive the world through multiple senses (including audition and touch).
The author or co-author of over 300 papers, Torralba has been cited over 71,000 times on Google Scholar. He is an associate editor of the International Journal in Computer Vision, and served as program chair for the Computer Vision and Pattern Recognition conference in 2015. He has received the 2008 National Science Foundation Career Award, the Best Student Paper Award at the IEEE Conference on Computer Vision and Pattern Recognition in 2009, and the 2010 J. K. Aggarwal Prize from the International Association for Pattern Recognition. In 2017, he received the Frank Quick Faculty Research Innovation Fellowship and the Louis D. Smullin ('39) Award for Teaching Excellence. Earlier in 2020, he received the PAMI Mark Everingham Prize.


",Professor Antonio Torralba elected 2021 AAAI Fellow,2021-01-12,['Jane Halpern'],"Faculty/Awards, honors and fellowships/School of Engineering/MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Brain and cognitive sciences/Computer Science and Artificial Intelligence Laboratory (CSAIL)/MIT-IBM Watson AI Lab/Quest for Intelligence/Center for Brains Minds and Machines/Computer vision/Machine learning","['recognition', 'torralba', 'computer', '2021', 'engineering', 'aaai', 'mit', 'antonio', 'pattern', 'elected', 'professor', 'vision', 'fellow', 'intelligence', 'science', 'received']","Antonio Torralba, faculty head of Artificial Intelligence and Decision Making within the Department of Electrical Engineering and Computer Science (EECS) and the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science, has been selected as a 2021 Fellow by the Association for the Advancement of Artificial Intelligence (AAAI).
Torralba received a degree in telecommunications engineering from Telecom BCN in Spain in 1994 and a PhD in signal, image, and speech processing from the Institut National Polytechnique de Grenoble, France in 2000.
From 2000 to 2005, he received postdoc training at both the Department of Brain and Cognitive Sciences and the Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT.
The author or co-author of over 300 papers, Torralba has been cited over 71,000 times on Google Scholar.
He is an associate editor of the International Journal in Computer Vision, and served as program chair for the Computer Vision and Pattern Recognition conference in 2015.",Mit
21,https://news.mit.edu/2021/ten-keys-reality-wilczek-0112,"


In the spring of 1970, colleges across the country erupted with student protests in response to the Vietnam War and the National Guard’s shooting of student demonstrators at Kent State University. At the University of Chicago, where Frank Wilczek was an undergraduate, regularly scheduled classes were “improvised and semivoluntary” amid the turmoil, as he recalls.
It was during this turbulent time that Wilczek found unexpected comfort, and a new understanding of the world, in mathematics. He had decided to sit in on a class by physics professor Peter Freund, who, with a zeal “bordering on rapture,” led students through mathematical theories of symmetry and ways in which these theories can predict behaviors in the physical world.
In his new book, “Fundamentals: Ten Keys to Reality,” published today by Penguin Press, Wilczek writes that the lessons were a revelation: “To experience the deep harmony between two different universes — the universe of beautiful ideas and the universe of physical behavior — was for me a kind of spiritual awakening. It became my vocation. I haven’t been disappointed.”
Wilczek, who is the Herman Feshbach Professor of Physics at MIT, has since made groundbreaking contributions to our fundamental understanding of the physical world, for which he has been widely recognized, most notably in 2004 with the Nobel Prize in Physics, which he shared with physicists David Gross and David Politzer. He has also authored several popular science books on physics and the history of science.
In his new book, he distills scientists’ collective understanding of the physical world into 10 broad philosophical themes, using the fundamental theories of physics, from cosmology to quantum mechanics, to reframe ideas of space, time, and our place in the universe.
“People wrestle with what the world is all about,” Wilczek tells MIT News. “They’re not concerned with knowing precisely what Coulomb’s law is, but want to know more about questions like the ancient Greeks asked: What is space? What is time? So in the end, I came up with 10 assertions, at the levels of philosophy but backed up by very concrete facts, to organize what we know.”
A rollercoaster reborn
Wilczek wrote the bulk of the book earlier this spring, in the midst of another tumultuous time, at the start of a global pandemic. His grandson had been born as Wilczek was laying out the structure for his book, and in the preface, the physicist writes that he watched as the baby began building up a model of the world, based on his observations and interactions with the environment, “with insatiable curiosity and few preconceptions.”
Wilczek says that scientists may take a cue from the way babies learn — by building and pruning more detailed models of the world, with a similar unbiased, open outlook. He can recall times when he felt his own understanding of the world fundamentally shift. The college course on mathematical symmetry was an early instance. More recently, the rise of artificial intelligence and machine learning has prompted him to rethink “what knowledge is, and how it’s acquired.”
He writes: “The process of being born again can be disorienting. But, like a roller- coaster ride, it can also be exhilarating. And it brings this gift: To those who are born again, in the way of science, the world comes to seem fresh, lucid, and wonderfully abundant.”
“Patterns in matter”
Wilczek’s book contains ample opportunity for readers to reframe their view of the physical world. For instance, in a chapter entitled “There’s Plenty of Space,” he writes that, while the universe is vast, there is another scale of vastness in ourselves. To illustrate his point, he calculates that there are roughly 10 octillion atoms that make up the human body. That’s about 1 million times the number of stars in the visible universe. The multitudes within and beyond us are not contradictory, he says, but can be explained by the same set of physical rules.
And in fact, the universe, in all its diversity, can be described by a surprisingly few set of rules, collectively known as the Standard Model of Physics, though Wilczek prefers to call it by another name.
“The so-called Standard Model is the culmination of millenia of investigation, allowing us to understand how matter works, very fully,” Wilczek says. “So calling it a model, and standard, is kind of a lost opportunity to really convey to people the magnitude of what’s been achieved by humanity. That’s why I like to call it the ‘Core.’ It’s a central body of understanding that we can build out from.”
Wilczek takes the reader through many of the key experiments, theories, and revelations that physicists have made in building and validating the Standard Model, and its mathematical descriptions of the universe.
Included in this often joyful scientific tour are brief mentions of Wilczek’s own contributions, such as his Nobel-winning work establishing the theory of quantum chromodynamics; his characterization of the axion, a theoretical particle that he named after a laundry detergent by the same name (“It was short, catchy, and would fit in nicely alongside proton, neutron, electron, and pion,” he writes); and his introduction of the anyon — an entirely new kind of particle that is neither a fermion or a boson.
In April, and then separately in July, scientists made the first observations of anyons, nearly 40 years after Wilczek first proposed their existence.
“I was beginning to think it would never happen,” says Wilczek, who was finishing up his book when the discoveries were made public. “When it finally did, it was a beautiful surprise.”
The discovery of anyons opens possibilities for the particles to be used as building blocks for quantum computers, and marks another milestone in our understanding of the universe.
In closing his book, Wilczek writes about “complementarity” — a concept in physics that refers to two seemingly contrasting theories, such as the wave and particle theories of light, that can separately explain the same set of phenomena. He points to many complementary theories of physics throughout the book and extends the idea to philosophy and ways in which accepting contrasting views of the world can help us to expand our experience.
“With progress, we’ve come to consider people and creatures as having intrinsic value and being worthy of profound respect, just like ourselves,” he writes. “When we see ourselves as patterns in matter, it is natural to draw our circle of kinship very wide indeed.”


",Ten “keys to reality” from Nobel laureate Frank Wilczek,2021-01-12,['Jennifer Chu'],Books and authors/Center for Theoretical Physics/Faculty/History of science/Physics/School of Science/Science writing/Laboratory for Nuclear Science,"['book', 'nobel', 'model', 'keys', 'frank', 'reality', 'world', 'theories', 'laureate', 'universe', 'writes', 'physical', 'physics', 'wilczek', 'understanding']","At the University of Chicago, where Frank Wilczek was an undergraduate, regularly scheduled classes were “improvised and semivoluntary” amid the turmoil, as he recalls.
It was during this turbulent time that Wilczek found unexpected comfort, and a new understanding of the world, in mathematics.
“People wrestle with what the world is all about,” Wilczek tells MIT News.
“The so-called Standard Model is the culmination of millenia of investigation, allowing us to understand how matter works, very fully,” Wilczek says.
“I was beginning to think it would never happen,” says Wilczek, who was finishing up his book when the discoveries were made public.",Mit
22,https://news.mit.edu/2021/school-engineering-third-fourth-quarter-2020-awards-0107,"


Members of the MIT engineering faculty receive many awards in recognition of their scholarship, service, and overall excellence. The School of Engineering periodically recognizes their achievements by highlighting the honors, prizes, and medals won by faculty working in our academic departments, labs, and centers. The following are awards presented to engineering faculty in the 3rd and 4th quarters of 2020.

Pukit Agrawal of the Department of Electrical Engineering and Computer Science won the Amazon Research Award on June 17, and the AWS Machine Learning Research Award on Sept. 2.
	 
Robert Armstrong of the Department of Chemical Engineering received the AIChE Founders Award for Outstanding Contributions to the Field of Chemical Engineering on Sept. 1.
	 
Regina Barzilay of the Department of Electrical Engineering and Computer Science won the Inaugural AAAI Squirrel AI Award for Artificial Intelligence for the Benefit of Humanity on Sept. 23.
	 
Alexis Bateman of the Department of Mechanical Engineering was named one of the 100 Most Influential Women in Supply Chain list by B2G on Sept. 9.
	 
Geoffrey S.D. Beach of the Department of Materials Science and Engineering was named a Fellow of the American Physical Society on Sept. 28.
	 
Fikile Brushett of the Department of Chemical Engineering received the NOBBChE Lloyd N. Ferguson Young Scientist Award on Sept. 24.
	 
Yet-Ming Chiang of the Department of Materials Science and Engineering was named a fellow of the Electrochemical Society on July 31.
	 
James Collins, of the Institute of Medical Engineering and Science and the Department of Biological Engineering, won the Dickson Prize for Medicine on Oct. 26.
	 
Dirk Englund of the Department of Electrical Engineering and Computer Science won the Humboldt Research Fellowship for the fields of Optics and Atomic Physics on Sept. 8.
	 
James G. Fujimoto of the Department of Electrical Engineering and Computer Science won the The Sanford and Sue Greenberg Prize to End Blindness on Dec. 3.
	 
Robert Langer of the Department of Chemical Engineering received the APGI 2020 Maurice-Marie Janot Award on July 7.
	 
Nancy Lynch of the Department of Electrical Engineering and Computer Science won the CONCUR Test-of-Time Award on Sept. 2.
	 
Karthish Manthiram of the Department of Chemical Engineering received the AIChE 35 Under 35 Award on Sept. 16, and the 2020 DOE Early Career Research Award on Oct. 22.
	 
Jelena Notaros of the Department of Electrical Engineering and Computer Science was named in Forbes 30 under 30 list on Dec. 1.
	 
Eva Ponce of the Department of Mechanical Engineering was named one of the 100 Most Influential Women in Supply Chain List by B2G on Sept. 9.
	 
Ellen Roche of the Department of Mechanical Engineering won the NIBIB Trailblazer Award for New and Early Stage Investigators on Sept. 9.
	 
Maria Jesus Saenz of the Department of Mechanical Engineering was awarded the Distinguished Educator Award by the International Engineering and Operations Management Society on Aug. 15.
	 
Yang Shao-Horn of the of the Department of Mechanical Engineering was named a National Academy of Inventors Fellow on Dec. 8.
	 
Max Shulaker of the Department of Electrical Engineering and Computer Science won the 2020 IEEE Electron Devices Society Early Career Award on Sept. 23.
	 
Julian Shun of the Department of Electrical Engineering and Computer Science won the Google Faculty Research Award on Feb. 24.
	 
Zachary Smith of the Department of Chemical Engineering received the AIChE 35 Under 35 Award on Sept. 16.
	 
Giovanni Traverso of the Department of Mechanical Engineering was named by WIRED as one of the top 32 innovators who are building a better future on Dec. 3.
	 
Domitilla del Vecchio of the Department of Mechanical Engineering won the Newton Award for Transformative Ideas during the Covid-19 Pandemic on July 16, and was named an IEEE 2021 Fellow on Nov. 29.



",School of Engineering third and fourth quarter 2020 awards,2021-01-07,[],"School of Engineering/Biological engineering/Chemical engineering/Electrical Engineering & Computer Science (eecs)/Mechanical engineering/Materials Science and Engineering/Institute for Medical Engineering and Science (IMES)/Faculty/Awards, honors and fellowships/DMSE","['awards', 'scholarship', 'recognition', 'fourth', '2020', 'faculty', 'engineering', 'won', 'recognizes', 'school', 'service', 'quarter', 'working']","Members of the MIT engineering faculty receive many awards in recognition of their scholarship, service, and overall excellence.
The School of Engineering periodically recognizes their achievements by highlighting the honors, prizes, and medals won by faculty working in our academic departments, labs, and centers.
The following are awards presented to engineering faculty in the 3rd and 4th quarters of 2020.",Mit
23,https://news.mit.edu/2021/delivering-life-saving-oxygen-during-pandemic-0105,"


At the peak of the Covid-19 outbreak in Italy last spring, doctors and health care professionals were faced with harrowing decisions. Hospitals were running out of ventilators, forcing doctors to choose which patients had the best chance of survival, and which didn’t.
“It was a very difficult time for Italy,” recalls Daniele Vivona, a mechanical engineering graduate student from Italy. In early March, Vivona and a team of researchers at MIT’s Electrochemical Energy Lab (EEL) started to devise a plan to develop an oxygen concentrator that might one day help hospitals, like those in Italy, deliver oxygen to patients who so desperately need it.
“Traditionally, our lab uses electrons to break molecules that generate energy carriers,” explains Yang Shao-Horn, professor of mechanical engineering and EEL’s director. “We wanted to figure out how to take our expertise in electrochemistry and use it to create a device to make an oxygen concentrator that can be delivered to patients.”
Shao-Horn’s team is one of several groups that have been developing technologies to help hospitals around the world provide life-saving oxygen to patients with Covid-19 and other respiratory illnesses.
A low-cost portable oxygen concentrator
As a starting point, Shao-Horn and her team at EEL reached out to Boston-area doctors, as well as doctors in Italy and South Korea, to better understand their needs. They set out to make a low-cost and portable oxygen concentrator to improve clinical management in hospitals that were overwhelmed with Covid-19 patients, in addition to providing solutions that could be adopted in places with limited infrastructure, such as field hospitals or developing countries.
The resulting device resembles a typical electrochemical cell battery. Water and air are pumped through a cell with a cathode that produces negative electrons. The water is passed through a catalytic H2O2 membrane that helps separate oxygen from the air before being positively charged by an anode. After passing through an oxygen compressor, pure oxygen then flows to an oxygen tank, where it can be readily delivered and used to treat patients.
Postdoc C. John Eom has been leading efforts to improve the cathode and anode in the device, while fellow postdoc Yunguang Zhu has been focusing on the chemistry involved in the H2O2 membrane.
As the team continues to work on the concentrator, they are looking into various ways it can help doctors save lives — including while transporting patients from the intensive care unit (ICU) to the operating room.
“We’re hoping to have something portable enough that patients could potentially use the device at home and we provide doctors with more options to address diverse situations that require the delivery of oxygen to patients,” says Eom.
Open-source ventilator designs
Stories of Italian hospitals running out of ventilators were the impetus for another MIT-led project known as the MIT Emergency Ventilator Team. “This project started around the time of news reports from Italy describing ventilators being rationed due to shortages, and available data at that time suggested about 10 percent of Covid patients would require an ICU,” alum Alexander Slocum Jr. ’08, SM ’10, PhD ’13 said in April.
Slocum Jr. worked with his father Alexander Slocum Sr., the Walter M. May and A. Hazel May Professor of Mechanical Engineering, as well as research scientist Nevan Hanumara SM ’06, PhD ’12, and together they developed a plan to release an open-source design that companies worldwide could then use to manufacture low-cost ventilators for emergency use.
“We realized that as researchers, our best role would be supporting other people who had more capabilities to execute and produce ventilators than we did,” recalls Hanumara. “So, we focused heavily on developing the base requirements for safe low-cost ventilation and, following from this, a reference hardware and software design.”
The team grew to include MIT graduate students and alumni, including a trio from Professor Daniela Rus’s group in MIT's Computer Science and Artificial Intelligence Laboratory. They used a design developed in the mechanical engineering class 2.75 (Medical Device Design) back in 2010 as a starting point. Graduate student Kimberly Jung, a West Point graduate who has served in the U.S. Army, acted as the “executive officer,” holding the team together.
With insights gathered from the clinical community, they developed multiple prototype iterations, wrote code, and conducted animal studies. As the work progressed, it was posted to an open-source site. Within a few months, over 24,000 people had registered to gain access to the site.
“Since March, there has been a tremendous and humbling international response to our work,” adds Hanumara. The team has refocused to help groups around the world refine the designs and deploy ventilators. From an all-girl robotics team in Afghanistan to groups in New York, Ireland, India, Chile, and Morocco, Hanumara and the MIT Emergency Ventilator team have been helping others develop solutions that fit their own country’s needs.
Splitting ventilators to treat multiple patients
Giovanni Traverso, the Karl Van Tassel (1925) Career Development Professor of Mechanical Engineering, together with collaborators from Brigham and Women’s Hospital, Massachusetts General Hospital, and Philips, took a different approach in trying to solve the ventilator shortage. Their effort, led by postdoc Shriya Srinivasan PhD ’20, developed a method to split a ventilator so it can treat two, or potentially more, patients at a time instead of one. Their approach is meant only as a last resort when there aren’t enough ventilators to meet the need.
“We saw this as an opportunity where we might be able to help hospitals facing ventilator shortages due to Covid-19,” says Traverso. “While other teams were developing new ventilators, our approach was to address situations where people can’t make their own ventilators or augment the capacity of all ventilators. We wanted to help inform how they could amplify their current capacity further.”
Splitting ventilators between two patients provides a host of logistical issues, including matching flow rates and delivering the same amount of oxygen to two patients who may have different needs. “Previous designs haven’t provided the ability to customize the treatment to each patient, who will invariably present with variable needs,” says Srinivasan. “Our approach focused closely on this aspect and enabled the customization of volume and pressure for each patient.”
“We knew splitting ventilators was a major challenge, so we aimed to understand what the challenges were and address them to make it feasible to treat multiple patients using one ventilator,” adds Traverso
To tackle these challenges, Srinivasan and Traverso added two flow valves to the split ventilator. Health-care professionals can use these valves to tailor the flow of oxygen to each individual patient. The team also added new safety measures, including pressure release valves and alarms, to make sure patients don’t receive too much or too little oxygen as their condition changes.
The research team was able to successfully test their new method with the help of an artificial lung and through simultaneous ventilation of two pigs. As with the MIT Emergency Ventilator Team, the team is working with international groups to bring the split ventilator technology to countries that need additional infrastructure to treat patients with respiratory diseases like Covid-19. Their research was published in Science Translational Medicine and the team started a nonprofit, Project Prana, to help support the dissemination of the work.
“We’re also working with large health care systems and startups in India, Bangladesh, and Venezuela to bring the system to the rural towns that have run out of ventilators and cannot afford emergency ventilators,” Srinivasan says.
While the methods were different, these three research teams share a central purpose: to provide oxygen to those whose lives depend on it. Whether it’s through electrochemical reactions, open-source ventilator designs, or splitting ventilators, this research could help hospitals weather further spikes in Covid-19 cases and put solutions in place in the event of future pandemics.


",Delivering life-saving oxygen during a pandemic,2021-01-05,['Mary Beth Gallagher'],School of Engineering/MIT Schwarzman College of Computing/Mechanical engineering/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Covid-19/Disease/Design/Health care/Public health/Medical devices/Health sciences and technology/Pandemic,"['oxygen', 'lifesaving', 'ventilator', 'delivering', 'hospitals', 'ventilators', 'pandemic', 'help', 'doctors', 'patients', 'covid19', 'italy', 'team']","Hospitals were running out of ventilators, forcing doctors to choose which patients had the best chance of survival, and which didn’t.
After passing through an oxygen compressor, pure oxygen then flows to an oxygen tank, where it can be readily delivered and used to treat patients.
Open-source ventilator designsStories of Italian hospitals running out of ventilators were the impetus for another MIT-led project known as the MIT Emergency Ventilator Team.
“We saw this as an opportunity where we might be able to help hospitals facing ventilator shortages due to Covid-19,” says Traverso.
While the methods were different, these three research teams share a central purpose: to provide oxygen to those whose lives depend on it.",Mit
24,https://news.mit.edu/2020/community-year-review-1222,"


We’ve reached a reflective time in an especially reflective year for the MIT community. The challenges of 2020 have been unique, shaped in countless ways by the Covid-19 pandemic, renewed calls for social justice, an unprecedented election cycle, and more. Here are some of the top stories in the MIT community this year.
Adapting to Covid-19
Like everyone else, the Institute was forced to reckon with the Covid-19 pandemic — though it accomplished this with a distinctive MIT flair that included high-impact Covid-19 research; faculty creating new modes of teaching; heroic efforts from MIT Medical, including the development of unique testing facilities; the virtualization of everything from Campus Preview Weekend to Commencement to the Great Glass Pumpkin Patch; MIT-affiliated startups working to solve challenges brought on by the pandemic; and creative community-building efforts from students and others, including MIT Minecraft and a revamped first-year orientation.
The Institute recommits itself to supporting racial justice
Shortly after MIT hired a new Institute Community and Equity Officer, John Dozier, the community mourned together in the aftermath of high-profile killings of Black Americans at the hands of law enforcement. President Reif followed up with renewed commitments to address systemic racism at the Institute, and activist and scholar Angela Davis spoke with community members about the work ahead.
Nergis Mavalvala appointed dean of School of Science
Astrophysicist and Professor Nergis Mavalvala is renowned for her pioneering work in gravitational-wave detection, which she conducted as a leading member of LIGO, the Laser Interferometer Gravitational-Wave Observatory. In September, Mavalvala was appointed to serve as dean of the MIT School of Science, becoming the first woman in this role.
MIT and Harvard University take legal action against visa policy affecting foreign students
MIT and Harvard University filed a lawsuit challenging a policy that would have had the effect of banning any international student with an F-1 student visa if their classes were fully online as a result of Covid-19. The policy, which was issued by the U.S. Immigration and Customs Enforcement (ICE) and the Department of Homeland Security, was rescinded shortly after the lawsuit was filed.
MIT ends contract negotiations with Elsevier
Standing by its commitment to provide equitable and open access to scholarship, MIT ended negotiations with Elsevier for a new journals contract. Elsevier was not able to present a proposal that aligned with the principles of the MIT Framework for Publisher Contracts, which is grounded in the conviction that openly sharing research and educational materials is key to the Institute’s mission of advancing knowledge and bringing that knowledge to bear on the world’s greatest challenges.
Andrea Ghez ’87 wins Nobel Prize in Physics
Astronomer and Department of Physics alumna Andrea Ghez ’87 shared the 2020 Nobel Prize in Physics “for the discovery of a supermassive compact object at the center of our galaxy.” Ghez is known for her pioneering work using high spatial-resolution imaging techniques to study star-forming regions and the supermassive black hole known as Sagittarius A* at the center of the Milky Way Galaxy. She is the fourth woman to win a Nobel Prize in the physics category and the second MIT alumna to win a Nobel.
The Media Lab charts a course for the future
Today, the Media Lab announced the appointment of its new director, Dava Newman. The move serves as an important step in the lab’s plans to rebuild a culture of trust and support while retaining its culture of creativity. Earlier in January, a third-party review of MIT’s engagements with Jeffrey Epstein shed light on the Institute’s actions pertaining to Epstein donations that MIT received between 2002 and 2017, as well as visits that Epstein made to campus. MIT subsequently made donations totaling $850,000 to nonprofits supporting survivors of sexual abuse, while the Media Lab took initial steps toward establishing a supportive culture through self-examination and group discourse.
MIT adds important voices to the 2020 election discourse
Charles Stewart, Ariel White, Adam Berinsky, and others from around the Institute provided important viewpoints on this year’s historic elections — from the election process to voting rights to assessing voter intent — while students and others around the community led civic engagement efforts, including nonpartisan get-out-the-vote campaigns.
Students win an impressive number of distinguished fellowships
Despite a number of extraordinary pressures this year, MIT students continued to shine. Undergraduates were awarded two Rhodes Scholarships, two Marshall Scholarships, one Mitchell Scholarship, and four Schwarzman Scholarships.
Remembering those we’ve lost
Among community members who died this year were Angelika Amon, Arnold Demain, Michael Hawley, Tunney Lee, Mario Molina, Arthur Samberg, Judith Jarvis Thomson, Mary Frances Wagley, and Daniel Wang. A longer list of 2020 obituaries is available on MIT News.
In case you missed it…
Additional top community stories of 2020 include the MIT Task Force on the Work of the Future final report; a warning about deepfakes using art and artificial intelligence; the selection of four MIT-connected astronauts as the next possible moon walkers; a new approach to sustainable buildings in Boston; and MIT students (again!) dominating the Putnam Math Competition.


",MIT community in 2020: A year in review,2020-12-22,[],School of Engineering/School of Science/MIT Sloan School of Management/School of Architecture and Planning/School of Arts Humanities and Social Sciences/MIT Schwarzman College of Computing/President L. Rafael Reif/Diversity and inclusion/Covid-19/Social justice/Community/Administration,"['institute', 'community', '2020', 'work', 'policy', 'review', 'win', 'prize', 'covid19', 'mit', 'students']","We’ve reached a reflective time in an especially reflective year for the MIT community.
Here are some of the top stories in the MIT community this year.
The Institute recommits itself to supporting racial justiceShortly after MIT hired a new Institute Community and Equity Officer, John Dozier, the community mourned together in the aftermath of high-profile killings of Black Americans at the hands of law enforcement.
Students win an impressive number of distinguished fellowshipsDespite a number of extraordinary pressures this year, MIT students continued to shine.
A longer list of 2020 obituaries is available on MIT News.",Mit
25,https://news.mit.edu/2020/top-research-stories-1222,"


Although 2020 has been a year most of us would prefer to forget, it still featured a number of research breakthroughs worth celebrating. Despite the new challenges brought on by Covid-19 — and sometimes because of them — MIT’s community achieved important milestones on the frontiers of science and engineering.
The following 10 research-related stories published in the previous 12 months received top views on MIT News. (We’ve also rounded up the year’s top MIT community-related stories.)
10. How quarantines impact Covid-19’s spread. A team of MIT engineers developed a model that uses data from the Covid-19 pandemic in conjunction with a neural network to determine the efficacy of quarantine measures and better predict the spread of the virus. The researchers say their model is the first to have integrated machine learning with epidemiology.
9. A flat fisheye lens. Engineers at MIT and elsewhere designed the first flat lens that can produce crisp, 180-degree panoramic images similar to those produced by the curved glass of traditional fisheye lenses. The design consists of a single flat, millimeter-thin piece of glass covered on one side with tiny structures that precisely scatter incoming light to produce panoramic images.
8. Blocking coronaviruses’ ability to enter human cells. MIT chemists designed a drug candidate that can bind to the viral protein coronaviruses use to enter human cells, potentially disarming it. The potential drug is a short protein fragment, or peptide, that mimics a protein found on the surface of human cells.
7. Why motivation to learn declines with age. MIT neuroscientists found that aging negatively affects a brain circuit critical for maintaining motivation to learn new things and engage in everyday activities. They also showed they could boost older mice’s motivation to engage in certain learning activities by reactivating this circuit.
6. A black hole’s corona plays peekaboo. In a first, astronomers at MIT and elsewhere watched a supermassive black hole’s corona — the ring of high-energy particles that encircles a black hole’s horizon — disappear and then reappear. Although the cause of the transformation is unclear, the researchers guess it may have been caused by a star caught in the black hole’s gravitational pull.
5. Simple, solar-powered water desalination. Researchers at MIT and in China developed a completely passive, solar-powered water desalination system that achieves a new level of efficiency in turning seawater into fresh, potable water using the energy of sunlight. The system could provide more than 1.5 gallons of fresh drinking water per hour for every square meter of solar collecting area.
4. Model identifies a powerful new antibiotic. An MIT-developed deep learning model identified a new drug compound capable of killing many species of antibiotic-resistant bacteria. In lab tests, the compound killed many of the world’s most problematic disease-causing bacteria, including some strains that are resistant to all known antibiotics.
3. Deploying an open-source, low-cost ventilator. A team including MIT engineers, physicians, and computer scientists designed an inexpensive, open-source ventilator to address a global shortage of the life-saving machines brought by the Covid-19 pandemic. The team’s ventilator can be produced using common materials to enable rapid deployment. The team also published details on their website to facilitate parallel efforts by other experts.
2. A signature of life on Venus. Astronomers from MIT and elsewhere found evidence of phosphine, a gas associated with living organisms, in the habitable region of Venus’ atmosphere. If their observation is indeed associated with life, it must be some sort of “aerial” life-form in Venus’ clouds, the team concluded.
1. Covid-19 detection through cough recordings. MIT researchers trained an artificial intelligence model to distinguish asymptomatic people infected with Covid-19 from healthy individuals through forced-cough recordings. In experiments, the model accurately identified 98.5 percent of participants who were confirmed to have Covid-19, based on the recordings they submitted.
In case you missed it…
Additional top research stories of 2020 include a study of economic recovery after the 1918 flu pandemic; a look at the “blue shift” and vote-counting in presidential elections; a progress report on the new MIT-designed fusion experiment; a prototype for a reusable silicone rubber face mask; and an answer to why shaving dulls even the sharpest of razors.


",Top MIT research stories of 2020,2020-12-22,[],MIT Sloan School of Management/School of Science/School of Architecture and Planning/School of Engineering/Research/School of Arts Humanities and Social Sciences/Covid-19/Pandemic,"['research', 'ventilator', 'model', 'holes', '2020', 'water', 'researchers', 'black', 'covid19', 'venus', 'mit', 'team']","Although 2020 has been a year most of us would prefer to forget, it still featured a number of research breakthroughs worth celebrating.
The following 10 research-related stories published in the previous 12 months received top views on MIT News.
In a first, astronomers at MIT and elsewhere watched a supermassive black hole’s corona — the ring of high-energy particles that encircles a black hole’s horizon — disappear and then reappear.
Astronomers from MIT and elsewhere found evidence of phosphine, a gas associated with living organisms, in the habitable region of Venus’ atmosphere.
MIT researchers trained an artificial intelligence model to distinguish asymptomatic people infected with Covid-19 from healthy individuals through forced-cough recordings.",Mit
26,https://news.mit.edu/2020/want-cheaper-nuclear-energy-turn-design-process-game-1217,"


Nuclear energy provides more carbon-free electricity in the United States than solar and wind combined, making it a key player in the fight against climate change. But the U.S. nuclear fleet is aging, and operators are under pressure to streamline their operations to compete with coal- and gas-fired plants.
One of the key places to cut costs is deep in the reactor core, where energy is produced. If the fuel rods that drive reactions there are ideally placed, they burn less fuel and require less maintenance. Through decades of trial and error, nuclear engineers have learned to design better layouts to extend the life of pricey fuel rods. Now, artificial intelligence is poised to give them a boost.
Researchers at MIT and Exelon show that by turning the design process into a game, an AI system can be trained to generate dozens of optimal configurations that can make each rod last about 5 percent longer, saving a typical power plant an estimated $3 million a year, the researchers report. The AI system can also find optimal solutions faster than a human, and quickly modify designs in a safe, simulated environment. Their results appear this month in the journal Nuclear Engineering and Design.
“This technology can be applied to any nuclear reactor in the world,” says the study’s senior author, Koroush Shirvan, an assistant professor in MIT’s Department of Nuclear Science and Engineering. “By improving the economics of nuclear energy, which supplies 20 percent of the electricity generated in the U.S., we can help limit the growth of global carbon emissions and attract the best young talents to this important clean-energy sector.”
In a typical reactor, fuel rods are lined up on a grid, or assembly, by their levels of uranium and gadolinium oxide within, like chess pieces on a board, with radioactive uranium driving reactions, and rare-earth gadolinium slowing them down. In an ideal layout, these competing impulses balance out to drive efficient reactions. Engineers have tried using traditional algorithms to improve on human-devised layouts, but in a standard 100-rod assembly there might be an astronomical number of options to evaluate. So far, they’ve had limited success.
The researchers wondered if deep reinforcement learning, an AI technique that has achieved superhuman mastery at games like chess and Go, could make the screening process go faster. Deep reinforcement learning combines deep neural networks, which excel at picking out patterns in reams of data, with reinforcement learning, which ties learning to a reward signal like winning a game, as in Go, or reaching a high score, as in Super Mario Bros.
Here, the researchers trained their agent to position the fuel rods under a set of constraints, earning more points with each favorable move. Each constraint, or rule, picked by the researchers reflects decades of expert knowledge rooted in the laws of physics. The agent might score points, for example, by positioning low-uranium rods on the edges of the assembly, to slow reactions there; by spreading out the gadolinium “poison” rods to maintain consistent burn levels; and by limiting the number of poison rods to between 16 and 18.
“After you wire in rules, the neural networks start to take very good actions,” says the study’s lead author Majdi Radaideh, a postdoc in Shirvan’s lab. “They’re not wasting time on random processes. It was fun to watch them learn to play the game like a human would.”
Through reinforcement learning, AI has learned to play increasingly complex games as well as or better than humans. But its capabilities remain relatively untested in the real world. Here, the researchers show that reinforcement learning has potentially powerful applications.
“This study is an exciting example of transferring an AI technique for playing board games and video games to helping us solve practical problems in the world,” says study co-author Joshua Joseph, a research scientist at the MIT Quest for Intelligence.
Exelon is now testing a beta version of the AI system in a virtual environment that mimics an assembly within a boiling water reactor, and about 200 assemblies within a pressurized water reactor, which is globally the most common type of reactor. Based in Chicago, Illinois, Exelon owns and operates 21 nuclear reactors across the United States. It could be ready to implement the system in a year or two, a company spokesperson says.
The study’s other authors are Isaac Wolverton, a MIT senior who joined the project through the Undergraduate Research Opportunities Program; Nicholas Roy and Benoit Forget of MIT; and James Tusar and Ugi Otgonbaatar of Exelon.


",Want cheaper nuclear energy? Turn the design process into a game,2020-12-17,['Kim Martineau'],School of Engineering/MIT Schwarzman College of Computing/Nuclear science and engineering/Quest for Intelligence/Undergraduate Research Opportunities Program (UROP)/Energy/Nuclear power and reactors/Research/Artificial intelligence/Computing/Algorithms/Data/Gaming,"['process', 'rods', 'reactor', 'cheaper', 'energy', 'learning', 'researchers', 'reactions', 'fuel', 'ai', 'reinforcement', 'system', 'turn', 'nuclear', 'design', 'game']","If the fuel rods that drive reactions there are ideally placed, they burn less fuel and require less maintenance.
Through decades of trial and error, nuclear engineers have learned to design better layouts to extend the life of pricey fuel rods.
The AI system can also find optimal solutions faster than a human, and quickly modify designs in a safe, simulated environment.
“This technology can be applied to any nuclear reactor in the world,” says the study’s senior author, Koroush Shirvan, an assistant professor in MIT’s Department of Nuclear Science and Engineering.
Deep reinforcement learning combines deep neural networks, which excel at picking out patterns in reams of data, with reinforcement learning, which ties learning to a reward signal like winning a game, as in Go, or reaching a high score, as in Super Mario Bros.",Mit
27,https://news.mit.edu/2020/brain-reading-computer-code-1215,"


In some ways, learning to program a computer is similar to learning a new language. It requires learning new symbols and terms, which must be organized correctly to instruct the computer what to do. The computer code must also be clear enough that other programmers can read and understand it.

In spite of those similarities, MIT neuroscientists have found that reading computer code does not activate the regions of the brain that are involved in language processing. Instead, it activates a distributed network called the multiple demand network, which is also recruited for complex cognitive tasks such as solving math problems or crossword puzzles.

However, although reading computer code activates the multiple demand network, it appears to rely more on different parts of the network than math or logic problems do, suggesting that coding does not precisely replicate the cognitive demands of mathematics either.

“Understanding computer code seems to be its own thing. It’s not the same as language, and it’s not the same as math and logic,” says Anna Ivanova, an MIT graduate student and the lead author of the study.

Evelina Fedorenko, the Frederick A. and Carole J. Middleton Career Development Associate Professor of Neuroscience and a member of the McGovern Institute for Brain Research, is the senior author of the paper, which appears today in eLife. Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory and Tufts University were also involved in the study.

Language and cognition

A major focus of Fedorenko’s research is the relationship between language and other cognitive functions. In particular, she has been studying the question of whether other functions rely on the brain’s language network, which includes Broca’s area and other regions in the left hemisphere of the brain. In previous work, her lab has shown that music and math do not appear to activate this language network.

“Here, we were interested in exploring the relationship between language and computer programming, partially because computer programming is such a new invention that we know that there couldn’t be any hardwired mechanisms that make us good programmers,” Ivanova says.

There are two schools of thought regarding how the brain learns to code, she says. One holds that in order to be good at programming, you must be good at math. The other suggests that because of the parallels between coding and language, language skills might be more relevant. To shed light on this issue, the researchers set out to study whether brain activity patterns while reading computer code would overlap with language-related brain activity.

The two programming languages that the researchers focused on in this study are known for their readability — Python and ScratchJr, a visual programming language designed for children age 5 and older. The subjects in the study were all young adults proficient in the language they were being tested on. While the programmers lay in a functional magnetic resonance (fMRI) scanner, the researchers showed them snippets of code and asked them to predict what action the code would produce.

The researchers saw little to no response to code in the language regions of the brain. Instead, they found that the coding task mainly activated the so-called multiple demand network. This network, whose activity is spread throughout the frontal and parietal lobes of the brain, is typically recruited for tasks that require holding many pieces of information in mind at once, and is responsible for our ability to perform a wide variety of mental tasks.

“It does pretty much anything that’s cognitively challenging, that makes you think hard,” Ivanova says.

Previous studies have shown that math and logic problems seem to rely mainly on the multiple demand regions in the left hemisphere, while tasks that involve spatial navigation activate the right hemisphere more than the left. The MIT team found that reading computer code appears to activate both the left and right sides of the multiple demand network, and ScratchJr activated the right side slightly more than the left. This finding goes against the hypothesis that math and coding rely on the same brain mechanisms.

Effects of experience

The researchers say that while they didn’t identify any regions that appear to be exclusively devoted to programming, such specialized brain activity might develop in people who have much more coding experience.

“It’s possible that if you take people who are professional programmers, who have spent 30 or 40 years coding in a particular language, you may start seeing some specialization, or some crystallization of parts of the multiple demand system,” Fedorenko says. “In people who are familiar with coding and can efficiently do these tasks, but have had relatively limited experience, it just doesn’t seem like you see any specialization yet.”

In a companion paper appearing in the same issue of eLife, a team of researchers from Johns Hopkins University also reported that solving code problems activates the multiple demand network rather than the language regions.

The findings suggest there isn’t a definitive answer to whether coding should be taught as a math-based skill or a language-based skill. In part, that’s because learning to program may draw on both language and multiple demand systems, even if — once learned — programming doesn’t rely on the language regions, the researchers say.

“There have been claims from both camps — it has to be together with math, it has to be together with language,” Ivanova says. “But it looks like computer science educators will have to develop their own approaches for teaching code most effectively.”

The research was funded by the National Science Foundation, the Department of the Brain and Cognitive Sciences at MIT, and the McGovern Institute for Brain Research.


","To the brain, reading computer code is not the same as reading language",2020-12-15,['Anne Trafton'],Research/Brain and cognitive sciences/Learning/McGovern Institute/National Science Foundation (NSF)/Linguistics/Neuroscience/STEM education/Computer science and technology/Technology and society/Computer Science and Artificial Intelligence Laboratory (CSAIL)/School of Science/MIT Schwarzman College of Computing,"['coding', 'code', 'language', 'computer', 'researchers', 'multiple', 'math', 'network', 'demand', 'reading', 'brain']","The computer code must also be clear enough that other programmers can read and understand it.
In spite of those similarities, MIT neuroscientists have found that reading computer code does not activate the regions of the brain that are involved in language processing.
“Understanding computer code seems to be its own thing.
The other suggests that because of the parallels between coding and language, language skills might be more relevant.
To shed light on this issue, the researchers set out to study whether brain activity patterns while reading computer code would overlap with language-related brain activity.",Mit
28,https://news.mit.edu/2020/building-machines-better-understand-human-goals-1214,"


In a classic experiment on human social intelligence by psychologists Felix Warneken and Michael Tomasello, an 18-month old toddler watches a man carry a stack of books towards an unopened cabinet. When the man reaches the cabinet, he clumsily bangs the books against the door of the cabinet several times, then makes a puzzled noise. 
Something remarkable happens next: the toddler offers to help. Having inferred the man's goal, the toddler walks up to the cabinet and opens its doors, allowing the man to place his books inside. But how is the toddler, with such limited life experience, able to make this inference? 
Recently, computer scientists have redirected this question toward computers: How can machines do the same? 
The critical component to engineering this type of understanding is arguably what makes us most human: our mistakes. Just as the toddler could infer the man’s goal merely from his failure, machines that infer our goals need to account for our mistaken actions and plans. 
In the quest to capture this social intelligence in machines, researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Department of Brain and Cognitive Sciences created an algorithm capable of inferring goals and plans, even when those plans might fail. 
This type of research could eventually be used to improve a range of assistive technologies, collaborative or caretaking robots, and digital assistants like Siri and Alexa. 
“This ability to account for mistakes could be crucial for building machines that robustly infer and act in our interests,” says Tan Zhi-Xuan, PhD student in MIT’s Department of Electrical Engineering and Computer Science (EECS) and the lead author on a new paper about the research. “Otherwise, AI systems might wrongly infer that, since we failed to achieve our higher-order goals, those goals weren't desired after all. We've seen what happens when algorithms feed on our reflexive and unplanned usage of social media, leading us down paths of dependency and polarization. Ideally, the algorithms of the future will recognize our mistakes, bad habits, and irrationalities and help us avoid, rather than reinforce, them.” 
To create their model the team used Gen, a new AI programming platform recently developed at MIT, to combine symbolic AI planning with Bayesian inference. Bayesian inference provides an optimal way to combine uncertain beliefs with new data, and is widely used for financial risk evaluation, diagnostic testing, and election forecasting. 
The team’s model performed 20 to 150 times faster than an existing baseline method called Bayesian Inverse Reinforcement Learning (BIRL), which learns an agent’s objectives, values, or rewards by observing its behavior, and attempts to compute full policies or plans in advance. The new model was accurate 75 percent of the time in inferring goals. 
“AI is in the process of abandoning the ‘standard model’ where a fixed, known objective is given to the machine,” says Stuart Russell, the Smith-Zadeh Professor of Engineering at the University of California at Berkeley. “Instead, the machine knows that it doesn't know what we want, which means that research on how to infer goals and preferences from human behavior becomes a central topic in AI. This paper takes that goal seriously; in particular, it is a step towards modeling — and hence inverting — the actual process by which humans generate behavior from goals and preferences.""
How it works 
While there’s been considerable work on inferring the goals and desires of agents, much of this work has assumed that agents act optimally to achieve their goals. 
However, the team was particularly inspired by a common way of human planning that’s largely sub-optimal: not to plan everything out in advance, but rather to form only partial plans, execute them, and then plan again from there. While this can lead to mistakes from not thinking enough “ahead of time,” it also reduces the cognitive load. 
For example, imagine you're watching your friend prepare food, and you would like to help by figuring out what they’re cooking. You guess the next few steps your friend might take: maybe preheating the oven, then making dough for an apple pie. You then “keep” only the partial plans that remain consistent with what your friend actually does, and then you repeat the process by planning ahead just a few steps from there. 
Once you've seen your friend make the dough, you can restrict the possibilities only to baked goods, and guess that they might slice apples next, or get some pecans for a pie mix. Eventually, you'll have eliminated all the plans for dishes that your friend couldn't possibly be making, keeping only the possible plans (i.e., pie recipes). Once you're sure enough which dish it is, you can offer to help.
The team’s inference algorithm, called “Sequential Inverse Plan Search (SIPS)”, follows this sequence to infer an agent's goals, as it only makes partial plans at each step, and cuts unlikely plans early on. Since the model only plans a few steps ahead each time, it also accounts for the possibility that the agent — your friend — might be doing the same. This includes the possibility of mistakes due to limited planning, such as not realizing you might need two hands free before opening the refrigerator. By detecting these potential failures in advance, the team hopes the model could be used by machines to better offer assistance.
“One of our early insights was that if you want to infer someone’s goals, you don’t need to think further ahead than they do. We realized this could be used not just to speed up goal inference, but also to infer intended goals from actions that are too shortsighted to succeed, leading us to shift from scaling up algorithms to exploring ways to resolve more fundamental limitations of current AI systems,” says Vikash Mansinghka, a principal research scientist at MIT and one of Tan Zhi-Xuan's co-advisors, along with Joshua Tenenbaum, MIT professor in brain and cognitive sciences. “This is part of our larger moonshot — to reverse-engineer 18-month-old human common sense.” 
The work builds conceptually on earlier cognitive models from Tenenbaum's group, showing how simpler inferences that children and even 10-month-old infants make about others' goals can be modeled quantitatively as a form of Bayesian inverse planning.
While to date the researchers have explored inference only in relatively small planning problems over fixed sets of goals, through future work they plan to explore richer hierarchies of human goals and plans. By encoding or learning these hierarchies, machines might be able to infer a much wider variety of goals, as well as the deeper purposes they serve.
“Though this work represents only a small initial step, my hope is that this research will lay some of the philosophical and conceptual groundwork necessary to build machines that truly understand human goals, plans and values,” says Xuan. “This basic approach of modeling humans as imperfect reasoners feels very promising. It now allows us to infer when plans are mistaken, and perhaps it will eventually allow us to infer when people hold mistaken beliefs, assumptions, and guiding principles as well.”
Zhi-Xuan, Mansinghka, and Tenenbaum wrote the paper alongside EECS graduate student Jordyn Mann and PhD student Tom Silver. They virtually presented their work last week at the Conference on Neural Information Processing Systems (NeurIPS 2020).
This work was funded, in part, by the DARPA Machine Common Sense program, the Aphorism Foundation, the Siegel Family Foundation, the MIT-IBM Watson AI Lab, and the Intel Probabilistic Computing Center. Tom Silver is supported by an NSF Graduate Research Fellowship. 


",Building machines that better understand human goals,2020-12-14,['Rachel Gordon'],Brain and cognitive sciences/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Research/Machine learning/Artificial intelligence/Algorithms/Computer science and technology/School of Engineering/School of Science/MIT Schwarzman College of Computing/MIT-IBM Watson AI Lab/Defense Advanced Research Projects Agency (DARPA),"['human', 'plans', 'research', 'model', 'goals', 'infer', 'work', 'friend', 'ai', 'better', 'machines', 'building', 'understand']","Just as the toddler could infer the man’s goal merely from his failure, machines that infer our goals need to account for our mistaken actions and plans.
“Otherwise, AI systems might wrongly infer that, since we failed to achieve our higher-order goals, those goals weren't desired after all.
By detecting these potential failures in advance, the team hopes the model could be used by machines to better offer assistance.
While to date the researchers have explored inference only in relatively small planning problems over fixed sets of goals, through future work they plan to explore richer hierarchies of human goals and plans.
“Though this work represents only a small initial step, my hope is that this research will lay some of the philosophical and conceptual groundwork necessary to build machines that truly understand human goals, plans and values,” says Xuan.",Mit
29,https://news.mit.edu/2020/automating-material-matching-movies-video-games-1210,"


Very few of us who play video games or watch computer-generated image-filled movies ever take the time to sit back and appreciate all the handiwork that make their graphics so thrilling and immersive. 
One key aspect of this is texture. The glossy pictures we see on our screens often appear seamlessly rendered, but they require huge amounts of work behind the scenes. When effects studios create scenes in computer-assisted design programs, they first 3D model all the objects that they plan to put in the scene, and then give a texture to each generated object: for example, making a wood table appear to be glossy, polished, or matte.
If a designer is trying to recreate a particular texture from the real world, they may find themselves digging around online trying to find a close match that can be stitched together for the scene. But most of the time you can’t just take a photo of an object and use it in a scene — you have to create a set of “maps” that quantify different properties like roughness or light levels.
There are programs that have made this process easier than ever before, like the Adobe Substance software that helped propel the photorealistic ruins of Las Vegas in “Blade Runner 2049”. However, these so-called “procedural” programs can take months to learn, and still involve painstaking hours or even days to create a particular texture.










              

            Even the design of a simple leather shoe can be made up of dozens of different textures.        

          




















Previous item
Next item

















A team led by researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) has developed an approach that they say can make texturing even less tedious, to the point where you can snap a picture of something you see in a store, and then go recreate the material on your home laptop. 
“Imagine being able to take a photo of a pair of jeans that you can then have your character wear in a video game,” says PhD student Liang Shi, lead author of a paper about the new “MATch” project. “We believe this system would further close the gap between ‘virtual’ and ‘reality.’”
Shi says that the goal of MATch is to “significantly simplify and expedite the creation of synthetic materials using machine learning.” The team evaluated MATch on both rendered synthetic materials and real materials captured on camera, and showed that it can reconstruct materials more accurately and at a higher resolution than existing state-of-the-art methods. 
A collaboration with researchers at Adobe, one core element is a new library called “DiffMat” that essentially provides the various building blocks for constructing different textured materials. 
The team’s framework involves dozens of so-called “procedural graphs” made up of different nodes that all act like mini-Instagram filters: they take some input and transform it in a certain artistic way to produce an output. 
“A graph simply defines a way to combine hundreds of such filters to achieve a very complex visual effect, like a particular texture,” says Shi. “The neural network selects the most appropriate combinations of filter nodes until it perceptually matches the appearance of the user’s input image.”
Moving forward, Shi says that the team would like to go beyond inputting just a single flat sample, and to instead be able to capture materials from images of curved objects, or with multiple materials in the image.
They also hope to expand the pipeline to handle more complex materials that have different properties depending on how they are pointed. (For example, with a piece of wood you can see lines going in one direction ""with the grain""; wood is stronger with the grain compared to ""against the grain"").
Shi co-wrote the paper with MIT Professor Wojciech Matusik, alongside MIT graduate student research scientist Beichen Li and Adobe researchers Miloš Hašan, Kalyan Sunkavali, Radomír Měch, and Tamy Boubekeur. The paper will be presented virtually this month at the SIGGRAPH Asia computer graphics conference.
The work is supported, in part, by the National Science Foundation.


",Automating material-matching for movies and video games,2020-12-10,['Adam Conner-Simons'],Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Computer science and technology/Computer modeling/Research/School of Engineering/MIT Schwarzman College of Computing/Imaging/Simulation/National Science Foundation (NSF)/Film and Television/Video games/Arts/Technology and society/Industry,"['games', 'movies', 'automating', 'materials', 'shi', 'programs', 'researchers', 'different', 'texture', 'match', 'wood', 'video', 'materialmatching', 'scene', 'team']","However, these so-called “procedural” programs can take months to learn, and still involve painstaking hours or even days to create a particular texture.
The glossy pictures we see on our screens often appear seamlessly rendered, but they require huge amounts of work behind the scenes.
Very few of us who play video games or watch computer-generated image-filled movies ever take the time to sit back and appreciate all the handiwork that make their graphics so thrilling and immersive.
“A graph simply defines a way to combine hundreds of such filters to achieve a very complex visual effect, like a particular texture,” says Shi.
They also hope to expand the pipeline to handle more complex materials that have different properties depending on how they are pointed.",Mit
30,https://news.mit.edu/2020/two-mit-seniors-named-2021-marshall-scholars-1207,"


Katherine “Katie” Collins and Marla Evelyn Odell have been awarded Marshall Scholarships and will begin graduate studies in the United Kingdom. next fall. The MIT seniors were selected through a rigorous national process that evaluates applicants on the basis of academic merit, leadership, and ambassadorial potential.
Funded by the British government, the Marshall Scholarship provides exceptional American students with the opportunity to pursue two years of advanced study in any field at any university in the U.K. Up to 50 Marshall Scholars are selected each year from a pool of over 1,000 applicants.
MIT’s endorsed Marshall candidates were advised and supported by the distinguished fellowships team, led by Assistant Dean Kim Benard in Career Advising and Professional Development. They were also mentored by the MIT Presidential Committee on Distinguished Fellowships, co-chaired by professors Will Broadhead and Tamar Schapiro.
“We are very proud of all the MIT students who applied for the Marshall Scholarship this year. The combination of intellectual excellence, civic-mindedness, and good humor they embody is a reminder of how lucky we are to work with such extraordinary students,” Broadhead says. “Katie and Marla have been a source of inspiration for all of us on the committee and could not be more richly deserving of their places among this year’s cohort of scholars. We offer them our warmest congratulations!”
Katherine “Katie” Collins
Hailing from Newton, Massachusetts, Collins is majoring in brain and cognitive sciences (BCS) with minors in computer science and biomedical engineering. For her first year as a Marshall Scholar, she will complete an MPhil in machine learning and machine intelligence at Cambridge University, followed by an MS in mind, language, and embedded cognition at the University of Edinburgh in her second year. Collins aspires to a future career as a professor, leading a lab at the intersection of artificial intelligence and cognitive science. She plans to continue spearheading efforts to bring more women and young girls into computer science. 
Collins was named a Goldwater Scholar and a Johnson and Johnson Research Scholar, and has won awards for academic achievement from the BCS department. She has been working on several projects at the cusp of cognitive sciences, computation, and biomedicine. With Professor Josh Tenenbaum at the MIT Computational Cognitive Science group, Collins is building computational models to reverse-engineer how humans make inferences about their environment. She has also co-developed deep learning frameworks for the design of medical diagnostics with Professor Tim Lu of the MIT Synthetic Biology Center and in her internships at the Wyss Institute for Biologically Inspired Design and the Broad Institute of MIT and Harvard. 
Collins has competed nationally as a member of the MIT varsity women’s cross country and track and field teams, earning NCAA All-American and Academic All-American recognition. She is the founder and co-president of MITxHarvard Women in Artificial Intelligence and a research editor at the MIT Undergraduate Research Journal. She also serves on the Dean’s Action Group on Social and Ethical Computing, and the MIT Undergraduate Innovation Committee.
Marla Evelyn Odell 
Odell, from Edmonds, Washington, is majoring in computer science, economics, and data science, and minoring in mathematics and public policy. For her Marshall degree programs, she will pursue a MASt in mathematical statistics at Cambridge University and then an MS in the social science of the internet at Oxford University. Following her Marshall fellowship, she intends to undertake a career advocating for policies that encourage the development of more inclusive and transparent technologies. 
As a research intern at the Brookings Institution, Odell shaped recommendations for federal privacy legislation. In the MIT Computer Science and Artificial Intelligence Laboratory, Odell researched explainable algorithms that can increase the accountability of black-box AI systems. With a passion for bridging policy and engineering, she helped organize the MIT Artificial Intelligence Ethics Reading Group and establish the MIT Science Policy Review.
Throughout her time at MIT, Odell has been a champion of inclusion. As a first-year student, Odell founded MIT Women in Electrical Engineering and Computer Science to provide professional, social, and mentorship opportunities for undergraduate women. Later, she helped write the Department of Electrical Engineering and Computer Science (EECS)’s statement on diversity and inclusion. She currently serves as the inaugural undergraduate representative on the EECS Committee for Diversity, Equity, and Inclusion.
As a lifelong student-athlete, Odell rowed for MIT's Division I crew team and served as a managing director for Amphibious Achievement, a dual rowing-academic mentorship program for high school students. Previously, she was president of MIT's computer science honor society, a Gordon-MIT Engineering Leader, and an Addir Interfaith Fellow.


",Two MIT seniors named 2021 Marshall Scholars,2020-12-07,['Julia Mongo'],"School of Engineering/School of Science/Brain and cognitive sciences/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Broad Institute/Awards, honors and fellowships/Undergraduate","['named', 'odell', 'seniors', 'women', 'computer', '2021', 'scholars', 'engineering', 'marshall', 'undergraduate', 'university', 'intelligence', 'science', 'mit']","She also serves on the Dean’s Action Group on Social and Ethical Computing, and the MIT Undergraduate Innovation Committee.
Marla Evelyn OdellOdell, from Edmonds, Washington, is majoring in computer science, economics, and data science, and minoring in mathematics and public policy.
In the MIT Computer Science and Artificial Intelligence Laboratory, Odell researched explainable algorithms that can increase the accountability of black-box AI systems.
With a passion for bridging policy and engineering, she helped organize the MIT Artificial Intelligence Ethics Reading Group and establish the MIT Science Policy Review.
As a first-year student, Odell founded MIT Women in Electrical Engineering and Computer Science to provide professional, social, and mentorship opportunities for undergraduate women.",Mit
31,https://news.mit.edu/2020/better-learning-shape-shifting-objects-1207,"


Have you ever seen a fancy ergonomic chair that seems to magically mold to a person’s body? Such products got researchers at MIT’s Computer Science and Artificial Intelligence Lab (CSAIL) thinking about other everyday objects that could be made to shape-shift to help their users — not only to get things done, but to actually improve their skills in particular areas. 
One idea they came up with: a basketball hoop that helps you train more effectively by shrinking and raising as you make shots more consistently.
The thinking is that a beginner could start by using the basket at a lower height and with a wider hoop diameter. As they proceed to make baskets more consistently, the hoop automatically shrinks and rises until it reaches regulation size. 







Play video




      

            An Adaptive Basketball Hoop for Training Motor Skills        

    



Led by MIT Professor Stefanie Mueller, the researchers say that these sorts of adaptive tools could help people who can’t afford coaches or personal trainers to learn different skills or train for sports. They hope that the idea might be particularly timely during the pandemic, with the sudden cancellation of so many in-person gym classes.
Mueller and colleagues have already started to develop several other prototype tools, including a bicycle with raisable training wheels, an armband that helps golfers keep their arms straight, and even adaptive life jackets and high-heeled shoes. 
With the basketball hoop, the CSAIL team tested it under two different conditions. In “manually adaptive” mode, the user is the one who changes the hoop’s height and width; in “auto-adaptive” mode, the hoop itself automatically adjusts so that the user is always learning at an ""optimal challenge point” where the task is neither too easy nor too hard. 
Experimental results showed that training on the auto-adaptive hoop led to better performance than with either the static hoop or the manually-adaptive mode — which lead author Dishita Turakhia says is an indication that people often over- or under-challenge themselves and “aren’t all that good at assessing their skill levels.” 
Users found that, compared to adjusting the hoop themselves, the auto-adaptive system was not just more effective, but more enjoyable and less distracting, since it removed them from having to constantly make decisions about whether to make the task more difficult.
“It’s interesting in that it’s objectively measuring performance,” says Fraser Anderson, a senior principal research scientist at Autodesk who was not involved in the study. “You don’t have to rely on your own sense of whether or not you’ve mastered a skill: the system can do that and take out the self-doubt, overconfidence, or guesswork.”
The system’s algorithm for determining shot accuracy is somewhat crude at the moment: It essentially gives the shooter one point if the ball goes through the net, and half a point if it hits the backboard. If the shooter’s average after at least four shots is 0.75 points or greater, the hoop will shrink and rise a set amount, and the whole process will then repeat. (Turakhia says that, with a greater number of sensors and cameras, the hoop could sense a wider range of skills and adapt accordingly.) 
The team plans to continue to work on adaptive tools for other use cases, including rehabilitation and workplace training. Anderson says he could even imagine an adaptive approach being used in medical schools to help surgeons improve their skills.
Turakhia and Mueller wrote the paper alongside master’s student Andrew Wong and former graduate students Yini Qi '17, MNG '18 and Lotta Blumberg '18, MNG '19. They will present the paper virtually in February at the Association for Computing Machinery’s Conference on Tangible, Embedded and Embodied Interaction (TEI). The project was supported, in part, by the MIT Integrated Learning Initiative.


",Better learning with shape-shifting objects,2020-12-07,['Adam Conner-Simons'],MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Research/Design/Manufacturing/Sports/Learning/Human-computer interaction,"['adaptive', 'point', 'mode', 'shapeshifting', 'learning', 'skills', 'tools', 'autoadaptive', 'better', 'wider', 'help', 'objects', 'training', 'hoop']","The thinking is that a beginner could start by using the basket at a lower height and with a wider hoop diameter.
As they proceed to make baskets more consistently, the hoop automatically shrinks and rises until it reaches regulation size.
(Turakhia says that, with a greater number of sensors and cameras, the hoop could sense a wider range of skills and adapt accordingly.)
The team plans to continue to work on adaptive tools for other use cases, including rehabilitation and workplace training.
Anderson says he could even imagine an adaptive approach being used in medical schools to help surgeons improve their skills.",Mit
32,https://news.mit.edu/2020/indigenous-knowledge-technology-mit-is-it-wise-1203,"


In November, 10 Indigenous media scholars and artists convened at MIT — virtually — for the inaugural Indigenous Digital Delegation. In a week-long series of gatherings, the delegation met with over 60 MIT scientists, staff, fellows, and students. The theme of the gathering was “Indigenous Knowledge, Artificial Intelligence, and Digital Worlds.”
“Wisdom is not a topic that is taught or studied in the curriculum in our schools or universities, nor is it a practice in modern life,” said Ojibwe elder, artist, and scholar Duke Redbird, in the delegation’s keynote public lecture, titled “Dish with One Spoon.” “Technology can put a man in space or a nano-computer in every creature on Earth. Yet technology cannot answer this question that should be asked of anything. And it is an Indigenous question: ‘Is it wise?’”







Play video




      

            Ojibwe elder, artist, and scholar Duke Redbird speaks at the inaugural Indigenous Digital Delegation.        

    



Delegates met with MIT scholars to discuss diverse domains, from the decolonization of space, to re-imagining Indigenous architecture, to the role of community-based governance in the genetic modification of invasive species.
“This has been an incredible opportunity for Indigenous scholars and creators to connect with folks working in our field of digital and new media, as our decolonial tools will allow for deep connections through practice and critical thinking, transforming the field and the MIT campus,” says Dr. Julie Nagam, associate professor of art history at the University of Winnipeg. Nagam co-leads the delegation with the Indigenous Screen Office's Kerry Swanson, who adds, “The knowledge with which we return to our home environments and institutions will greatly impact our work, moving forward into the future.”
In the opening session, Professor Wesley Harris, MIT representative for the American Indian Science and Engineering Society, welcomed the delegation by emphasizing the need to include an “A” in the acronym STEM, to put the arts in the teachings of science, technology, engineering, and math.
Throughout the week of workshops, lab visits, and pairings, delegates were matched with relevant labs and researchers across MIT to brainstorm their current works, including art gallery and site-specific installations, a Sundance-backed documentary film, research projects such as Indigenous Protocol-based Artificial Intelligence, and Indigenous-led emergent media laboratories and education programs.
“We're headed toward a world where people are being partially programmed by algorithms,” said Redbird. “In the past, Indigenous people were programmed by our symbolic symbiotic relationship with the Earth. Today, a lot of what the average person will do will be designed and motivated by algorithms, rather than by nature.”
In one workshop, a team explored building an artificial intelligence system based in Indigenous-protocols. Delegates Jason Lewis and Scott Benesiianaabandan met with MIT scholar James Glass, of the Spoken Languages Systems Group, whose latest research interest involves supporting low-resourced languages. The delegate team shared aspects of Anishinaabe worldview, knowledge generation, and dissemination protocol to ask if “values might be articulated in a manner that retains their cultural integrity” rather than having algorithms rely on translation from a high-resourced language.
“The energy and enthusiasm across MIT for this gathering has been unparalleled,” said Kat Cizek, delegation event designer and artistic director at Co-Creation Studio at MIT Open Documentary Lab. “There’s huge interest in learning from Indigenous scholars and artists and together transforming understandings and practices of science, arts, and technologies.”
Other MIT labs participating in meetings included Space Enabled Research Group, CoLab, Game Lab, Opera of the Future, Fluid Interfaces, Sculpting Evolution Lab, and the CAST Visiting Artists Program, among others.
Redbird emphasized the importance of these exchanges. “It is imperative that the children of the 21st century have access to a worldview that celebrates the idea of a compassionate communion with all living things,” he said. “We want the generation of the future to apply the traditional values and wisdom of our ancestors and uphold the sacred covenant to family community in nature. It is incumbent upon this speed-of-light generation, born after 1995, to explore an Indigenous worldview and use technologies to change negative patterns and rethink the manner in which we engage the environment.”
The delegation included Elder Duke Redbird; co-leads Kerry Swanson from Indigenous Screen Office and Julie Nagam of The Space Between Us at the University of Winnipeg; scholars Heather Igloliorte and Jason Lewis from Concordia University; Jackson 2bears from the University of Lethbridge; L. Sarah Todd from the IM4 Lab at Emily Carr University; and artists Scott Bensiianaabandan, Lisa Jackson, Caroline Monnet, and Nyla Innuksuk.
This first Indigenous Delegation of its kind at MIT was originally scheduled as an in-person event in spring 2020. The delegates will continue in relationship with MIT as ISO-MIT Co-Creation Fellows at Open Documentary Lab for the next year. A second, more international version is being planned for 2021-22, on campus.


",Indigenous knowledge and technology at MIT: “Is it wise?”,2020-12-03,[],"Comparative Media Studies/Writing/MIT Center for Art, Science & Technology (CAST)/Game Lab/Diversity and inclusion/Arts/Art, Culture and Technology/Artificial intelligence/Special events and guest speakers/Media/School of Architecture and Planning/School of Humanities Arts and Social Sciences/Technology and society","['space', 'technology', 'indigenous', 'knowledge', 'delegation', 'scholars', 'wise', 'artists', 'lab', 'university', 'worldview', 'mit']","And it is an Indigenous question: ‘Is it wise?’”In November, 10 Indigenous media scholars and artists convened at MIT — virtually — for the inaugural Indigenous Digital Delegation.
In a week-long series of gatherings, the delegation met with over 60 MIT scientists, staff, fellows, and students.
The theme of the gathering was “Indigenous Knowledge, Artificial Intelligence, and Digital Worlds.”Delegates met with MIT scholars to discuss diverse domains, from the decolonization of space, to re-imagining Indigenous architecture, to the role of community-based governance in the genetic modification of invasive species.
This first Indigenous Delegation of its kind at MIT was originally scheduled as an in-person event in spring 2020.
The delegates will continue in relationship with MIT as ISO-MIT Co-Creation Fellows at Open Documentary Lab for the next year.",Mit
33,https://news.mit.edu/2020/object-recognition-v1-1203,"


Computer vision models known as convolutional neural networks can be trained to recognize objects nearly as accurately as humans do. However, these models have one significant flaw: Very small changes to an image, which would be nearly imperceptible to a human viewer, can trick them into making egregious errors such as classifying a cat as a tree.

A team of neuroscientists from MIT, Harvard University, and IBM have developed a way to alleviate this vulnerability, by adding to these models a new layer that is designed to mimic the earliest stage of the brain’s visual processing system. In a new study, they showed that this layer greatly improved the models’ robustness against this type of mistake.

“Just by making the models more similar to the brain’s primary visual cortex, in this single stage of processing, we see quite significant improvements in robustness across many different types of perturbations and corruptions,” says Tiago Marques, an MIT postdoc and one of the lead authors of the study.

Convolutional neural networks are often used in artificial intelligence applications such as self-driving cars, automated assembly lines, and medical diagnostics. Harvard graduate student Joel Dapello, who is also a lead author of the study, adds that “implementing our new approach could potentially make these systems less prone to error and more aligned with human vision.”

“Good scientific hypotheses of how the brain’s visual system works should, by definition, match the brain in both its internal neural patterns and its remarkable robustness. This study shows that achieving those scientific gains directly leads to engineering and application gains,” says James DiCarlo, the head of MIT’s Department of Brain and Cognitive Sciences, an investigator in the Center for Brains, Minds, and Machines and the McGovern Institute for Brain Research, and the senior author of the study.

The study, which is being presented at the NeurIPS conference this month, is also co-authored by MIT graduate student Martin Schrimpf, MIT visiting student Franziska Geiger, and MIT-IBM Watson AI Lab Co-director David Cox.











Mimicking the brain

Recognizing objects is one of the visual system’s primary functions. In just a small fraction of a second, visual information flows through the ventral visual stream to the brain’s inferior temporal cortex, where neurons contain information needed to classify objects. At each stage in the ventral stream, the brain performs different types of processing. The very first stage in the ventral stream, V1, is one of the most well-characterized parts of the brain and contains neurons that respond to simple visual features such as edges.

“It’s thought that V1 detects local edges or contours of objects, and textures, and does some type of segmentation of the images at a very small scale. Then that information is later used to identify the shape and texture of objects downstream,” Marques says. “The visual system is built in this hierarchical way, where in early stages neurons respond to local features such as small, elongated edges.”

For many years, researchers have been trying to build computer models that can identify objects as well as the human visual system. Today’s leading computer vision systems are already loosely guided by our current knowledge of the brain’s visual processing. However, neuroscientists still don’t know enough about how the entire ventral visual stream is connected to build a model that precisely mimics it, so they borrow techniques from the field of machine learning to train convolutional neural networks on a specific set of tasks. Using this process, a model can learn to identify objects after being trained on millions of images.

Many of these convolutional networks perform very well, but in most cases, researchers don’t know exactly how the network is solving the object-recognition task. In 2013, researchers from DiCarlo’s lab showed that some of these neural networks could not only accurately identify objects, but they could also predict how neurons in the primate brain would respond to the same objects much better than existing alternative models. However, these neural networks are still not able to perfectly predict responses along the ventral visual stream, particularly at the earliest stages of object recognition, such as V1.

These models are also vulnerable to so-called “adversarial attacks.” This means that small changes to an image, such as changing the colors of a few pixels, can lead the model to completely confuse an object for something different — a type of mistake that a human viewer would not make.

As a first step in their study, the researchers analyzed the performance of 30 of these models and found that models whose internal responses better matched the brain’s V1 responses were also less vulnerable to adversarial attacks. That is, having a more brain-like V1 seemed to make the model more robust. To further test and take advantage of that idea, the researchers decided to create their own model of V1, based on existing neuroscientific models, and place it at the front of convolutional neural networks that had already been developed to perform object recognition.

When the researchers added their V1 layer, which is also implemented as a convolutional neural network, to three of these models, they found that these models became about four times more resistant to making mistakes on images perturbed by adversarial attacks. The models were also less vulnerable to misidentifying objects that were blurred or distorted due to other corruptions.

“Adversarial attacks are a big, open problem for the practical deployment of deep neural networks. The fact that adding neuroscience-inspired elements can improve robustness substantially suggests that there is still a lot that AI can learn from neuroscience, and vice versa,” Cox says.

Better defense

Currently, the best defense against adversarial attacks is a computationally expensive process of training models to recognize the altered images. One advantage of the new V1-based model is that it doesn’t require any additional training. It is also better able to handle a wide range of distortions, beyond adversarial attacks.

The researchers are now trying to identify the key features of their V1 model that allows it to do a better job resisting adversarial attacks, which could help them to make future models even more robust. It could also help them learn more about how the human brain is able to recognize objects.

“One big advantage of the model is that we can map components of the model to particular neuronal populations in the brain,” Dapello says. “We can use this as a tool for novel neuroscientific discoveries, and also continue developing this model to improve its performance under this challenging task.”

The research was funded by the PhRMA Foundation Postdoctoral Fellowship in Informatics, the Semiconductor Research Corporation, DARPA, the MIT Shoemaker Fellowship, the U.S. Office of Naval Research, the Simons Foundation, and the MIT-IBM Watson AI Lab.


",Neuroscientists find a way to make object-recognition models perform better,2020-12-03,['Anne Trafton'],McGovern Institute/Brain and cognitive sciences/Research/Quest for Intelligence/MIT-IBM Watson AI Lab/Center for Brains Minds and Machines/School of Science/MIT Schwarzman College of Computing/Artificial intelligence/Machine learning/Defense Advanced Research Projects Agency (DARPA),"['neuroscientists', 'visual', 'model', 'objectrecognition', 'v1', 'researchers', 'objects', 'models', 'brains', 'perform', 'way', 'networks', 'better', 'brain', 'neural']","Convolutional neural networks are often used in artificial intelligence applications such as self-driving cars, automated assembly lines, and medical diagnostics.
A team of neuroscientists from MIT, Harvard University, and IBM have developed a way to alleviate this vulnerability, by adding to these models a new layer that is designed to mimic the earliest stage of the brain’s visual processing system.
Computer vision models known as convolutional neural networks can be trained to recognize objects nearly as accurately as humans do.
Today’s leading computer vision systems are already loosely guided by our current knowledge of the brain’s visual processing.
“Adversarial attacks are a big, open problem for the practical deployment of deep neural networks.",Mit
34,https://news.mit.edu/2020/3d-printer-gloss-1202,"


Shape, color, and gloss.

Those are an object’s three most salient visual features. Currently, 3D printers can reproduce shape and color reasonably well. Gloss, however, remains a challenge. That’s because 3D printing hardware isn’t designed to deal with the different viscosities of the varnishes that lend surfaces a glossy or matte look.

MIT researcher Michael Foshey and his colleagues may have a solution. They’ve developed a combined hardware and software printing system that uses off-the-shelf varnishes to finish objects with realistic, spatially varying gloss patterns. Foshey calls the advance “a chapter in the book of how to do high-fidelity appearance reproduction using a 3D printer.”

He envisions a range of applications for the technology. It might be used to faithfully reproduce fine art, allowing near-flawless replicas to be distributed to museums without access to originals. It might also help create more realistic-looking prosthetics. Foshey hopes the advance represents a step toward visually perfect 3D printing, “where you could almost not tell the difference between the object and the reproduction.”

Foshey, a mechanical engineer in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), will present the paper at next month’s SIGGRAPH Asia conference, along with lead author Michal Piovarči of the University of Lugano in Switzerland. Co-authors include MIT’s Wojciech Matusik, Vahid Babaei of the Max Planck Institute, Szymon Rusinkiewicz of Princeton University, and Piotr Didyk of the University of Lugano.

Glossiness is simply a measure of how much light is reflected from a surface. A high gloss surface is reflective, like a mirror. A low gloss, or matte, surface is unreflective, like concrete. Varnishes that lend a glossy finish tend to be less viscous and to dry into a smooth surface. Varnishes that lend a matte finish are more viscous — closer to honey than water. They contain large polymers that, when dried, protrude randomly from the surface and absorb light. “You have a bunch of these particles popping out of the surface, which gives you that roughness,” says Foshey. 

But those polymers pose a dilemma for 3D printers, whose skinny fluid channels and nozzles aren’t built for honey. “They’re very small, and they can get clogged easily,” says Foshey.

The state-of-the-art way to reproduce a surface with spatially varying gloss is labor-intensive: The object is initially printed with high gloss and with support structures covering the spots where a matte finish is ultimately desired. Then the support material is removed to lend roughness to the final surface. “There’s no way of instructing the printer to produce a matte finish in one area, or a glossy finish in another,” says Foshey. So, his team devised one.

They designed a printer with large nozzles and the ability to deposit varnish droplets of varying sizes. The varnish is stored in the printer’s pressurized reservoir, and a needle valve opens and closes to release varnish droplets onto the printing surface. A variety of droplet sizes is achieved by controlling factors like the reservoir pressure and the speed of the needle valve’s movements. The more varnish released, the larger the droplet deposited. The same goes for the speed of the droplet’s release. “The faster it goes, the more it spreads out once it impacts the surface,” says Foshey. “So we essentially vary all these parameters to get the droplet size we want.”

The printer achieves spatially varying gloss through halftoning. In this technique, discrete varnish droplets are arranged in patterns that, when viewed from a distance, appear like a continuous surface. “Our eyes actually do the mixing itself,” says Foshey. The printer uses just three off-the-shelf varnishes — one glossy, one matte, and one in between. By incorporating these varnishes into its preprogrammed halftoning pattern, the printer can yield continuous, spatially varying shades of glossiness across the printing surface.

Along with the hardware, Foshey’s team produced a software pipeline to control the printer’s output. First, the user indicates their desired gloss pattern on the surface to be printed. Next, the printer runs a calibration, trying various halftoning patterns of the three supplied varnishes. Based on the reflectance of those calibration patterns, the printer determines the proper halftoning pattern to use on the final print job to achieve the best possible reproduction. The researchers demonstrated their results on a variety of “2.5D” objects — mostly-flat printouts with textures that varied by half a centimeter in height. “They were impressive,” says Foshey. “They definitely have more of a feel of what you’re actually trying to reproduce.”

The team plans to continue developing the hardware for use on fully-3D objects. Didyk says “the system is designed in such a way that the future integration with commercial 3D printers is possible.”

This work was supported by the National Science Foundation and the European Research council.


",This 3D printer doesn’t gloss over the details,2020-12-02,['Daniel Ackerman'],3-D printing/automation/Computer science and technology/Manufacturing/MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/National Science Foundation (NSF),"['printer', 'details', 'finish', 'varying', 'gloss', 'varnishes', 'varnish', 'surface', '3d', 'foshey', 'matte', 'doesnt']","They’ve developed a combined hardware and software printing system that uses off-the-shelf varnishes to finish objects with realistic, spatially varying gloss patterns.
A high gloss surface is reflective, like a mirror.
A low gloss, or matte, surface is unreflective, like concrete.
“There’s no way of instructing the printer to produce a matte finish in one area, or a glossy finish in another,” says Foshey.
“So we essentially vary all these parameters to get the droplet size we want.”The printer achieves spatially varying gloss through halftoning.",Mit
35,https://news.mit.edu/2020/neural-model-language-1201,"


You don’t need a sledgehammer to crack a nut.
Jonathan Frankle is researching artificial intelligence — not noshing pistachios — but the same philosophy applies to his “lottery ticket hypothesis.” It posits that, hidden within massive neural networks, leaner subnetworks can complete the same task more efficiently. The trick is finding those “lucky” subnetworks, dubbed winning lottery tickets.
In a new paper, Frankle and colleagues discovered such subnetworks lurking within BERT, a state-of-the-art neural network approach to natural language processing (NLP). As a branch of artificial intelligence, NLP aims to decipher and analyze human language, with applications like predictive text generation or online chatbots. In computational terms, BERT is bulky, typically demanding supercomputing power unavailable to most users. Access to BERT’s winning lottery ticket could level the playing field, potentially allowing more users to develop effective NLP tools on a smartphone — no sledgehammer needed.
“We’re hitting the point where we’re going to have to make these models leaner and more efficient,” says Frankle, adding that this advance could one day “reduce barriers to entry” for NLP.
Frankle, a PhD student in Michael Carbin’s group at the MIT Computer Science and Artificial Intelligence Laboratory, co-authored the study, which will be presented next month at the Conference on Neural Information Processing Systems. Tianlong Chen of the University of Texas at Austin is the lead author of the paper, which included collaborators Zhangyang Wang, also of UT Austin, as well as Shiyu Chang, Sijia Liu, and Yang Zhang, all of the MIT-IBM Watson AI Lab.
You’ve probably interacted with a BERT network today. It’s one of the technologies that underlies Google’s search engine, and it has sparked excitement among researchers since Google released BERT in 2018. BERT is a method of creating neural networks — algorithms that use layered nodes, or “neurons,” to learn to perform a task through training on numerous examples. BERT is trained by repeatedly attempting to fill in words left out of a passage of writing, and its power lies in the gargantuan size of this initial training dataset. Users can then fine-tune BERT’s neural network to a particular task, like building a customer-service chatbot. But wrangling BERT takes a ton of processing power.
“A standard BERT model these days — the garden variety — has 340 million parameters,” says Frankle, adding that the number can reach 1 billion. Fine-tuning such a massive network can require a supercomputer. “This is just obscenely expensive. This is way beyond the computing capability of you or me.”
Chen agrees. Despite BERT’s burst in popularity, such models “suffer from enormous network size,” he says. Luckily, “the lottery ticket hypothesis seems to be a solution.”
To cut computing costs, Chen and colleagues sought to pinpoint a smaller model concealed within BERT. They experimented by iteratively pruning parameters from the full BERT network, then comparing the new subnetwork’s performance to that of the original BERT model. They ran this comparison for a range of NLP tasks, from answering questions to filling the blank word in a sentence.
The researchers found successful subnetworks that were 40 to 90 percent slimmer than the initial BERT model, depending on the task. Plus, they were able to identify those winning lottery tickets before running any task-specific fine-tuning — a finding that could further minimize computing costs for NLP. In some cases, a subnetwork picked for one task could be repurposed for another, though Frankle notes this transferability wasn’t universal. Still, Frankle is more than happy with the group’s results.
“I was kind of shocked this even worked,” he says. “It’s not something that I took for granted. I was expecting a much messier result than we got.”
This discovery of a winning ticket in a BERT model is “convincing,” according to Ari Morcos, a scientist at Facebook AI Research. “These models are becoming increasingly widespread,” says Morcos. “So it’s important to understand whether the lottery ticket hypothesis holds.” He adds that the finding could allow BERT-like models to run using far less computing power, “which could be very impactful given that these extremely large models are currently very costly to run.”
Frankle agrees. He hopes this work can make BERT more accessible, because it bucks the trend of ever-growing NLP models. “I don’t know how much bigger we can go using these supercomputer-style computations,” he says. “We’re going to have to reduce the barrier to entry.” Identifying a lean, lottery-winning subnetwork does just that — allowing developers who lack the computing muscle of Google or Facebook to still perform cutting-edge NLP. “The hope is that this will lower the cost, that this will make it more accessible to everyone … to the little guys who just have a laptop,” says Frankle. “To me that’s really exciting.”
This research was funded, in part, by the MIT-IBM Watson AI Lab.


",Shrinking massive neural networks used to model language,2020-12-01,['Daniel Ackerman'],Artificial intelligence/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Algorithms/Language/Electrical Engineering & Computer Science (eecs)/MIT Schwarzman College of Computing/School of Engineering/Machine learning,"['massive', 'bert', 'model', 'language', 'subnetworks', 'models', 'lottery', 'frankle', 'nlp', 'network', 'networks', 'task', 'ticket', 'shrinking', 'used', 'neural']","In a new paper, Frankle and colleagues discovered such subnetworks lurking within BERT, a state-of-the-art neural network approach to natural language processing (NLP).
You’ve probably interacted with a BERT network today.
Users can then fine-tune BERT’s neural network to a particular task, like building a customer-service chatbot.
Luckily, “the lottery ticket hypothesis seems to be a solution.”To cut computing costs, Chen and colleagues sought to pinpoint a smaller model concealed within BERT.
They experimented by iteratively pruning parameters from the full BERT network, then comparing the new subnetwork’s performance to that of the original BERT model.",Mit
36,https://news.mit.edu/2020/3-questions-joshua-cohen-qualities-good-jobs-1130,"


Joshua Cohen, a member of the MIT Task Force on the Work of the Future Research Advisory Board and a distinguished senior fellow in law, philosophy, and political science at the University of California at Berkeley recently wrote a research brief, “Good Jobs,” that describes the qualities inherent in “good jobs” and how they fit into a larger world of education, training, consumption, finance, firm organization, and worker representation.

Here, Cohen describes some of the main takeaways from his brief, including what exactly a ""good job"" is and how private-public policies and leadership can foster the creation of good jobs.

Q: Your brief refers to “good jobs.” Can you define what that means?
A: Because the nature of work is changing so much — and because it is so important in so many of our lives — we have to embrace a broad and ambitious understanding of “good jobs.” On this broad understanding, good jobs have familiar features that we look for in work: decent compensation, stability of hours, health and safety protections, and opportunities for acquiring new skills and responsibilities. All of those conditions are incredibly important, but they are not ambitious enough. Good jobs also ensure collective worker voice, so that people can protect themselves and not be subject to arbitrary authority. They serve a worthy purpose, so that people feel that their time is well-spent and are more motivated to attentive performance. Moreover, good jobs have a kind of complexity that makes work more engaging and enjoyable — thus more powerfully motivating. Goods jobs allow for greater chances for experiences of freshness, enable people to find their own distinctive ways of doing the job, create greater possibilities for creativity, and provide possibilities for a sense of evolving mastery in the face of a challenging task. In his 1973 book “Working,” Studs Terkel interviewed a woman named Nora Watson who complained about jobs that “are not big enough for people.” A good job, in brief, is a job that is “big enough for people.”

Q: What private-public policies can we recommend that will foster the creation of good jobs in the United States — especially related to how we shape technologies?
A: Good jobs are good for the people with the jobs. Firms are not always motivated to create good jobs, because the benefits flow to workers and to the larger society. To increase those motives, we need a favorable public policy environment, including minimum wage laws, which diminish the attractions of “low-road” strategies that do not provide good jobs; social protections (including income security) that reduce incentives for workers to accept bad jobs; active labor market policies that ensure appropriate skills, both cognitive and emotional, including the essential ability of learning how to learn and to collaborate; support (including tax incentives) for new firms using new, more productive technologies; and a research and development environment (including research grants and strong public R&D labs) that assists technological innovation by incumbent firms. Along with the policy environment, firms have an essential role to play. Zeynep Ton, in operations management at MIT Sloan, argues that a firm-level “good jobs strategy” requires a series of coordinated and mutually reinforcing strategic decisions: both investing in employees (good hiring, decent compensation, clear performance standards, and well-defined career paths) and developing operational strengths, including focus, cross-training, and a willingness to operate with sufficient slack to adopt to changing circumstances.

Q: Your brief makes the point that new technology might enhance the capability of providing good jobs. Can you share how private and public sector leadership could effect change?
A: Private and public sector leadership can focus on developing the kinds of policies I just described. More than that, we need them to think in a different way about technology, and about work. First, they should reject the technological determinist idea that the course of technological change is fixed, and that job design must simply adapt to it. In the case of artificial intelligence, for example, this means focusing very deliberately on the design and development of “humanistic AI,” which is understood as an augmentation or amplification of human capabilities, not as a substitute for them. Second, they should think about jobs in the way that Nora Watson did: as big enough for people. Jobs are an important part of human life; good jobs are jobs that contribute to living well. They should think about what they love about their own jobs, and aim to create jobs that other people can love just as much.


",3 Questions: Joshua Cohen on the qualities of good jobs,2020-11-30,[],Jobs/Policy/Business and management/3 Questions/Technology and society/Industry,"['good', 'qualities', 'job', 'research', 'public', 'firms', 'questions', 'cohen', 'including', 'work', 'brief', 'joshua', 'policies', 'jobs']","Here, Cohen describes some of the main takeaways from his brief, including what exactly a ""good job"" is and how private-public policies and leadership can foster the creation of good jobs.
Good jobs also ensure collective worker voice, so that people can protect themselves and not be subject to arbitrary authority.
A: Good jobs are good for the people with the jobs.
Firms are not always motivated to create good jobs, because the benefits flow to workers and to the larger society.
Jobs are an important part of human life; good jobs are jobs that contribute to living well.",Mit
37,https://news.mit.edu/2020/computer-aided-robot-design-1130,"


So, you need a robot that climbs stairs. What shape should that robot be? Should it have two legs, like a person? Or six, like an ant?
Choosing the right shape will be vital for your robot’s ability to traverse a particular terrain. And it’s impossible to build and test every potential form. But now an MIT-developed system makes it possible to simulate them and determine which design works best.
You start by telling the system, called RoboGrammar, which robot parts are lying around your shop — wheels, joints, etc. You also tell it what terrain your robot will need to navigate. And RoboGrammar does the rest, generating an optimized structure and control program for your robot.
The advance could inject a dose of computer-aided creativity into the field. “Robot design is still a very manual process,” says Allan Zhao, the paper’s lead author and a PhD student in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). He describes RoboGrammar as “a way to come up with new, more inventive robot designs that could potentially be more effective.”
Zhao is the lead author of the paper, which he will present at this month’s SIGGRAPH Asia conference. Co-authors include PhD student Jie Xu, postdoc Mina Konaković-Luković, postdoc Josephine Hughes, PhD student Andrew Spielberg, and professors Daniela Rus and Wojciech Matusik, all of MIT.
Ground rules
Robots are built for a near-endless variety of tasks, yet “they all tend to be very similar in their overall shape and design,” says Zhao. For example, “when you think of building a robot that needs to cross various terrains, you immediately jump to a quadruped,” he adds, referring to a four-legged animal like a dog. “We were wondering if that’s really the optimal design.”
Zhao’s team speculated that more innovative design could improve functionality. So they built a computer model for the task — a system that wasn’t unduly influenced by prior convention. And while inventiveness was the goal, Zhao did have to set some ground rules.
The universe of possible robot forms is “primarily composed of nonsensical designs,” Zhao writes in the paper. “If you can just connect the parts in arbitrary ways, you end up with a jumble,” he says. To avoid that, his team developed a “graph grammar” — a set of constraints on the arrangement of a robot’s components. For example, adjoining leg segments should be connected with a joint, not with another leg segment. Such rules ensure each computer-generated design works, at least at a rudimentary level.
Zhao says the rules of his graph grammar were inspired not by other robots but by animals — arthropods in particular. These invertebrates include insects, spiders, and lobsters. As a group, arthropods are an evolutionary success story, accounting for more than 80 percent of known animal species. “They’re characterized by having a central body with a variable number of segments. Some segments may have legs attached,” says Zhao. “And we noticed that that’s enough to describe not only arthropods but more familiar forms as well,” including quadrupeds. Zhao adopted the arthropod-inspired rules thanks in part to this flexibility, though he did add some mechanical flourishes. For example, he allowed the computer to conjure wheels instead of legs.







Play video






A phalanx of robots
Using Zhao’s graph grammar, RoboGrammar operates in three sequential steps: defining the problem, drawing up possible robotic solutions, then selecting the optimal ones. Problem definition largely falls to the human user, who inputs the set of available robotic components, like motors, legs, and connecting segments. “That’s key to making sure the final robots can actually be built in the real world,” says Zhao. The user also specifies the variety of terrain to be traversed, which can include combinations of elements like steps, flat areas, or slippery surfaces.
With these inputs, RoboGrammar then uses the rules of the graph grammar to design hundreds of thousands of potential robot structures. Some look vaguely like a racecar. Others look like a spider, or a person doing a push-up. “It was pretty inspiring for us to see the variety of designs,” says Zhao. “It definitely shows the expressiveness of the grammar.” But while the grammar can crank out quantity, its designs aren’t always of optimal quality.
Choosing the best robot design requires controlling each robot’s movements and evaluating its function. “Up until now, these robots are just structures,” says Zhao. The controller is the set of instructions that brings those structures to life, governing the movement sequence of the robot’s various motors. The team developed a controller for each robot with an algorithm called Model Predictive Control, which prioritizes rapid forward movement.
“The shape and the controller of the robot are deeply intertwined,” says Zhao, “which is why we have to optimize a controller for every given robot individually.” Once each simulated robot is free to move about, the researchers seek high-performing robots with a “graph heuristic search.” This neural network algorithm iteratively samples and evaluates sets of robots, and it learns which designs tend to work better for a given task. “The heuristic function improves over time,” says Zhao, “and the search converges to the optimal robot.”
This all happens before the human designer ever picks up a screw.
“This work is a crowning achievement in the a 25-year quest to automatically design the morphology and control of robots,” says Hod Lipson, a mechanical engineer and computer scientist at Columbia University, who was not involved in the project. “The idea of using shape-grammars has been around for a while, but nowhere has this idea been executed as beautifully as in this work. Once we can get machines to design, make and program robots automatically, all bets are off.”
Zhao intends the system as a spark for human creativity. He describes RoboGrammar as a “tool for robot designers to expand the space of robot structures they draw upon.” To show its feasibility, his team plans to build and test some of RoboGrammar’s optimal robots in the real world. Zhao adds that the system could be adapted to pursue robotic goals beyond terrain traversing. And he says RoboGrammar could help populate virtual worlds. “Let’s say in a video game you wanted to generate lots of kinds of robots, without an artist having to create each one,” says Zhao. “RoboGrammar would work for that almost immediately.”
One surprising outcome of the project? “Most designs did end up being four-legged in the end,” says Zhao. Perhaps manual robot designers were right to gravitate toward quadrupeds all along. “Maybe there really is something to it.”


",Computer-aided creativity in robot design,2020-11-30,['Daniel Ackerman'],Robotics/Design/Algorithms/Robots/Bioinspiration/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/MIT Schwarzman College of Computing/School of  Engineering,"['shape', 'robogrammar', 'student', 'creativity', 'segments', 'robots', 'design', 'computeraided', 'zhao', 'system', 'robot', 'rules']","But now an MIT-developed system makes it possible to simulate them and determine which design works best.
You start by telling the system, called RoboGrammar, which robot parts are lying around your shop — wheels, joints, etc.
The advance could inject a dose of computer-aided creativity into the field.
“Robot design is still a very manual process,” says Allan Zhao, the paper’s lead author and a PhD student in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL).
Ground rulesRobots are built for a near-endless variety of tasks, yet “they all tend to be very similar in their overall shape and design,” says Zhao.",Mit
38,https://news.mit.edu/2020/how-humans-use-objects-novel-ways-ssup-mit-1124,"


Human beings are naturally creative tool users. When we need to drive in a nail but don’t have a hammer, we easily realize that we can use a heavy, flat object like a rock in its place. When our table is shaky, we quickly find that we can put a stack of paper under the table leg to stabilize it. But while these actions seem so natural to us, they are believed to be a hallmark of great intelligence — only a few other species use objects in novel ways to solve their problems, and none can do so as flexibly as people. What provides us with these powerful capabilities for using objects in this way?
In a new paper published in the Proceedings of the National Academy of Sciences describing work conducted at MIT’s Center for Brains, Minds and Machines, researchers Kelsey Allen, Kevin Smith, and Joshua Tenenbaum study the cognitive components that underlie this sort of improvised tool use. They designed a novel task, the Virtual Tools game, that taps into tool-use abilities: People must select one object from a set of “tools” that they can place in a two-dimensional, computerized scene to accomplish a goal, such as getting a ball into a certain container. Solving the puzzles in this game requires reasoning about a number of physical principles, including launching, blocking, or supporting objects.
The team hypothesized that there are three capabilities that people rely on to solve these puzzles: a prior belief that guides people’s actions toward those that will make a difference in the scene, the ability to imagine the effect of their actions, and a mechanism to quickly update their beliefs about what actions are likely to provide a solution. They built a model that instantiated these principles, called the “Sample, Simulate, Update,” or “SSUP,” model, and had it play the same game as people. They found that SSUP solved each puzzle at similar rates and in similar ways as people did. On the other hand, a popular deep learning model that could play Atari games well but did not have the same object and physical structures was unable to generalize its knowledge to puzzles it was not directly trained on.







Play video




      

            Rapid trial-and-error learning with simulation supports flexible tool use and physical reasoning. Video by Kris Brewer.        

    



This research provides a new framework for studying and formalizing the cognition that supports human tool use. The team hopes to extend this framework to not just study tool use, but also how people can create innovative new tools for new problems, and how humans transmit this information to build from simple physical tools to complex objects like computers or airplanes that are now part of our daily lives.
Kelsey Allen, a PhD student in the Computational Cognitive Science Lab at MIT, is excited about how the Virtual Tools game might support other cognitive scientists interested in tool use: “There is just so much more to explore in this domain. We have already started collaborating with researchers across multiple different institutions on projects ranging from studying what it means for games to be fun, to studying how embodiment affects disembodied physical reasoning. I hope that others in the cognitive science community will use the game as a tool to better understand how physical models interact with decision-making and planning.”
Joshua Tenenbaum, professor of computational cognitive science at MIT, sees this work as a step toward understanding not only an important aspect of human cognition and culture, but also how to build more human-like forms of intelligence in machines. “Artificial Intelligence researchers have been very excited about the potential for reinforcement learning (RL) algorithms to learn from trial-and-error experience, as humans do, but the real trial-and-error learning that humans benefit from unfolds over just a handful of trials — not millions or billions of experiences, as in today’s RL systems,” Tenenbaum says. “The Virtual Tools game allows us to study this very rapid and much more natural form of trial-and-error learning in humans, and the fact that the SSUP model is able to capture the fast learning dynamics we see in humans suggests it may also point the way towards new AI approaches to RL that can learn from their successes, their failures, and their near misses as quickly and as flexibly as people do.” 


",How humans use objects in novel ways to solve problems,2020-11-24,[],Center for Brains Minds and Machines/McGovern Institute/School of Science/Brain and cognitive sciences/Artificial intelligence/Machine learning/Learning/Research,"['solve', 'table', 'puzzles', 'model', 'humans', 'tool', 'actions', 'tools', 'object', 'novel', 'problems', 'objects', 'update', 'game', 'ways']","When our table is shaky, we quickly find that we can put a stack of paper under the table leg to stabilize it.
But while these actions seem so natural to us, they are believed to be a hallmark of great intelligence — only a few other species use objects in novel ways to solve their problems, and none can do so as flexibly as people.
Solving the puzzles in this game requires reasoning about a number of physical principles, including launching, blocking, or supporting objects.
They built a model that instantiated these principles, called the “Sample, Simulate, Update,” or “SSUP,” model, and had it play the same game as people.
They found that SSUP solved each puzzle at similar rates and in similar ways as people did.",Mit
39,https://news.mit.edu/2020/center-advance-predictive-simulation-research-established-mit-schwarzman-college-computing-1124,"


Understanding the degradation of materials in extreme environments is a scientific problem with major technological applications, ranging from spaceflight to industrial and nuclear safety. Yet it presents an intrinsic challenge: Researchers cannot easily reproduce these environments in the laboratory or observe essential degradation processes in real-time. Computational modeling and simulation have consequently become indispensable tools in helping to predict the behavior of complex materials across a range of strenuous conditions.
At MIT, a new research effort aims to advance the state-of-the-art in predictive simulation as well as shape new interdisciplinary graduate education programs at the intersection of computational science and computer science.
Strengthening engagement with the sciences
The Center for Exascale Simulation of Materials in Extreme Environments (CESMIX) — based at the Center for Computational Science and Engineering (CCSE) within the MIT Stephen A. Schwarzman College of Computing — will bring together researchers in numerical algorithms and scientific computing, quantum chemistry, materials science, and computer science to connect quantum and molecular simulations of materials with advanced programming languages, compiler technologies, and software performance engineering tools, underpinned by rigorous approaches to statistical inference and uncertainty quantification.
“One of the goals of CESMIX is to build a substantive link between computer science and computational science and engineering, something that historically has been hard to do, but is sorely needed,” says Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing. “The center will also provide opportunities for faculty, researchers, and students across MIT to interact intellectually and create a new synthesis of different disciplines, which is central to the mission of the college.”
Leading the project as principal investigator is Youssef Marzouk, professor of aeronautics and astronautics and co-director of CCSE, which was renamed from the Center of Computational Engineering in January to reflect its strengthening engagement with the sciences at MIT. Marzouk, who is also a member of the Statistics and Data Science Center, notes that “CESMIX is trying to do two things simultaneously. On the one hand, we want to solve an incredibly challenging multiscale simulation problem, harnessing quantum mechanical models of complex materials to achieve unprecedented accuracy at the engineering scale. On the other hand, we want to create tools that make development and holistic performance engineering of the associated software stack as easy as possible, to achieve top performance on the coming generation of exascale computational hardware.”
The project involves participation from an interdisciplinary cohort of eight faculty members, serving as co-PIs, and research scientists spanning multiple labs and departments at MIT. The full list of participants includes:

Youssef Marzouk, PI, professor of aeronautics and astronautics and co-director of CCSE;
Saman Amarasinghe, co-PI, professor of computer science and engineering;
Alan Edelman, co-PI, professor of applied mathematics;
Nicolas Hadjiconstantinou, co-PI, professor of mechanical engineering and co-director of CCSE;
Asegun Henry, co-PI, associate professor of mechanical engineering;
Heather Kulik, co-PI, associate professor of chemical engineering;
Charles Leiserson, co-PI, the Edwin Sibley Webster Professor of Electrical Engineering;
Jaime Peraire, co-PI, the H.N. Slater Professor of Aeronautics and Astronautics;
Cuong Nguyen, principal research scientist of aeronautics and astronautics;
Tao B. Schardl, research scientist in the Computer Science and Artificial Intelligence Laboratory; and
Mehdi Pishahang, research scientist of mechanical engineering.

MIT was among a total of nine universities selected as part of the Predictive Science Academic Alliance Program (PSAAP) III to form a new center to support science-based modeling and simulation and exascale computing technologies. This is the third time that PSAAP centers have been awarded by the U.S. Department of Energy’s National Nuclear Security Administration (DoE/NNSA) since the program launched in 2008 and is the first time that the Institute has ever been selected. MIT is one of just two institutions nationwide chosen to establish a Single-Discipline Center in this round and will receive up to $9.5 million in funding through a cooperative agreement over five years.
Advancing predictive simulation
CESMIX will focus on exascale simulation of materials in hypersonic flow environments. It will also drive the development of new predictive simulation paradigms and computer science tools for the exascale. Researchers will specifically aim to predict the degradation of complex (disordered and multi-component) materials under extreme loading inaccessible to direct experimental observation — an application representing a technology domain of intense current interest, and one that ­­­exemplifies an important class of scientific problems involving material interfaces in extreme environments.
“A big challenge here is in being able to predict what reactions will occur and what new molecules will form under these conditions. While quantum mechanical modeling will enable us to predict these events, we also need to be able to address the times and length scales of these processes,” says Kulik, who is also a faculty member of CCSE. “Our efforts will be focused on developing the needed software and machine learning tools that tell us when more affordable physical models can address the length scale challenge and when we need quantum mechanics to address the accuracy challenge.”
CESMIX researchers plan on disseminating their results via multiple open-source software projects, engaging their developer and user communities. The project will also support the work of postdocs, graduate students, and research scientists at MIT with the overarching goal of creating new paradigms of practice for the next generation of computational scientists.


",Center to advance predictive simulation research established at MIT Schwarzman College of Computing,2020-11-24,['Terri Park'],"MIT Schwarzman College of Computing/School of Engineering/School of Science/Aeronautical and astronautical engineering/Mechanical engineering/Chemical engineering/Electrical Engineering & Computer Science (eecs)/Mathematics/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Education, teaching, academics/Faculty/Collaboration/Department of Energy (DoE)/Algorithms/Computer modeling/Data/Computer science and technology/Grants/Research/Software","['research', 'simulation', 'materials', 'engineering', 'computing', 'science', 'professor', 'predictive', 'copi', 'college', 'computational', 'advance', 'established', 'schwarzman', 'mit', 'center']","Computational modeling and simulation have consequently become indispensable tools in helping to predict the behavior of complex materials across a range of strenuous conditions.
At MIT, a new research effort aims to advance the state-of-the-art in predictive simulation as well as shape new interdisciplinary graduate education programs at the intersection of computational science and computer science.
Marzouk, who is also a member of the Statistics and Data Science Center, notes that “CESMIX is trying to do two things simultaneously.
Advancing predictive simulationCESMIX will focus on exascale simulation of materials in hypersonic flow environments.
It will also drive the development of new predictive simulation paradigms and computer science tools for the exascale.",Mit
40,https://news.mit.edu/2020/six-faculty-2020-aaas-fellows-1124,"


Six MIT faculty members have been elected as fellows of the American Association for the Advancement of Science (AAAS).
The new fellows are among a group of 489 AAAS members elected by their peers in recognition of their scientifically or socially distinguished efforts to advance science.
A virtual induction ceremony for the new fellows will be held on Feb. 13, 2021. 

Nazli Choucri is a professor of political science, a senior faculty member at the Center of International Studies (CIS), and a faculty affiliate at the Institute for Data, Science, and Society (IDSS). She works in the areas of international relations, conflict and violence, and the international political economy, with a focus on cyberspace and the global environment. Her current research is on cyberpolitics in international relations, focusing on integrating cyberspace into the fabric of international relations.

Catherine Drennan is a professor in the departments of Biology and Chemistry. Her research group seeks to understand how nature harnesses and redirects the reactivity of enzyme metallocenters in order to perform challenging reactions. By combining X-ray crystallography with other biophysical methods, the researchers’ goal is to “visualize” molecular processes by obtaining snapshots of enzymes in action.

Peter Fisher is a professor in the Department of Physics and currently serves as department head. He carries out research in particle physics in the areas of dark matter detection and the development of new kinds of particle detectors. He is also interested in compact energy supplies and wireless energy transmission.

Neil Gershenfeld is the director of MIT's Center for Bits and Atoms, which works to break down boundaries between the digital and physical worlds, from pioneering quantum computing to digital fabrication to the “internet of things.” He is the founder of a global network of over 1,000 fab labs, chairs the Fab Foundation, and leads the Fab Academy.

Ju Li is the Battelle Energy Alliance Professor of Nuclear Science and Engineering and a professor of materials science and engineering. He studies how atoms and electrons behave and interact, to inform the design new materials from the atomic level on up. His research areas include overcoming timescale challenges in atomistic simulations, energy storage and conversion, and materials in extreme environments and far from equilibrium.

Daniela Rus is the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science and director of the Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT. Her research interests include robotics, mobile computing, and data science. Rus is a Class of 2002 MacArthur Fellow, a fellow of ACM, AAAI and IEEE, and a member of the National Academy of Engineering, and the American Academy for Arts and Science.
This year’s fellows will be formally announced in the AAAS News and Notes section of Science on Nov. 27.


",Six MIT faculty elected 2020 AAAS Fellows,2020-11-24,[],"Awards, honors and fellowships/Faculty/School of Engineering/School of Humanities Arts and Social Sciences/School of Science/School of Architecture and Planning/MIT Schwarzman College of Computing/Political science/Biology/Chemistry/Physics/Center for Bits and Atoms/Nuclear science and engineering/DMSE/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)","['fellows', 'research', 'materials', 'faculty', '2020', 'energy', 'engineering', 'elected', 'professor', 'international', 'fab', 'aaas', 'mit', 'science']","Six MIT faculty members have been elected as fellows of the American Association for the Advancement of Science (AAAS).
The new fellows are among a group of 489 AAAS members elected by their peers in recognition of their scientifically or socially distinguished efforts to advance science.
Nazli Choucri is a professor of political science, a senior faculty member at the Center of International Studies (CIS), and a faculty affiliate at the Institute for Data, Science, and Society (IDSS).
Ju Li is the Battelle Energy Alliance Professor of Nuclear Science and Engineering and a professor of materials science and engineering.
This year’s fellows will be formally announced in the AAAS News and Notes section of Science on Nov. 27.",Mit
41,https://news.mit.edu/2020/lincoln-laboratory-establishes-biotechnology-and-human-systems-division-1123,"


MIT Lincoln Laboratory has established a new research and development division, the Biotechnology and Human Systems Division. The division will address emerging threats to both national security and humanity. Research and development will encompass advanced technologies and systems for improving chemical and biological defense, human health and performance, and global resilience to climate change, conflict, and disasters.
“We strongly believe that research and development in biology, biomedical systems, biological defense, and human systems is a critically important part of national and global security. The new division will focus on improving human conditions on many fronts,"" says Eric Evans, Lincoln Laboratory director.
The new division unifies four research groups: Humanitarian Assistance and Disaster Relief (HADR) Systems, Counter-Weapons of Mass Destruction Systems, Biological and Chemical Technologies, and Human Health and Performance Systems.
""We are in a historic moment in the country, and it is a historic moment for Lincoln Laboratory to create a new division. The nation and laboratory are faced with several growing security threats, and there is a pressing need to focus our research and development efforts to address these challenges,"" says Edward Wack, who is head of the division.
The laboratory began its initial work in biotechnology in 1995, through several programs that leveraged expertise in sensors and signal processing for chemical and biological defense systems. Work has since grown to include prototyping systems for protecting high-value facilities and transportation systems, architecting integrated early-warning biodefense systems for the U.S. Department of Defense (DoD), and applying artificial intelligence and synthetic biology technologies to accelerate the development of new drugs. In recent years, synthetic biology programs have expanded to include complex metabolic engineering for the production of novel materials and therapeutic molecules. 
“The ability to leverage the laboratory’s deep technical expertise to solve today’s challenges has long laid the foundation for the new division,” says Christina Rudzinski, who is an assistant head of the division and formerly led the Counter-Weapons of Mass Destruction Systems Group.
In recent years, the laboratory has also been growing its work for improving the health and performance of service members, veterans, and civilians. Laboratory researchers have applied decades of expertise in human language technology to understand disorders and injuries of the brain. Other programs have used physiological signals captured with wearable devices to detect heat strain, injury, and infection. The laboratory’s AI and robotics expertise has been leveraged to create prototypes of semi-autonomous medical interventions to help medics save lives on the battlefield and in disaster environments.
The laboratory's transition to disaster response technology extends over the past decade. Its rich history developing sensors and decision-support software translated well to the area of emergency response, leading to the development in 2010 of an emergency communications platform now in use worldwide, and the deployment of its advanced laser detection and ranging imaging system to quickly assess earthquake damage in Haiti. In 2015, the HADR Systems Group was established to build on this work.
Today, the group develops novel sensors, communication tools, and decision-support systems to aid national and global responses to disasters and humanitarian crises. Last year, the group launched its climate change initiative to develop new programs to monitor, predict, and address current and future climate change impacts.
Through these initiatives, the laboratory has come to view its work not only in the context of national security, but also global security.
""Pandemics and climate change can cause instability, and that instability can breed conflict,” says Wack. ""It benefits the United States to have a stable world. To the degree that we can, mitigating future pandemics and reducing the impacts of climate change would improve global stability and national security.""
In anticipation of the growing importance of these global security issues, the laboratory has been significantly increasing program development, strategic hiring, and investment in biotechnology and human systems research over the past few years. Now, that strategic planning and investment in biotechnology research has come to fruition.
One of the division's initial goals is to continue to build relationships with MIT partners, including the Department of Biological Engineering, the Institute for Medical Engineering and Science, and the McGovern Institute for Brain Research, as well as Harvard University and local hospitals such as Massachusetts General Hospital. These collaborators have helped bring the laboratory's sensor technology and algorithms to clinical applications for Covid-19 diagnostics, lung and liver disorders, bone injury, and spinal surgical tools. “We can have a bigger impact by drawing on some of the great expertise on campus and in our Boston medical ecosystem,” says Wack. 
Another goal is to lead the nation in research surrounding the intersection of AI and biology. This research includes developing advanced AI algorithms for analyzing multimodal biological data, prototyping intelligent autonomous systems, and making AI-enabled biotechnology that is ethical and transparent.
“Because of our extensive experience supporting the DoD, the laboratory is in a unique position to translate this cutting-edge research, including that from the commercial sector, into a government and national security context,” says Bill Streilein, principal staff in the Biotechnology and Human System Division. “This means not only addressing typical AI application issues of data collection and curation, model selection and training, and human-machine teaming, but also issues related to traceability, explainability, and fairness.”
Leadership also sees this new division as an opportunity to continue to shape an innovative, diverse, and inclusive culture at the laboratory. They will be emphasizing the importance of an interdisciplinary approach to solving the complex research challenges the division faces. 
“We want help from the rest of the laboratory,” says Jeffrey Palmer, an assistant head of the division who previously led the Human Health and Performance Systems Group. “I think there are many ways that we can help other divisions in their missions, and we absolutely need them for success in ours. These challenges are too big to face without applying the combined capabilities of the entire laboratory.”
The Biotechnology and Human Systems Division joins Lincoln Laboratory's eight other divisions: Advanced Technology; Air, Missile, and Maritime Defense Technology; Communication Systems; Cyber Security and Information Sciences; Engineering; Homeland Protection and Air Traffic Control; ISR and Tactical Systems; and Space Systems and Technology. Lincoln Laboratory is a federally funded research and development center.


",Lincoln Laboratory establishes Biotechnology and Human Systems Division,2020-11-23,['Rowena Lindsay'],Lincoln Laboratory/Administration/Biological engineering/Institute for Medical Engineering and Science (IMES)/McGovern Institute/Bioengineering and biotechnology/Health/Climate change/Disaster response/Artificial intelligence/Security studies and military/Systems engineering/Technology and society/Medicine/Human-computer interaction/Disease/Covid-19/Pandemic/Global/International relations/School of Engineering/School of Science/Department of Defense (DOD),"['technology', 'human', 'systems', 'research', 'security', 'national', 'biotechnology', 'division', 'establishes', 'laboratory', 'lincoln', 'development']","MIT Lincoln Laboratory has established a new research and development division, the Biotechnology and Human Systems Division.
“We strongly believe that research and development in biology, biomedical systems, biological defense, and human systems is a critically important part of national and global security.
The new division will focus on improving human conditions on many fronts,"" says Eric Evans, Lincoln Laboratory director.
The new division unifies four research groups: Humanitarian Assistance and Disaster Relief (HADR) Systems, Counter-Weapons of Mass Destruction Systems, Biological and Chemical Technologies, and Human Health and Performance Systems.
Now, that strategic planning and investment in biotechnology research has come to fruition.",Mit
42,https://news.mit.edu/2020/3-questions-christine-walley-evolving-perception-robots-us-1123,"


Christine J. Walley, professor of anthropology at MIT and member of the MIT Task Force on the Work of the Future, explores how robots have often been a symbol for anxiety about artificial intelligence and automation. Walley provides a unique perspective in the recent research brief “Robots as Symbols and Anxiety Over Work Loss.” She highlights the historical context of technology and job displacement and illustrates examples of how other countries approach policies regarding robots, skills, and learning. Here, Walley provides an overview of the brief.

Q: How are robots seen as a symbol when we think about the changing nature of work in the United States?  

A: In the media, there has been a great deal of concern about robots taking people’s jobs, but, as became clear during conversations with robotics experts for MIT’s Task Force on the Work of the Future, the concerns have outstripped what the technologies are at this point actually capable of. For an anthropologist, however, the point is not that people’s concerns are “irrational,” but that robots have become symbolic encapsulations of much broader anxieties about the changing nature of work in the United States. These anxieties are well-founded. In order to put the technology questions into perspective, however, we have to confront more explicitly the dynamics that are creating more precarious forms of employment, particularly for those on the lower end of the economic spectrum, who are most vulnerable to displacement by AI and automation.

Q: What can history and anthropology teach us about job displacement and technology and how this affects current anxiety about AI and automation today?

A: First, we have to remember that technologies are inherently social. How and why they get created or used depends, of course, on what people or corporations want to do with them and what legal, cultural, and institutional frameworks allow or encourage. From the point of view of the companies, they can be used either to complement what workers do in order to increase productivity or be used to displace workers as a cost-cutting measure. There is a need for policies that encourage the former.

My own research uses both history and ethnography to study former industrial communities in the United States. In the late 19th century, mechanization was used in many industries to displace skilled workers, who were more likely to be unionized and have higher wages. Our recent era has had a strong emphasis on shareholder value and what management scholar David Weil calls “the fissured workplace” — settings in which previously in-house work gets externalized through subcontracting and other non-standard work arrangements. Consequently, there is again a strong tendency to view workers primarily as costs to be eliminated. So, there is good reason for people to be anxious. However, we have to keep in mind that these are primarily political and social questions that need to be addressed, rather than anything inevitable about the technology itself.

Earlier ethnographies of industrial workplaces found that even with dangerous and repetitive jobs, workers often managed to find ways to take pride in their work and make those jobs meaningful, often through social relationships forged with co-workers. Ethnographies of deindustrialization have also shown how devastating the effects of job loss can be, including long-term transgenerational or cumulative effects on families and entire regions. These effects are found across ethnic and racial groups, with those of color particularly hard hit. The upshot is two-fold. First, we have to be aware of socially and politically destabilizing long-term effects of job loss. There is a need for policies that are better at minimizing this kind of displacement for emergent forms of automation and AI than what we saw with early rounds of deindustrialization in the 1980s and 1990s — particularly since the new jobs being created due to technological innovation won’t necessarily go to those who are losing their jobs. And, second, we need to be thinking not only about numbers of jobs, but how emergent technologies influence workplace sociality and what makes labor meaningful to workers — realities that are crucial to creating a more vibrant future economy that works for ordinary people, and not just Wall Street and corporations.

Q: What are some of the key takeaways, including policies, that the United States can learn from other countries in the way they think about technology, skills, and learning?

A: Not everyone in the world is as afraid of job displacement by robots or automation as workers are in the United States. This is not surprising, given that among wealthier countries the United States is an outlier in terms of its lack of universal health-care coverage and often in terms of other benefits and protections. Since health-care coverage in the U.S. is often provided through employers, it makes the possibility of being displaced by robots or automation that much more anxiety-provoking (just as it puts companies that provide health care at a disadvantage by saddling them with rising costs, contributing to the desire to save money by replacing workers with automation). In addition, the U.S. public school system is based on local taxes and is highly inequitable along lines of race and class, with relatively little spent on job retraining or vocational education in comparison to many European countries. Given employers’ need for more educated workers and given rapid technological change and job turnover, this puts many Americans at a strong disadvantage. It’s not surprising that we’re seeing declining social mobility rates in the United States in comparison to many other wealthy countries.

Policy differences make a substantial difference in how technologies are taken up and the impact they have, or will have, on workers. Some European countries, like Germany and Sweden, have policies in which workers select representatives who participate in decision-making on shop floors or even on management boards, increasing worker input into how new technologies will be used. Some countries, particularly Nordic ones, have also made social benefits more flexible, just as corporations have become more flexible, and are emphasizing continuing education and job retraining as technological transformation creates more job turnover. Although we have seen economic inequality on the rise in many parts of the world, it’s been particularly severe in the U.S. — and emergent technologies are poised to contribute to that. So, it is key for the U.S. to look seriously at what policies are working better in other countries and what we might learn from them.


",3Q: Christine Walley on the evolving perception of robots in the US,2020-11-23,[],School of Humanities Arts and Social Sciences/Anthropology/Robots/Labor/Learning/Artificial intelligence/automation/Policy/Technology and society/3 Questions/Faculty/Global/STEM education,"['christine', 'job', 'countries', 'jobs', 'united', 'technologies', 'walley', 'perception', '3q', 'workers', 'robots', 'work', 'evolving', 'policies', 'states']","Q: How are robots seen as a symbol when we think about the changing nature of work in the United States?
For an anthropologist, however, the point is not that people’s concerns are “irrational,” but that robots have become symbolic encapsulations of much broader anxieties about the changing nature of work in the United States.
My own research uses both history and ethnography to study former industrial communities in the United States.
A: Not everyone in the world is as afraid of job displacement by robots or automation as workers are in the United States.
It’s not surprising that we’re seeing declining social mobility rates in the United States in comparison to many other wealthy countries.",Mit
43,https://news.mit.edu/2020/forum-automation-work-future-1120,"


“Pop culture does a great job of scaring us that AI will take over the world,” said Professor Daniela Rus, speaking at a virtual MIT event on Wednesday. But realistically, said Rus, who directs the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), robots aren’t going to steal everyone’s jobs overnight — they’re not yet good enough at tasks requiring high dexterity or generalized processing of different kinds of information.

Still, automation has crept into some workplaces in recent years, a trend that’s likely to continue. Throughout the daylong conference, the “AI and the Work of the Future Congress,” which convened speakers from academia, industry, and government, one key theme consistently emerged: Task automation shouldn’t be viewed as a replacement for human work, but a partner for it. With the exception of some middle-skilled manufacturing jobs, automation has generally improved human productivity, not eliminated the need for it. If people thoughtfully guide the development and deployment of new workplace technologies, the speakers agreed, we could see improvements in both productivity and well-being.

The daylong event was organized by MIT’s Task Force on the Work of the Future, which released its final report this week, along with the Initiative on the Digital Economy and CSAIL. During the forum, task force participants and other science and industry leaders discussed both the [social] and technological dimensions of these changes.

Narrow AI

Rus emphasized that current industrial applications of artificial intelligence are relatively narrow. “What today’s AI systems can do is specialized intelligence, or the ability to solve a very fixed, limited number of problems,” she said. In select industries like insurance and health care, artificial intelligence has been used to boost efficiency for individual tasks, but it hasn’t generally displaced human workers. Fully automated systems, like driverless cars, remain decades in the future. 

While the rise of artificial intelligence in industry remains gradual, multiple speakers noted how other technologies have rocketed to widespread adoption due to the Covid-19 pandemic. Microsoft CEO Satya Nadella described how videoconferencing and related technologies have enabled the transmission of potentially lifesaving information. “The expert can be remote, but can perhaps more seamlessly transfer their knowledge to the person on the front line,” he said.

Nadella added that, since so many companies have grown used to videoconferencing, they may never return to 100 percent face-to-face interactions. “There’s going to be real, structural change,” he said. “People are going to question what really requires presence that is physical, versus telepresence. And I think the workflow will adjust.” He noted that workplaces would have to be more intentional about fostering social cohesion among workers in lieu of casual in-person conversations.

Pandemic aside, some speakers pointed out that automation’s impact on work, though generally positive, has been unequal. Some middle-skill manufacturing jobs have been lost due to automation. But those losses aren’t inevitable — they can be avoided through careful deployment of automation, said Bosch CEO Volkmar Denner. “You could go a very aggressive path and say ‘the robot finally could replace human workers,’” said Denner. “The path we chose was completely different.” Robots on Bosch’s manufacturing line are designed not to oust humans, but to make them even more valuable by assisting with particular tasks to make them more efficient overall.

“We can find a balance between the economic aspects — introducing automation — and also the social aspects — keeping workers in work,” he said. “Technology always should serve human beings and not vice versa.”

Other industry leaders agreed. Jeanne Magoulick, engineering manager for Ford Motor Company, said her team is developing artificial intelligence for predictive maintenance of machinery. “It’s going to notify us when a machine seems to be trending out of control, and then we can schedule that for maintenance during the next available window,” she said. “It’s going to make us more efficient.”

“It’s a choice”

Rus also discussed the use of machines as guardian systems — safeguards that help ensure human workers are performing at their best. She cited a study where radiologists and an artificial intelligence algorithm were separately shown images of lymph node cells and tasked with determining whether they were cancerous or not. The humans’ error rate was 7.5 percent, and the computer’s was 3.5 percent. However, when an image was scanned by both a human and a computer, the resulting error rate was just 0.5 percent, “which is extraordinary,” said Rus.

Julie Shah, MIT associate professor in the Department of Aeronautics and Astronautics, added that this sort of “guardian” relationship between humans and automation could extend to many domains, including self-driving cars and manufacturing systems.

Nadella envisioned that one day the very tools of automation — the ability to design and program computers and robots — will become accessible to those without specialized training. He pointed to examples, like word processing and spreadsheet programs like Excel, where automation turbocharged productivity without requiring users to learn computer code.

“Knowledge work got fundamentally transformed,” said Nadella. In the future, “this notion of a citizen-app developer, a citizen-data scientist — I think it’s real.”

Denner also cautioned, however, that certain tasks — like valuing human lives in an automated driving scenario — are best left to ethicists and society as a whole, not to industrial programmers.

In an afternoon panel about shaping workplace technologies in the future, MIT professor of economics Daron Acemoglu reiterated the refrain that technology isn’t an inevitable force — it’s shaped by humans. Ultimately, he said policymakers and managers will decide how automation fits into the workplace. “There isn’t an ironclad rule of what it is that humans can do and technologies cannot do. They are both fluid. It depends on what we value and how we use technology,” Acemoglu said. “It’s a choice.”


",MIT forum examines the rise of automation in the workplace,2020-11-20,['Daniel Ackerman'],Economics/Jobs/Robots/Artificial intelligence/Special events and guest speakers/President L. Rafael Reif/Policy/Industry/Business and management/Robotics/Manufacturing/MIT Schwarzman College of Computing/Poverty/Computer Science and Artificial Intelligence Laboratory (CSAIL)/School of Humanities Arts and Social Sciences/School of Engineering/School of Architecture and Planning/Sloan School of Management,"['tasks', 'human', 'examines', 'technologies', 'forum', 'humans', 'rise', 'work', 'workers', 'artificial', 'intelligence', 'workplace', 'automation', 'mit', 'going']","With the exception of some middle-skilled manufacturing jobs, automation has generally improved human productivity, not eliminated the need for it.
Narrow AIRus emphasized that current industrial applications of artificial intelligence are relatively narrow.
While the rise of artificial intelligence in industry remains gradual, multiple speakers noted how other technologies have rocketed to widespread adoption due to the Covid-19 pandemic.
“You could go a very aggressive path and say ‘the robot finally could replace human workers,’” said Denner.
Jeanne Magoulick, engineering manager for Ford Motor Company, said her team is developing artificial intelligence for predictive maintenance of machinery.",Mit
44,https://news.mit.edu/2020/future-work-conference-1120,"


The American workforce is at a crossroads. Digitization and automation have replaced millions of middle-class jobs, while wages have stagnated for many who remain employed. A lot of labor has become insecure, low-income freelance work.

Yet there is reason for optimism on behalf of workers, as scholars and business leaders outlined in an MIT conference on Wednesday. Automation and artificial intelligence do not just replace jobs; they also create them. And many labor, education, and safety-net policies could help workers greatly as well.

That was the outlook of many participants at the conference, the “AI and the Work of the Future Congress,” marking the release of the final report of MIT’s Task Force on the Work of the Future. The report concludes that there is no technology-driven jobs wipeout on the horizon, but new policies are needed to match the steady march of innovation; technology has mostly helped white-collar workers, but not the rest of the work force in the U.S.

“We’re not going to run out of work,” Elisabeth Beck Reynolds, executive director of the task force, and executive director of the MIT Industrial Performance Center, said Wednesday.

She added: “Clearly the distributional effects of technological change are uneven. We’ve seen the reduction of middle-skill jobs [due] to automation, [along with] jobs in manufacturing, administration, in clerical work, while we’ve seen an increase in jobs for those with higher education and higher skill sets. … Our challenge is to try to train [workers] and make sure we have workers in good positions for those jobs.”

Indeed, the notion of social responsibility was a leading motif of the conference, which drew an audience of about 1,500 online viewers. 

“I believe that those of us who are technologists, and who educate tomorrow’s technologists, have a special role to play,” said MIT President L. Rafael Reif, in his introductory remarks at the conference. “It means that, while we are teaching students, in every field, to be fluent in the use of AI strategies and tools, we must be sure that we equip tomorrow’s technologists with equal fluency in the cultural values and ethical principles that should ground and govern how those tools are designed and how they’re used.”

The daylong event was organized by MIT’s Task Force on the Work of the Future, along with the Initiative on the Digital Economy and the Computer Science and Artificial Intelligence Laboratory.
Conditions on the ground
The report notes that over the last four decades, innovation has driven increases in productivity, but that earnings have not followed in step. Since 1978, overall U.S. productivity has risen by 66 percent; yet over the same time, compensation for production and nonsupervisory workers has only risen by 10 percent.

“Work has become a lot more fragile,” said James Manyika, a senior partner at the consulting firm McKinsey and Company, chair of the McKinsey Global Institute, and a member of McKinsey's board of directors. “This has affected both middle-wage and lower-wage workers.”

To be sure, information technology in particular has helped people in engineering, design, medicine, marketing, and many other white-collar fields; and while middle-income jobs have become more scarce, service-sector jobs have expanded but tend to be lower-income.

“Certainly the United States is a good place for high-wage workers to be, but not for lower-wage [workers] and those in the middle,” said Susan Houseman, vice president and director of research at the W.E. Upjohn Institute for Employment Research. “We should be concerned about the growth of nontraditional work arangements.”

Moreover, “The U.S. doesn’t seem to be getting a very positive return on its inequality,” said David Autor, the Ford Professor of Economics at MIT, associate head of MIT’s Department of Economics, and a co-chair of the task force. “That is, we have a lot of inequality, but we do not have faster growth.”

In general, most workers are “not seeming to share in the prosperity that improved technology has got us,” said Robert M. Solow, Institute Professor Emeritus and 1987 Nobel laureate in economics, in recorded remarks shown during the conference.

That said, Solow observed, “There’s room for a lot of ingenuity here, because since the nature of employment has changed, as we become a service economy rather than a goods-producing economy, there’s room for innovation in how to organize union work. … More active enforcement of antitrust laws, to try to increase the degree of competition in the production of goods and services, would also have the effect of improving the prospects for wages and salaries.”

He added: “The main factor in the disturbance in the distribution of incomes is probably not technological change.”

What are the next steps?

But if there is room for policy interventions to ease the social jolts resulting from technology, which ones make the most sense? In general terms, some conference participants advocated for an openness to market-driven technological change, paired with a substantial safety net to help people handle those disruptive waves of innovation.

“The real fundamental shift is, we have to think of service jobs the way 100 years ago we thought about manufacturing jobs. In other words, we have to start putting in place … protections and benefits,” said Fareed Zakaria, author and host of the CNN show, “Fareed Zakaria GPS.” He added, “Ultimately, that is the only way you are going to really address this problem. We are not going to bring back tens of millions of manufacturing jobs to the United States. We are going to take these service jobs and make them better jobs. And companies can do that.”

One conference panel focused on the support of education, particularly public universities and community colleges, where traditionally overlooked pools of workplace talent reside.

“One of the most important skills or approaches that we need to talk about is how to make sure that people know how to think, how to learn, how to adapt,” said Freeman Hrabowski, president of the University of Maryland at Baltimore County. That said, he noted, people receiving a broad college education can also receive specialist certificates and credentials in particular technical areas and add layers to their skills that are more closely linked to evolving job opportunities. “Both are very important,” he noted.

Juan Salgado, chancellor of the City Colleges of Chicago, a group of community colleges, pointed out that there are 11.8 million community college students in America — many of whom already hold jobs and have workplace skills in addition to the academic skills they are acquiring.

“It’s about the assets that are in our institutions, our students, and the fact that we’re not paying enough attention to them,” said Salgado.

“We know what works,” said Paul Osterman, a professor of human resources and management at the MIT Sloan School of Management, pointing out that many training programs, internships, and other work-directed educational programs have been rigorously assessed and proven to be effective. “It’s taking what we know works and making it work at scale.”

Saru Jayaraman, president of the advocacy group One Fair Wage and director of the Food Labor Research Center at the University of California at Berkeley, noted that simply raising the minimum wage, especially for food service workers, would have multiple benefits that only start with the increased earnings for roughly 10 percent of the workforce.

“Increased wages reduce turnover in an industry that has some of the highest turnover rates in any industry in the United States,” said Jayaraman, adding that better wages have “increased employee morale, [and] increased employee productivity and consumer service.”

Karen Mills, a senior fellow at the Harvard Business School and a former administrator of the Small Business Administration, suggested that good policies are especially important for small businesses, which may not be able to capitalize on technology as much as bigger firms.

“In the jobs of the future, not all robots are going to be serving you coffee,” said Mills. “There’s still going to be Main Street.” She emphasized the continued need for supportive policies for small businesses, including access to health care for employees and access to capital for firm founders, which would also help small businesses owned by women and people of color.

Rep. Lisa Blunt Rochester of Delaware, who will start her third term as a congresswoman in January, helped found the Congressional Future of Work Caucus, and suggested there is more bipartisan support for federal action than observers may suspect. 

“We launched the caucus right before Covid-19 struck,” she said. “We literally had standing room only. Democrats, Republicans, we had the council on Black mayors, we had the unions, AFL-CIO, just this diversity, academics — I held up your [interim] report — there was this common agreement that we need to have the conversation.”

“Something we shape and create”

The conference also included extended discussion about the state of technology itself, especially artificial intelligence, examining its paths of progress and forms of deployment.

“Technology is not something that happens to us,” said David Mindell, task force co-chair, professor of aeronautics and astronautics, and the Dibner Professor of the History of Engineering and Manufacturing at MIT. “It’s something we shape and create.”

“You can’t say, ‘AI did it,’” said Microsoft CEO Satya Nadella, in a taped conversation with Autor.  “We, as creators of AI, first and foremost have a set of design principles. … We have to go from ethics to actual engineering and design and [a] process that allows us to be more accountable.”

Related: MIT forum examines the rise of automation in the workplace

A number of conference participants suggested that we should be careful to construct policies that don’t rein in technological advances, but can ameliorate their effects.

“I don’t think we should constrain technological progress, because it is a competitive advantage of nations, and we have to let innovation thrive. We have to let technology proceed,” said Indra Nooyi, the former chairman and CEO of Pepsico. “At best, what we can do is anticipate the negative consequences of technology ... and put in some checks and balances.”

As a few conference panelists noted throughout the event, the overlapping issues of work, technology, and inequality have become even more complicated and relevant during the Covid-19 pandemic, with roughly one-third of the work force able to work more securely from home, while many service workers and others have to perform their jobs in person.

Surveying the employment landscape of 2020, Nooyi noted, “In many ways Covid has exacerbated all the societal divides.” Indeed, Reynolds said, “We believe this work is more important, not less important, in the time of Covid.”

Overall, the task force members noted, making the work of the future better is a task that starts today.

“I really come away from this concerned about the direction [of work], but optimistic about our ability to change it,” Autor said.


",Why we shouldn’t fear the future of work,2020-11-20,['Peter Dizikes'],Economics/Jobs/Artificial intelligence/Special events and guest speakers/President L. Rafael Reif/Industry/Business and management/Government/Robotics/Manufacturing/MIT Schwarzman College of Computing/Sloan School of Management/School of Humanities Arts and Social Science/School of Engineering/Poverty/School of Science/School of Architecture and Planning,"['technology', 'future', 'work', 'shouldnt', 'workers', 'force', 'task', 'fear', 'going', 'conference', 'mit', 'jobs']","A lot of labor has become insecure, low-income freelance work.
Yet there is reason for optimism on behalf of workers, as scholars and business leaders outlined in an MIT conference on Wednesday.
That was the outlook of many participants at the conference, the “AI and the Work of the Future Congress,” marking the release of the final report of MIT’s Task Force on the Work of the Future.
“In the jobs of the future, not all robots are going to be serving you coffee,” said Mills.
“I really come away from this concerned about the direction [of work], but optimistic about our ability to change it,” Autor said.",Mit
45,https://news.mit.edu/2020/neural-network-uncertainty-1120,"


Increasingly, artificial intelligence systems known as deep learning neural networks are used to inform decisions vital to human health and safety, such as in autonomous driving or medical diagnosis. These networks are good at recognizing patterns in large, complex datasets to aid in decision-making. But how do we know they’re correct? Alexander Amini and his colleagues at MIT and Harvard University wanted to find out.

They’ve developed a quick way for a neural network to crunch data, and output not just a prediction but also the model’s confidence level based on the quality of the available data. The advance might save lives, as deep learning is already being deployed in the real world today. A network’s level of certainty can be the difference between an autonomous vehicle determining that “it’s all clear to proceed through the intersection” and “it’s probably clear, so stop just in case.” 

Current methods of uncertainty estimation for neural networks tend to be computationally expensive and relatively slow for split-second decisions. But Amini’s approach, dubbed “deep evidential regression,” accelerates the process and could lead to safer outcomes. “We need the ability to not only have high-performance models, but also to understand when we cannot trust those models,” says Amini, a PhD student in Professor Daniela Rus’ group at the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL).

“This idea is important and applicable broadly. It can be used to assess products that rely on learned models. By estimating the uncertainty of a learned model, we also learn how much error to expect from the model, and what missing data could improve the model,” says Rus.

Amini will present the research at next month’s NeurIPS conference, along with Rus, who is the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science, director of CSAIL, and deputy dean of research for the MIT Stephen A. Schwarzman College of Computing; and graduate students Wilko Schwarting of MIT and Ava Soleimany of MIT and Harvard.

Efficient uncertainty

After an up-and-down history, deep learning has demonstrated remarkable performance on a variety of tasks, in some cases even surpassing human accuracy. And nowadays, deep learning seems to go wherever computers go. It fuels search engine results, social media feeds, and facial recognition. “We’ve had huge successes using deep learning,” says Amini. “Neural networks are really good at knowing the right answer 99 percent of the time.” But 99 percent won’t cut it when lives are on the line.

“One thing that has eluded researchers is the ability of these models to know and tell us when they might be wrong,” says Amini. “We really care about that 1 percent of the time, and how we can detect those situations reliably and efficiently.”

Neural networks can be massive, sometimes brimming with billions of parameters. So it can be a heavy computational lift just to get an answer, let alone a confidence level. Uncertainty analysis in neural networks isn’t new. But previous approaches, stemming from Bayesian deep learning, have relied on running, or sampling, a neural network many times over to understand its confidence. That process takes time and memory, a luxury that might not exist in high-speed traffic.

The researchers devised a way to estimate uncertainty from only a single run of the neural network. They designed the network with bulked up output, producing not only a decision but also a new probabilistic distribution capturing the evidence in support of that decision. These distributions, termed evidential distributions, directly capture the model's confidence in its prediction. This includes any uncertainty present in the underlying input data, as well as in the model’s final decision. This distinction can signal whether uncertainty can be reduced by tweaking the neural network itself, or whether the input data are just noisy.

Confidence check

To put their approach to the test, the researchers started with a challenging computer vision task. They trained their neural network to analyze a monocular color image and estimate a depth value (i.e. distance from the camera lens) for each pixel. An autonomous vehicle might use similar calculations to estimate its proximity to a pedestrian or to another vehicle, which is no simple task.

Their network’s performance was on par with previous state-of-the-art models, but it also gained the ability to estimate its own uncertainty. As the researchers had hoped, the network projected high uncertainty for pixels where it predicted the wrong depth. “It was very calibrated to the errors that the network makes, which we believe was one of the most important things in judging the quality of a new uncertainty estimator,” Amini says.

To stress-test their calibration, the team also showed that the network projected higher uncertainty for “out-of-distribution” data — completely new types of images never encountered during training. After they trained the network on indoor home scenes, they fed it a batch of outdoor driving scenes. The network consistently warned that its responses to the novel outdoor scenes were uncertain. The test highlighted the network’s ability to flag when users should not place full trust in its decisions. In these cases, “if this is a health care application, maybe we don’t trust the diagnosis that the model is giving, and instead seek a second opinion,” says Amini.

The network even knew when photos had been doctored, potentially hedging against data-manipulation attacks. In another trial, the researchers boosted adversarial noise levels in a batch of images they fed to the network. The effect was subtle — barely perceptible to the human eye — but the network sniffed out those images, tagging its output with high levels of uncertainty. This ability to sound the alarm on falsified data could help detect and deter adversarial attacks, a growing concern in the age of deepfakes.

Deep evidential regression is “a simple and elegant approach that advances the field of uncertainty estimation, which is important for robotics and other real-world control systems,” says Raia Hadsell, an artificial intelligence researcher at DeepMind who was not involved with the work. “This is done in a novel way that avoids some of the messy aspects of other approaches —  e.g. sampling or ensembles — which makes it not only elegant but also computationally more efficient — a winning combination.”

Deep evidential regression could enhance safety in AI-assisted decision making. “We’re starting to see a lot more of these [neural network] models trickle out of the research lab and into the real world, into situations that are touching humans with potentially life-threatening consequences,” says Amini. “Any user of the method, whether it’s a doctor or a person in the passenger seat of a vehicle, needs to be aware of any risk or uncertainty associated with that decision.” He envisions the system not only quickly flagging uncertainty, but also using it to make more conservative decision making in risky scenarios like an autonomous vehicle approaching an intersection.

“Any field that is going to have deployable machine learning ultimately needs to have reliable uncertainty awareness,” he says.

This work was supported, in part, by the National Science Foundation and Toyota Research Institute through the Toyota-CSAIL Joint Research Center.


",A neural network learns when it should not be trusted,2020-11-20,['Daniel Ackerman'],Technology and society/Machine learning/Artificial intelligence/Algorithms/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/MIT Schwarzman College of Computing,"['data', 'uncertainty', 'learning', 'trusted', 'learns', 'models', 'network', 'decision', 'deep', 'networks', 'amini', 'neural']","Increasingly, artificial intelligence systems known as deep learning neural networks are used to inform decisions vital to human health and safety, such as in autonomous driving or medical diagnosis.
But previous approaches, stemming from Bayesian deep learning, have relied on running, or sampling, a neural network many times over to understand its confidence.
The researchers devised a way to estimate uncertainty from only a single run of the neural network.
This distinction can signal whether uncertainty can be reduced by tweaking the neural network itself, or whether the input data are just noisy.
They trained their neural network to analyze a monocular color image and estimate a depth value (i.e.",Mit
46,https://news.mit.edu/2020/phiala-shanahan-receives-kenneth-wilson-award-1118,"


Class of 1957 Career Development Assistant Professor of Physics Phiala Shanahan will receive the 2020 Kenneth G. Wilson Award for Excellence in Lattice Field Theory.
The award, given by the international lattice field theory community, recognizes her research of hadrons and nuclei using the tools of lattice Quantum Chromodynamics, or lattice QCD, and her pioneering application of machine learning and artificial intelligence techniques to lattice field theory.
Shanahan’s research interests are focused around theoretical nuclear and particle physics, specifically regarding the structure and interactions of hadrons and nuclei from the fundamental (quark and gluon) degrees of freedom encoded in the Standard Model of particle physics.
In recent work she has used supercomputers to reveal the role of gluons, the force carriers of the strong interactions described by QCD, in hadron and nuclear structure. She and her group recently also achieved the first calculation of the gluon structure of light nuclei, making predictions that will be testable in new experiments proposed at Jefferson National Accelerator Facility and at the planned Electron-Ion Collider. 
“To be recognized by those closest to my work on a technical level is an incredible honor,” says Shanahan, who is also a researcher in the Center for Theoretical Physics within the Laboratory for Nuclear Science. “The award reflects not only my work, but the efforts of my awesome students and postdocs, as well as my wonderful colleagues in the Center for Theoretical Physics who create such a vibrant and positive community to work in.”
This year's award ceremony will be part of the Nov. 12 virtual Bethe colloquium series, where Shanahan will receive a certificate citing her contributions, a modest monetary award, and the opportunity to present her cited work.
Shanahan’s work to understand the structure of matter from first principles also aims to enrich nuclear physics experimental programs seeking to constrain physics beyond the current Standard Model, such as dark matter. The group’s research into nuclear structure and reactions “are at the cusp of entering the beginning of a precision era of understanding how nuclei emerge from particle physics,” says Shanahan. “It is just so exciting to begin to bridge that gap.” 
For the third area of her cited research, machine learning, she says, “We are working hard to reinvent how numerical lattice field theory calculations are done with new algorithms to enable studies that are computationally intractable right now. When Aurora, which will be the new largest supercomputer in the world, comes online in the next couple of years, we plan to be ready to exploit it in a new way.”
Shanahan obtained her BS in 2012 and her PhD in 2015 from Australia’s University of Adelaide. After graduation she began at MIT as a postdoc, then held a joint position as assistant professor at the College of William & Mary and senior staff scientist at the Thomas Jefferson National Accelerator Facility until she came back to MIT in 2018. Shanahan is the recipient of a National Science Foundation CAREER award as well as a U.S. Department of Energy Early Career Award, was named as an Emmy Noether fellow in 2018, and was listed in the Forbes Magazine 30 under 30 in Science in 2017 and as one of Science News’ 10 Scientists to Watch in 2020.
Since its inception in 2011, the annual Kenneth G. Wilson Award for Excellence in Lattice Field Theory has recognized physicists who have made recent, outstanding contributions to lattice field theory. The award is named after Nobel laureate Kenneth Wilson (1936–2013), who in 1974 founded lattice gauge theory, permitting such theories to be studied numerically using powerful computers.


",Phiala Shanahan receives Kenneth G. Wilson Award for work in lattice field theory,2020-11-18,['Sandi Miller'],"School of Science/Physics/Laboratory for Nuclear Science/Faculty/Awards, honors and fellowships/Machine learning/Artificial intelligence","['phiala', 'nuclear', 'lattice', 'receives', 'work', 'g', 'field', 'award', 'science', 'structure', 'wilson', 'physics', 'theory', 'kenneth', 'shanahan']","Class of 1957 Career Development Assistant Professor of Physics Phiala Shanahan will receive the 2020 Kenneth G. Wilson Award for Excellence in Lattice Field Theory.
The award, given by the international lattice field theory community, recognizes her research of hadrons and nuclei using the tools of lattice Quantum Chromodynamics, or lattice QCD, and her pioneering application of machine learning and artificial intelligence techniques to lattice field theory.
In recent work she has used supercomputers to reveal the role of gluons, the force carriers of the strong interactions described by QCD, in hadron and nuclear structure.
Since its inception in 2011, the annual Kenneth G. Wilson Award for Excellence in Lattice Field Theory has recognized physicists who have made recent, outstanding contributions to lattice field theory.
The award is named after Nobel laureate Kenneth Wilson (1936–2013), who in 1974 founded lattice gauge theory, permitting such theories to be studied numerically using powerful computers.",Mit
47,https://news.mit.edu/2020/advancing-artificial-intelligence-research-1118,"


The broad applicability of artificial intelligence in today’s society necessitates the need to develop and deploy technologies that can build trust in emerging areas, counter asymmetric threats, and adapt to the ever-changing needs of complex environments.
As part of a new collaboration to advance and support AI research, the MIT Stephen A. Schwarzman College of Computing and the Defense Science and Technology Agency in Singapore are awarding funding to 13 projects led by researchers within the college that target one or more of the following themes: trustworthy AI, enhancing human cognition in complex environments, and AI for everyone. The 13 research projects selected are highlighted below.
“SYNTHBOX: Establishing Real-World Model Robustness and Explainability Using Synthetic Environments” by Aleksander Madry, professor of computer science. Emerging machine learning technology has the potential to significantly help with and even fully automate many tasks that have confidently been entrusted only to humans so far. Leveraging recent advances in realistic graphics rendering, data modeling, and inference, Madry’s team is building a radically new toolbox to fuel streamlined development and deployment of trustworthy machine learning solutions.
“Next-Generation NLP Technologies for Low-Resource Tasks” by Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science; and Tommi Jaakkola, the Thomas Siebel Professor of Electrical Engineering and Computer Science. In natural language technologies, most languages in the world are not richly annotated. This lack of direct supervision often results in inaccurate, indefensible, and brittle outputs. In a project led by Barzilay and Jaakkola, researchers are developing new text-generation tools for controlled style transfer and novel algorithms for detecting misinformation or suspicious news online. 
“Computationally-Supported Role-playing for Social Perspective Taking” by D. Fox Harrell, professor of digital media and artificial intelligences. Drawing on computer science and social science approaches, this project aims to create tools, techniques, and methods to model social phenomena for users of computer-supported role-playing systems — online gaming, augmented reality, and virtual reality — to better understand the perspectives of others with different social identities.
“Improving Situational Awareness for Collaborative Human-Machine First Responder Teams” by Nick Roy, professor of aeronautics and astronautics. When responding to emergencies in urban environments, achieving situational awareness is essential. In a project led by Roy, researchers are developing a multi-agent system that encompasses a team of autonomous air and ground vehicles designed to arrive at the scene of an emergency, a map of the scene to provide a situation report to the first responders in advance, and the ability to search for people and entities of interest.
“New Representations for Vision” by William Freeman, the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science; and Josh Tenenbaum, professor of cognitive science and computation. An unrealized goal of AI is to model the rich and complicated shapes and textures of real-world scenes depicted in an image. This project will focus on developing neural network representations for images which are better suited to the requirements for image representations in vision and graphics to represent a 3D world efficiently, capturing its richness.
“Data-driven Optimization Under Categorical Uncertainty, and Applications to Smart City Operations” by Alexandre Jacquillat, assistant professor of operations research and statistics. Smart city technologies can help aid major metropolitan areas that are facing increasing pressure to manage congestion, cut greenhouse gas emissions, improve public safety, and enhance health-care delivery. In a project led by Jacquillat, researchers are working on new AI tools to help manage the cyber-physical infrastructure in smart cities and on the development and deployment of automated decision tools for smart city operations.
“Provably Robust Reinforcement Learning” by Ankur Moitra, the Rockwell International Career Development Associate Professor of Applied Mathematics. Moitra and his team are building on their new framework for robust supervised learning to explore more complex learning problems, including the design of robust algorithms for reinforcement learning in Massart noise models, a space that has yet to be fully explored.
“Audio Forensics” by James Glass, senior research scientist. The ongoing improvements in capabilities that manipulate or generate multi-media content such as speech, images, and video are resulting in ever-more natural and realistic “deepfake” content that is increasingly difficult to discern from the real thing. In a project led by Glass, researchers are developing a set of deep learning models that can be used to identify manipulated or synthetic speech content, as well as detect the nature of deepfakes to help analysts better understand the underlying objective of the manipulation and how much effort is required to create the fake content.
“Building Dependable Autonomous Systems through Learning Certified Decisions and Control” by Chuchu Fan, assistant professor of aeronautics and astronautics. Machine learning creates unprecedented opportunities for achieving full autonomy, but ­learning-based methods in autonomous systems can and do fail, due to poor-quality data, modeling errors, the coupling with other agents, and the complex interaction with human and computer systems in modern operational environments. Fan and her research group are building a framework consisting of algorithms, theories, and software tools for learning certified planner and control, as well as developing firmware platforms for the automatic plug-and-play design of quadrotors and the formation control of mixed ground and aerial vehicles.
“Online Learning and Decision-making Under Uncertainty in Complex Environments” by Patrick Jaillet, the Dugald C. Jackson Professor of Electrical Engineering and Computer Science. Technical advances in computing, telecommunication, sensing capabilities, and other information technologies provide tremendous opportunities to use dynamic information in order to enhance productivity, optimize performance, and solve new complex online problems of great practical interests. However, many of these opportunities bring significant methodological challenges on how to formulate and solve these new problems. In a project led by Jaillet, researchers are using machine learning techniques to systematically integrate online optimization and online learning in order to help human decision-making under uncertainty.
“Analytics-Guided Communication to Counteract Filter Bubbles and Echo Chambers” by Deb Roy, professor of media arts and sciences. Social media technologies that promised to open up our worlds have instead driven us algorithmically into cocoons of homogeneity. Roy and his team are developing language models and methods to counteract the effects of these technologies that has exacerbated socioeconomic divides and limited exposure to different perspectives, curbing opportunities for users to learn from others who may not necessarily look, think, or live like them.
“Decentralized Learning with Diverse Data” by Costis Daskalakis, professor of electrical engineering and computer science; Asu Ozdaglar, the MathWorks Professor of Electrical Engineering and Computer Science, department head of electrical engineering and computer science, and deputy dean of academics for MIT Schwarzman College of Computing; and Russ Tedrake, Toyota Professor of Electrical Engineering and Computer Science. In many AI settings, it is important to combine diverse experiences of, and decentralized data collected by, heterogeneous agents in order to develop better models for predictions and decision-making in the various different new tasks these agents are performing. Bringing tools from machine learning, optimization, control, statistics, statistical physics, and game theory, this project aims to advance the fundamental science of federated or fleet learning – learning from decentralized agents with diverse data — using robotics as an application area to provide a rich and relevant source of data.
“Trustworthy, Deployable 3D Scene Perception via Neuro-symbolic Probabilistic Programs” by Vikash Mansinghka, principal research scientist; Joshua Tenenbaum, professor of cognitive science and computation; and Antonio Torralba, Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science. To be deployable in the real world, 3D scene perception systems need to generalize across environments and sensor configurations, and adapt to scene and environment changes, without costly re-training or fine-tuning. Building on the researchers’ breakthroughs in probabilistic programming and in real-time neural Monte Carlo inference for symbolic generative models, the project team is developing a domain-general approach to trustworthy, deployable 3D scene perception that addresses fundamental limitations of state-of-the-art deep learning systems.



",Advancing artificial intelligence research,2020-11-18,['Terri Park'],Electrical Engineering & Computer Science (eecs)/Aeronautical and astronautical engineering/Brain and cognitive sciences/Comparative Media Studies/Writing/Mathematics/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Artificial intelligence/Machine learning/Funding/Faculty/Technology and society/MIT Schwarzman College of Computing/School of Engineering/School of Science/School of Humanities Arts and Social Sciences,"['research', 'electrical', 'computer', 'technologies', 'learning', 'researchers', 'engineering', 'advancing', 'professor', 'developing', 'artificial', 'project', 'intelligence', 'science']","“SYNTHBOX: Establishing Real-World Model Robustness and Explainability Using Synthetic Environments” by Aleksander Madry, professor of computer science.
“Next-Generation NLP Technologies for Low-Resource Tasks” by Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science; and Tommi Jaakkola, the Thomas Siebel Professor of Electrical Engineering and Computer Science.
“New Representations for Vision” by William Freeman, the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science; and Josh Tenenbaum, professor of cognitive science and computation.
“Online Learning and Decision-making Under Uncertainty in Complex Environments” by Patrick Jaillet, the Dugald C. Jackson Professor of Electrical Engineering and Computer Science.
“Trustworthy, Deployable 3D Scene Perception via Neuro-symbolic Probabilistic Programs” by Vikash Mansinghka, principal research scientist; Joshua Tenenbaum, professor of cognitive science and computation; and Antonio Torralba, Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science.",Mit
48,https://news.mit.edu/2020/bhavik-nagda-1118,"


“Academia,” “government,” “industry” — Bhavik Nagda squinted closely as his professor pointed to each word on the diagram of the American economy’s core components. Between each word sprouted dozens of arrows, illustrating the complex interactions between the three institutions.
“There were just so many arrows,” says Nagda, recalling the presentation during MIT’s Science Policy Bootcamp. “I was blown away. It gave a voice to the way I think about systemic issues and how America has built its economy.”
A senior majoring in computer science, Nagda had always been fascinated by futuristic technologies. Upon coming to MIT he quickly took on research roles in everything from artificial intelligence to computational cognitive science. But he found himself coming back to a key question: What led impossible-sounding ideas to become real products?
The pieces finally connected when he attended the bootcamp, taught by Bill Bonvillian, the former director of the MIT Washington Office, during the Institute’s Independent Activities Period (IAP) in January. Nagda had already observed the importance of cooperation between innovators and policymakers during several internships, in roles as an engineer and a technology investor. The bootcamp crystallized his understanding of how critical this cooperation is to the U.S. economy — and he began to envision a future for himself working at the intersection of technology, innovation, and policy.
A key concept from the course, explains Nagda, was the “valley of death,” which describes the difficulty a research idea often faces in receiving enough funding to continue with development. He learned how programs such as the Defense Advanced Research Projects Agency, which helped launch major inventions such as GPS and the internet, are crucial drivers for the economy.
Nagda agreed — he had seen firsthand how few ideas make it past this valley and reach commercialization. For the past few months, he had been working at Bessemer Venture Partners. The firm is famous for its cloud investments, such as Pinterest, Twilio, and Twitch. Nagda worked as a technology investor to find and recommend emerging companies.
The experience showed him that while the venture community funds a variety of ideas, scalable “software-as-a-service” (SAAS) products and biotech products have remained the most lucrative for the last decade. He became fascinated with ways governments can use their expansive resources to support early-stage research in fields like clean energy and precision medicine.
But in addition to funding new ideas, governments must also anticipate the pitfalls of technology and create policies to protect the public, says Nagda. He witnessed collaborations between politicians and innovators during an earlier sophomore-year internship, working as a software engineer at Cruise Automation, a company that has been introducing self-driving cars into cities that can be hailed via phone app.
Prior to the company’s launch, many policymakers were worried about public safety. A single flaw in a vehicle’s design could lead to severe danger for both passengers and pedestrians, a prospect the engineers took very seriously.
For example, “One of the challenges is making an accurate sensor,” says Nagda. “The lidar and stereo camera imagery and inertial measurements must help the computer estimate the location and speed of the vehicle. It’s very challenging.” As an intern, he noted with interest how Cruise’s engineers worked with policymakers to ensure the technology would meet predetermined safety conditions.
He also witnessed the company develop coalitions across San Francisco. Employees from the government and community relations teams spoke with community members of all backgrounds, from biking commuters to homeless people. The goal was to interpret the concerns of everyday people about autonomous vehicles and consider their thoughts into the car’s design.
This focus on societal impact by tech companies has grown due to the recent national scrutiny of industry leaders, such as Amazon, Apple, Google, and Facebook. “The congressional hearings have shown us there’s a lot of work Silicon Valley has to do. There’s now a focus for tech companies to think about their stakeholders as opposed to just directly maximizing share value,” he says.
Nagda’s interests in technology and government were also fueled by a summer he spent working for the Federal Communications Commission (FCC), with support from MIT’s PKG Center. He helped to research robotic telemarketing, or “robocalling.”
“When robocalling was first invented, it was very exciting to people. But in the last decade, we’ve started to get around three to four calls a day,” says Nagda. “There are so many innocent people who get hacked into revealing their bank numbers.”
Nagda’s team focused on helping to authenticate callers who had been incorrectly blocked and labeled as robocallers. Their response code helped to recognize this error and provide users a message to automatically reverse the mistake. The work was presented to the Internet Engineering Task Force.
After meeting policymakers in person, Nagda was surprised to see a level of government cooperation rarely portrayed in the media. “It was amazing to see delegates from both sides of the aisle work together on this issue.”
During his time at MIT, Nagda has also conducted research in the labs of Professor Tomas Lozano-Perez of the Computer Science and Artificial Intelligence Laboratory and as a research assistant for economics Professor Jonathan Gruber. Since last year, he has worked in the lab of brain and cognitive sciences Professor Joshua Tenenbaum, through the SuperUROP program; his research has included work on an artificial intelligence system that can “learn” to play Atari video games.
He also taught an engineering bootcamp in Soroti, Uganda for students ages 12-19 during IAP this past year through MIT’s D-Lab Development program. And, he has participated in the MIT Driverless Team, which builds cars and races them in international competitions.
In the future, Nagda hopes to return to Washington and leverage his technical background. He views the area as similar to MIT — a place where ideas flow and can have wide-scale impact. His experience also showed him the greater need for engineers on Capitol Hill.
“For the last decade, it’s been clear that technology is impacting society in often detrimental ways. It’s a front and center issue in politics right now. I think to push the needle forward, we need more technologists in the room while policies are formulated.”
In terms of whether he plans to be involved in research, policy, or business, Nagda is still unsure. “I don’t know where I’ll be, but I know I’ll be thinking about these three issues for the rest of my life.”


",Bhavik Nagda: Delving into the deployment of new technologies,2020-11-18,['Hannah Meiseles'],Students/Profile/Undergraduate/Electrical Engineering & Computer Science (eecs)/Policy/Economics/Brain and cognitive sciences/Government/Technology and society/Innovation and Entrepreneurship (I&E)/Public Service Center (PSC)/School of  Engineering/MIT Schwarzman College of Computing/School of Humanities Arts and Social Sciences/Defense Advanced Research Projects Agency (DARPA),"['technology', 'research', 'technologies', 'delving', 'work', 'working', 'nagda', 'policymakers', 'professor', 'ideas', 'bhavik', 'science', 'mit', 'deployment']","“Academia,” “government,” “industry” — Bhavik Nagda squinted closely as his professor pointed to each word on the diagram of the American economy’s core components.
It gave a voice to the way I think about systemic issues and how America has built its economy.”A senior majoring in computer science, Nagda had always been fascinated by futuristic technologies.
Nagda had already observed the importance of cooperation between innovators and policymakers during several internships, in roles as an engineer and a technology investor.
Nagda agreed — he had seen firsthand how few ideas make it past this valley and reach commercialization.
After meeting policymakers in person, Nagda was surprised to see a level of government cooperation rarely portrayed in the media.",Mit
49,https://news.mit.edu/2020/stem-week-event-encourages-students-see-themselves-science-technology-careers-1117,"


Covid-19 has given the public a crash course in what it is like to be a medical researcher. The evening news displays graphs and charts describing case counts and statistical data, while the status of vaccine trials is front page news. Now, more than ever, the public is seeing how STEM (science, technology, engineering, and math) fields are rising to the challenge of Covid-19.
It is in this spirit that MIT and the Massachusetts STEM Advisory Council encouraged students to “see themselves in STEM” by producing a week of programming aimed at fostering a lifelong love of STEM.
Partnering across the Commonwealth
The Massachusetts STEM Week kicked-off Oct. 19 with opening remarks by MIT President L. Rafael Reif. Speaking to a stream of over 480 viewers, President Reif reflected on how reading MIT textbooks in his native Venezuela put him on a pathway to a career in STEM. “I realized there was a world of other people who loved science and engineering, and places like MIT where I could join them,” he said. President Reif challenged students to participate in STEM spaces, declaring “we need you to do more than see yourself in STEM, we invite you to step up and take your place.”
Governor Charlie Baker joined the kickoff event, describing the importance of the Massachusetts STEM economy and its global impact. “Massachusetts is enormously lucky to have MIT among the constellation of amazing colleges and universities that are a part of this Commonwealth,” the governor said, noting that STEM institutions “provide an incredible collection of ideas, gadgets, and solutions that become a big part of the way the world works.”
Lieutenant Governor Karyn Polito, a co-chair of the STEM Advisory Council, echoed those sentiments, adding “with the pipeline of talent we have, we need to make sure that pipeline includes more women and communities of color.” Fellow STEM Advisory Council co-chairs Jeffrey Leiden of Vertex Pharmaceuticals and U.S. Congressman Joe Kennedy also offered inspiring welcoming remarks. 
MIT Media Lab Associate Director Cynthia Breazeal presented the kickoff’s keynote address, diving into the number of ways that artificial intelligence permeates the platforms that students frequently utilize. Noting examples of how AI can sometimes encode bias into software, Breazeal argued that the antidote is to prepare students to be “AI-Literate” and encourage them toward the field. Expanding access to open K-12 curriculum, educator resources, and easy-to-use platforms that introduce AI concepts will excite “a far more diverse and inclusive group of students [who] have the potential to become the ethical designers of the AI solutions of tomorrow.”
70 years of supporting student scientists through the Massachusetts Science and Engineering Fair
The second hour of the kickoff event celebrated student scientists by featuring two prize-winning projects from this year’s Massachusetts Science and Engineering Fair (MSEF). MIT’s relationship with MSEF spans over 70 years, starting with a small gathering on the dirt floor of Rockwell Cage in 1949. The fair was started by the American Academy of Arts and Sciences, MIT professors, and a group of pioneering K-12 science educators organized as the Massachusetts Science Fair Committee. 
The first two presenters were Hopkinton High School students Archita Nemalikanti and Sreeja Bolla, winners of the Sanofi Genzyme award. Their invention combined light sensors, computer programming, and a heavily researched set of calculations to create a non-invasive test for anemia in newborns, similar to current O2 finger sensors. Archita and Sreeja described the months-long process of developing the device and demonstrated their expertise to an international audience.
Mathworks prize winner and Westfield High School student Suvin Sundararajan presented his work on testing different additives to create safer and more environmentally friendly plastics for 3D printing. His biodegradable, flame retardant, non-toxic plastic was synthesized using lab equipment and guidance from the Emrick Group in the Department of Polymer Science and Engineering at the University of Massachusetts at Amherst. Reflecting on how the group’s mentorship made the project possible, Suvin recounted how he was trained on multimillion-dollar equipment typical in chemistry labs, which “allowed me to understand how they function and provided an opportunity to generate more ideas” for his work. 
More #MassSTEMWeek events across campus
In addition to Monday’s kickoff event, members of the MIT community participated in a range of Massachusetts STEM Week events across the Commonwealth.

Former astronaut and aeronautics and astronautics Professor Jeff Hoffman discussed NASA’s latest Mars mission and the Mars 2020 Perseverance Rover on “AstroNights: LIVE  Mars Mania!”
Professor of comparative media studies Fox Harrell, an affiliate of the MIT Computer Science and Artificial Intelligence Laboratory, participated in a panel discussion with Lt. Governor Polito and four inspiring Boston Public Schools students organized by the United Way’s BoSTEM program.
Lemelson-MIT held two STEM Week events, “Helping Youth See Themselves in Biotech” and “The Wonderful World of Biotech,” co-sponsored by the Massachusetts Black and Latino Legislative Caucus.
MIT Media Lab Research Scientist Katlyn Turner presented “Antiracism in Technology Design.”
The MIT Museum held two Virtual Idea Hubs, which encourage creating, tinkering, and engineering using everyday materials around your home. Participating families built whimsical structures that explored balance and centers of gravity. 
Brian Mernoff, an educator at the MIT Museum, participated in the MetroNorth/Region IV MSEF event. This panel — Supporting Science at Home, featuring MSEF’s Region IV Science Fair representatives — included a robust conversation about supporting students through the science fair process and how university and corporate partners add perspective and value along the way.

In addition to these events, undergraduate students led over 250 hours of hands-on STEM explorations with 60 high school students around the Commonwealth as part of MIT's Full STEAM Into Fall after-school program.
The Massachusetts STEM Week is organized by the Massachusetts Executive Office of Education and the STEM Advisory Council.


",STEM Week event encourages students to see themselves in science and technology careers,2020-11-17,[],"Special events and guest speakers/K-12 education/STEM education/Community/Mentoring/Education, teaching, academics/Aeronautical and astronautical engineering/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Media Lab/Lemelson-MIT/MIT Museum/President L. Rafael Reif/Cambridge, Boston and region/Collaboration/Government/Comparative Media Studies/Writing/School of Engineering/School of Humanities Arts and Social Sciences/School of Architecture and Planning/MIT Schwarzman College of Computing","['technology', 'event', 'massachusetts', 'careers', 'encourages', 'week', 'engineering', 'stem', 'fair', 'events', 'science', 'mit', 'students']","Now, more than ever, the public is seeing how STEM (science, technology, engineering, and math) fields are rising to the challenge of Covid-19.
Partnering across the CommonwealthThe Massachusetts STEM Week kicked-off Oct. 19 with opening remarks by MIT President L. Rafael Reif.
More #MassSTEMWeek events across campusIn addition to Monday’s kickoff event, members of the MIT community participated in a range of Massachusetts STEM Week events across the Commonwealth.
This panel — Supporting Science at Home, featuring MSEF’s Region IV Science Fair representatives — included a robust conversation about supporting students through the science fair process and how university and corporate partners add perspective and value along the way.
The Massachusetts STEM Week is organized by the Massachusetts Executive Office of Education and the STEM Advisory Council.",Mit
50,https://news.mit.edu/2020/mitnano-immersion-lab-gaming-program-awards-seed-grants-1117,"


MIT.nano has announced its second annual seed grants to support hardware and software research related to sensors, 3D/4D interaction and analysis, augmented and virtual reality (AR/VR), and gaming. The grants are awarded through the MIT.nano Immersion Lab Gaming Program, a four-year collaboration between MIT.nano and video game development company NCSOFT, a founding member of the MIT.nano Consortium.
“We are thrilled to award seven grants this year in support of research that will shape how people interact with the digital world and each other,” says MIT.nano Associate Director Brian W. Anthony. “The MIT.nano Immersion Lab Gaming Program encourages cross-collaboration between disciplines. Together, musicians and engineers, performing artists and data scientists will work to change the way humans think, study, interact, and play with data and information.”
The MIT.nano Immersion Lab is a new, two-story immersive space dedicated to visualizing, understanding, and interacting with large data or synthetic environments, and to measuring human-scale motions and maps of the world. Outfitted with equipment and software tools for motion capture, photogrammetry, and visualization, the facility is available for use by researchers and educators interested in using and creating new experiences, including the seed grant projects.
This year, the following seven projects have been selected to receive seed grants.
Mohammad Alizadeh: a neural-enhanced teleconferencing system
Seeking to improve teleconferencing systems that often suffer from poor quality due to network congestion, Department of Electrical Engineering and Computer Science (EECS) Associate Professor Mohammad Alizadeh proposes using artificial intelligence to create a neural-enhanced system.
This project, co-investigated by EECS professors Hari Balakrishnan, Fredo Durand, and William T. Freeman, uses person-specific face image generation models to handle deteriorated network conditions. Before a session, participants exchange their personalized generative models. If the network is congested during the session, clients use these models to reconstruct a high-quality video of each person from a low bit-rate video encoding or just their audio.
The team believes that, if successful, this research could help solve the major challenge of network bottlenecks that negatively impact remote teaching and learning, virtual reality experiences, and online multiplayer games.
Luca Daniel: dance-inspired models for representing intent
Dancers have an amazing ability to represent intent and emotions very clearly and powerfully using their bodies. EECS Professor Luca Daniel, along with mechanical engineering Research Scientist Micha Feigin-Almon, aim to study and generate a modeling framework explaining and capturing how dancers communicate intent and emotions through their movements.
This is the second NCSOFT seed grant awarded to Daniel and Feigin, who received funding in the 2019 inaugural round. Over the past year, they have used the motion capture equipment and software tools of the Immersion Lab to study how the movements of trained dancers differ from those of untrained individuals. The research indicates that trained dancers are producing elegant movements by efficiently storing and retrieving elastic energy from their muscles and connective tissue.
Daniel and Feigin are now formalizing their stretch-stabilization model of human movement, which has broad applicability in the fields of gaming, health care, robotics, and neuroscience. Of particular interest is generating elegant, realistic movement trajectories for virtual reality characters.
Frédo Durand: inverse rendering for photorealistic computer graphics and digital avatars
The field of computer graphics has created advanced algorithms to simulate images, given a 3D model of a scene. However, the realism of the final image is only as good as the input model, and the creation of sophisticated 3D scenes remains a huge challenge, especially in the case of digital humans.
Frédo Durand, the Amar Bose Professor of Computing in EECS, strives to improve 3D models to satisfy all lighting phenomena and faithfully reproduce real scenes and humans by developing inverse rendering techniques to generate photorealistic 3D models from input photographs. The end result could help create compelling digital avatars for gaming or video conferencing, reproduce scenes for telepresence, or create rich realistic environments for video games.
Jeehwan Kim: electronic skin-based long-term imperceptible and controller-free AR/VR motion tracker
The most widely used input devices for AR/VR interactions are handheld controllers, which are difficult for new users to manipulate and inconvenient to hold, interrupting immersive experiences. Associate professor of mechanical engineering Jeehwan Kim proposes electronic skin (e-skin)-based human motion tracking hardware that can imperceptibly adhere to human skin and accurately detect human motions using skin-conformable optical markers and strain sensors.
Kim’s e-skin would enable detection and input of very subtle human movements, helping to improve the accuracy of the AR/VR experience and facilitate deeper user immersion. Additionally, the integration of LED markers, strain sensors, and EP sensors on the e-skin will combine to provide tracking of large range of motions, ranging from the movements of the joints to those of skin surfaces, as well as emotional changes.
Will Oliver: Qubit Arcade
Qubit Arcade, MIT’s VR quantum computing experience, teaches core principles of quantum computing via a virtual reality demonstration. Users can create Bloch spheres, control qubit states, measure results, and compose quantum circuits in a 3D representation, with a small selection of quantum gates.
In this next phase, EECS Associate Professor Will Oliver, with co-investigators MIT Open Learning Media Development Director Chris Boebel and virtual reality developer Luis Zanforlin, aims to turn Qubit Arcade into a gamified multiplayer educational tool that can be used in a classroom setting or at home, on the Oculus Quest platform. Their project includes adding more quantum gates to the circuit builder, and model quantum parallelism and interference — fundamental characteristics of quantum algorithms.
The team believes learnings from the educational application of Qubit Arcade have the potential to be applied to a variety of other subjects involving abstract visualizations, mathematical modeling, and circuit design.
Jay Scheib: augmenting opera: “Parsifal”
Music and Theater Arts Professor Jay Scheib’s work focuses on make opera and theater a more immersive event by bringing live cinematic technology and techniques into theatrical forms. In his productions, the audience experiences two layers of reality — the making of the performance and the performance itself, through live production and simultaneous broadcast.
Scheib’s current project is to create a production using AR techniques that would make use of existing extended reality (XR) technology, such as live video effects and live special effects processed in real time and projected back onto a surface or into headsets. He intends to develop an XR prototype for an augmented live performance of Richard Wagner’s opera “Parsifal” through the use of game engines for real-time visual effects processing in live performance environments.
Justin Solomon: AI for designing usable virtual assets
Virtual worlds, video games, and other 3D digital environments are built on huge collections of assets — 3D models of the objects, scenery, and characters that populate each scene. Artists and game designers use specialized, difficult-to-use modeling tools to design these assets, while non-experts are confined to rudimentary mix-and-match interfaces combining simplistic pre-defined pieces.
EECS Associate Professor Justin Solomon seeks to democratize content creation in virtual worlds by developing AI tools for designing virtual assets. In this project, Solomon explores designing algorithms that learn from existing datasets of expert-created 3D models to assist non-expert users in contributing objects to 3D environments. He envisions a flexible toolbox for producing assets from high-level user guidance, with varying part structure and detail.


",MIT.nano Immersion Lab Gaming Program awards 2020 seed grants,2020-11-17,[],School of Engineering/School of Humanities Arts and Social Sciences/School of Science/Mechanical engineering/Music and theater arts/Electrical Engineering & Computer Science (eecs)/MIT.nano/Video games/Grants/Computer science and technology/Augmented and virtual reality/Nanoscience and nanotechnology/Research/Funding/Technology and society/Collaboration/Industry,"['immersion', 'grants', 'gaming', 'awards', 'using', 'seed', '2020', 'virtual', 'models', 'reality', 'live', 'lab', 'professor', '3d', 'quantum', 'mitnano', 'video', 'program']","MIT.nano has announced its second annual seed grants to support hardware and software research related to sensors, 3D/4D interaction and analysis, augmented and virtual reality (AR/VR), and gaming.
The grants are awarded through the MIT.nano Immersion Lab Gaming Program, a four-year collaboration between MIT.nano and video game development company NCSOFT, a founding member of the MIT.nano Consortium.
Of particular interest is generating elegant, realistic movement trajectories for virtual reality characters.
Will Oliver: Qubit ArcadeQubit Arcade, MIT’s VR quantum computing experience, teaches core principles of quantum computing via a virtual reality demonstration.
In this project, Solomon explores designing algorithms that learn from existing datasets of expert-created 3D models to assist non-expert users in contributing objects to 3D environments.",Mit
51,https://news.mit.edu/2020/work-of-future-final-report-1117,"


Decades of technological change have polarized the earnings of the American workforce, helping highly educated white-collar workers thrive, while hollowing out the middle class. Yet present-day advances like robots and artificial intelligence do not spell doom for middle-tier or lower-wage workers, since innovations create jobs as well. With better policies in place, more people could enjoy good careers even as new technology transforms workplaces.
That’s the conclusion of the final report from MIT’s Task Force on the Work of the Future, which summarizes over two years of research on technology and jobs. The report, “The Work of the Future: Building Better Jobs in an Age of Intelligent Machines,” was released today, and the task force is hosting an online conference on Wednesday, the “AI & the Future of Work Congress.”
At the core of the task force’s findings: A robot-driven jobs apocalypse is not on the immediate horizon. As technology takes jobs away, it provides new opportunities; about 63 percent of jobs performed in 2018 did not exist in 1940. Rather than a robot revolution in the workplace, we are witnessing a gradual tech evolution. At issue is how to improve the quality of jobs, particularly for middle- and lower-wage workers, and ensure there is greater shared prosperity than the U.S. has seen in recent decades.
“The sky is not falling, but it is slowly lowering,” says David Autor, the Ford Professor of Economics at MIT, associate head of MIT’s Department of Economics, and a co-chair of the task force. “We need to respond. The world is gradually changing in very important ways, and if we just keep going in the direction we’re going, it is going to produce bad outcomes.”
That starts with a realistic understanding of technological change, say the task force leaders.
The task force aimed “to move past the hype about what [technologies] might be here, and now we’re looking at what can we feasibly do to move things forward for workers,” says Elisabeth Beck Reynolds, executive director of the task force as well as executive director of the MIT Industrial Performance Center. “We looked across a range of industries and examined the numerous factors — social, cognitive, organizational, economic — that shape how firms adopt technology.”
“We want to inject into the public discourse a more nuanced way of talking about technology and work,” adds David Mindell, task force co-chair, professor of aeronautics and astronautics, and the Dibner Professor of the History of Engineering and Manufacturing at MIT. “It’s not that the robots are coming tomorrow and there’s nothing we can do about it. Technology is an aggregate of human choices.”
The report also addresses why Americans may be concerned about work and the future. It states: “Where innovation fails to drive opportunity, it generates a palpable fear of the future: the suspicion that technological progress will make the country wealthier while threatening the people’s livelihoods. This fear exacts a high price: political and regional divisions, distrust of institutions, and mistrust of innovation itself. The last four decades of economic history give credence to that fear.”
""Automation is transforming our work, our lives, our society,"" says MIT President L. Rafael Reif, who initiated the formation of the task force in 2017. ""Fortunately, the harsh societal consequences that concern us all are not inevitable. How we design tomorrow’s technologies, and the policies and practices we build around them, will profoundly shape their impact.""
Reif adds: ""Getting this right is among the most important and inspiring challenges of our time — and it should be a priority for everyone who hopes to enjoy the benefits of a society that’s healthy and stable, because it offers opportunity for all.""
Six big conclusions
The task force, an Institute-wide group of scholars and researchers, spent over two years studying work and technology in depth. The final report presents six overarching conclusions and a set of policy recommendations. The conclusions:
1) Technological change is simultaneously replacing existing work and creating new work. It is not eliminating work altogether. 
Over the last several decades, technology has significantly changed many workplaces, especially through digitization and automation, which have replaced clerical, administrative, and assembly-line workers across the country. But the overall percentage of adults in paid employment has largely risen for over a century. In theory, the report states, there is “no instrinsic conflict between technological change, full employment, and rising earnings.”
In practice, however, technology has polarized the economy. White-collar workers — in medicine, marketing, design, research, and more — have become more productive and richer, while middle-tier workers have lost out. Meanwhile, there has been growth in lower-paying service-industry jobs where digitization has little impact — such as food services, janitors, and drivers. Since 1978, aggregate U.S. productivity has risen by 66 percent, while compensation for production and nonsupervisory workers has risen by only 10 percent. Wage gaps also exist by race and gender, and cities do not provide the “escalator” to the middle class they once did.
While innovations have replaced many receptionists, clerks, and assembly-line workers, they have simultaneously created new occupations. Since the middle of the 20th century, the U.S. has seen major growth in the computer industry, renewable energy, medical specialties, and many areas of design, engineering, marketing, and health care. These industries can support many middle-income jobs as well — while the services sector keeps growing.
As the task force leaders state in the report, “The dynamic interplay among task automation, innovation, and new work creation, while always disruptive, is a primary wellspring of rising productivity. Innovation improves the quantity, quality, and variety of work that a worker can accomplish in a given time. This rising productivity, in turn, enables improving living standards and the flourishing of human endeavors.”
However, a bit ruefully, the authors also note that “in what should be a virtuous cycle, rising productivity provides society with the resources to invest in those whose livelihoods are disrupted by the changing structure of work.”
But this has not come to pass, as the distribution of value from these jobs has been lopsided. In the U.S., lower-skill jobs only pay 79 percent as much when compared to Canada, 74 percent compared to the U.K., and 57 percent compared to Germany.
“People understand that automation can make the country richer and make them poorer, and that they’re not sharing in those gains,” Autor says. “We think that can be fixed.”
2) Momentous impacts of technological change are unfolding gradually.
Time and again, media coverage about technology and jobs focuses on dramatic scenarios in which robots usurp people, and we face a future without work.
But this picture elides a basic point: Technologies mimicking human actions are difficult to build, and expensive. It is generally cheaper to simply hire people for those tasks. On the other hand, technologies that augment human abilities — like tools that let doctors make diagnoses — help those workers become more productive. Apart from clerical and assembly-line jobs, many technologies exist in concert with workers, not as a substitute for them.
Thus workplace technology usually involves “augmentation tasks more than replacement tasks,” Mindell says. The task force report surveys technology adoption in industries including insurance, health care, manufacturing, and autonomous vehicles, finding growth in “narrow” AI systems that complement workers. Meanwhile, technologists are working on difficult problems like better robotic dexterity, which could lead to more direct replacement of workers, but such advances at a high level are further off in the future.
“That’s what technological adoption looks like,” Mindell says. “It’s uneven, it’s lumpy, it goes in fits and starts.” The key question is how innovators at MIT and elsewhere can shape new technology to broad social benefit.
3) Rising labor productivity has not translated into broad increases in incomes because societal institutions and labor market policies have fallen into disrepair. 
While the U.S. has witnessed a lot of technological innovation in recent decades, it has not seen as much policy innovation, particularly on behalf of workers. The polarizing effects of technology on jobs would be lessened if middle- and lower-income workers had relatively better support in other ways. Instead, in terms of pay, working environment, termination notice time, paid vacation time, sick time, and family leave, “less-educated and low-paid U.S. workers fare worse than comparable workers in other wealthy industrialized nations,” the report notes. The adjusted gross hourly earnings of lower-skill workers in the U.S. in 2015 averaged $10.33, compared to $24.28 in Denmark, $18.18 in Germany, and $17.61 in Australia.
“It’s untenable that the labor market has this growing gulf without shared prosperity,” Autor says. “We need to restore the synergy between rising productivity and improvements in labor market opportunity.” He adds: “We’ve had real institutional failure, and it’s within our hands to change it. … That includes worker voice, minimum wages, portable benefits, and incentives that cause companies to invest in workers.”
Looking ahead, the report cautions, “If those technologies deploy into the labor institutions of today, which were designed for the last century, we will see similar effects to recent decades: downward pressure on wages, skills, and benefits, and an increasingly bifurcated labor market.” The task force argues instead for institutional innovations that complement technological change.
4) Improving the quality of jobs requires innovation in labor market institutions. 
The task force contends the U.S. needs to modernize labor policies on several fronts, including restoring the federal minimum wage to a reasonable percentage of the national median wage and, crucially, indexing it to inflation. 
The report also suggests upgrading unemployment insurance in several ways, including: using very recent earnings to determine eligibility or linking eligibility to hours worked, not earnings; making it easier to receive partial benefits in case of events like loss of a second job; and dropping the requirement that people need to seek full-time work to receive benefits, since so many people hold part-time positions. 
The report also observes that U.S. collective bargaining law and processes are antiquated. The authors argue that workers need better protection of their current collective bargaining rights; new forms of workplace representation beyond traditional unions; and legal protections allowing groups to organize that include home-care workers, farmworkers, and independent contractors.
5) Fostering opportunity and economic mobility necessitates cultivating and refreshing worker skills.
Technological advancement may often be incremental, but changes happen often enough that workers’ skills and career paths can become obsolete. The report emphasizes that U.S. workers need more opportunities to add new skills — whether through the community college system, online education, company-based retraining, or other means.  
The report calls for making ongoing skills development accessible, engaging, and cost-effective. This requires buttressing what already works, while advancing new tools: blended online and in-person offerings, machine-supervised learning, and augmented and virtual reality learning environments.
The greatest needs are among workers without four-year college degrees. “We need to focus on those who are between high school and the four-year degree,” Reynolds says. “There should be pathways for those people to increase their skill set and make it meaningful to the labor market. We really need a shift that makes this a high priority.”
6) Investing in innovation will drive new job creation, speed growth, and meet rising competitive challenges.
The rate of new-job creation over the last century is heavily driven by technological innovation, the report notes, with a considerable portion of that stemming from federal investment in R&D, which has helped produce many forms of computing and medical advances, among other things. As of 2015, the U.S. invested 2.7 percent of its GDP in R&D, compared to 2.9 percent in Germany and 2.1 percent in China. But the public share of that R&D investment has fallen from 40 percent in 1985 to 25 percent in 2015. The task force calls for a recommitment to this federal support.
“Innovation has a key role in job creation and growth,” Autor says.
Given the significance of innovation to job and wealth creation, the report calls for increased overall federal research funding; targeted assistance that helps small- and medium-sized businesses adopt technology; policies creating a wider geographical spread of innovation in the U.S.; and policies that enhance investment in workers, not just capital, including the elimination of accelerated capital depreciation claims, and an employer training tax credit that functions like the R&D tax credit.
Global issues, U.S. suggestions 
In addition to Reynolds, Autor, and Mindell, MIT’s Task Force on the Work of the Future consisted of a group of 18 MIT professors representing all five Institute schools and the MIT Schwarzman College of Computing; a 22-person advisory board drawn from the ranks of industry leaders, former government officials, and academia; a 14-person research board of scholars; and over 20 graduate students. The task force also consulted with business executives, labor leaders, and community college leaders, among others. The final document includes case studies from specific firms and sectors as well, and the Task Force is publishing nearly two dozen research briefs that go into the primary research in more detail. 
The task force observed global patterns at play in the way technology is adopted and diffused through the workplace, although its recommendations are focused on U.S. policy issues.
“While our report is very geared toward the U.S. in policy terms, it clearly is speaking to a lot of trends and issues that exist globally,” Reynolds said. “The message is not just for the U.S. Many of the challenges we outline are found in other countries too, albeit to lesser degrees. As we wrote in the report, ‘the central challenge ahead, indeed the work of the future, is to advance labor market opportunity to meet, complement, and shape technological innovations.’”
The task force intends to circulate ideas from the report among policymakers and politicians, corporate leaders and other business managers, and researchers, as well as anyone with an interest in the condition of work in the 21st century.
“I hope people are receptive,” Reynolds adds. “We have made forceful recommendations that tie together different policy areas — skills, job quality, and innovation. These issues are critical, particularly as we think about recovery and rebuilding in the age of Covid-19. I hope our message will be picked up by both the public sector and private sector leaders, because both of those are essential to forge the path forward.”


","Report outlines route toward better jobs, wider prosperity",2020-11-17,['Peter Dizikes'],Economics/Jobs/Robots/Artificial intelligence/Machine learning/President L. Rafael Reif/Policy/Industrial Performance Center/Industry/Faculty/Administration/Technology and society/Global/Innovation and Entrepreneurship (I&E)/Poverty/Business and management/Robotics/Research/Manufacturing/School of Humanities Arts and Social Sciences/Sloan School of Management/School of Engineering/School of Science/School of Architecture and Planning/MIT Schwarzman College of Computing,"['technology', 'innovation', 'prosperity', 'work', 'workers', 'route', 'force', 'wider', 'task', 'labor', 'outlines', 'better', 'report', 'technological', 'jobs']","That’s the conclusion of the final report from MIT’s Task Force on the Work of the Future, which summarizes over two years of research on technology and jobs.
As technology takes jobs away, it provides new opportunities; about 63 percent of jobs performed in 2018 did not exist in 1940.
Six big conclusionsThe task force, an Institute-wide group of scholars and researchers, spent over two years studying work and technology in depth.
The task force report surveys technology adoption in industries including insurance, health care, manufacturing, and autonomous vehicles, finding growth in “narrow” AI systems that complement workers.
The polarizing effects of technology on jobs would be lessened if middle- and lower-income workers had relatively better support in other ways.",Mit
52,https://news.mit.edu/2020/iot-deep-learning-1113,"


Deep learning is everywhere. This branch of artificial intelligence curates your social media and serves your Google search results. Soon, deep learning could also check your vitals or set your thermostat. MIT researchers have developed a system that could bring deep learning neural networks to new — and much smaller — places, like the tiny computer chips in wearable medical devices, household appliances, and the 250 billion other objects that constitute the “internet of things” (IoT).
The system, called MCUNet, designs compact neural networks that deliver unprecedented speed and accuracy for deep learning on IoT devices, despite limited memory and processing power. The technology could facilitate the expansion of the IoT universe while saving energy and improving data security.
The research will be presented at next month’s Conference on Neural Information Processing Systems. The lead author is Ji Lin, a PhD student in Song Han’s lab in MIT’s Department of Electrical Engineering and Computer Science. Co-authors include Han and Yujun Lin of MIT, Wei-Ming Chen of MIT and National University Taiwan, and John Cohn and Chuang Gan of the MIT-IBM Watson AI Lab.
The Internet of Things
The IoT was born in the early 1980s. Grad students at Carnegie Mellon University, including Mike Kazar ’78, connected a Cola-Cola machine to the internet. The group’s motivation was simple: laziness. They wanted to use their computers to confirm the machine was stocked before trekking from their office to make a purchase. It was the world’s first internet-connected appliance. “This was pretty much treated as the punchline of a joke,” says Kazar, now a Microsoft engineer. “No one expected billions of devices on the internet.”
Since that Coke machine, everyday objects have become increasingly networked into the growing IoT. That includes everything from wearable heart monitors to smart fridges that tell you when you’re low on milk. IoT devices often run on microcontrollers — simple computer chips with no operating system, minimal processing power, and less than one thousandth of the memory of a typical smartphone. So pattern-recognition tasks like deep learning are difficult to run locally on IoT devices. For complex analysis, IoT-collected data is often sent to the cloud, making it vulnerable to hacking.
“How do we deploy neural nets directly on these tiny devices? It’s a new research area that’s getting very hot,” says Han. “Companies like Google and ARM are all working in this direction.” Han is too.
With MCUNet, Han’s group codesigned two components needed for “tiny deep learning” — the operation of neural networks on microcontrollers. One component is TinyEngine, an inference engine that directs resource management, akin to an operating system. TinyEngine is optimized to run a particular neural network structure, which is selected by MCUNet’s other component: TinyNAS, a neural architecture search algorithm.
System-algorithm codesign 
Designing a deep network for microcontrollers isn’t easy. Existing neural architecture search techniques start with a big pool of possible network structures based on a predefined template, then they gradually find the one with high accuracy and low cost. While the method works, it’s not the most efficient. “It can work pretty well for GPUs or smartphones,” says Lin. “But it’s been difficult to directly apply these techniques to tiny microcontrollers, because they are too small.”
So Lin developed TinyNAS, a neural architecture search method that creates custom-sized networks. “We have a lot of microcontrollers that come with different power capacities and different memory sizes,” says Lin. “So we developed the algorithm [TinyNAS] to optimize the search space for different microcontrollers.” The customized nature of TinyNAS means it can generate compact neural networks with the best possible performance for a given microcontroller — with no unnecessary parameters. “Then we deliver the final, efficient model to the microcontroller,” say Lin.
To run that tiny neural network, a microcontroller also needs a lean inference engine. A typical inference engine carries some dead weight — instructions for tasks it may rarely run. The extra code poses no problem for a laptop or smartphone, but it could easily overwhelm a microcontroller. “It doesn’t have off-chip memory, and it doesn’t have a disk,” says Han. “Everything put together is just one megabyte of flash, so we have to really carefully manage such a small resource.” Cue TinyEngine.
The researchers developed their inference engine in conjunction with TinyNAS. TinyEngine generates the essential code necessary to run TinyNAS’ customized neural network. Any deadweight code is discarded, which cuts down on compile-time. “We keep only what we need,” says Han. “And since we designed the neural network, we know exactly what we need. That’s the advantage of system-algorithm codesign.” In the group’s tests of TinyEngine, the size of the compiled binary code was between 1.9 and five times smaller than comparable microcontroller inference engines from Google and ARM. TinyEngine also contains innovations that reduce runtime, including in-place depth-wise convolution, which cuts peak memory usage nearly in half. After codesigning TinyNAS and TinyEngine, Han’s team put MCUNet to the test.
MCUNet’s first challenge was image classification. The researchers used the ImageNet database to train the system with labeled images, then to test its ability to classify novel ones. On a commercial microcontroller they tested, MCUNet successfully classified 70.7 percent of the novel images — the previous state-of-the-art neural network and inference engine combo was just 54 percent accurate. “Even a 1 percent improvement is considered significant,” says Lin. “So this is a giant leap for microcontroller settings.”
The team found similar results in ImageNet tests of three other microcontrollers. And on both speed and accuracy, MCUNet beat the competition for audio and visual “wake-word” tasks, where a user initiates an interaction with a computer using vocal cues (think: “Hey, Siri”) or simply by entering a room. The experiments highlight MCUNet’s adaptability to numerous applications.







Play video






“Huge potential”
The promising test results give Han hope that it will become the new industry standard for microcontrollers. “It has huge potential,” he says.
The advance “extends the frontier of deep neural network design even farther into the computational domain of small energy-efficient microcontrollers,” says Kurt Keutzer, a computer scientist at the University of California at Berkeley, who was not involved in the work. He adds that MCUNet could “bring intelligent computer-vision capabilities to even the simplest kitchen appliances, or enable more intelligent motion sensors.”
MCUNet could also make IoT devices more secure. “A key advantage is preserving privacy,” says Han. “You don’t need to transmit the data to the cloud.”
Analyzing data locally reduces the risk of personal information being stolen — including personal health data. Han envisions smart watches with MCUNet that don’t just sense users’ heartbeat, blood pressure, and oxygen levels, but also analyze and help them understand that information. MCUNet could also bring deep learning to IoT devices in vehicles and rural areas with limited internet access.
Plus, MCUNet’s slim computing footprint translates into a slim carbon footprint. “Our big dream is for green AI,” says Han, adding that training a large neural network can burn carbon equivalent to the lifetime emissions of five cars. MCUNet on a microcontroller would require a small fraction of that energy. “Our end goal is to enable efficient, tiny AI with less computational resources, less human resources, and less data,” says Han.


",System brings deep learning to “internet of things” devices,2020-11-13,['Daniel Ackerman'],Artificial intelligence/Machine learning/internet of things/Algorithms/Electrical Engineering & Computer Science (eecs)/MIT Schwarzman College of Computing/School of Engineering,"['microcontrollers', 'things', 'devices', 'learning', 'tinynas', 'microcontroller', 'lin', 'network', 'deep', 'system', 'run', 'brings', 'tinyengine', 'internet', 'neural']","Deep learning is everywhere.
Soon, deep learning could also check your vitals or set your thermostat.
MIT researchers have developed a system that could bring deep learning neural networks to new — and much smaller — places, like the tiny computer chips in wearable medical devices, household appliances, and the 250 billion other objects that constitute the “internet of things” (IoT).
The system, called MCUNet, designs compact neural networks that deliver unprecedented speed and accuracy for deep learning on IoT devices, despite limited memory and processing power.
So pattern-recognition tasks like deep learning are difficult to run locally on IoT devices.",Mit
53,https://news.mit.edu/2020/staying-ahead-ai-curve-renzo-zagni-1109,"


In August, the young artificial intelligence process automation company Intelenz, Inc. announced its first U.S. patent, an AI-enabled software-as-a-service application for automating repetitive activities, improving process execution, and reducing operating costs. For company co-founder Renzo Zagni, the patent is a powerful testament to the value of his MIT educational experience.
Over the course of his two-decade career at Oracle, Zagni worked his way from database administrator to vice president of Enterprise Applications-IT. After spending seven years in his final role, he was ready to take on a new challenge by starting his own company.
From employee to entrepreneur 
Zagni launched Intelenz in 2017 with a goal of keeping his company on the cutting edge. Doing so required that he stay up to date on the latest machine learning knowledge and techniques. At first, that meant exploring new concepts on his own. But to get to the next level, he realized he needed a little more formal education. That’s when he turned to MIT.
“When I discovered that I could take courses at MIT, I thought, ‘What better place to learn about artificial intelligence and machine learning?’” he says. “Access to MIT faculty was something that I simply couldn’t pass up.”
Zagni enrolled in MIT Professional Education’s Professional Certificate Program in Machine Learning and Artificial Intelligence, traveling from California to Cambridge, Massachusetts, to attend accelerated courses on the MIT campus.
As he continued to build his startup, one key to demystifying machine learning came from MIT Professor Regina Barzilay, a Delta Electronics professor in the Department of Electrical Engineering and Computer Science and a member of MIT’s Computer Science and Artificial Intelligence Laboratory. “Professor Barzilay used real-life examples in a way that helped us quickly understand very complex concepts behind machine learning and AI,” Zagni says. “And her passion and vision to use the power of machine learning to help win the fight against cancer was commendable and inspired us all.”  
The insights Zagni gained from Barzilay and other machine learning/AI faculty members helped him shape Intelenz’ early products — and continue to influence his company’s product development today — most recently, in his patented technology, the ""Service Tickets Early Warning System.” The technology is an important representation of Intelenz’ ability to develop AI models aimed at automating and improving business processes at the enterprise level.
“We had a problem we wanted to solve and knew that artificial intelligence and machine learning could possibly address it. And MIT gave me the tools and the methodologies to translate these needs into a machine learning model that ended up becoming a patent,” Zagni says.
Driving machine learning with innovation
As an entrepreneur looking to push the boundaries of information technology, Zagni wasn’t content to simply use existing solutions; innovation became a key goal very early in the process.
“For professionals like me who work in information technology, innovation and artificial intelligence go hand-in-hand,” Zagni says.
While completing machine learning courses at MIT, Zagni simultaneously enrolled in MIT Professional Education’s Professional Certificate Program in Innovation and Technology. Combining his new AI knowledge with the latest approaches in innovation was a game-changer.
“During my first year with MIT, I was putting together the Intelenz team, hiring developers, and completing designs. What I learned in the innovation courses helped us a lot,” Zagni says. “For instance, Blake Kotelly‘s Mastering Innovation and Design Thinking course made a huge difference in how we develop our solutions and engage our customers. And our customers love the design-thinking approach.”
Looking forward
While his progress at Intelenz is exciting, Zagni is anything but done. As he continues to develop his organization and its AI-enabled offerings, he’s looking ahead to additional opportunities for growth.  
“We’re already looking for the next technology that is going to allow us to disrupt the market,” Zagni says. “We’re hearing a lot about quantum computing and other technology innovations. It’s very important for us to stay on top of them if we want to remain competitive.”
He remains committed to lifelong learning, and says he will definitely be looking to future MIT courses — and he recommends other professionals in his field do the same.
“Being part of the MIT ecosystem has really put me ahead of the curve by providing access to the latest information, tools, and methodologies,” Zagni says. “And on top of that, the faculty are very helpful and truly want to see participants succeed.”


",Staying ahead of the artificial intelligence curve with help from MIT,2020-11-09,[],School of Engineering/MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/MIT Professional Education/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Machine learning/Artificial intelligence/Innovation and Entrepreneurship (I&E)/Leadership,"['technology', 'innovation', 'staying', 'curve', 'intelenz', 'learning', 'courses', 'ahead', 'help', 'artificial', 'machine', 'intelligence', 'mit', 'zagni']","Doing so required that he stay up to date on the latest machine learning knowledge and techniques.
“When I discovered that I could take courses at MIT, I thought, ‘What better place to learn about artificial intelligence and machine learning?’” he says.
“We had a problem we wanted to solve and knew that artificial intelligence and machine learning could possibly address it.
“For professionals like me who work in information technology, innovation and artificial intelligence go hand-in-hand,” Zagni says.
While completing machine learning courses at MIT, Zagni simultaneously enrolled in MIT Professional Education’s Professional Certificate Program in Innovation and Technology.",Mit
54,https://news.mit.edu/2020/algorithm-reduces-use-of-riskier-antibiotics-utis-1105,"


One paradox about antibiotics is that, broadly speaking, the more we use them, the less they continue to work. The Darwinian process of bacteria growing resistant to antibiotics means that, when the drugs don't work, we can no longer treat infections, leading to groups like the World Health Organization warning about our ability to control major public health threats.
Because of its ubiquity, one topic that’s particularly concerning is urinary tract infections (UTIs), which affect half of all women and add almost $4 billion a year in unnecessary health-care costs. Doctors often treat UTIs using antibiotics called fluoroquinolones that are inexpensive and generally effective. However, they have also been found to put women at risk of becoming infected with other difficult-to-treat bacteria, such as C. difficile and certain species of Staphylococcus, and also to increase their risk of tendon injuries and life-threatening conditions like aortic tears. 
As a result of this, medical associations have issued guidelines recommending fluoroquinolones as “second-line treatments” that should only be used on a patient when other antibiotics are ineffective or have adverse reactions. All the while, doctors with limited time and resources continue to prescribe them at high rates. 
A team led by MIT scientists believes that this conundrum opens up an opportunity for a data-driven tool that could help doctors make safer, more customized decisions for patients. 
In a new paper, the researchers present a recommendation algorithm that predicts the probability that a patient’s UTI can be treated by first- or second-line antibiotics. With this information, the model then makes a recommendation for a specific treatment that selects a first-line agent as frequently as possible, without leading to an excess of treatment failures.  
The team showed that the model would allow clinicians to reduce the use of second-line antibiotics 67 percent. For patients where clinicians chose a second-line drug but the algorithm chose a first-line drug, the first-line drug ended up working more than 90 percent of the time. When clinicians chose an inappropriate first-line drug, the algorithm chose an appropriate first-line drug almost half of the time. 
MIT Professor David Sontag says that a system like this could be used when a patient comes into the emergency room or their primary physician’s office with a suspected UTI. Even when the infection is confirmed, the specific bacterium is still unknown, making it difficult to choose a treatment plan. That’s where the algorithm comes in, and makes a suggestion using electronic health record (EHR) data from more than 10,000 patients from Brigham & Women’s Hospital (BWH) and Massachusetts General Hospital (MGH). 
Presented in an article appearing this week in the Science Translational Medicine journal, the team’s system features a thresholding algorithm that the team hopes will be intuitive for clinicians to apply to a wide range of drugs that all face a similar dilemma: how to balance the need for an effective treatment with the desire to minimize the use of second-line antibiotics. They also structured their model to be directly embedded into the EHR, eliminating unnecessary steps and additional workflows.
As an example of how the threshold algorithm works, UTI treatments are extremely unlikely to lead to life-threatening side effects, so a doctor might set the threshold treatment failure at a relatively high number like 10 percent. In contrast, treatments for certain bloodstream infections have a much higher risk of death, so in those cases a doctor would likely set the treatment failure much lower, such as 1 percent. (Even at such a low threshold for failure, the researchers say the algorithm could lead to additional improvements, but that requires further study.)
The project is part of a larger wave of machine learning models that have been used to predict antibiotic resistance in infectious syndromes such as bloodstream infections and using pathogen genomic data. While many of these approaches provide new clinical information, most of them haven’t been widely adopted due to their lack of interpretability, difficulty integrating into clinical workflows, and absence of evidence proving that they work in actual hospital settings. 
“What’s exciting about this research is that it presents a blueprint for the right way to do retrospective evaluation,” says Sontag, a professor in the MIT Department of Electrical Engineering and Computer Science. “We do this by showing that one can do an apples-to-apples comparison within the existing clinical practice. When we say we can reduce second-line antibiotic use and inappropriate treatment by certain percentages, we have confidence in those numbers relative to clinicians.” 
“With this algorithm, we can actually ask the doctor what specific probability of treatment failure they’re willing to risk in order to reduce the use of second-line drugs by a certain amount,” adds Sanjat Kanjilal, a Harvard Medical School lecturer, infectious diseases physician, and associate medical director of microbiology at the BWH. Kanjilal and Sontag co-wrote the new paper with researchers at Carnegie Mellon University and MGH.
The team is quick to point out that they haven’t tested their algorithm on more complicated forms of UTIs that involve pre-existing conditions, and that the ultimate proof of utility can only be assessed using a randomized controlled trial. However, they say that the vast majority of UTI cases are compatible with the system. 
Moving forward, Sontag says that future efforts will focus on doing a randomized controlled trial comparing usual practice to algorithm-supported decisions. They also plan to increase the diversity of their sample size to improve recommendations across race, ethnicity, socioeconomic status, and more complex health backgrounds. 
Sontag and Kanjilal co-wrote the paper with MIT graduate student Michael Oberst, MIT undergraduate Sooraj Boominathan, Carnegie Mellon PhD student Helen Zhou, and David C. Hooper, chief of MGH’s infection control unit. Sontag has a dual affiliation with both the Computer Science and Artificial Intelligence Laboratory and the Institute for Medical Engineering and Science.
The project was supported, in part, by the MGH-MIT Grand Challenges Award, a Harvard Catalyst grant, and a National Science Foundation CAREER award.


",Algorithm reduces use of riskier antibiotics for UTIs,2020-11-05,"['Adam Conner-Simons', 'Rachel Gordon']",Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Institute for Medical Engineering and Science (IMES)/Computer science and technology/Algorithms/Research/Medicine/Health care/Artificial intelligence/Machine learning/Antibiotics/Imaging/MIT Schwarzman College of Computing/School of Engineering,"['drug', 'antibiotics', 'algorithm', 'treatment', 'using', 'riskier', 'reduces', 'secondline', 'sontag', 'firstline', 'utis', 'uti', 'team']","Doctors often treat UTIs using antibiotics called fluoroquinolones that are inexpensive and generally effective.
In a new paper, the researchers present a recommendation algorithm that predicts the probability that a patient’s UTI can be treated by first- or second-line antibiotics.
The team showed that the model would allow clinicians to reduce the use of second-line antibiotics 67 percent.
For patients where clinicians chose a second-line drug but the algorithm chose a first-line drug, the first-line drug ended up working more than 90 percent of the time.
When clinicians chose an inappropriate first-line drug, the algorithm chose an appropriate first-line drug almost half of the time.",Mit
55,https://news.mit.edu/2020/new-world-warcraft-erik-lin-greenberg-1103,"


In the past decade, high tech tools have proliferated in the world’s fighting forces. At least 80 nations can now deploy remote-controlled drones. Will the widespread use of digitally enhanced arsenals prove a destabilizing, if not destructive, element in the complex struggles among states? Not necessarily, argues assistant professor of political science Erik Lin-Greenberg ’09, SM ’09.
“I’ve learned that in some circumstances, remote war-fighting technologies such as drones can lead to a ratcheting down of tensions, and that it might even serve strategically as a way to de-escalate global crises,” he says.
Lin-Greenberg, who joined MIT in July, is investigating how new technologies affect military decision-making and the use of force. He wants to understand, for instance, whether allies employing different technologies face obstacles working together, or if the high-speed tempo of military operations executed by artificial intelligence might prove challenging to officers accustomed to making decisions in longer time frames. Pinning down answers to these questions is a matter of some urgency.
“Given the massive amount of technological innovation under way, it’s clear that this is the direction war is moving,” says Lin-Greenberg. “I’m trying to figure out how the systems that are becoming dominant will shape both interactions between states and their leaders’ decisions — factors that may determine whether there’s war or peace.”
Managing intelligence
The impetus for Lin-Greenberg’s scholarly inquiry came from his own experiences as an active duty U.S. Air Force officer, when he was engaged in a series of critical military intelligence roles. He was, for instance, responsible for the deployment of intelligence assets and personnel in Kandahar, Afghanistan. He also led a team of analysts providing real-time support to national agencies and coalition combat forces during surveillance and reconnaissance operations, and, as a political-military affairs analyst, provided intelligence insights to senior Air Force leaders. And today, as a major in the Air Force Reserve, Lin-Greenberg supports strategy and policy work at the Joint Staff, the headquarters team of the Pentagon.
On his first active duty assignment, after nine months of intelligence training, Lin-Greenberg was at a ground station supporting remotely piloted drone sorties. “It was really the height of operations in Afghanistan, before troop withdrawal,” he recalls. “We carried out 50 to 60 combat air patrols of remote aircraft to surveil and execute airstrikes — drones were seen as an easy fix to everything.”
But while these remotely piloted drones performed effectively in uncontested airspace in Afghanistan, Lin-Greenberg began thinking about how they might function in other contexts. “I wondered how we would respond when a more advanced adversary shot one of them down,” he recalls. “I wanted to know what these new systems meant for the future of warfare after Afghanistan.”
These big-picture questions increasingly preoccupied Lin-Greenberg as he progressed up the ladder in intelligence work. “When I sat in meetings discussing AI and big data, people didn’t always comprehend how these technologies played out in an operational setting,” he says. “This motivated my research as an academic.”

From battlefield to classroom
Lin-Greenberg’s fascination for these topics was deep enough that he left active duty in order to study political science at Columbia University. There, he turned his attention to the ways in which remotely operated weapons systems alter the risk calculus in military operations.
“Political science theory suggested that all these tools that make it easier to harm the enemy without putting your own troops in the line of fire would make offensive actions easier,” says Lin-Greenberg. But his research has revealed some contrary evidence.
In June 2019, for example, when Iran sought to signal its displeasure with U.S. activity in the Persian Gulf, it shot down an unpiloted drone rather than a piloted reconnaissance aircraft. In response, the Trump administration announced that there would be no lethal retaliation, since no U.S. lives were lost. In a piece for Foreign Policy describing this and similar episodes, Lin-Greenberg argued that use of pilotless weapons might create “zones of escalation” that avoid riskier, more destructive conflict. “They might actually have a stabilizing effect,” he says.
Lin-Greenberg bolsters his research, often constructed as case studies, with observational data from archives, and with the use of innovative war-gaming techniques. In a recent project, for instance, he created a simulation where military officers participating in the game responded either to the shoot-down of a drone or an attack by an adversary. As research assistants jotted down notes in real time, these military personnel revealed what they might actually do given the different scenarios.
With regional antagonists such as Pakistan and India building up their technological capabilities in uncrewed weaponry, threatening to ignite larger conflicts, Lin-Greenberg hopes to get his insights about the use of such systems in front of policymakers on a timely basis. In addition to his Joint Staff advising, he writes regularly for the foreign policy-based web platform War on the Rocks and for a variety of journals. He also has a book project in progress, “Remote Controlled Escalation: Drones, Cyber Warfare, and Interstate Crises,” which is based on his dissertation.
Transformed by 9/11
His deep commitment to national defense, as both a scholar and service member, was triggered by the 9/11 attacks. A first-year student in a New Jersey town right outside of New York, Lin-Greenberg watched fighter jets buzzing overhead. “Walking home that day, the world seemed uncertain, but I looked up and felt at least some degree of safety — folks were trying to defend the country.”
Soon he began exploring military careers, and settled on the Air Force, gravitating toward the intelligence branches: “Maybe I watched too many James Bond movies in high school, but I definitely read a lot about the Cold War and history,” says Lin-Greenberg. But rather than attend a service academy, he chose MIT, under the Air Force ROTC program.
A political science major, Lin-Greenberg credits classes with such professors as Stephen Van Evera, Fotini Christia, Barry Posen, and M. Taylor Fravel for giving him a grounding in security studies, U.S. and Chinese foreign policy, and regional conflicts, which provided an essential intellectual foundation for his military career.
Now back at MIT and teaching with his former mentors, Lin-Greenberg says he is “humbled, excited, and a bit intimidated.” This fall, he is teaching the graduate class 17.432 (Causes of War), drawing in part on his own military history, while pursuing new research avenues. One area of interest: developing international agreements on the use of AI, similar to the law of the sea and law of war. “Having guidelines on automated systems, a set of best practices, would be incredibly useful to decrease inadvertent escalations, and to tell states how to interact when something goes awry,” he says.
Although his return to Cambridge, Massachusetts, comes in the midst of a pandemic, Lin-Greenberg has managed to rediscover old and beloved haunts, as he settles in to remote teaching and research. “I did a Mike’s Pastry run days after I got here, and I am gorging myself on food in the North End,” he says.


",A new world of warcraft,2020-11-03,['Leda Zimmerman'],School of Humanities Arts and Social Sciences/Political science/ROTC/Center for International Studies/Security studies and military/Drones/Faculty/Afghanistan/Classes and programs/Profile,"['research', 'systems', 'war', 'air', 'technologies', 'military', 'warcraft', 'world', 'force', 'lingreenberg', 'intelligence', 'drones']","Lin-Greenberg, who joined MIT in July, is investigating how new technologies affect military decision-making and the use of force.
“Given the massive amount of technological innovation under way, it’s clear that this is the direction war is moving,” says Lin-Greenberg.
And today, as a major in the Air Force Reserve, Lin-Greenberg supports strategy and policy work at the Joint Staff, the headquarters team of the Pentagon.
On his first active duty assignment, after nine months of intelligence training, Lin-Greenberg was at a ground station supporting remotely piloted drone sorties.
But rather than attend a service academy, he chose MIT, under the Air Force ROTC program.",Mit
56,https://news.mit.edu/2020/3-questions-sanjay-sarma-bill-bonvillian-applying-new-tech-workforce-education-1102,"


Sanjay Sarma, MIT vice president for MIT Open Learning and the Fred and Daniel Fort Flowers Professor of Mechanical Engineering, and William B. Bonvillian, lecturer in the MIT Program in Science, Technology, and Society and former director of MIT’s Washington D.C. office, recently produced a new research brief, “Applying New Education Technologies to Meet Workforce Education Needs.”

The publication, part of the MIT Task Force on the Work of the Future’s series of research briefs, asks: What lessons from learning science and new technologies might be drawn to make online education, including workforce training, more effective? The brief argues that the U.S. needs to make workforce education a policy priority, to upgrade skills for those being left behind, and to help others shift job sectors to areas where there will be work. Internet-based tools will be critical in enabling workforce education to meet growing needs. However, the brief argues that workforce education will not be able to scale unless it provides quality training, and to do this effectively, online training must incorporate lessons from the best science-based teaching practices.

Sarma and Bonvillian have been leading a research project on workforce education. A preliminary report on that project is available via MIT Open Learning, and a final report will be published as a book by MIT Press in January 2021.  

Q: Why is it especially important now to make workforce education a policy priority?   
Sarma: Workforce education programs in the U.S. are trying to make up for an education system that does not serve the needs of all. They are an educational safety net system for those being left behind, but it’s not that much of a fallback. Our education system has an early 20th-century design that winnows out the “capable” from the rest, in an archaic sense, and in the process locks in some fundamental imbalances. Our workforce education system needs to reduce the difference and improve the imbalance, but that will take rethinking and redesign.

Bonvillian: To complicate the problem, our workforce has been upskilling, with rote jobs in decline and jobs that require more skills increasing. We are a long way from the robots seizing our jobs, but there is an ongoing gradual shift with the introduction of IT and related technologies into the workplace, as the interim report of the Work of the Future Task Force set out. So, there’s a big job ahead in educating for new skills. On top of that, over 50 million have filed for unemployment since the coronavirus began, and when it lets up we will need to face a massive resorting of our workforce. Covid-19 has accelerated the decline of the in-person retail sector, which employed some 15 million in 2019. Restaurant, hospitality, and travel sectors have been slammed as well, and many of these jobs will not return. After World War II, the country had to cope with the return of 16 million soldiers and sailors, and in response passed one of the most important pieces of social legislation in our history, the GI Bill. It brought new education, and with it a route into the middle class to millions. The redeployment problem after Covid-19 may be larger and may require equally creative programs for workforce education to bring new skills to the millions who will need them.

Q: Your brief describes the current gaps in workforce education, from under-investment to a disconnect between the still-separate worlds of work and learning. Can you describe some of these challenges?

Bonvillian: The gaps are many. There has been disinvestment in recent decades by both government and employers in workforce education, and our federal government programs have real limits. The Department of Labor’s training programs don’t reach the oncoming higher technical skills or help incumbent workers acquire them. In turn, the Department of Education’s programs focus on college, not workforce education, and don’t mesh with the labor programs. We have a vocational education system in high schools that has largely been dismantled, and have underfunded community colleges that lack the resources to provide advanced training in emerging fields. Most colleges and universities don’t see workforce education as their problem so aren’t linked to the other participants in the system. Overall, the education system is disconnected from the workplace, and a system for lifelong learning is missing. In a period of growing economic inequality in our society, we need to fix these problems.

Sarma: Let me add one more gap — the gap in scale. The need is growing, but the existing workforce education system operates at too small a scale to meet these needs. The system needs not only reforms, but also the ability to reach many more, more effectively. Online is a new tool that can help with the scale-up — if we apply it right.

Q: What are the key lessons from learning science and new technologies that could make online education and workforce training more effective? 

Sarma: That’s the core subject of this brief. Online education has been evolving for many years — but with the widespread availability of broadband technology through laptops and cell phones, it reached an inflection point in 2012 with the development of MOOCs [massive open online classes], and the establishment of edX by MIT and Harvard. EdX has now reached over 100 million enrolled users. Online has had limited use so far in training, but could be a new workforce education tool. But if online simply tries to replicate current classroom practices, which have had only limited change over the decades, and apply them to workforce needs, it won’t work well. With the new education technology, we need new pedagogical practices. If we collect together lessons from the fields of cognitive science and education, there is a new science of learning emerging. What are some of those lessons? We need to provide learning in segments, in 10-minute chunks rather than in lengthier lectures by talking heads, and we need to use “spaced practice” so that learning occurs and is reiterated over a period of weeks and months — it’s no longer studying for the single exam. Researchers have found that learning occurs best when the student has to struggle a bit — but not too much — to acquire it. This means building “desirable difficulties” into learning to challenge us. We need to use frequent low-stakes testing, with periodic assessment and feedback. And rather than teaching content areas in single, separate blocks, we need to “interleave” the subjects, mixing fields, moving from one to another and back again.
All of these ideas can be systematically introduced into both online courses and in-person workforce training modules. Online learning can also include computer gaming and simulations that encourage more creative problem-solving as a learning task evolves. And online can offer collaborative tools where fellow learners help each other by providing coaching and practice. Hands-on, active approaches to learning can provide benefits, and can be furthered by simulations, software prototyping, and incorporating new technologies like virtual and augmented reality into online offerings. These advances will be particularly critical in providing workforce skills, which must emphasize hands-on aspects. Artificial intelligence is only starting to evolve in education, but potentially can be applied to enable digital tutors and personalized coaching tools. 
To summarize, improved workforce education looks like it will be critical for a better society, and we have a new set of tools we can apply. We hope MIT can play a role in validating this new tool set.


",3 Questions: Sanjay Sarma and Bill Bonvillian on new technologies in workforce education,2020-11-02,[],"School of Engineering/School of Humanities Arts and Social Sciences/Mechanical engineering/Program in STS/online learning/Open access/EdX/Education, teaching, academics/3 Questions/Covid-19/Pandemic/Government/Jobs","['bonvillian', 'bill', 'technologies', 'need', 'learning', 'sanjay', 'questions', 'needs', 'skills', 'sarma', 'workforce', 'online', 'system', 'training', 'mit', 'education']","Sarma and Bonvillian have been leading a research project on workforce education.
Q: Why is it especially important now to make workforce education a policy priority?
Sarma: Workforce education programs in the U.S. are trying to make up for an education system that does not serve the needs of all.
Our workforce education system needs to reduce the difference and improve the imbalance, but that will take rethinking and redesign.
In turn, the Department of Education’s programs focus on college, not workforce education, and don’t mesh with the labor programs.",Mit
57,https://news.mit.edu/2020/covid-19-cough-cellphone-detection-1029,"


Asymptomatic people who are infected with Covid-19 exhibit, by definition, no discernible physical symptoms of the disease. They are thus less likely to seek out testing for the virus, and could unknowingly spread the infection to others.
But it seems those who are asymptomatic may not be entirely free of changes wrought by the virus. MIT researchers have now found that people who are asymptomatic may differ from healthy individuals in the way that they cough. These differences are not decipherable to the human ear. But it turns out that they can be picked up by artificial intelligence.
In a paper published recently in the IEEE Journal of Engineering in Medicine and Biology, the team reports on an AI model that distinguishes asymptomatic people from healthy individuals through forced-cough recordings, which people voluntarily submitted through web browsers and devices such as cellphones and laptops.
The researchers trained the model on tens of thousands of samples of coughs, as well as spoken words. When they fed the model new cough recordings, it accurately identified 98.5 percent of coughs from people who were confirmed to have Covid-19, including 100 percent of coughs from asymptomatics — who reported they did not have symptoms but had tested positive for the virus.
The team is working on incorporating the model into a user-friendly app, which if FDA-approved and adopted on a large scale could potentially be a free, convenient, noninvasive prescreening tool to identify people who are likely to be asymptomatic for Covid-19. A user could log in daily, cough into their phone, and instantly get information on whether they might be infected and therefore should confirm with a formal test.
“The effective implementation of this group diagnostic tool could diminish the spread of the pandemic if everyone uses it before going to a classroom, a factory, or a restaurant,” says co-author Brian Subirana, a research scientist in MIT’s Auto-ID Laboratory.
Subirana’s co-authors are Jordi Laguarta and Ferran Hueto, of MIT’s Auto-ID Laboratory.

 New AI model detects asymptomatic Covid-19 infections through device-recorded coughs
Vocal sentiments
Prior to the pandemic’s onset, research groups already had been training algorithms on cellphone recordings of coughs to accurately diagnose conditions such as pneumonia and asthma. In similar fashion, the MIT team was developing AI models to analyze forced-cough recordings to see if they could detect signs of Alzheimer’s, a disease associated with not only memory decline but also neuromuscular degradation such as weakened vocal cords.
They first trained a general machine-learning algorithm, or neural network, known as ResNet50, to discriminate sounds associated with different degrees of vocal cord strength. Studies have shown that the quality of the sound “mmmm” can be an indication of how weak or strong a person’s vocal cords are. Subirana trained the neural network on an audiobook dataset with more than 1,000 hours of speech, to pick out the word “them” from other words like “the” and “then.”
The team trained a second neural network to distinguish emotional states evident in speech, because Alzheimer’s patients — and people with neurological decline more generally — have been shown to display certain sentiments such as frustration, or having a flat affect, more frequently than they express happiness or calm. The researchers developed a sentiment speech classifier model by training it on a large dataset of actors intonating emotional states, such as neutral, calm, happy, and sad.
The researchers then trained a third neural network on a database of coughs in order to discern changes in lung and respiratory performance.
Finally, the team combined all three models, and overlaid an algorithm to detect muscular degradation. The algorithm does so by essentially simulating an audio mask, or layer of noise, and distinguishing strong coughs — those that can be heard over the noise — over weaker ones.
With their new AI framework, the team fed in audio recordings, including of Alzheimer’s patients, and found it could identify the Alzheimer’s samples better than existing models. The results showed that, together, vocal cord strength, sentiment, lung and respiratory performance, and muscular degradation were effective biomarkers for diagnosing the disease.
When the coronavirus pandemic began to unfold, Subirana wondered whether their AI framework for Alzheimer’s might also work for diagnosing Covid-19, as there was growing evidence that infected patients experienced some similar neurological symptoms such as temporary neuromuscular impairment.
“The sounds of talking and coughing are both influenced by the vocal cords and surrounding organs. This means that when you talk, part of your talking is like coughing, and vice versa. It also means that things we easily derive from fluent speech, AI can pick up simply from coughs, including things like the person’s gender, mother tongue, or even emotional state. There’s in fact sentiment embedded in how you cough,” Subirana says. “So we thought, why don’t we try these Alzheimer’s biomarkers [to see if they’re relevant] for Covid.”
“A striking similarity”
In April, the team set out to collect as many recordings of coughs as they could, including those from Covid-19 patients. They established a website where people can record a series of coughs, through a cellphone or other web-enabled device. Participants also fill out a survey of symptoms they are experiencing, whether or not they have Covid-19, and whether they were diagnosed through an official test, by a doctor’s assessment of their symptoms, or if they self-diagnosed. They also can note their gender, geographical location, and native language.
To date, the researchers have collected more than 70,000 recordings, each containing several coughs, amounting to some 200,000 forced-cough audio samples, which Subirana says is “the largest research cough dataset that we know of.” Around 2,500 recordings were submitted by people who were confirmed to have Covid-19, including those who were asymptomatic.
The team used the 2,500 Covid-associated recordings, along with 2,500 more recordings that they randomly selected from the collection to balance the dataset. They used 4,000 of these samples to train the AI model. The remaining 1,000 recordings were then fed into the model to see if it could accurately discern coughs from Covid patients versus healthy individuals.
Surprisingly, as the researchers write in their paper, their efforts have revealed “a striking similarity between Alzheimer’s and Covid discrimination.”
Without much tweaking within the AI framework originally meant for Alzheimer’s, they found it was able to pick up patterns in the four biomarkers — vocal cord strength, sentiment, lung and respiratory performance, and muscular degradation — that are specific to Covid-19. The model identified 98.5 percent of coughs from people confirmed with Covid-19, and of those, it accurately detected all of the asymptomatic coughs.
“We think this shows that the way you produce sound, changes when you have Covid, even if you’re asymptomatic,” Subirana says.
Asymptomatic symptoms
The AI model, Subirana stresses, is not meant to diagnose symptomatic people, as far as whether their symptoms are due to Covid-19 or other conditions like flu or asthma. The tool’s strength lies in its ability to discern asymptomatic coughs from healthy coughs.  
The team is working with a company to develop a free pre-screening app based on their AI model. They are also partnerning with several hospitals around the world to collect a larger, more diverse set of cough recordings, which will help to train and strengthen the model’s accuracy.
As they propose in their paper, “Pandemics could be a thing of the past if pre-screening tools are always on in the background and constantly improved.”
Ultimately, they envision that audio AI models like the one they’ve developed may be incorporated into smart speakers and other listening devices so that people can conveniently get an initial assessment of their disease risk, perhaps on a daily basis.
This research was supported, in part, by Takeda Pharmaceutical Company Limited.


",Artificial intelligence model detects asymptomatic Covid-19 infections through cellphone-recorded coughs,2020-10-29,['Jennifer Chu'],Covid-19/Computer modeling/Artificial intelligence/Sensors/Health/Medicine/Crowdsourcing/Mechanical engineering/School of Engineering/Machine learning/Pandemic/Public health,"['infections', 'asymptomatic', 'model', 'subirana', 'coughs', 'recordings', 'cellphonerecorded', 'ai', 'vocal', 'detects', 'artificial', 'intelligence', 'covid19', 'alzheimers', 'team']","They used 4,000 of these samples to train the AI model.
The model identified 98.5 percent of coughs from people confirmed with Covid-19, and of those, it accurately detected all of the asymptomatic coughs.
“We think this shows that the way you produce sound, changes when you have Covid, even if you’re asymptomatic,” Subirana says.
The tool’s strength lies in its ability to discern asymptomatic coughs from healthy coughs.
The team is working with a company to develop a free pre-screening app based on their AI model.",Mit
58,https://news.mit.edu/2020/ai-cures-data-driven-clinical-solutions-covid-19-1027,"


Modern health care has been reinvigorated by the widespread adoption of artificial intelligence. From speeding image analysis for radiology to advancing precision medicine for personalized care, AI has countless applications, but can it rise to the challenge in the fight against Covid-19?
Researchers from the Abdul Latif Jameel Clinic for Machine Learning in Health (Jameel Clinic), now housed within the MIT Stephen A. Schwarzman College of Computing, say the ongoing public health crisis provides ample opportunities for leveraging AI technologies, such as accelerating the search for effective therapeutics and drugs that can treat the disease, and are actively working to translate this potential to success.
AI Cures
When Covid-19 began to spread worldwide, Jameel Clinic’s community of machine learning and life science researchers redirected their work and began exploring how they can collaborate on the search for solutions by tapping into their collective knowledge and expertise. The ensuing discussions led to the launch of AI Cures, an initiative dedicated to developing machine learning methods for finding promising antiviral molecules for Covid-19 and other emerging pathogens, and to lower the barrier for people from varied backgrounds to get involved by inviting them to contribute to the effort.
As part of the mission of AI Cures to have broad impact and engagement, Jameel Clinic brought together researchers, clinicians, and public health specialists for a conference focused on the development of AI algorithms for the clinical management of Covid-19 patients, early detection and monitoring of the disease, preventing future outbreaks, and ways in which these technologies have been utilized in patient care.
Data-driven clinical solutions
On Sept. 29, over 650 people representing 50 countries and 70 organizations logged on from around the globe for the virtual AI Cures Conference: Data-driven Clinical Solutions for Covid-19.
In welcoming the audience, Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing, remarked that “AI in health care is moving beyond the use of computing as just simple tools, to capabilities that really aid in the processes of discovery, diagnosis, and care. The potential for AI-accelerated discovery is particularly relevant in times such as these.”
Attendees heard from 14 other speakers, including MIT researchers, on technologies they developed over the past six months in response to the pandemic — from epidemiological models created using clinical data to predict the risk of both infection and death for individual patients, to a wireless device that allows doctors to monitor Covid-19 patients from a distance, to a machine learning model that pinpoints patients at risk for intubation before they crash.
James Collins, the Termeer Professor of Medical Engineering and Science in MIT’s Institute for Medical Engineering and Science (IMES) and Department of Biological Engineering, and faculty co-lead of life sciences for Jameel Clinic, gave the first talk of the day on harnessing synthetic biology to develop diagnostics to address Covid-19 and how his lab is using deep learning to enhance the design of such systems. Collins and his team are utilizing AI techniques to create a set of algorithms to effectively predict the efficacy of RNA-based sensors. The sensors, first developed in 2014 to detect the Ebola virus and later tailored for the Zika virus in 2016, were designed and optimized for a Covid-19 diagnostic, and related CRISPR-based biosensors are being used in a mask developed in Collins’ lab that produces a detectable signal when a person with the virus breathes, coughs, or sneezes.
While AI has proven to be an effective tool in health care, a model requires good data for it to be valuable and useful. With Covid-19 being a new disease, limited amounts of information are available to researchers, and in order to advance even more efforts to combat the virus, Collins notes that “we need to put in place and secure the resources to generate and collect large amounts of well-characterized data to train deep learning models. At present we generally don’t have such large datasets. In the system we developed, our dataset consists of about 91,000 RNA elements, which is currently the largest available for RNA synthetic biology, but it should be larger and expanded to many more different sensors.”
Offering perspective from the clinical side, Constance Lehman, a professor at Harvard Medical School (HMS), discussed the ways in which she’s implementing AI tools in her work as director of breast imaging at Massachusetts General Hospital (MGH). In collaboration with Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science and faculty co-lead of AI for Jameel Clinic, Lehman designs machine learning models to aid in breast cancer detection, which became a critical tool when mammography screenings were put on hold during the emergency stay-at-home-order issued in Massachusetts last March. By the time screenings reopened in May, around 15,000 mammograms had been cancelled. MGH is gradually rescheduling patients using a model developed by Lehman and Barzilay to help ease the process. “We took those women that had been diverted from screening and ranked them by their AI risk models and we reached out to them, inviting them back in.”
However, according to Lehman, many are choosing to opt out of screening and, in particular, fewer women of color are returning. “There are many determinants of who returns for screening. Social determinants can swamp all of our best, most scientific evidence-based approaches to effective and equitable health care. We’re delighted that our risk model is equally predictive across races, but I am dismayed to see that we are screening more white women than women of color during these times. Those are social determinants, which we are working very hard on.”
The conference culminated in a panel discussion with those who are at the front line of the pandemic. The panelists — Gabriella Antici, founder of the Protea Institute in Brazil; Rajesh Gandhi, a professor at HMS and an infectious disease physician at MGH; Guillermo Torre, a professor of cardiology and president of TEC Salud in Mexico; and Karen Wong, data science unit lead for the Covid-19 clinical team at the U.S. Centers for Disease Control and Prevention — shared their experiences in handling the crisis and had an open conversation with Barzilay, the panel’s moderator, on the limitations of AI and what is currently not being addressed.
“Those from the AI community like myself are always asking ourselves if we are solving the right problems,” says Barzilay. “We hope to come up with new ideas for AI solutions and what we can do in the future to help.”
Gandhi offered that “we need more refined and sophisticated approaches to deciding when to use different drugs and how to use them in combination.” He also suggested that integrating physiologic data could be useful in considering how to treat individual patients from different age ranges exhibiting a variety of Covid-19 symptoms, from mild to severe.
In her closing remarks, Barzilay expressed hope that the conference “illustrates the types of problems that we need to be addressing on the AI side” and notes that Jameel Clinic will widely share any new data they obtain so that everyone can benefit to help patients suffering from Covid-19.
The event was the first in a pair of conferences that took place as part of the AI Cures initiative. The next event, AI Cures Drug Discovery Conference, which will focus on cutting-edge AI approaches in this area developed by MIT researchers and their collaborators, will be held virtually on Oct. 30.
AI Cures: Data-driven Clinical Solutions was organized by Jameel Clinic, MIT Schwarzman College of Computing, and Institute for Medical Engineering and Sciences. Additional support was provided by the Patrick J. McGovern Foundation.


",AI Cures: data-driven clinical solutions for Covid-19,2020-10-27,['Terri Park'],MIT Schwarzman College of Computing/Biological engineering/Electrical Engineering & Computer Science (eecs)/Institute for Medical Engineering and Science (IMES)/J-Clinic/Artificial intelligence/Machine learning/Disease/Medicine/Health care/Health sciences and technology/Covid-19/Pandemic/Special events and guest speakers/Collaboration/Research/School of Engineering,"['data', 'covid19', 'jameel', 'learning', 'clinical', 'solutions', 'clinic', 'ai', 'cures', 'patients', 'health', 'datadriven', 'developed']","Data-driven clinical solutionsOn Sept. 29, over 650 people representing 50 countries and 70 organizations logged on from around the globe for the virtual AI Cures Conference: Data-driven Clinical Solutions for Covid-19.
“Those from the AI community like myself are always asking ourselves if we are solving the right problems,” says Barzilay.
The event was the first in a pair of conferences that took place as part of the AI Cures initiative.
The next event, AI Cures Drug Discovery Conference, which will focus on cutting-edge AI approaches in this area developed by MIT researchers and their collaborators, will be held virtually on Oct. 30.
AI Cures: Data-driven Clinical Solutions was organized by Jameel Clinic, MIT Schwarzman College of Computing, and Institute for Medical Engineering and Sciences.",Mit
59,https://news.mit.edu/2020/defextiles-leveraging-3d-printer-defect-to-create-quasi-textiles-1020,"


Sometimes 3D printers mess up. They extrude too much material, or too little, or deposit material in the wrong spot. But what if this bug could be turned into a (fashionable) feature?
Introducing DefeXtiles, a tulle-like textile that MIT Media Lab graduate student Jack Forman developed by controlling a common 3D printing defect — the under-extrusion of polymer filament.
Forman used a standard, inexpensive 3D printer to produce sheets and complex 3D geometries with a woven-like structure based on the “glob-stretch” pattern produced by under-extrusion. Forman has printed these flexible and thin sheets into an interactive lampshade, full-sized skirts, a roll of fabric long enough to stretch across a baseball diamond, and intricately patterned lace, among other items.










Forman, who works in the Tangible Media research group with Professor Hiroshi Ishii, presented and demonstrated the DefeXtiles research on Oct. 20 at the Association for Computing Machinery Symposium on User Interface Software and Technology. The material may prove immediately useful for prototyping and customizing in fashion design, Forman says, but future applications also could include 3D-printed surgical mesh with tunable mechanical properties, among other items.
“In general, what excites me most about this work is how immediately useful it can be to many makers,” Forman says. “Unlike previous work, the fact that no custom software or hardware is needed — just a relatively cheap $250 printer, the most common type of printer used — really makes this technique accessible to millions of people.”
“We envision that the materials of the future will be dynamic and computational,” says Ishii. “We call it ‘Radical Atoms.’ DefeXtiles is an excellent example of Radical Atoms, a programmable matter that emulates the properties of existing materials and goes beyond. We can touch, feel, wear, and print them.”
Joining Forman and Ishii on the project are Computer Science and Artificial Intelligence Laboratory and Department of Electrical Engineering and Computer Science graduate student Mustafa Doga Dogan, and Hamilton Forsythe, an MIT Department of Architecture undergraduate researcher.
Filaments to fabric
Forman had been experimenting with 3D printing during the media arts and sciences class MAS.863 / 4.140 / 6.943 (How to Make (Almost) Anything), led by Professor Neil Gershenfeld, director of the MIT Center for Bits and Atoms. Forman's experiments were inspired by the work of a friend from his undergraduate days at Carnegie Mellon University, who used under-extruded filament to produce vases. With his first attempts at under-extruding, ""I was annoyed because the defects produced were perfect and periodic,” he says, “but then when I started playing with it, bending it and even stretching it, I was like, ‘whoa, wait, this is a textile. It looks like it, feels likes it, bends like it, and it prints really quickly.”
“I brought a small sample to my class for show and tell, not really thinking much of it, and Professor Gershenfeld saw it and he was excited about it,” Forman adds.
When a 3D printer under-extrudes material, it produces periodic gaps in the deposited material. Using an inexpensive fused deposition modeling 3D printer, Forman developed an under-extruding process called “glob-stretch,” where globs of thermoplastic polymer are connected by fine strands. The process produces a flexible, stretchy textile with an apparent warp and weft like a woven fabric. Forman says it feels something like a mesh jersey fabric.
“Not only are these textiles thinner and faster to print than other approaches, but the complexity of demonstrated forms is also improved. With this approach we can print 3D dimensional shell forms with a normal 3D printer and no special slicer software,” says Forman. “This is exciting because there’s a lot of opportunities with 3D printing fabric, but it’s really hard for it to be easily disseminated, since a lot of it uses expensive machinery and special software or special commands that are generally specific to a printer.”
The new textile can be sewn, de-pleated, and heat-bonded like an iron-on patch. Forman and his colleagues have printed the textiles using many common 3D printing materials, including a conductive filament that they used to produce a lamp that can be lit and dimmed by touching pleats in the lampshade. The researchers suggest that other base materials or additives could produce textiles with magnetic or optical properties, or textiles that are more biodegradable by using algae, coffee grounds, or wood.
According to Scott Hudson, a professor at Carnegie Mellon University's Human-Computer Interaction Institute, Forman's work represents a very interesting addition to the expanding set of 3D-printing techniques. 
“This work is particularly important because it functions within the same print process as more conventional techniques,” notes Hudson, who was not part of the study. “This will allow us to integrate custom 3D-printed textile components — components that can be flexible and soft — into objects, along with more conventional hard parts.”
Lab @home
When MIT closed down at the start of the Covid-19 pandemic, Forman was in the midst of preparing for the ACM symposium submission. He relocated his full lab set up to the basement of his parents’ cabin near Lake Placid, New York.
“It’s not a lot of large equipment, but it’s lots of little tools, pliers, filaments,” he explains. “I had to set up two 3D printers, a soldering station, a photo backdrop — just because the work is so multidisciplinary.”
At the cabin, “I was able to hone in and focus on the research while the world around me was on fire, and it was actually a really good distraction,” Forman says. “It was also interesting to be working on a project that was so tech-focused, and then look out the window and see nature and trees — the tension between the two was quite inspiring.”
It was an experience for his parents as well, who got to see him “at my most intense and focused, and the hardest I’ve worked,” he recalls. “I’d be going upstairs at 5 a.m. for a snack when my dad was coming down for breakfast.""  
“My parents became part of the act of creation, where I’d print something and go, ‘look at this,’” he says. “I don’t know if I’ll ever have the opportunity again to have my parents so closely touch what I do every day.”
One of the more unusual aspects of the project has been what to call the material. Forman and his colleagues use the term “quasi-textile” because DefeXtiles doesn’t have all the same physical qualities of a usual textile, such as a bias in both directions and degree of softness. But some skeptics have been converted when they feel the material, Forman says.
The experience reminds him of the famous René Magritte painting “The Treachery of Objects (This Is Not a Pipe),” where the illustration of a pipe prompts a discussion about whether a representation can fully encompass all of an object’s meanings. “I’m interested in the coupling between digital bits and the materials experience by computationally fabricating high-fidelity materials with controllable forms and mechanical properties,” Forman explains.
“It makes me think about when the reference of the thing becomes accepted as the thing,” he adds. “It’s not the decision people make, but the reasoning behind it that interests me, and finding what causes them to accept it or reject it as a textile material.”


",Leveraging a 3D printer “defect” to create a new quasi-textile,2020-10-26,['Becky Ham'],"Media Lab/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Architecture/Center for Bits and Atoms/3-D printing/Polymers/Students/Graduate, postdoctoral/MIT Schwarzman College of Computing/School of Architecture and Planning/School of Engineering/Research/Covid-19/Pandemic","['printer', 'create', 'quasitextile', 'materials', 'forman', 'print', 'really', 'work', 'material', 'textile', 'defect', 'leveraging', '3d', 'used']","Forman used a standard, inexpensive 3D printer to produce sheets and complex 3D geometries with a woven-like structure based on the “glob-stretch” pattern produced by under-extrusion.
When a 3D printer under-extrudes material, it produces periodic gaps in the deposited material.
Using an inexpensive fused deposition modeling 3D printer, Forman developed an under-extruding process called “glob-stretch,” where globs of thermoplastic polymer are connected by fine strands.
With this approach we can print 3D dimensional shell forms with a normal 3D printer and no special slicer software,” says Forman.
But some skeptics have been converted when they feel the material, Forman says.",Mit
60,https://news.mit.edu/2020/stressed-job-ai-teammate-may-know-how-help-1026,"


Humans have been teaming up with machines throughout history to achieve goals, be it by using simple machines to move materials or complex machines to travel in space. But advances in artificial intelligence today bring possibilities for even more sophisticated teamwork — true human-machine teams that cooperate to solve complex problems.
Much of the development of these human-machine teams focuses on the machine, tackling the technology challenges of training AI algorithms to perform their role in a mission effectively. But less focus, MIT Lincoln Laboratory researchers say, has been given to the human side of the team. What if the machine works perfectly, but the human is struggling?
""In the area of human-machine teaming, we often think about the technology — for example, how do we monitor it, understand it, make sure it's working right. But teamwork is a two-way street, and these considerations aren't happening both ways. What we're doing is looking at the flip side, where the machine is monitoring and enhancing the other side — the human,"" says Michael Pietrucha, a tactical systems specialist at the laboratory. 
Pietrucha is among a team of laboratory researchers that aims to develop AI systems that can sense when a person's cognitive fatigue is interfering with their performance. The system would then suggest interventions, or even take action in dire scenarios, to help the individual recover or to prevent harm. 
""Throughout history, we see human error leading to mishaps, missed opportunities, and sometimes disastrous consequences,"" says Megan Blackwell, former deputy lead of internally funded biological science and technology research at the laboratory. ""Today, neuromonitoring is becoming more specific and portable. We envision using technology to monitor for fatigue or cognitive overload. Is this person attending to too much? Will they run out of gas, so to speak? If you can monitor the human, you could intervene before something bad happens.""
This vision has its roots in decades-long research at the laboratory in using technology to ""read"" a person's cognitive or emotional state. By collecting biometric data — such as video and audio recordings of a person speaking — and processing these data with advanced AI algorithms, researchers have uncovered biomarkers of various psychological and neurobehavioral conditions. These biomarkers have been used to train models that can accurately estimate the level of a person's depression, for example.
In this work, the team will apply their biomarker research to AI that can analyze an individual's cognitive state, encapsulating how fatigued, stressed, or overloaded a person is feeling. The system will use biomarkers derived from physiological data such as vocal and facial recordings, heart rate, EEG and optical indications of brain activity, and eye movement to gain these insights.
The first step will be to build a cognitive model of an individual. ""The cognitive model will integrate the physiological inputs and monitor the inputs to see how they change as a person performs particular fatiguing tasks,"" says Thomas Quatieri, who leads several neurobehavioral biomarker research efforts at the laboratory. ""Through this process, the system can establish patterns of activity and learn a person's baseline cognitive state involving basic task-related functions needed to avoid injury or undesirable outcomes, such as auditory and visual attention and response time.""
Once this individualized baseline is established, the system can start to recognize deviations from normal and predict if those deviations will lead to mistakes or poor performance.
""Building a model is hard. You know you got it right when it predicts performance,"" says William Streilein, principal staff in the Lincoln Lab's Homeland Protection and Air Traffic Control Division. ""We've done well if the system can identify a deviation, and then actually predict that the deviation is going to interfere with the person's performance on a task. Humans are complex; we compensate naturally to stress or fatigue. What's important is building a system that can predict when that deviation won't be compensated for, and to only intervene then.""
The possibilities for interventions are wide-ranging. On one end of the spectrum are minor adjustments a human can make to restore performance: drink coffee, change the lighting, get fresh air. Other interventions could suggest a shift change or transfer of a task to a machine or other teammate. Another possibility is using transcranial direct current stimulation, a performance-restoring technique that uses electrodes to stimulate parts of the brain and has been show to be more effective than caffeine in countering fatigue, with fewer side effects.
On the other end of the spectrum, the machine might take actions necessary to ensure the survival of the human team member when the human is incapable of doing so. For example, an AI teammate could make the ""ejection decision"" for a fighter pilot who has lost consciousness or the physical ability to eject themselves. Pietrucha, a retired colonel in the U.S. Air Force who has had many flight hours as a fighter/attack aviator, sees the promise of such a system that ""goes beyond the mere analysis of flight parameters and includes analysis of the cognitive state of the aircrew, intervening only when the aircrew can't or wont,"" he says. 
Determining the most helpful intervention, and its effectiveness, depends on a number of factors related to the task at hand, dosage of the intervention, and even a user's demographic background. ""There's a lot of work to be done still in understanding the effects of different interventions and validating their safety,"" Streilein says. ""Eventually, we want to introduce personalized cognitive interventions and assess their effectiveness on mission performance.""
Beyond its use in combat aviation, the technology could benefit other demanding or dangerous jobs, such as those related to air traffic control, combat operations, disaster response, or emergency medicine. ""There are scenarios where combat medics are vastly outnumbered, are in taxing situations, and are as every bit as tired as everyone else. Having this kind of over-the-shoulder help, something to help monitor their mental status and fatigue, could help prevent medical errors or even alert others to their level of fatigue,"" Blackwell says.
Today, the team is pursuing sponsorship to help develop the technology further. The coming year will be focused on collecting data to train their algorithms. The first subjects will be intelligence analysts, outfitted with sensors as they play a serious game that simulates the demands of their job. ""Intelligence analysts are often overwhelmed by data and could benefit from this type of system,"" Streilein says. ""The fact that they usually do their job in a 'normal' room environment, on a computer, allows us to easily instrument them to collect physiological data and start training.""
""We'll be working on a basic set of capabilities in the near term,"" Quatieri says, ""but an ultimate goal would be to leverage those capabilities so that, while the system is still individualized, it could be a more turnkey capability that could be deployed widely, similar to how Siri, for example, is universal but adapts quickly to an individual."" In the long view, the team sees the promise of a universal background model that could represent anyone and be adapted for a specific use. 
Such a capability may be key to advancing human-machine teams of the future. As AI progresses to achieve more human-like capabilities, while being immune from the human condition of mental stress, it's possible that humans may present the greatest risk to mission success. An AI teammate may know just how to lift its partner up.


",Stressed on the job? An AI teammate may know how to help,2020-10-26,['Kylie Foy'],Lincoln Laboratory/Human-computer interaction/Health/Artificial intelligence/Research/Mental health/Machine learning,"['cognitive', 'stressed', 'job', 'human', 'technology', 'data', 'persons', 'know', 'fatigue', 'ai', 'monitor', 'help', 'system', 'teammate', 'team']","Pietrucha is among a team of laboratory researchers that aims to develop AI systems that can sense when a person's cognitive fatigue is interfering with their performance.
The system would then suggest interventions, or even take action in dire scenarios, to help the individual recover or to prevent harm.
This vision has its roots in decades-long research at the laboratory in using technology to ""read"" a person's cognitive or emotional state.
On the other end of the spectrum, the machine might take actions necessary to ensure the survival of the human team member when the human is incapable of doing so.
An AI teammate may know just how to lift its partner up.",Mit
61,https://news.mit.edu/2020/autonomous-boats-could-be-your-next-ride-1026,"


The feverish race to produce the shiniest, safest, speediest self-driving car has spilled over into our wheelchairs, scooters, and even golf carts. Recently, there’s been movement from land to sea, as marine autonomy stands to change the canals of our cities, with the potential to deliver goods and services and collect waste across our waterways. 
In an update to a five-year project from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Senseable City Lab, researchers have been developing the world's first fleet of autonomous boats for the City of Amsterdam, the Netherlands, and have recently added a new, larger vessel to the group: “Roboat II.” Now sitting at 2 meters long, which is roughly a “Covid-friendly” 6 feet, the new robotic boat is capable of carrying passengers.
This work was supported by grant from the Amsterdam Institute for Advanced Metropolitan Solutions (AMS) in Netherlands.







Play video




      

            Roboat, the autonomous robotic boat        

    



Alongside the Amsterdam Institute for Advanced Metropolitan Solutions, the team also created navigation and control algorithms to update the communication and collaboration among the boats. 
“Roboat II navigates autonomously using algorithms similar to those used by self-driving cars, but now adapted for water,” says MIT Professor Daniela Rus, a senior author on a new paper about Roboat and the director of CSAIL. “We’re developing fleets of Roboats that can deliver people and goods, and connect with other Roboats to form a range of autonomous platforms to enable water activities.” 
Self-driving boats have been able to transport small items for years, but adding human passengers has felt somewhat intangible due to the current size of the vessels. Roboat II is the “half-scale” boat in the growing body of work, and joins the previously developed quarter-scale Roboat, which is 1 meter long. The third installment, which is under construction in Amsterdam and is considered to be “full scale,” is 4 meters long and aims to carry anywhere from four to six passengers. 
Aided by powerful algorithms, Roboat II autonomously navigated the canals of Amsterdam for three hours collecting data, and returned back to its start location with an error margin of only 0.17 meters, or fewer than 7 inches. 
“The development of an autonomous boat system capable of accurate mapping, robust control, and human transport is a crucial step towards having the system implemented in the full-scale Roboat,” says senior postdoc Wei Wang, lead author on a new paper about Roboat II. “We also hope it will eventually be implemented in other boats in order to make them autonomous.”
Wang wrote the paper alongside MIT Senseable City Lab postdoc Tixiao Shan, research fellow Pietro Leoni, postdoc David Fernandez-Gutierrez, research fellow Drew Meyers, and MIT professors Carlo Ratti and Daniela Rus. The work was supported by a grant from the Amsterdam Institute for Advanced Metropolitan Solutions in the Netherlands. A paper on Roboat II will be virtually presented at the International Conference on Intelligent Robots and Systems. 
To coordinate communication among the boats, another team from MIT CSAIL and Senseable City Lab, also led by Wang, came up with a new control strategy for robot coordination. 
With the intent of self-assembling into connected, multi-unit trains — with distant homage to children’s train sets — “collective transport” takes a different path to complete various tasks. The system uses a distributed controller, which is a collection of sensors, controllers, and associated computers distributed throughout a system), and a strategy inspired by how a colony of ants can transport food without communication. Specifically, there’s no direct communication among the connected robots — only one leader knows the destination. The leader initiates movement to the destination, and then the other robots can estimate the intention of the leader, and align their movements accordingly. 
“Current cooperative algorithms have rarely considered dynamic systems on the water,” says Ratti, the Senseable City Lab director. “Cooperative transport, using a team of water vehicles, poses unique challenges not encountered in aerial or ground vehicles. For example, inertia and load of the vehicles become more significant factors that make the system harder to control. Our study investigates the cooperative control of the surface vehicles and validates the algorithm on that.” 
The team tested their control method on two scenarios: one where three robots are connected in a series, and another where three robots are connected in parallel. The results showed that the coordinated group was able to track various trajectories and orientations in both configurations, and that the magnitudes of the followers’ forces positively contributed to the group — indicating that the follower robots helped the leader. 
Wang wrote a paper about collective transport alongside Stanford University PhD student Zijian Wang, MIT postdoc Luis Mateos, MIT researcher Kuan Wei Huang, Stanford Assistant Professor Mac Schwager, Ratti, and Rus. 
Roboat II
In 2016, MIT researchers tested a prototype that could move “forward, backward, and laterally along a pre-programmed path in the canals.” Three years later, the team’s robots were updated to “shapeshift” by autonomously disconnecting and reassembling into a variety of configurations. 
Now, Roboat II has scaled up to explore transportation tasks, aided by updated research. These include a new algorithm for Simultaneous Localization and Mapping (SLAM), a model-based optimal controller called nonlinear model predictive controller, and an optimization-based state estimator, called moving horizon estimation. 
Here’s how it works: When a passenger pickup task is required from a user at a specific position, the system coordinator will assign the task to an unoccupied boat that’s closest to the passenger. As Roboat II picks up the passenger, it will create a feasible path to the desired destination, based on the current traffic conditions. 
Then, Roboat II, which weighs more than 50 kilograms, will start to localize itself by running the SLAM algorithm and utilizing lidar and GPS sensors, as well as an inertial measurement unit for localization, pose, and velocity. The controller then tracks the reference trajectories from the planner, which updates the path to avoid obstacles that are detected to avoid potential collisions.  
The team notes that the improvements in their control algorithms have made the obstacles feel like less of a giant iceberg since their last update; the SLAM algorithm provides a higher localization accuracy for Roboat, and allows for online mapping during navigation, which they didn’t have in previous iterations. 
Increasing the size of Roboat also required a larger area to conduct the experiments, which began in the MIT pools and subsequently moved to the Charles River, which cuts through Boston and Cambridge, Massachusetts.
While navigating the congested roads of cities alike can lead drivers to feel trapped in a maze, canals largely avoid this. Nevertheless, tricky scenarios in the waterways can still emerge. Given that, the team is working on developing more efficient planning algorithms to let the vessel handle more complicated scenarios, by applying active object detection and identification to improve Roboat’s understanding of its environment. The team plans to estimate disturbances such as currents and waves, to further improve the tracking performance in more noisy waters. 
“All of these expected developments will be incorporated into the first prototype of the full-scale Roboat and tested in the canals of the City of Amsterdam,” says Rus. 
Collective transport 
Making our intuitive abilities a reality for machines has been the persistent intention since the birth of the field, from straightforward commands for picking up items to the nuances of organizing in a group. 
One of the main goals of the project is enabling self-assembly to complete the aforementioned tasks of collecting waste, delivering items, and transporting people in the canals — but controlling this movement on the water has been a challenging obstacle. Communication in robotics can often be unstable or have delays, which may worsen the robot coordination. 
Many control algorithms for this collective transport require direct communication, the relative positions in the group, and the destination of the task — but the team’s new algorithm simply needs one robot to know the desired trajectory and orientation. 
Normally, the distributed controller running on each robot requires the velocity information of the connected structure (represented by the velocity of the center of the structure), but this requires that each robot knows the relative position to the center of the structure. In the team’s algorithm, they don’t need the relative position, and each robot simply uses its local velocity instead of the velocity of the center of the structure.
When the leader initiates the movement to the destination, the other robots can therefore estimate the intention of the leader and align their movements. The leader can also steer the rest of the robots by adjusting its input, without any communication between any two robots. 
In the future, the team plans to use machine learning to estimate (online) the key parameters of the robots. They’re also aiming to explore adaptive controllers that allow for dynamic change to the structure when objects are placed on the boat. Eventually, the boats will also be extended to outdoor water environments, where large disturbances such as currents and waves exist.


",Autonomous boats could be your next ride,2020-10-26,['Rachel Gordon'],MIT Schwarzman College of Computing/Urban studies and planning/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Research/Robotics/Robots/Computer science and technology/Algorithms/Autonomous vehicles/Transportation/Sensors/School of Architecture and Planning,"['control', 'water', 'autonomous', 'algorithms', 'robots', 'ride', 'roboat', 'communication', 'transport', 'boats', 'ii', 'mit', 'team']","Alongside the Amsterdam Institute for Advanced Metropolitan Solutions, the team also created navigation and control algorithms to update the communication and collaboration among the boats.
Roboat II is the “half-scale” boat in the growing body of work, and joins the previously developed quarter-scale Roboat, which is 1 meter long.
A paper on Roboat II will be virtually presented at the International Conference on Intelligent Robots and Systems.
Now, Roboat II has scaled up to explore transportation tasks, aided by updated research.
As Roboat II picks up the passenger, it will create a feasible path to the desired destination, based on the current traffic conditions.",Mit
62,https://news.mit.edu/2020/morphsensor-morphs-interactive-objects-1022,"


We’ve come a long way since the first 3D-printed item came to us by way of an eye wash cup, to now being able to rapidly fabricate things like car parts, musical instruments, and even biological tissues and organoids. 
While much of these objects can be freely designed and quickly made, the addition of electronics to embed things like sensors, chips, and tags usually requires that you design both separately, making it difficult to create items where the added functions are easily integrated with the form. 
Now, a 3D design environment from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) lets users iterate an object’s shape and electronic function in one cohesive space, to add existing sensors to early-stage prototypes.
The team tested the system, called MorphSensor, by modeling an N95 mask with a humidity sensor, a temperature-sensing ring, and glasses that monitor light absorption to protect eye health.







Play video






MorphSensor automatically converts electronic designs into 3D models, and then lets users iterate on the geometry and manipulate active sensing parts. This might look like a 2D image of a pair of AirPods and a sensor template, where a person could edit the design until the sensor is embedded, printed, and taped onto the item. 
To test the effectiveness of MorphSensor, the researchers created an evaluation based on standard industrial assembly and testing procedures. The data showed that MorphSensor could match the off-the-shelf sensor modules with small error margins, for both the analog and digital sensors.
“MorphSensor fits into my long-term vision of something called ‘rapid function prototyping’, with the objective to create interactive objects where the functions are directly integrated with the form and fabricated in one go, even for non-expert users,” says CSAIL PhD student Junyi Zhu, lead author on a new paper about the project. “This offers the promise that, when prototyping, the object form could follow its designated function, and the function could adapt to its physical form.” 
MorphSensor in action 
Imagine being able to have your own design lab where, instead of needing to buy new items, you could cost-effectively update your own items using a single system for both design and hardware. 
For example, let’s say you want to update your face mask to monitor surrounding air quality. Using MorphSensor, users would first design or import the 3D face mask model and sensor modules from either MorphSensor's database or online open-sourced files. The system would then generate a 3D model with individual electronic components (with airwires connected between them) and color-coding to highlight the active sensing components.  
Designers can then drag and drop the electronic components directly onto the face mask, and rotate them based on design needs. As a final step, users draw physical wires onto the design where they want them to appear, using the system’s guidance to connect the circuit. 
Once satisfied with the design, the ""morphed sensor"" can be rapidly fabricated using an inkjet printer and conductive tape, so it can be adhered to the object. Users can also outsource the design to a professional fabrication house.  
To test their system, the team iterated on EarPods for sleep tracking, which only took 45 minutes to design and fabricate. They also updated a “weather-aware” ring to provide weather advice, by integrating a temperature sensor with the ring geometry. In addition, they manipulated an N95 mask to monitor its substrate contamination, enabling it to alert its user when the mask needs to be replaced.
In its current form, MorphSensor helps designers maintain connectivity of the circuit at all times, by highlighting which components contribute to the actual sensing. However, the team notes it would be beneficial to expand this set of support tools even further, where future versions could potentially merge electrical logic of multiple sensor modules together to eliminate redundant components and circuits and save space (or preserve the object form). 
Zhu wrote the paper alongside MIT graduate student Yunyi Zhu; undergraduates Jiaming Cui, Leon Cheng, Jackson Snowden, and Mark Chounlakone; postdoc Michael Wessely; and Professor Stefanie Mueller. The team will virtually present their paper at the ACM User Interface Software and Technology Symposium. 
This material is based upon work supported by the National Science Foundation.


",Electronic design tool morphs interactive objects,2020-10-22,['Rachel Gordon'],MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Mechanical engineering/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Research/3-D printing/Design/Manufacturing/electronics/Sensors/National Science Foundation (NSF),"['using', 'mask', 'users', 'tool', 'morphs', 'morphsensor', 'function', 'sensor', 'system', 'electronic', 'objects', 'design', 'interactive', 'team']","Now, a 3D design environment from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) lets users iterate an object’s shape and electronic function in one cohesive space, to add existing sensors to early-stage prototypes.
MorphSensor automatically converts electronic designs into 3D models, and then lets users iterate on the geometry and manipulate active sensing parts.
Using MorphSensor, users would first design or import the 3D face mask model and sensor modules from either MorphSensor's database or online open-sourced files.
Designers can then drag and drop the electronic components directly onto the face mask, and rotate them based on design needs.
To test their system, the team iterated on EarPods for sleep tracking, which only took 45 minutes to design and fabricate.",Mit
63,https://news.mit.edu/2020/translating-lost-languages-using-machine-learning-1021,"


Recent research suggests that most languages that have ever existed are no longer spoken. Dozens of these dead languages are also considered to be lost, or “undeciphered” — that is, we don’t know enough about their grammar, vocabulary, or syntax to be able to actually understand their texts.
Lost languages are more than a mere academic curiosity; without them, we miss an entire body of knowledge about the people who spoke them. Unfortunately, most of them have such minimal records that scientists can’t decipher them by using machine-translation algorithms like Google Translate. Some don’t have a well-researched “relative” language to be compared to, and often lack traditional dividers like white space and punctuation. (To illustrate, imaginetryingtodecipheraforeignlanguagewrittenlikethis.)
However, researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) recently made a major development in this area: a new system that has been shown to be able to automatically decipher a lost language, without needing advanced knowledge of its relation to other languages. They also showed that their system can itself determine relationships between languages, and they used it to corroborate recent scholarship suggesting that the language of Iberian is not actually related to Basque.
The team’s ultimate goal is for the system to be able to decipher lost languages that have eluded linguists for decades, using just a few thousand words.
Spearheaded by MIT Professor Regina Barzilay, the system relies on several principles grounded in insights from historical linguistics, such as the fact that languages generally only evolve in certain predictable ways. For instance, while a given language rarely adds or deletes an entire sound, certain sound substitutions are likely to occur. A word with a “p” in the parent language may change into a “b” in the descendant language, but changing to a “k” is less likely due to the significant pronunciation gap.
By incorporating these and other linguistic constraints, Barzilay and MIT PhD student Jiaming Luo developed a decipherment algorithm that can handle the vast space of possible transformations and the scarcity of a guiding signal in the input. The algorithm learns to embed language sounds into a multidimensional space where differences in pronunciation are reflected in the distance between corresponding vectors. This design enables them to capture pertinent patterns of language change and express them as computational constraints. The resulting model can segment words in an ancient language and map them to counterparts in a related language.  
The project builds on a paper Barzilay and Luo wrote last year that deciphered the dead languages of Ugaritic and Linear B, the latter of which had previously taken decades for humans to decode. However, a key difference with that project was that the team knew that these languages were related to early forms of Hebrew and Greek, respectively.
With the new system, the relationship between languages is inferred by the algorithm. This question is one of the biggest challenges in decipherment. In the case of Linear B, it took several decades to discover the correct known descendant. For Iberian, the scholars still cannot agree on the related language: Some argue for Basque, while others refute this hypothesis and claim that Iberian doesn’t relate to any known language. 
The proposed algorithm can assess the proximity between two languages; in fact, when tested on known languages, it can even accurately identify language families. The team applied their algorithm to Iberian considering Basque, as well as less-likely candidates from Romance, Germanic, Turkic, and Uralic families. While Basque and Latin were closer to Iberian than other languages, they were still too different to be considered related. 
In future work, the team hopes to expand their work beyond the act of connecting texts to related words in a known language — an approach referred to as “cognate-based decipherment.” This paradigm assumes that such a known language exists, but the example of Iberian shows that this is not always the case. The team’s new approach would involve identifying semantic meaning of the words, even if they don’t know how to read them. 
“For instance, we may identify all the references to people or locations in the document which can then be further investigated in light of the known historical evidence,” says Barzilay. “These methods of ‘entity recognition’ are commonly used in various text processing applications today and are highly accurate, but the key research question is whether the task is feasible without any training data in the ancient language.”      .
The project was supported, in part, by the Intelligence Advanced Research Projects Activity (IARPA).


",Translating lost languages using machine learning,2020-10-21,['Adam Conner-Simons'],MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Research/Artificial intelligence/Big data/Computer science and technology/Machine learning/Linguistics/Algorithms/Language/History,"['algorithm', 'barzilay', 'using', 'team', 'language', 'iberian', 'known', 'learning', 'words', 'lost', 'related', 'system', 'machine', 'languages', 'translating']","Lost languages are more than a mere academic curiosity; without them, we miss an entire body of knowledge about the people who spoke them.
The team’s ultimate goal is for the system to be able to decipher lost languages that have eluded linguists for decades, using just a few thousand words.
The resulting model can segment words in an ancient language and map them to counterparts in a related language.
The proposed algorithm can assess the proximity between two languages; in fact, when tested on known languages, it can even accurately identify language families.
While Basque and Latin were closer to Iberian than other languages, they were still too different to be considered related.",Mit
64,https://news.mit.edu/2020/openspace-construction-1021,"


People who work behind a computer screen all day take it for granted that everyone’s work will be tracked and accessible when they collaborate with others. But if your job takes place out in the real world, managing projects can require a lot more effort.
In construction, for example, general contractors and real estate developers often need someone to be physically present on a job site to verify work is done correctly and on time. They might also rely on a photographer or smartphone images to document a project’s progress. Those imperfect solutions can lead to accountability issues, unnecessary change orders, and project delays.
Now the startup OpenSpace is bringing some of the benefits of digital work to the real world with a solution that uses 360-degree cameras and computer vision to create comprehensive, time-stamped digital replicas of construction sites.
All customers need to do is walk their job site with a small 360-degree camera on their hard hat. The OpenSpace Vision Engine maps the photos to work plans automatically, creating a Google Streetview-like experience for people to remotely tour work sites at different times as if they were physically present.
The company is also deploying analytics solutions that help customers track progress and search for objects on their job sites. To date, OpenSpace has helped customers map more than 1.5 billion square feet of construction projects, including bridges, hospitals, football stadiums, and large residential buildings.
The solution is helping workers in the construction industry improve accountability, minimize travel, reduce risks, and more.
“The core product we have today is a simple idea: It allows our customers to have a complete visual record of any space, indoor or outdoor, so they can see what’s there from anywhere at any point in time,” says OpenSpace cofounder and CEO Jeevan Kalanithi SM ’07. “They can teleport into the site to inspect the actual reality, but they can also see what was there yesterday or a week ago or five years ago. It brings this ground truth record to the site.”
Shining a light on construction sites
The founders of OpenSpace originally met during their time at MIT. At the Media Lab, Kalanithi and David Merrill SM ’06, PhD ’09 built a gaming system based on small cubes that used LCD touch screens and motion sensors to encourage kids to develop critical thinking skills. They spun the idea into a company, Sifteo, which created multiple generations of its toys.
In 2014, Sifteo was bought by 3D Robotics, then a drone company that would go on to focus on drone inspection software for construction, engineering, and mining firms. Kalanithi stayed with 3D Robotics for over two years, eventually serving as president of the company.
In the summer of 2016, Kalanithi left 3D Robotics with the intention of spending more time with friends and family. He reconnected with two friends from MIT, Philip DeCamp ’05, SM ’08, PhD ’13 and Michael Fleischman PhD ’08, who had researched new machine vision and AI techniques in their PhD research. Fleischman had started a social media analytics company he sold to Twitter.
At the time, DeCamp and Fleischman were considering ways to use machine vision advances with 360-degree cameras. Kalanithi, who had helped guide 3D Robotics toward the construction industry, thought he had the perfect application.
People have long used photographs to document construction projects, and many times contracts for large construction projects require photos of progress to be taken. But the photos never document the entire site, and they aren’t taken frequently enough to capture every phase of work.
Early versions of the OpenSpace solution required someone to set up a tripod in every space of a construction project. A breakthrough came when one early user, a straight-talking project manager, gave the founders some useful feedback.
“I was showing him the output of our product at the time, which looks similar to now, and he says, ‘This is great. How long did it take you?’ When I told him he said, ‘Well that’s cool Jeevan, but there’s no way we’re going to use that,’” Kalanithi recalls. “I thought maybe this idea isn’t so good after all. But then he gave us the idea. He said, ‘What would be great is if I could just wear that little camera and walk around. I walk around the job site all the time.’”
The founders took the advice and repurposed their solution to work with off-the-shelf 360-degree cameras and slightly modified hard hats. The cameras take pictures every half second and use artificial intelligence techniques to identify the camera’s precise location, even indoors. Once a few tours of the job site have been uploaded to OpenSpace’s platform, it can map pictures onto site plans within 15 minutes.
Kalanithi still remembers the excitement the founders felt the first time they saved a customer money, helping to settle a dispute between a general contractor and a drywall specialist. Since then they’ve gotten a lot of those calls, in some cases saving companies millions of dollars. Kalanithi says saving builders costs helps the construction industry meet growing needs related to aging infrastructure and housing shortages.
Helping nondigital workers
OpenSpace’s analytics solutions, which the company calls its ClearSight suite of products, have not been rolled out to every customer yet. But Kalanithi believes they will bring even more value to people managing work sites.
“If you have someone walking around the project all the time, we can start classifying and computing what they’re seeing,” Kalanithi says. “So, we can see how much framing and drywall is being installed, how quickly, how much material was used. That’s the basis for how people get paid in this industry: How much work did you do?”
Kalanithi believes Clearsight is the beginning of a new phase for OpenSpace, where the company can use AI and computer vision to give customers a new perspective on what’s going on at their job site.
“The product experience today, where you look around to see the site, will be something people sometimes do on OpenSpace, but they may be spending more time looking at productivity charts and little OpenSpace verified payment buttons, and maybe sometimes they’ll drill down to look at the actual images,” Kalanithi says.
The Covid-19 pandemic accelerated some companies’ adoption of digital solutions to help cut down on travel and physical contact. But even in states that have resumed construction, Kalanithi says customers are continuing to use OpenSpace, a key indicator of the value it brings.
Indeed, the vast majority of the information captured by OpenSpace was never available before, and it brings with it the potential for major improvements in the construction industry and beyond.
“If the last decade was defined by the cloud and mobile technology being the real enabling technologies, I think this next decade will be innovations that affect people in the real physical world,” Kalanithi says. “Because cameras and computer vision are getting better, so for a lot of people who have been ignored or left behind by technology based on the work they do, we’ll have the opportunity to make some amends and build some stuff that will make those folks' lives easier.”


",Bringing construction projects to the digital world,2020-10-21,['Zach Winn'],Building/Innovation and Entrepreneurship (I&E)/Startups/Alumni/ae/Cities/Media Lab/Computer vision/Artificial intelligence/School of School of Architecture and Planning,"['customers', 'bringing', 'job', 'digital', 'construction', 'site', 'work', 'projects', 'world', 'openspace', 'vision', 'company', 'kalanithi', 'cameras']","Now the startup OpenSpace is bringing some of the benefits of digital work to the real world with a solution that uses 360-degree cameras and computer vision to create comprehensive, time-stamped digital replicas of construction sites.
All customers need to do is walk their job site with a small 360-degree camera on their hard hat.
People have long used photographs to document construction projects, and many times contracts for large construction projects require photos of progress to be taken.
Once a few tours of the job site have been uploaded to OpenSpace’s platform, it can map pictures onto site plans within 15 minutes.
But even in states that have resumed construction, Kalanithi says customers are continuing to use OpenSpace, a key indicator of the value it brings.",Mit
65,https://news.mit.edu/2020/neural-pathway-crucial-successful-rapid-object-recognition-primates-1020,"


MIT researchers have identified a brain pathway critical in enabling primates to effortlessly identify objects in their field of vision. The findings enrich existing models of the neural circuitry involved in visual perception and help to further unravel the computational code for solving object recognition in the primate brain.
Led by Kohitij Kar, a postdoc at the McGovern Institute for Brain Research and Department of Brain and Cognitive Sciences, the study looked at an area called the ventrolateral prefrontal cortex (vlPFC), which sends feedback signals to the inferior temporal (IT) cortex via a network of neurons. The main goal of this study was to test how the back-and-forth information processing of this circuitry — that is, this recurrent neural network — is essential to rapid object identification in primates.
The current study, published in Neuron and available via open access, is a followup to prior work published by Kar and James DiCarlo, the Peter de Florez Professor of Neuroscience, the head of MIT’s Department of Brain and Cognitive Sciences, and an investigator in the McGovern Institute and the Center for Brains, Minds, and Machines.
Monkey versus machine 
In 2019, Kar, DiCarlo, and colleagues identified that primates must use some recurrent circuits during rapid object recognition. Monkey subjects in that study were able to identify objects more accurately than engineered “feed-forward” computational models, called deep convolutional neural networks, that lacked recurrent circuitry.
Interestingly, specific images for which models performed poorly compared to monkeys in object identification, also took longer to be solved in the monkeys’ brains — suggesting that the additional time might be due to recurrent processing in the brain. Based on the 2019 study, it remained unclear, though, exactly which recurrent circuits were responsible for the delayed information boost in the IT cortex. That’s where the current study picks up.
“In this new study, we wanted to find out: Where are these recurrent signals in IT coming from?” Kar says. “Which areas reciprocally connected to IT, are functionally the most critical part of this recurrent circuit?”
To determine this, researchers used a pharmacological agent to temporarily block the activity in parts of the vlPFC in macaques while they engaged in an object discrimination task. During these tasks, monkeys viewed images that contained an object, such as an apple, a car, or a dog; then, researchers used eye tracking to determine if the monkeys could correctly indicate what object they had previously viewed when given two object choices.
“We observed that if you use pharmacological agents to partially inactivate the vlPFC, then both the monkeys’ behavior and IT cortex activity deteriorates, but more so for certain specific images. These images were the same ones we identified in the previous study — ones that were poorly solved by ‘feed-forward’ models and took longer to be solved in the monkey’s IT cortex,” says Kar.
“These results provide evidence that this recurrently connected network is critical for rapid object recognition, the behavior we're studying. Now, we have a better understanding of how the full circuit is laid out, and what are the key underlying neural components of this behavior.”
The full study, entitled “Fast recurrent processing via ventrolateral prefrontal cortex is needed by the primate ventral stream for robust core visual object recognition,” will run in print Jan. 6, 2021.
“This study demonstrates the importance of prefrontal cortical circuits in automatically boosting object recognition performance in a very particular way,” DiCarlo says. “These results were obtained in nonhuman primates and thus are highly likely to also be relevant to human vision.”
The present study makes clear the integral role of the recurrent connections between the vlPFC and the primate ventral visual cortex during rapid object recognition. The results will be helpful to researchers designing future studies that aim to develop accurate models of the brain, and to researchers who seek to develop more human-like artificial intelligence.


",Neural pathway crucial to successful rapid object recognition in primates,2020-10-20,['Alison Gold'],School of Science/Brain and cognitive sciences/McGovern Institute/Center for Brains Minds and Machines/Neuroscience/Vision/Research/Artificial intelligence,"['recognition', 'pathway', 'researchers', 'monkeys', 'models', 'object', 'primates', 'vlpfc', 'brain', 'crucial', 'cortex', 'study', 'rapid', 'successful', 'recurrent', 'neural']","MIT researchers have identified a brain pathway critical in enabling primates to effortlessly identify objects in their field of vision.
The findings enrich existing models of the neural circuitry involved in visual perception and help to further unravel the computational code for solving object recognition in the primate brain.
Monkey versus machineIn 2019, Kar, DiCarlo, and colleagues identified that primates must use some recurrent circuits during rapid object recognition.
“These results provide evidence that this recurrently connected network is critical for rapid object recognition, the behavior we're studying.
“This study demonstrates the importance of prefrontal cortical circuits in automatically boosting object recognition performance in a very particular way,” DiCarlo says.",Mit
66,https://news.mit.edu/2020/lincoln-laboratory-technologies-rd-100-award-winners-1020,"


Eight technologies developed by MIT Lincoln Laboratory researchers, either wholly or in collaboration with researchers from other organizations, were among the winners of the 2020 R&D 100 Awards. Annually since 1963, these international R&D awards recognize 100 technologies that a panel of expert judges selects as the most revolutionary of the past year.
Six of the laboratory’s winning technologies are software systems, a number of which take advantage of artificial intelligence techniques. The software technologies are solutions to difficulties inherent in analyzing large volumes of data and to problems in maintaining cybersecurity. Another technology is a process designed to assure secure fabrication of integrated circuits, and the eighth winner is an optical communications technology that may enable future space missions to transmit error-free data to Earth at significantly higher rates than currently possible.
CyberPow
To enable timely, effective responses to post-disaster large-scale power outages, Lincoln Laboratory created a system that rapidly estimates and maps the extent and location of power outages across geographic boundaries. Cyber Sensing for Power Outage Detection, nicknamed CyberPow, uses pervasive, internet-connected devices to produce near-real-time situational awareness to inform decisions about allocating personnel and resources.
The system performs active scanning of the IP networks in a targeted region to identify changes in network device availability as an indicator of power loss. CyberPow is a low-cost, efficient alternative to current approaches that rely on piecing together outage data from disparate electric utilities with varying ability to assess their own outages.
FOVEA
The Forensic Video Exploitation and Analysis (FOVEA) tool suite, developed by the laboratory under the sponsorship of the Department of Homeland Security Science and Technology Directorate, enables users to efficiently analyze video captured by existing large-scale closed-circuit television systems. The FOVEA tools expedite daily tasks, such as searching through video, investigating abandoned objects, or piecing together activity from multiple cameras. A video summarization tool condenses all motion activity within a long time frame into a very short visual summary, transforming, for example, one hour of raw video into a three-minute summary that also acts as a clickable index into the original video sequence.
To allow analysts to track the onsite history of a suspicious object, a “jump back” feature automatically scans to the segment of video in which an idle or suspicious object first appeared. Because analysts can quickly navigate a camera network through the use of transition zones — clickable overlays that mark common entry/exit zones — FOVEA makes it easy to follow a person of interest through many camera views. Video data from each camera combined on the fly in chronological order can be exported easily. Highly efficient algorithms mean that no specialized hardware is required; thus, FOVEA software tools can add strong forensic capabilities to any video streaming system.
Keylime
An open-source key bootstrapping and integrity management software architecture, Keylime is designed to increase the security and privacy of edge, cloud, and internet-of-things devices. Keylime enables users to securely upload cryptographic keys, passwords, and certificates into their machines without unnecessarily divulging these secrets. In addition, Keylime enables users to continuously verify trust in their computing resources without relying on their service providers to guarantee security.
Keylime leverages the Trusted Platform Module, an industry-standard hardware security chip, but eliminates the complexity, compatibility, and performance issues that the module introduces. Keylime has fostered a vibrant, growing open-source community with the help of Red Hat, a multinational software company, and has been accepted as a Sandbox technology in the Cloud Native Computing Foundation, a Linux Foundation project.
LAVA
A team from Lincoln Laboratory, New York University, and Northeastern University developed the Large-scale Vulnerability Addition (LAVA) technique, which injects numerous bugs into a program to create ground truth for evaluating bug-finding systems. The technique inserts bugs at known locations in a program and constructs triggering inputs for each bug. A bug finder’s ability to discover LAVA bugs in a program can be used to estimate the finder’s false negative and false positive rates.
LAVA addresses the critical need for technology that can discover new approaches to finding bugs in software programs. Despite decades of research into building stable software, bugs still plague modern programs, and current approaches to bug discovery have relied on analyzing programs against programs that have either no known bugs or previously discovered bugs. Manually creating programs with known bugs is laborious and cannot be done at a large scale. LAVA is the only system capable of injecting essentially an unlimited number of bugs into real programs given the program’s source code. It has been used to evaluate bug finders, both human and automated, since 2017.
RIO
The Reconnaissance of Influence Operations (RIO) software system automates the detection of disinformation narratives, networks, and inﬂuential actors. The system is designed to address the growing threat posed by adversarial countries that exploit social media and digital communications to achieve political objectives.
The unprecedented scales, speeds, and reach of disinformation campaigns present a rising threat to global stability, and especially to democratic societies. The RIO system integrates natural language processing, machine learning, graph analytics, and novel network causal inference to quantify the impact of individual actors in spreading the disinformation narrative. By providing situational awareness of inﬂuence campaigns and knowledge of the mechanisms behind social inﬂuence, RIO offers capabilities that can aid in crafting responses to dangerous influence operations.
TRACER
Many popular closed-source computer applications, such as browsers or productivity applications running on the Windows operating system, are vulnerable to large-scale cyber attacks through which adversaries use previously discovered entry into the applications’ data to take control of a computer. Enabling the severity of these attacks is the homogeneity of the targets: Because all installations of an application look alike, it can be easy for attackers to simultaneously compromise millions of computers, remotely exfiltrating sensitive information or stealing user data.
Lincoln Laboratory’s Timely Randomization Applied to Commodity Executables at Runtime (TRACER) technique protects closed-source Windows applications against sophisticated attacks by automatically and transparently re-randomizing the applications’ sensitive internal data and layout every time any output is generated. TRACER therefore assures that leaked information quickly becomes stale and that attacks cannot bypass a one-time randomization, as is the case in one-time randomization defenses.
Defensive Wire Routing for Untrusted Integrated Circuit Fabrication
Lincoln Laboratory researchers developed the Defensive Wire Routing for Untrusted Integrated Circuit Fabrication techniques to deter an outsourced foundry from maliciously tampering with or modifying the security-critical components of a digital circuit design. For example, a trusted design could be changed by a fabricator who inserts a “hardware Trojan” or “backdoor” that can compromise the downstream system security.
The Defensive Wire Routing technology augments standard wire routing processes to make complex integrated circuits (ICs) inspectable and/or tamper-evident post fabrication. The need for such defensive techniques has arisen because of increasing commercial and government use of outsourced third-party IC foundries for advanced high-performance IC fabrication.
TBIRD
Sponsored by NASA, Lincoln Laboratory developed TeraByte InfraRed Delivery (TBIRD), a technology that enables error-free transmission of data from satellites in low Earth orbit (LEO) at a rate of 200 gigabits per second. The current approach to LEO data delivery, generally a combination of RF communication, networks of ground stations, and onboard data compression, will become less able to efficiently and accurately handle the volume of data sent from the increasing numbers of LEO satellites sharing a crowded RF spectrum.
TBIRD is an optical communications alternative that leverages the high bandwidths and unregulated spectrum available in the optical frequencies. It combines a custom-designed, innovative transmit/receive system in conjunction with commercial transceivers to provide high-rate, error-free data links through the dynamic atmosphere. Because TBIRD enables extremely high data-volume transfers that can occur over different atmospheric conditions (horizontal-link or LEO-to-ground), it has the potential to transform satellite operations in all scientific, commercial, and defense applications.
Because of the Covid-19 pandemic, editors of R&D World, an online publication that promotes the award program, announced the winners at virtual ceremonies broadcast on Sept. 29-30 and Oct. 1. Since 2010, Lincoln Laboratory has had 66 technologies recognized with R&D 100 Awards. 


",Eight Lincoln Laboratory technologies named 2020 R&D 100 Award winners,2020-10-20,['Dorothy Ryan'],"Lincoln Laboratory/Awards, honors and fellowships/Artificial intelligence/Algorithms/Cyber security/Communications/NASA/Satellites/Video/Cryptography/Disaster response/Social media","['winners', '100', 'data', 'named', 'technology', 'technologies', '2020', 'programs', 'award', 'rd', 'laboratory', 'system', 'software', 'lincoln', 'video', 'bugs']","Eight technologies developed by MIT Lincoln Laboratory researchers, either wholly or in collaboration with researchers from other organizations, were among the winners of the 2020 R&D 100 Awards.
The software technologies are solutions to difficulties inherent in analyzing large volumes of data and to problems in maintaining cybersecurity.
Video data from each camera combined on the fly in chronological order can be exported easily.
RIOThe Reconnaissance of Influence Operations (RIO) software system automates the detection of disinformation narratives, networks, and inﬂuential actors.
Since 2010, Lincoln Laboratory has had 66 technologies recognized with R&D 100 Awards.",Mit
67,https://news.mit.edu/2020/global-collaboration-moving-ai-principles-to-practice-1019,"


Today, artificial intelligence — and the computing systems that underlie it — are more than just matters of technology; they are matters of state and society, of governance and the public interest. The choices that technologists, policymakers, and communities make in the next few years will shape the relationship between machines and humans for decades to come.
The rapidly increasing applicability of AI has prompted a number of organizations to develop high-level principles on social and ethical issues such as privacy, fairness, bias, transparency, and accountability. Building on those broader principles, the AI Policy Forum, a global effort convened by the MIT Stephen A. Schwarzman College of Computing, will provide an overarching policy framework and tools for governments and companies to implement in concrete ways.
“Our goal is to help policymakers in making practical decisions about AI policy,” says Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing. “We are not trying to develop another set of principles around AI, several of which already exist, but rather provide context and guidelines specific to a field of use of AI to help policymakers around the world with implementation.”
“Moving beyond principles means understanding trade-offs and identifying the technical tools and the policy levers to address them. We created the college to examine and address these types of issues, but this can’t be a siloed effort. We need for this to be a global collaboration and engage scientists, technologists, policymakers, and business leaders,” says MIT Provost Martin Schmidt. “This is a challenging and complex process for which we need all hands-on deck.”
The AI Policy Forum is designed as a yearlong process. Activities associated with this effort will be distinguished by their focus on tangible outcomes — their engagement with key government officials at the local, national, and international level charged with designing those public policies, and their deep technical grounding in the latest advances in the science of AI. The measure of success will be whether these efforts have bridged the gap between these communities, translated principled agreement into actionable outcomes, and helped create the conditions for deeper trust between humans and machines.
The global collaboration will begin in late 2020 and early 2021 with a series of AI Policy Forum Task Forces, chaired by MIT researchers and bringing together the world’s leading technical and policy experts on some of the most pressing issues of AI policy, starting with AI in finance and mobility. Further task forces throughout 2021 will convene more communities of practice with the shared aim of designing the next chapter of AI: one that both delivers on AI’s innovative potential and responds to society’s needs.
Each task force will produce results that inform concrete public policies and frameworks for the next chapter of AI, and help define the roles that the academic and business communities, civil society, and governments will need to play in making it a reality. Research from the task forces will feed into the development of the AI Policy Framework, a dynamic assessment tool that will help governments gauge their own progress on AI policy-making goals and guide application of best practices appropriate to their own national priorities.
On May 6–7, 2021, MIT will host — most likely online — the first AI Policy Forum Summit, a two-day collaborative gathering to discuss the progress of the task forces towards equipping high-level decision-makers with a deeper understanding of the tools at their disposal — and trade-offs to be made — to produce better public policy around AI, and better AI systems with concern for public policy. Then, in fall 2021, a follow-on event at MIT will bring together leaders from across sectors and countries and, built atop the leading research from the task forces, the forum will provide a focal point for work to move from AI principles to AI practice, and serve as a springboard to global efforts to design the future of AI.


",A global collaboration to move artificial intelligence principles to practice,2020-10-19,[],MIT Schwarzman College of Computing/Artificial intelligence/Machine learning/Algorithms/Research/Technology and society/Ethics/Policy/Government/Industry/Collaboration/Global/Law/Computer science and technology/International relations,"['collaboration', 'principles', 'public', 'global', 'forum', 'practice', 'forces', 'policy', 'ai', 'policymakers', 'task', 'help', 'artificial', 'intelligence', 'mit']","Today, artificial intelligence — and the computing systems that underlie it — are more than just matters of technology; they are matters of state and society, of governance and the public interest.
Building on those broader principles, the AI Policy Forum, a global effort convened by the MIT Stephen A. Schwarzman College of Computing, will provide an overarching policy framework and tools for governments and companies to implement in concrete ways.
“Our goal is to help policymakers in making practical decisions about AI policy,” says Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing.
We need for this to be a global collaboration and engage scientists, technologists, policymakers, and business leaders,” says MIT Provost Martin Schmidt.
“This is a challenging and complex process for which we need all hands-on deck.”The AI Policy Forum is designed as a yearlong process.",Mit
68,https://news.mit.edu/2020/mit-full-steam-ahead-scalable-hands-on-remote-learning-1015,"


When we picture hands-on learning, usually a computer screen is nowhere in sight. But for the team behind MIT Full STEAM Ahead, “hands-on remote learning” may become the great new frontier for delivering quality K-12 online learning at scale.
Full STEAM Ahead began as an online resource hub to provide robust curated content to K-12 students, teachers, and parents during the first surge of the Covid-19 pandemic, when schools around the world started to shut down in rapid succession. With support from the Abdul Latif Jameel World Education Lab (J-WEL) and a monumental effort of coordination, the hub has grown into a vibrant learning community. Six months, 10 custom-created interactive learning packages, and two highly successful online summer programs later, the Full STEAM team has a lot to share about what makes engaging, effective remote learning experiences. Spoiler alert: it’s not high-tech gadgetry.
“Remote learning does not automatically mean ‘passive learning,’” says Claudia Urrea, senior associate director for pK-12 at J-WEL and co-leader of the project. “We’ve learned that with basic access to tech, and by using low-cost, widely available materials, learners can create and participate in effective hands-on learning, even at a distance.”
Spring: rapid response 
Perhaps MIT’s most ambitious response to date to the sudden crisis in pK-12 education, Full STEAM Ahead is a testament to the strength and ingenuity of MIT’s pK-12 community, and a demonstration of MIT's “mens et manus” (""mind and hand"") motto in action. Within days of the full-campus evacuation in mid-March, members of MIT’s pK-12 education community came together on Zoom to organize a coordinated response. It was Professor Eric Klopfer, director of the Scheller Teacher Education Program (STEP), who first suggested an online resource hub with a highly interactive hands-on learning component, and signed on to lead the project along with Urrea.
In a matter of weeks, the core team had built the site and were compiling resources in consultation with local school systems. Office of Government and Community Relations (OGCR) K-12 Outreach Administrator Rohan Kundargi communicated with colleagues in the Cambridge Public Schools in order to better understand their students’ and teachers’ needs. As a result, the interactive site was able to meet Massachusetts data privacy and security requirements, making it easier for teachers to turn to as a resource.
The content gathered on Full STEAM Ahead ranges from interactive programming languages geared toward young or beginner-level users, such as MIT App Inventor and Scratch; high-quality STEM-content video lessons (MIT BLOSSOMS); and course materials such as MIT OpenCourseWare Highlights for High School, all of which can be filtered by subject or interest area. Sponsored by J-WEL, the Full STEAM Ahead website also features curated MIT resources for learners at the higher education and workforce learning levels, in keeping with J-WEL’s commitment to transforming learning across those groups.
For each of 10 weeks between mid-April and the end of the academic year, Full STEAM Ahead released a new themed “Learning Package,” scaffolded by age group, for K-12 students and teachers. Different organizations across campus committed to curating each new package, an effort coordinated in large part by STEP Research Scientist Aditi Wagh. Lemelson-MIT, the Edgerton Center, the MIT Museum, the Education Arcade, three groups from the MIT Media Lab (Personal Robots, Lifelong Kindergarten, and Space Exploration Initiative), and others helped young learners explore topics such as spread of disease, invention education, nature photography, and artificial intelligence through engaging, low-cost interactive activities.










              

            Screenshot of a video by Leilani Roser, from the Edgerton Center ""Making Music and Sounds"" Learning Package        

          

            

            Image courtesy of MIT Full STEAM Ahead.        

          


















Previous item
Next item

















Making Music and Sounds, a two-part learning package from the Edgerton Center, encapsulated the program's guiding principles of accessibility, adaptability, and hands-on exploration of STEAM subjects through creative means. Through short video tutorials, learners were encouraged to explore music and sound-making by creating basic instruments or collections of sounds using household objects such as soda cans, rubber bands, or dishware. Downloadable activity packets, divided by age group, provided guidelines for engaging in offline hands-on learning experiences. Other videos in the packages prompted learners to consider questions such as “What is music?” or to explore the physical characteristics of the musicalized objects and the sounds they could make.
Over the course of the 10-week release, the Full STEAM Ahead website received 130,000 page views from 45,000 unique viewers. Learners in 150 countries accessed the learning packages, including Australia, Brazil, Canada, Hong Kong, India, Japan, Mexico, Turkey, and the United Kingdom.
Summer: keeping middle schoolers off the “Covid slide”
Recognizing that the potential for student learning loss would only grow as the universally haphazard school year ended, the Full STEAM Ahead team changed course heading into the summer. They reevaluated the needs of parents and teachers through conversations while gathering information from MIT students about their interest in mentorship opportunities, resulting in plans for an online summer program aimed at middle schoolers. The team worked with partner schools to target students at increased risk for ""Covid slide,"" or the phenomenon of falling behind because of the pandemic, as well as students from groups traditionally underrepresented in STEM fields.
The Full STEAM Ahead Into Summer (FSAIS) program received over 800 applications from students across the state of Massachusetts, ultimately welcoming 291 participants in grades 6-9 over the course of two three-week sessions. Thanks to funding from an anonymous donor and voluntary contributions from some participants’ families, FSAIS was able to send hands-on-learning kits to each participant, free of charge. MIT Open Learning Assistant Director for Special Projects Kirky DeLong oversaw efforts to assemble these kits, featuring an extensive collection of items ranging from basic office supplies to specialty items (cutting boards, small hacksaws, and water rocket launchers.) In addition to hands-on STEM activities, the program also offered a math-focused academic period run by STEP Program Manager Jenny Gardony and a book club run by STEP Research Scientist Meredith Thompson that featured the works of Christine Taylor-Butler '81.










              

            J-WEL’s Claudia Urrea and her daughter Martina work to assemble hands-on learning kits for use by students in the Full STEAM Ahead summer program. The kits were sent to each of the 291 participants free of charge, and included over 30 items ranging from general office supplies to specialty items such as hacksaws, cutting boards, and water rocket launchers.        

          

            

            Photo: Joe Diaz        

          


















Previous item
Next item

















The two programs kept students actively learning most of each day. According to student and parent feedback, there was nothing “passive” about their experiences; indeed, many of the comments could be describing traditional, in-person STEM summer camps. One parent remarked, “This was a phenomenal program for my son. I am so grateful for it, and I think it helped to build [his] confidence a bit where he was once defeated. He is more a bookworm than he is technical with his hands; however, this program allowed him ... to do activities with his hands which he wouldn't ordinarily do.”
Says pK-12 Program Coordinator Joe Diaz, “I was fairly skeptical about the prospect of bringing a true 'learning-by-doing' experience to the participating kids over an online platform, but between the guidance of the MIT student mentors and encouragement of the members of our team, it was great to witness the kids discovering skills with the same enthusiasm that we've come to expect in our in-person programs.”
Fall and beyond: expanding possibilities of hands-on remote learning
No one can say for sure how long parents and educators will need extra assistance delivering high-quality instruction to young learners. But it’s clear that the face of teaching and learning has been forever altered by the pandemic, and the lessons the Full STEAM Ahead team has learned will extend far into the future of 21st century education. 
For now, the group shows no signs of slowing down. This month, Full STEAM Ahead will host an online program for upwards of 600 international high school students living in Spain. Plans are also in progress for another program aimed at local high school students. 
Urrea, Klopfer, and core team members from STEP, J-WEL, MIT Open Learning and OGCR will share their insights in a forthcoming case study to be included in a peer-reviewed publication from Harvard Graduate School of Education and the Qatar Foundation. Their contribution will join 26 other such case studies from universities across the world, focusing on the role of higher education in supporting pK-12 learning during Covid-19.
For Urrea, there are two keys to the success of the program to date: the first is the team’s ability to work together to leverage existing resources, both in terms of talent and available tools. The second is its guiding principle of meeting learners where they are. “Everything we do is designed to be accessible across socioeconomic strata,” she says. “We see this as an extension of MIT values, including a commitment to access and equality.” 
There is an additional benefit to all this group work: a consolidated effort in which all members of MIT’s vast pK-12 community can participate. Klopfer notes that, although the many individuals, departments, labs, and centers have been coming together informally for years, ""we haven’t had a focal point for a lot of collaboration. I think it’s fantastic that we now have that in Full STEAM Ahead.” 


","MIT Full STEAM Ahead offers scalable, hands-on remote learning for K-12",2020-10-15,['Kate Stringer'],"School of Engineering/School of Humanities Arts and Social Sciences/School of Science/Office of Open Learning/Abdul Latif Jameel World Education Lab (J-WEL)/Lemelson-MIT/Edgerton/MIT Museum/Media Lab/online learning/STEM education/K-12 education/Mentoring/Education, teaching, academics/Cambridge, Boston and region/Classes and programs/Collaboration/Community/Covid-19/Pandemic","['education', 'remote', 'learning', 'ahead', 'steam', 'scalable', 'offers', 'handson', 'k12', 'summer', 'learners', 'program', 'mit', 'students']","When we picture hands-on learning, usually a computer screen is nowhere in sight.
But for the team behind MIT Full STEAM Ahead, “hands-on remote learning” may become the great new frontier for delivering quality K-12 online learning at scale.
Downloadable activity packets, divided by age group, provided guidelines for engaging in offline hands-on learning experiences.
Over the course of the 10-week release, the Full STEAM Ahead website received 130,000 page views from 45,000 unique viewers.
J-WEL’s Claudia Urrea and her daughter Martina work to assemble hands-on learning kits for use by students in the Full STEAM Ahead summer program.",Mit
69,https://news.mit.edu/2020/gaussian-machine-learning-tb-drug-1015,"


Machine learning is a computational tool used by many biologists to analyze huge amounts of data, helping them to identify potential new drugs. MIT researchers have now incorporated a new feature into these types of machine-learning algorithms, improving their prediction-making ability.
Using this new approach, which allows computer models to account for uncertainty in the data they’re analyzing, the MIT team identified several promising compounds that target a protein required by the bacteria that cause tuberculosis.
This method, which has previously been used by computer scientists but has not taken off in biology, could also prove useful in protein design and many other fields of biology, says Bonnie Berger, the Simons Professor of Mathematics and head of the Computation and Biology group in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL).
“This technique is part of a known subfield of machine learning, but people have not brought it to biology,” Berger says. “This is a paradigm shift, and is absolutely how biological exploration should be done.”
Berger and Bryan Bryson, an assistant professor of biological engineering at MIT and a member of the Ragon Institute of MGH, MIT, and Harvard, are the senior authors of the study, which appears today in Cell Systems. MIT graduate student Brian Hie is the paper’s lead author.
Better predictions
Machine learning is a type of computer modeling in which an algorithm learns to make predictions based on data that it has already seen. In recent years, biologists have begun using machine learning to scour huge databases of potential drug compounds to find molecules that interact with particular targets.
One limitation of this method is that while the algorithms perform well when the data they’re analyzing are similar to the data they were trained on, they’re not very good at evaluating molecules that are very different from the ones they have already seen.
To overcome that, the researchers used a technique called Gaussian process to assign uncertainty values to the data that the algorithms are trained on. That way, when the models are analyzing the training data, they also take into account how reliable those predictions are.
For example, if the data going into the model predict how strongly a particular molecule binds to a target protein, as well as the uncertainty of those predictions, the model can use that information to make predictions for protein-target interactions that it hasn’t seen before. The model also estimates the certainty of its own predictions. When analyzing new data, the model’s predictions may have lower certainty for molecules that are very different from the training data. Researchers can use that information to help them decide which molecules to test experimentally.
Another advantage of this approach is that the algorithm requires only a small amount of training data. In this study, the MIT team trained the model with a dataset of 72 small molecules and their interactions with more than 400 proteins called protein kinases. They were then able to use this algorithm to analyze nearly 11,000 small molecules, which they took from the ZINC database, a publicly available repository that contains millions of chemical compounds. Many of these molecules were very different from those in the training data.
Using this approach, the researchers were able to identify molecules with very strong predicted binding affinities for the protein kinases they put into the model. These included three human kinases, as well as one kinase found in Mycobacterium tuberculosis. That kinase, PknB, is critical for the bacteria to survive, but is not targeted by any frontline TB antibiotics.
The researchers then experimentally tested some of their top hits to see how well they actually bind to their targets, and found that the model’s predictions were very accurate. Among the molecules that the model assigned the highest certainty, about 90 percent proved to be true hits — much higher than the 30 to 40 percent hit rate of existing machine learning models used for drug screens.
The researchers also used the same training data to train a traditional machine-learning algorithm, which does not incorporate uncertainty, and then had it analyze the same 11,000 molecule library. “Without uncertainty, the model just gets horribly confused and it proposes very weird chemical structures as interacting with the kinases,” Hie says.
The researchers then took some of their most promising PknB inhibitors and tested them against Mycobacterium tuberculosis grown in bacterial culture media, and found that they inhibited bacterial growth. The inhibitors also worked in human immune cells infected with the bacterium.
A good starting point
Another important element of this approach is that once the researchers get additional experimental data, they can add it to the model and retrain it, further improving the predictions. Even a small amount of data can help the model get better, the researchers say.
“You don’t really need very large data sets on each iteration,” Hie says. “You can just retrain the model with maybe 10 new examples, which is something that a biologist can easily generate.”
This study is the first in many years to propose new molecules that can target PknB, and should give drug developers a good starting point to try to develop drugs that target the kinase, Bryson says. “We’ve now provided them with some new leads beyond what has been already published,” he says.
The researchers also showed that they could use this same type of machine learning to boost the fluorescent output of a green fluorescent protein, which is commonly used to label molecules inside living cells. It could also be applied to many other types of biological studies, says Berger, who is now using it to analyze mutations that drive tumor development.
The research was funded by the U.S. Department of Defense through the National Defense Science and Engineering Graduate Fellowship; the National Institutes of Health; the Ragon Institute of MGH, MIT, and Harvard’ and MIT’s Department of Biological Engineering.


",Machine learning uncovers potential new TB drugs,2020-10-15,['Anne Trafton'],Research/Mathematics/Electrical Engineering & Computer Science (eecs)/Biological engineering/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Broad Institute/School of Science/School of Engineering/MIT Schwarzman College of Computing/Drug discovery/Artificial intelligence/Machine learning/Medicine,"['data', 'protein', 'model', 'uncertainty', 'learning', 'researchers', 'drugs', 'predictions', 'used', 'potential', 'tb', 'uncovers', 'machine', 'molecules', 'mit']","Machine learning is a computational tool used by many biologists to analyze huge amounts of data, helping them to identify potential new drugs.
MIT researchers have now incorporated a new feature into these types of machine-learning algorithms, improving their prediction-making ability.
“This technique is part of a known subfield of machine learning, but people have not brought it to biology,” Berger says.
In recent years, biologists have begun using machine learning to scour huge databases of potential drug compounds to find molecules that interact with particular targets.
To overcome that, the researchers used a technique called Gaussian process to assign uncertainty values to the data that the algorithms are trained on.",Mit
70,https://news.mit.edu/2020/mit-proto-ventures-program-readies-new-startups-for-launch-1013,"


Powered by the MIT Innovation Initiative (MITii) and launched in October 2019, the MIT Proto Ventures program takes an entirely new approach to venture formation from within MIT. It oversees the accelerated emergence of new ventures along a full life cycle: from discovery of ideas and resources at MIT to exploration of the problem-solution space to a methodical de-risking process to helping build a “proto venture” with internal and external support that demonstrates the viability of the venture.
Under the leadership of MITii Venture Builder Luis Ruben Soenksen PhD '19, the program announced last week that two new MIT startups will launch as part of Proto Ventures.
“The Proto Ventures program has nurtured everything I know is relevant in order to develop high-impact scientific ventures in today’s world,” says Soenksen. “This time has been an extraordinary complement to my PhD studies and research activities at MIT and is leading me to pursue my passion around artificial intelligence and health care in the form of multiple startups. What more could I’ve hoped for?”
In its first year, the Proto Ventures program has led to hundreds of ecosystem interactions, and a log of 319 screened concepts needed to identify the kind of high-impact proto ventures that would attract long-term collaboration with multiple faculty, students, and staff across MIT.
The program hosted its AI+Healthcare forum in February, bringing together 48 representatives of corporations, startups, venture capital firms, local hospitals, the pharmaceutical industry, and MIT researchers. Thirty-eight curated “proto venture” ideas were discussed and screened through roundtables, expert discussions and a voting process to gather multi-sectoral input on the anticipated value and impact of the proposed ventures, initiatives, and organizations at the intersection of artificial intelligence and health care.
Two startups emerged from within the program’s AI+Healthcare track, which was sponsored by MIT’s Abdul Latif Jameel Clinic for Machine Learning in Health (Jameel Clinic):

TOCI develops state-of-the-art AI tools that help time-constrained doctors and care-givers to effortlessly regain quality time and human connection in front of their patients; and
	 
Medicall offers enjoyable one-shot telemedicine using its unique AI that fully automates the repetitive process of asynchronous information collection and differential pre-reporting even before the visit begins.

Going forward, Jameel Clinic will assume day-to-day management of this AI+Healthcare channel, promoting and accelerating these proto ventures along with further launches of the program.
MIT Innovation Initiative actively seeks new Venture Builders to develop additional channels within the MIT Proto Ventures program.
“The role of MIT Venture Builder is one of the most exciting opportunities for an entrepreneur in the MIT community”, says MIT Innovation Initiative Executive Director Gene Keselman. “It’s a unique license to explore and tap into the resources of the entire Institute, bring together people and organizations that may not otherwise have collaborated, to rapidly explore, invent, iterate, and launch.”
Visit the MIT Proto Ventures Program or email proto.ventures@mit.edu to learn more, including how to become a channel sponsor or the next Venture Builder.


",MIT Proto Ventures program readies new startups for launch,2020-10-13,['David Sweeney'],School of Engineering/Sloan School of Management/Innovation Initiative/Artificial intelligence/Health/Health care/Innovation and Entrepreneurship (I&E)/Startups/Classes and programs/Business and management,"['process', 'innovation', 'readies', 'jameel', 'proto', 'launch', 'initiative', 'ventures', 'startups', 'program', 'mit', 'venture']","Powered by the MIT Innovation Initiative (MITii) and launched in October 2019, the MIT Proto Ventures program takes an entirely new approach to venture formation from within MIT.
Under the leadership of MITii Venture Builder Luis Ruben Soenksen PhD '19, the program announced last week that two new MIT startups will launch as part of Proto Ventures.
“The Proto Ventures program has nurtured everything I know is relevant in order to develop high-impact scientific ventures in today’s world,” says Soenksen.
MIT Innovation Initiative actively seeks new Venture Builders to develop additional channels within the MIT Proto Ventures program.
“The role of MIT Venture Builder is one of the most exciting opportunities for an entrepreneur in the MIT community”, says MIT Innovation Initiative Executive Director Gene Keselman.",Mit
71,https://news.mit.edu/2020/less-scatterbrained-scatterplots-1007,"


Scatterplots. You might not know them by name, but if you spend more than 10 minutes online you’ll find them everywhere. They’re popular in news articles, in the data science community, and, perhaps most crucially, for internet memes about the digestive quality of pancakes. 
By depicting data as a mass of points across two axes, scatterplots are effective in visualizing trends, correlations, and anomalies. But using them for large datasets often leads to overlapping dots that make them more or less unreadable.
Researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) say they’ve solved this with a new open-source system that makes it possible to create interactive scatterplots based on large-scale datasets that have upwards of billions of distinct data points. 







Play video






Called “Kyrix-S,” the system has an interface that allows users to pan, zoom, and jump around a scatterplot as if they were looking at directions on Google Maps. Whereas other systems developed for large datasets often focus on very specific applications, Kyrix-S is generalizable enough to work for a wide range of visualization styles, including heat maps, pie charts, and radar-style graphics. (The team showed that the system allows users to create visualizations with 800 percent less code compared to a similar state-of-the-art authoring system.)
Users can produce a scatterplot by just writing a few dozen lines of JSON, a human-readable text format.
Lead developer Wenbo Tao, a PhD student at MIT CSAIL, gives the example of a static New York Times scatterplot that he says would improve by being made interactive via a system like Kyrix-S.










              

            A static New York Times scatterplot        

          




















Previous item
Next item

















“In these scatterplots, you are able to see overall trends and outliers, but the overplotting and the static nature of the plot limit the user's ability to interact with the chart,” says Tao. 
In contrast, Kyrix-S can produce a version (below) that puts data in several zoom levels, enabling interaction with each county. To avoid overplotting, Kyrix-S’ scatterplot also shows only the most important examples, like the most populous counties.
Kyrix-S is currently being used by Data Civilizer 2.0, a data integration platform developed at MIT. An earlier version was also employed to help Massachusetts General Hospital analyze a massive brain activity dataset (EEG) that clocks in at 30 terabytes — the equivalent of more than 50,000 hours of digital music. (The goal of that study was to train a model that predicts seizures, given a series of 2-second EEG segments.)
Moving forward, the researchers will be adapting Kyrix-S to work as part of a graphical user interface. They also plan to add functionality so that the system can handle data that are being continuously updated.
Tao wrote a paper about Kyrix-S alongside MIT Adjunct Professor Mike Stonebraker, researchers Xinli Hou and Adam Sah, Leilani Battle SM '13, PhD '17, and Professor Remco Chang of Tufts University. It will be presented virtually at IEEE’s VIS data visualization conference Oct. 25.


",Less scatterbrained scatterplots,2020-10-07,['Adam Conner-Simons'],MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Research/Design/Science communications/Data/Data visualization,"['data', 'datasets', 'users', 'static', 'times', 'scatterbrained', 'york', 'scatterplot', 'system', 'scatterplots', 'science']","Scatterplots.
They’re popular in news articles , in the data science community , and, perhaps most crucially, for internet memes about the digestive quality of pancakes .
By depicting data as a mass of points across two axes, scatterplots are effective in visualizing trends, correlations, and anomalies.
(The team showed that the system allows users to create visualizations with 800 percent less code compared to a similar state-of-the-art authoring system.)
Lead developer Wenbo Tao, a PhD student at MIT CSAIL, gives the example of a static New York Times scatterplot that he says would improve by being made interactive via a system like Kyrix-S.A static New York Times scatterplot Previous item Next item",Mit
72,https://news.mit.edu/2020/moral-decisions-universalization-1002,"


Imagine that one day you’re riding the train and decide to hop the turnstile to avoid paying the fare. It probably won’t have a big impact on the financial well-being of your local transportation system. But now ask yourself, “What if everyone did that?” The outcome is much different — the system would likely go bankrupt and no one would be able to ride the train anymore.
Moral philosophers have long believed this type of reasoning, known as universalization, is the best way to make moral decisions. But do ordinary people spontaneously use this kind of moral judgment in their everyday lives?
In a study of several hundred people, MIT and Harvard University researchers have confirmed that people do use this strategy in particular situations called “threshold problems.” These are social dilemmas in which harm can occur if everyone, or a large number of people, performs a certain action. The authors devised a mathematical model that quantitatively predicts the judgments they are likely to make. They also showed, for the first time, that children as young as 4 years old can use this type of reasoning to judge right and wrong.
“This mechanism seems to be a way that we spontaneously can figure out what are the kinds of actions that I can do that are sustainable in my community,” says Sydney Levine, a postdoc at MIT and Harvard and the lead author of the study.
Other authors of the study are Max Kleiman-Weiner, a postdoc at MIT and Harvard; Laura Schulz, an MIT professor of cognitive science; Joshua Tenenbaum, a professor of computational cognitive science at MIT and a member of MIT’s Center for Brains, Minds, and Machines and Computer Science and Artificial Intelligence Laboratory (CSAIL); and Fiery Cushman, an assistant professor of psychology at Harvard. The paper is appearing this week in the Proceedings of the National Academy of Sciences.
Judging morality
The concept of universalization has been included in philosophical theories since at least the 1700s. Universalization is one of several strategies that philosophers believe people use to make moral judgments, along with outcome-based reasoning and rule-based reasoning. However, there have been few psychological studies of universalization, and many questions remain regarding how often this strategy is used, and under what circumstances.
To explore those questions, the MIT/Harvard team asked participants in their study to evaluate the morality of actions taken in situations where harm could occur if too many people perform the action. In one hypothetical scenario, John, a fisherman, is trying to decide whether to start using a new, more efficient fishing hook that will allow him to catch more fish. However, if every fisherman in his village decided to use the new hook, there would soon be no fish left in the lake.
The researchers found that many subjects did use universalization to evaluate John’s actions, and that their judgments depended on a variety of factors, including the number of people who were interested in using the new hook and the number of people using it that would trigger a harmful outcome.
To tease out the impact of those factors, the researchers created several versions of the scenario. In one, no one else in the village was interested in using the new hook, and in that scenario, most participants deemed it acceptable for John to use it. However, if others in the village were interested but chose not to use it, then John’s decision to use it was judged to be morally wrong.
The researchers also found that they could use their data to create a mathematical model that explains how people take different factors into account, such as the number of people who want to do the action and the number of people doing it that would cause harm. The model accurately predicts how people’s judgments change when these factors change.
In their last set of studies, the researchers created scenarios that they used to test judgments made by children between the ages of 4 and 11. One story featured a child who wanted to take a rock from a path in a park for his rock collection. Children were asked to judge if that was OK, under two different circumstances: In one, only one child wanted a rock, and in the other, many other children also wanted to take rocks for their collections.
The researchers found that most of the children deemed it wrong to take a rock if everyone wanted to, but permissible if there was only one child who wanted to do it. However, the children were not able to specifically explain why they had made those judgments.
“What's interesting about this is we discovered that if you set up this carefully controlled contrast, the kids seem to be using this computation, even though they can't articulate it,” Levine says. “They can't introspect on their cognition and know what they're doing and why, but they seem to be deploying the mechanism anyway.”
In future studies, the researchers hope to explore how and when the ability to use this type of reasoning develops in children.
Collective action
In the real world, there are many instances where universalization could be a good strategy for making decisions, but it’s not necessary because rules are already in place governing those situations.
“There are a lot of collective action problems in our world that can be solved with universalization, but they're already solved with governmental regulation,” Levine says. “We don't rely on people to have to do that kind of reasoning, we just make it illegal to ride the bus without paying.”
However, universalization can still be useful in situations that arise suddenly, before any government regulations or guidelines have been put in place. For example, at the beginning of the Covid-19 pandemic, before many local governments began requiring masks in public places, people contemplating wearing masks might have asked themselves what would happen if everyone decided not to wear one.
The researchers now hope to explore the reasons why people sometimes don’t seem to use universalization in cases where it could be applicable, such as combating climate change. One possible explanation is that people don’t have enough information about the potential harm that can result from certain actions, Levine says.
The research was funded by the John Templeton Foundation, the Templeton World Charity Foundation, and the Center for Brains, Minds, and Machines.


",How we make moral decisions,2020-10-02,['Anne Trafton'],Research/Brain and cognitive sciences/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Center for Brains Minds and Machines/School of Science/School of  Engineering,"['decisions', 'reasoning', 'wanted', 'using', 'children', 'moral', 'researchers', 'number', 'judgments', 'rock', 'universalization', 'mit']","Moral philosophers have long believed this type of reasoning, known as universalization, is the best way to make moral decisions.
But do ordinary people spontaneously use this kind of moral judgment in their everyday lives?
Universalization is one of several strategies that philosophers believe people use to make moral judgments, along with outcome-based reasoning and rule-based reasoning.
One story featured a child who wanted to take a rock from a path in a park for his rock collection.
Collective actionIn the real world, there are many instances where universalization could be a good strategy for making decisions, but it’s not necessary because rules are already in place governing those situations.",Mit
73,https://news.mit.edu/2020/mit-researchers-collaborators-work-prepare-manufacturers-future-crises-1002,"


At the beginning of the Covid-19 crisis, the state of Massachusetts assembled a manufacturing emergency response team as part of its efforts to respond to the desperate need for personal protective equipment (PPE), particularly masks and gowns. The Massachusetts Emergency Response Team (M-ERT) — aided by MIT faculty, students, staff, and alumni — helped local manufacturers produce more than 9 million pieces of PPE as well as large volumes of hand sanitizer, disinfectants, and test swabs.

Building on the experiences and knowledge gained through the work of M-ERT, a new project, which was recently awarded funding from the National Science Foundation (NSF), is developing a network collaboration model designed to help ecosystems organize and enable manufacturers to rapidly “pivot,” in an emergency, from producing their standard products to producing PPE or other urgently needed goods. Elisabeth Reynolds, executive director of the MIT Task Force on the Work of the Future and the MIT Industrial Performance Center, John Hart, professor of mechanical engineering and director of the Laboratory for Manufacturing and Productivity, Ben Linville-Engler, industry and certificate director of the System Design and Management program, and Haden Quinlan, program manager for MIT’s Center for Additive and Digital Advanced Production Technologies, are collaborating with researchers from the University of Massachusetts at Lowell and the Worcester Polytechnic Institute, as well as the Massachusetts Technology Collaborative.

“The Massachusetts manufacturing ecosystem proved to be extremely valuable in response to Covid-19,” says Reynolds, “and it was activated in an important way because of the M-ERT collaboration.”

The NSF grant will allow researchers to gather and learn from the data from the recent emergency manufacturing effort, and also design a network and collaboration model applicable to manufacturing in future crises. The RESPOND network (Rapid Execution for Scaling Production Of Needed Designs) will support the establishment of a multidisciplinary, diverse stakeholder ecosystem that can help support the production of new products in large volumes during times of crises.

“This grant allows us to retrospectively study what we’ve done [with M-ERT],” says Linville-Engler, “so we can look at doing this proactively in the future, undertaking efforts of ecosystem engineering and manufacturing. We can look at how people operate in a network of information like this.”

Linville-Engler describes this type of network modeling as looking at the “network of networks,” with evolving uncertainties, needs, and demands across the different nodes.

“This project highlights important opportunities to use digital tools to advance  manufacturing in the regional and national spheres,” says Hart. “We hope to implement our learnings at scale, and help impart agility in the manufacturing ecosystem.”

In addition to the RESPOND network project, a number of other MIT-based manufacturing efforts have recently received federal funding. A new, online agile manufacturing course is being designed and taught by Hart, Linville-Engler, and Quinlan. Reynolds, along with Julie Shah, associate professor in the Department of Aeronautics and Astronautics and the Computer Science and Artificial Intelligence Laboratory, and Paul Osterman, professor of human resources and management at MIT Sloan, also received an NSF planning grant toward understanding the human-technology frontier as part of research related to the MIT Task Force on the Work of the Future. 

“These efforts represent a renewed commitment to manufacturing in this country,” says Reynolds. “We’re at a time of a real inflection point in the world of advanced manufacturing, punctuated by new data and new challenges.”


",MIT researchers and collaborators work to prepare manufacturers for future crises,2020-10-02,['Stefanie Koperniak'],"National Science Foundation (NSF)/MIT Sloan School of Management/Industrial Performance Center/Mechanical engineering/System Design and Management/Center for Additive and Digital Advanced Production Technologies (APT)/Manufacturing/Cambridge, Boston and region/Covid-19/Pandemic","['massachusetts', 'reynolds', 'efforts', 'future', 'collaborators', 'researchers', 'emergency', 'mert', 'work', 'crises', 'network', 'prepare', 'manufacturing', 'manufacturers', 'response', 'mit']","The Massachusetts Emergency Response Team (M-ERT) — aided by MIT faculty, students, staff, and alumni — helped local manufacturers produce more than 9 million pieces of PPE as well as large volumes of hand sanitizer, disinfectants, and test swabs.
“This project highlights important opportunities to use digital tools to advance manufacturing in the regional and national spheres,” says Hart.
“We hope to implement our learnings at scale, and help impart agility in the manufacturing ecosystem.”In addition to the RESPOND network project, a number of other MIT-based manufacturing efforts have recently received federal funding.
“These efforts represent a renewed commitment to manufacturing in this country,” says Reynolds.
“We’re at a time of a real inflection point in the world of advanced manufacturing, punctuated by new data and new challenges.”",Mit
74,https://news.mit.edu/2020/anticipating-heart-failure-machine-learning-1001,"


Every year, roughly one out of eight U.S. deaths is caused at least in part by heart failure. One of acute heart failure’s most common warning signs is excess fluid in the lungs, a condition known as “pulmonary edema.” 
A patient’s exact level of excess fluid often dictates the doctor’s course of action, but making such determinations is difficult and requires clinicians to rely on subtle features in X-rays that sometimes lead to inconsistent diagnoses and treatment plans.
To better handle that kind of nuance, a group led by researchers at MIT’s Computer Science and Artificial Intelligence Lab (CSAIL) has developed a machine learning model that can look at an X-ray to quantify how severe the edema is, on a four-level scale ranging from 0 (healthy) to 3 (very, very bad). The system determined the right level more than half of the time, and correctly diagnosed level 3 cases 90 percent of the time.
Working with Beth Israel Deaconess Medical Center (BIDMC) and Philips, the team plans to integrate the model into BIDMC’s emergency-room workflow this fall.
“This project is meant to augment doctors’ workflow by providing additional information that can be used to inform their diagnoses as well as enable retrospective analyses,” says PhD student Ruizhi Liao, who was the co-lead author of a related paper with fellow PhD student Geeticka Chauhan and MIT professors Polina Golland and Peter Szolovits. 
The team says that better edema diagnosis would help doctors manage not only acute heart issues, but other conditions like sepsis and kidney failure that are strongly associated with edema. 
As part of a separate journal article, Liao and colleagues also took an existing public dataset of X-ray images and developed new annotations of severity labels that were agreed upon by a team of four radiologists. Liao’s hope is that these consensus labels can serve as a universal standard to benchmark future machine learning development.
An important aspect of the system is that it was trained not just on more than 300,000 X-ray images, but also on the corresponding text of reports about the X-rays that were written by radiologists. The team was pleasantly surprised that their system found such success using these reports, most of which didn’t have labels explaining the exact severity level of the edema.
“By learning the association between images and their corresponding reports, the method has the potential for a new way of automatic report generation from the detection of image-driven findings,” says Tanveer Syeda-Mahmood, a researcher not involved in the project who serves as chief scientist for IBM’s Medical Sieve Radiology Grand Challenge. “Of course, further experiments would have to be done for this to be broadly applicable to other findings and their fine-grained descriptors.”
Chauhan’s efforts focused on helping the system make sense of the text of the reports, which could often be as short as a sentence or two. Different radiologists write with varying tones and use a range of terminology, so the researchers had to develop a set of linguistic rules and substitutions to ensure that data could be analyzed consistently across reports. This was in addition to the technical challenge of designing a model that can jointly train the image and text representations in a meaningful manner.
“Our model can turn both images and text into compact numerical abstractions from which an interpretation can be derived,” says Chauhan. “We trained it to minimize the difference between the representations of the X-ray images and the text of the radiology reports, using the reports to improve the image interpretation.”
On top of that, the team’s system was also able to “explain” itself, by showing which parts of the reports and areas of X-ray images correspond to the model prediction. Chauhan is hopeful that future work in this area will provide more detailed lower-level image-text correlations, so that clinicians can build a taxonomy of images, reports, disease labels and relevant correlated regions. 
“These correlations will be valuable for improving search through a large database of X-ray images and reports, to make retrospective analysis even more effective,” Chauhan says.
Chauhan, Golland, Liao and Szolovits co-wrote the paper with MIT Assistant Professor Jacob Andreas, Professor William Wells of Brigham and Women’s Hospital, Xin Wang of Philips, and Seth Berkowitz and Steven Horng of BIDMC. The paper will be presented Oct. 5 (virtually) at the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). 
The work was supported in part by the MIT Deshpande Center for Technological Innovation, the MIT Lincoln Lab, the National Institutes of Health, Philips, Takeda, and the Wistron Corporation.


",Anticipating heart failure with machine learning,2020-10-01,['Adam Conner-Simons'],MIT Schwarzman College of Computing/School of Engineering/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Algorithms/Research/Computer science and technology/Medicine/Health care/Artificial intelligence/Machine learning/Imaging,"['labels', 'anticipating', 'failure', 'heart', 'model', 'learning', 'images', 'text', 'level', 'reports', 'xray', 'system', 'machine', 'mit', 'team']","Every year, roughly one out of eight U.S. deaths is caused at least in part by heart failure.
The system determined the right level more than half of the time, and correctly diagnosed level 3 cases 90 percent of the time.
Liao’s hope is that these consensus labels can serve as a universal standard to benchmark future machine learning development.
“Our model can turn both images and text into compact numerical abstractions from which an interpretation can be derived,” says Chauhan.
“These correlations will be valuable for improving search through a large database of X-ray images and reports, to make retrospective analysis even more effective,” Chauhan says.",Mit
75,https://news.mit.edu/2020/provably-exact-artificial-intelligence-nuclear-particle-physics-0924,"


The Standard Model of particle physics describes all the known elementary particles and three of the four fundamental forces governing the universe; everything except gravity. These three forces — electromagnetic, strong, and weak — govern how particles are formed, how they interact, and how the particles decay.
Studying particle and nuclear physics within this framework, however, is difficult, and relies on large-scale numerical studies. For example, many aspects of the strong force require numerically simulating the dynamics at the scale of 1/10th to 1/100th the size of a proton to answer fundamental questions about the properties of protons, neutrons, and nuclei.
“Ultimately, we are computationally limited in the study of proton and nuclear structure using lattice field theory,” says assistant professor of physics Phiala Shanahan. “There are a lot of interesting problems that we know how to address in principle, but we just don’t have enough compute, even though we run on the largest supercomputers in the world.”
To push past these limitations, Shanahan leads a group that combines theoretical physics with machine learning models. In their paper “Equivariant flow-based sampling for lattice gauge theory,” published this month in Physical Review Letters, they show how incorporating the symmetries of physics theories into machine learning and artificial intelligence architectures can provide much faster algorithms for theoretical physics. 
“We are using machine learning not to analyze large amounts of data, but to accelerate first-principles theory in a way which doesn’t compromise the rigor of the approach,” Shanahan says. “This particular work demonstrated that we can build machine learning architectures with some of the symmetries of the Standard Model of particle and nuclear physics built in, and accelerate the sampling problem we are targeting by orders of magnitude.” 
Shanahan launched the project with MIT graduate student Gurtej Kanwar and with Michael Albergo, who is now at NYU. The project expanded to include Center for Theoretical Physics postdocs Daniel Hackett and Denis Boyda, NYU Professor Kyle Cranmer, and physics-savvy machine-learning scientists at Google Deep Mind, Sébastien Racanière and Danilo Jimenez Rezende.
This month’s paper is one in a series aimed at enabling studies in theoretical physics that are currently computationally intractable. “Our aim is to develop new algorithms for a key component of numerical calculations in theoretical physics,” says Kanwar. “These calculations inform us about the inner workings of the Standard Model of particle physics, our most fundamental theory of matter. Such calculations are of vital importance to compare against results from particle physics experiments, such as the Large Hadron Collider at CERN, both to constrain the model more precisely and to discover where the model breaks down and must be extended to something even more fundamental.”
The only known systematically controllable method of studying the Standard Model of particle physics in the nonperturbative regime is based on a sampling of snapshots of quantum fluctuations in the vacuum. By measuring properties of these fluctuations, one can infer properties of the particles and collisions of interest.
This technique comes with challenges, Kanwar explains. “This sampling is expensive, and we are looking to use physics-inspired machine learning techniques to draw samples far more efficiently,” he says. “Machine learning has already made great strides on generating images, including, for example, recent work by NVIDIA to generate images of faces 'dreamed up' by neural networks. Thinking of these snapshots of the vacuum as images, we think it's quite natural to turn to similar methods for our problem.”
Adds Shanahan, “In our approach to sampling these quantum snapshots, we optimize a model that takes us from a space that is easy to sample to the target space: given a trained model, sampling is then efficient since you just need to take independent samples in the easy-to-sample space, and transform them via the learned model.”
In particular, the group has introduced a framework for building machine-learning models that exactly respect a class of symmetries, called ""gauge symmetries,"" crucial for studying high-energy physics.
As a proof of principle, Shanahan and colleagues used their framework to train machine-learning models to simulate a theory in two dimensions, resulting in orders-of-magnitude efficiency gains over state-of-the-art techniques and more precise predictions from the theory. This paves the way for significantly accelerated research into the fundamental forces of nature using physics-informed machine learning.
The group’s first few papers as a collaboration discussed applying the machine-learning technique to a simple lattice field theory, and developed this class of approaches on compact, connected manifolds which describe the more complicated field theories of the Standard Model. Now they are working to scale the techniques to state-of-the-art calculations.
“I think we have shown over the past year that there is a lot of promise in combining physics knowledge with machine learning techniques,” says Kanwar. “We are actively thinking about how to tackle the remaining barriers in the way of performing full-scale simulations using our approach. I hope to see the first application of these methods to calculations at scale in the next couple of years. If we are able to overcome the last few obstacles, this promises to extend what we can do with limited resources, and I dream of performing calculations soon that give us novel insights into what lies beyond our best understanding of physics today.”

This idea of physics-informed machine learning is also known by the team as “ab-initio AI,” a key theme of the recently launched MIT-based National Science Foundation Institute for Artificial Intelligence and Fundamental Interactions (IAIFI), where Shanahan is research coordinator for physics theory.
Led by the Laboratory for Nuclear Science, the IAIFI is comprised of both physics and AI researchers at MIT and Harvard, Northeastern, and Tufts universities.
“Our collaboration is a great example of the spirit of IAIFI, with a team with diverse backgrounds coming together to advance AI and physics simultaneously” says Shanahan. As well as research like Shanahan’s targeting physics theory, IAIFI researchers are also working to use AI to enhance the scientific potential of various facilities, including the Large Hadron Collider and the Laser Interferometer Gravity Wave Observatory, and to advance AI itself. 


",Provably exact artificial intelligence for nuclear and particle physics,2020-09-24,['Sandi Miller'],School of Science/Physics/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Laboratory for Nuclear Science/Artificial intelligence/LIGO/Research,"['provably', 'particle', 'model', 'learning', 'theoretical', 'standard', 'exact', 'artificial', 'nuclear', 'machine', 'intelligence', 'physics', 'theory', 'sampling', 'shanahan']","The Standard Model of particle physics describes all the known elementary particles and three of the four fundamental forces governing the universe; everything except gravity.
This month’s paper is one in a series aimed at enabling studies in theoretical physics that are currently computationally intractable.
“Our aim is to develop new algorithms for a key component of numerical calculations in theoretical physics,” says Kanwar.
“These calculations inform us about the inner workings of the Standard Model of particle physics, our most fundamental theory of matter.
This paves the way for significantly accelerated research into the fundamental forces of nature using physics-informed machine learning.",Mit
76,https://news.mit.edu/2020/regina-barzilay-wins-aaai-squirrel-ai-award-artificial-intelligence-0923,"


For more than 100 years Nobel Prizes have been given out annually to recognize breakthrough achievements in chemistry, literature, medicine, peace, and physics. As these disciplines undoubtedly continue to impact society, newer fields like artificial intelligence (AI) and robotics have also begun to profoundly reshape the world.
In recognition of this, the world’s largest AI society — the Association for the Advancement of Artificial Intelligence (AAAI) — announced today the winner of their new Squirrel AI Award for Artificial Intelligence for the Benefit of Humanity, a $1 million award given to honor individuals whose work in the field has had a transformative impact on society.
The recipient, Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science at MIT and a member of MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), is being recognized for her work developing machine learning models to develop antibiotics and other drugs, and to detect and diagnose breast cancer at early stages.
In February, AAAI will officially present Barzilay with the award, which comes with an associated prize of $1 million provided by the online education company Squirrel AI. 
“Only world-renowned recognitions, such as the Association of Computing Machinery’s A.M. Turing Award and the Nobel Prize, carry monetary rewards at the million-dollar level,” says AAAI awards committee chair Yolanda Gil. “This award aims to be unique in recognizing the positive impact of artificial intelligence for humanity.” 
Barzilay has conducted research on a range of topics in computer science, ranging from explainable machine learning to deciphering dead languages. Since surviving breast cancer in 2014, she has increasingly focused her efforts on health care. She created algorithms for early breast cancer diagnosis and risk assessment that have been tested at multiple hospitals around the globe, including in Sweden, Taiwan, and at Boston’s Massachusetts General Hospital. She is now working with breast cancer organizations such as Institute Protea in Brazil to make her diagnostic tools available for underprivileged populations around the world. (She realized from doing her work that, if a system like hers had existed at the time, her doctors actually could have detected her cancer two or three years earlier.) 
In parallel, she has been working on developing machine learning models for drug discovery: with collaborators she’s created models for selecting molecule candidates for therapeutics that have been able to speed up drug development, and last year helped discover a new antibiotic called Halicin that was shown to be able to kill many species of disease-causing bacteria that are antibiotic-resistant, including Acinetobacter baumannii and clostridium difficile (“c-diff”). 
“Through my own life experience, I came to realize that we can create technology that can alleviate human suffering and change our understanding of diseases,“ says Barzilay, who is also a member of the Koch Institute for Integrative Cancer Research. “I feel lucky to have found collaborators who share my passion and who have helped me realize this vision.”
Barzilay also serves as a member of MIT’s Institute for Medical Engineering and Science, and as faculty co-lead for MIT’s Abdul Latif Jameel Clinic for Machine Learning in Health. One of the Jameel Clinic’s most recent efforts is “AI Cures,” a cross-institutional initiative focused on developing affordable Covid-19 antivirals. 
“Regina has made truly-changing breakthroughs in imaging breast cancer and predicting the medicinal activity of novel chemicals,” says MIT professor of biology Phillip Sharp, a Nobel laureate who has served as director of both the McGovern Institute for Brain Research and the MIT Center for Cancer Research, predecessor to the Koch Institute. “I am honored to have as a colleague someone who is such a pioneer in using deeply creative machine learning methods to transform the fields of health care and biological science.”
Barzilay joined the MIT faculty in 2003 after earning her undergraduate at Ben-Gurion University of the Negev, Israel and her PhD at Columbia University. She is also the recipient of a MacArthur “genius grant”, the National Science Foundation Career Award, a Microsoft Faculty Fellowship, multiple “best paper” awards in her field, and MIT’s Jamieson Award for excellence in teaching.







Play video






""We believe AI advances will benefit a great many fields, from health care and education to smart cities and the environment,"" says Derek Li, founder and chairman of Squirrel AI. “We believe that Dr. Barzilay and other future awardees will inspire the AI community to continue to contribute to and advance AI’s impact on the world.”
AAAI’s Gil says the organization was very excited to partner with Squirrel AI for this new award to recognize the positive impacts of artificial intelligence “to protect, enhance, and improve human life in meaningful ways.” With more than 300 elected fellows and 6,000 members from 50 countries across the globe, AAAI is the world’s largest scientific society devoted to artificial intelligence. Its officers have included many AI pioneers, including Allen Newell and John McCarthy. AAAI confers several influential AI awards including the Feigenbaum Prize, the Newell Award (jointly with ACM), and the Engelmore Award. 
“Regina has been a trailblazer in the field of health care AI by asking the important questions about how we can use machine learning to treat and diagnose diseases,” says Daniela Rus, director of CSAIL and the Andrew (1956) and Erna Viterbi Professor of Electrical Engineering and Computer Science. “She has been both a brilliant researcher and a devoted educator, and all of us at CSAIL are so inspired by her work and proud to have her as a colleague.” 


",Regina Barzilay wins $1M Association for the Advancement of Artificial Intelligence Squirrel AI award,2020-09-23,['Adam Conner-Simons'],"MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Koch Institute/McGovern Institute/Awards, honors and fellowships/Faculty/Artificial intelligence/Computer science and technology/Machine learning/Cancer/Antibiotics/Health care/Health sciences and technology/Pharmaceuticals/Medicine/Institute for Medical Engineering and Science (IMES)/School of  Engineering","['association', 'barzilay', 'institute', 'research', 'cancer', 'advancement', 'regina', 'learning', 'breast', 'award', 'ai', 'squirrel', 'artificial', 'machine', 'intelligence', 'science', 'wins']","As these disciplines undoubtedly continue to impact society, newer fields like artificial intelligence (AI) and robotics have also begun to profoundly reshape the world.
In recognition of this, the world’s largest AI society — the Association for the Advancement of Artificial Intelligence (AAAI) — announced today the winner of their new Squirrel AI Award for Artificial Intelligence for the Benefit of Humanity, a $1 million award given to honor individuals whose work in the field has had a transformative impact on society.
Since surviving breast cancer in 2014, she has increasingly focused her efforts on health care.
She created algorithms for early breast cancer diagnosis and risk assessment that have been tested at multiple hospitals around the globe, including in Sweden, Taiwan, and at Boston’s Massachusetts General Hospital.
She is now working with breast cancer organizations such as Institute Protea in Brazil to make her diagnostic tools available for underprivileged populations around the world.",Mit
77,https://news.mit.edu/2020/examining-racial-attitudes-in-virtual-spaces-through-gaming-0917,"


The national dialogue on race has progressed powerfully and painfully in the past year, and issues of racial bias in the news have become ubiquitous. However, for over a decade, researchers from MIT’s Imagination, Computation, and Expression Laboratory (ICE Lab) have been developing systems to model, simulate, and analyze such issues of identity. 
In recent years there’s been a rise in popularity of video games or virtual reality (VR) experiences addressing racial issues for educational or training purposes, coinciding with the rapid development of the academic field of serious or “impact” games such as “Walk a Mile in Digital Shoes” or “1000 Cut Journey.” 
Now researchers from the ICE Lab, part of MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the MIT Center for Advanced Virtuality, have updated a 2019 computational model to better understand our behavioral choices, by way of a video game simulation of a discriminatory racial encounter between a Black student and her white teacher. 
A paper on the game will be presented this week the 2020 Foundations of Digital Games conference. 
The system, which was informed by the social science research of collaborators at the University of Michigan's Engaging, Managing, and Bonding through Race (EMBRace) lab, is supported by the Racial Encounter Coping Appraisal and Socialization Theory (RECAST). RECAST provides a way of understanding how racial socialization, or the way one has been taught to think about race, cushions the influence between racial stress and coping.
The game, called “Passage Home,” is used to help understand the attitudes of PreK-12 educators, with the eventual goal of providing an innovative tool for clinicians to better understand the behavioral choices adolescents make when encountered with racial injustice. 
Following user studies conducted with the original version of Passage Home in 2019, the team worked with Riana Elyse Anderson, assistant professor in the Department of Health Behavior and Health Education at the University of Michigan's School of Public Health, and Nkemka Anyiwo, vice provost and National Science Foundation Postdoctoral Fellow in the Graduate School of Education at the University of Pennsylvania, to iterate on the original prototype and improve it to align more closely with RECAST theory. Since creating the latest version of “Passage Home” VR, they sought to understand the opportunities and challenges for using it as a tool for capturing insights about how individuals perceive and respond to racialized encounters. 
Experiments from “Passage Home” revealed that players' existing colorblind racial attitudes and their ethnic identity development hindered their ability to accurately interpret racist subtexts.
The interactive game puts the player into the first-person perspective of “Tiffany,” a Black student who is falsely accused of plagiarism by her white female English teacher, “Mrs. Smith.” In the game, Mrs. Smith holds the inherently racist belief that Black students are incapable of producing high-quality work as the basis of her accusation. 
“There has been much focus on understanding the efficacy of these systems as interventions to reduce racial bias, but there’s been less attention on how individuals’ prior physical-world racial attitudes influence their experiences of such games about racial issues,” says MIT CSAIL PhD student Danielle Olson, lead author on the paper being presented this week.
“Danielle Olson is at the forefront of computational modeling of social phenomena, including race and racialized experiences,” says her thesis supervisor D. Fox Harrell, professor of digital media and AI in CSAIL and director of the ICE Lab and MIT Center for Advanced Virtuality. “What is crucial about her dissertation research and system 'Passage Home' is that it does not only model race as physical experience, rather it simulates how people are socialized to think about race, which often has more a profound impact on their racial biases regarding others and themselves than merely what they look like.”
Many mainstream strategies for portraying race in VR experiences are often rooted in negative racial stereotypes, and the questions are often focused on “right” and “wrong” actions. In contrast, with “Passage Home,” the researchers aimed to take into account the nuance and complexity of how people think about race, which involves systemic social structures, history, lived experiences, interpersonal interactions, and discourse.
In the game, prior to the discriminatory interaction, the player is provided with a note that they (Tiffany) are  academically high-achieving and did not commit plagiarism. The player is prompted to make a series of choices to capture their thoughts, feelings, and desired actions in response to the allegation. 
The player then chooses which internal thoughts are most closely aligned with their own, and the verbal responses, body language, or gesture they want to express. These combinations contribute to how the narrative unfolds. 
One educator, for example, expressed that, “This situation could have happened to any student of any race, but the way [the student] was raised, she took it as being treated unfairly.” 
The game makes it clear that the student did not cheat, and the student never complains of unfairness, so in this case, the educator’s prior racial attitude results in not only misreading the situation, but actually imputing an attitude to the student that was never there. (The team notes that many people failed to recognize the racist nature of the comments because their racial literacy inhibited them from decoding anti-Black subtexts.)
The results of the game demonstrated statistically significant relationships within the following categories:

Competence (players’ feelings of skillfulness and success in the game)
	
Positively associated with unawareness of racial privilege




Negative affect (players’ feelings of boredom and monotony in the game)
	
Positively associated with unawareness of blatant racial issues




Empathy (players’ feelings of empathy towards Mrs. Smith, who is racially biased towards Tiffany)
	
Negatively associated with ethnic identity search, and positively associated with unawareness of racial privilege, blatant racial issues, and institutional discrimination


Perceived competence of Tiffany, the student (how well the player thought she handled the situation)
	
Negatively associated with unawareness of institutional discrimination and blatant racial issues


Perceived unfairness of Mrs. Smith, the teacher (questioning if Mrs. Smith was unfair to Tiffany)
	
Negatively associated with unawareness of racial privilege, institutional discrimination, and blatant racial issues



“Even if developers create these games to attempt to encourage white educators to understand how racism negatively impacts their Black students, their prior worldviews may cause them to identify with the teacher who is the perpetrator of racial violence, not the student who is the target,” says Olson. “These results can aid developers in avoiding assumptions about players’ racial literacy by creating systems informed by evidence-based research on racial socialization and coping.” 
While this work demonstrates a promising tool, the team notes that because racism exists at individual, cultural, institutional and systemic levels, there are limitations to which levels and how much impact emergent technologies such as VR can make. 
Future games could be personalized to attend to differences in players’ racial socialization and attitudes, rather than assuming players will interpret racialized content in a similar way. By improving players’ in-game experiences, the hope is that this will increase the possibility for transformative learning with educators, and aid in the pursuit of racial equity for students.
This material is based upon work supported by the following grant programs: National Science Foundation Graduate Research Fellowship Program, the Ford Foundation Predoctoral Fellowship Program, the MIT Abdul Latif Jameel World Education Lab pK-12 Education Innovation Grant, and the International Chapter of the P.E.O. Scholar Award. 


",Examining racial attitudes in virtual spaces through gaming,2020-09-17,['Rachel Gordon'],School of Humanities Arts and Social Sciences/MIT Schwarzman College of Computing/School of Engineering/Comparative Media Studies/Writing/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Abdul Latif Jameel World Education Lab (J-WEL)/Diversity and inclusion/Video games/Augmented and virtual reality/Race and gender,"['gaming', 'experiences', 'games', 'student', 'virtual', 'examining', 'associated', 'attitudes', 'way', 'race', 'racial', 'spaces', 'passage', 'game', 'players']","The national dialogue on race has progressed powerfully and painfully in the past year, and issues of racial bias in the news have become ubiquitous.
A paper on the game will be presented this week the 2020 Foundations of Digital Games conference.
RECAST provides a way of understanding how racial socialization, or the way one has been taught to think about race, cushions the influence between racial stress and coping.
Experiments from “Passage Home” revealed that players' existing colorblind racial attitudes and their ethnic identity development hindered their ability to accurately interpret racist subtexts.
Future games could be personalized to attend to differences in players’ racial socialization and attitudes, rather than assuming players will interpret racialized content in a similar way.",Mit
78,https://news.mit.edu/2020/realtime-robots-motion-0917,"


George Konidaris still remembers his disheartening introduction to robotics.
“When you’re a young student and you want to program a robot, the first thing that hits you is this immense disappointment at how much you can’t do with that robot,” he says.
Most new roboticists want to program their robots to solve interesting, complex tasks — but it turns out that just moving them through space without colliding with objects is more difficult than it sounds.
Fortunately, Konidaris is hopeful that future roboticists will have a more exciting start in the field. That’s because roughly four years ago, he co-founded Realtime Robotics, a startup that’s solving the “motion planning problem” for robots.
The company has invented a solution that gives robots the ability to quickly adjust their path to avoid objects as they move to a target. The Realtime controller is a box that can be connected to a variety of robots and deployed in dynamic environments.
“Our box simply runs the robot according to the customer’s program,” explains Konidaris, who currently serves as Realtime’s chief roboticist. “It takes care of the movement, the speed of the robot, detecting obstacles, collision detection. All [our customers] need to say is, ‘I want this robot to move here.’”
Realtime’s key enabling technology is a unique circuit design that, when combined with proprietary software, has the effect of a plug-in motor cortex for robots. In addition to helping to fulfill the expectations of starry-eyed roboticists, the technology also represents a fundamental advance toward robots that can work effectively in changing environments.
Helping robots get around
Konidaris was not the first person to get discouraged about the motion planning problem in robotics. Researchers in the field have been working on it for 40 years. During a four-year postdoc at MIT, Konidaris worked with School of Engineering Professor in Teaching Excellence Tomas Lozano-Perez, a pioneer in the field who was publishing papers on motion planning before Konidaris was born.
Humans take collision avoidance for granted. Konidaris points out that the simple act of grabbing a beer from the fridge actually requires a series of tasks such as opening the fridge, positioning your body to reach in, avoiding other objects in the fridge, and deciding where to grab the beer can.
“You actually need to compute more than one plan,” Konidaris says. “You might need to compute hundreds of plans to get the action you want. … It’s weird how the simplest things humans do hundreds of times a day actually require immense computation.”
In robotics, the motion planning problem revolves around the computational power required to carry out frequent tests as robots move through space. At each stage of a planned path, the tests help determine if various tiny movements will make the robot collide with objects around it. Such tests have inspired researchers to think up ever more complicated algorithms in recent years, but Konidaris believes that’s the wrong approach.
“People were trying to make algorithms smarter and more complex, but usually that’s a sign that you’re going down the wrong path,” Konidaris says. “It’s actually not that common that super technically sophisticated techniques solve problems like that.”
Konidaris left MIT in 2014 to join the faculty at Duke University, but he continued to collaborate with researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL). Duke is also where Konidaris met Realtime co-founders Sean Murray, Dan Sorin, and Will Floyd-Jones. In 2015, the co-founders collaborated to make a new type of computer chip with circuits specifically designed to perform the frequent collision tests required to move a robot safely through space. The custom circuits could perform operations in parallel to more efficiently test short motion collisions.
“When I left MIT for Duke, one thing bugging me was this motion planning thing should really be solved by now,” Konidaris says. “It really did come directly out of a lot of experiences at MIT. I wouldn’t have been able to write a single paper on motion planning before I got to MIT.”
The researchers founded Realtime in 2016 and quickly brought on robotics industry veteran Peter Howard SM ’87, who currently serves as Realtime’s CEO and is also considered a co-founder.
“I wanted to start the company in Boston because I knew MIT and lot of robotics work was happening there,” says Konidaris, who moved to Brown University in 2016. “Boston is a hub for robotics. There’s a ton of local talent, and I think a lot of that is because MIT is here — PhDs from MIT became faculty at local schools, and those people started robotics programs. That network effect is very strong.”
Removing robot restraints
Today the majority of Realtime’s customers are in the automotive, manufacturing, and logistics industries. The robots using Realtime’s solution are doing everything from spot welding to making inspections to picking items from bins.
After customers purchase Realtime’s control box, they load in a file describing the configuration of the robot’s work cell, information about the robot such as its end-of-arm tool, and the task the robot is completing. Realtime can also help optimally place the robot and its accompanying sensors around a work area. Konidaris says Realtime can shorten the process of deploying robots from an average of 15 weeks to one week.
Once the robot is up and running, Realtime’s box controls its movement, giving it instant collision-avoidance capabilities.
“You can use it for any robot,” Konidaris says. “You tell it where it needs to go and we’ll handle the rest.”
Realtime is part of MIT’s Industrial Liaison Program (ILP), which helps companies make connections with larger industrial partners, and it recently joined ILP’s STEX25 startup accelerator.
With a few large rollouts planned for the coming months, the Realtime team’s excitement is driven by the belief that solving a problem as fundamental as motion planning unlocks a slew of new applications for the robotics field.
“What I find most exciting about Realtime is that we are a true technology company,” says Konidaris. “The vast majority of startups are aimed at finding a new application for existing technology; often, there’s no real pushing of the technical boundaries with a new app or website, or even a new robotics ‘vertical.’ But we really did invent something new, and that edge and that energy is what drives us. All of that feels very MIT to me.”


",Helping robots avoid collisions,2020-09-17,['Zach Winn'],Research/Computer vision/Machine learning/Robotics/School of Engineering/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Startups/Innovation and Entrepreneurship (I&E)/Manufacturing,"['helping', 'konidaris', 'planning', 'work', 'robots', 'realtime', 'motion', 'realtimes', 'collisions', 'avoid', 'robotics', 'robot', 'mit']","That’s because roughly four years ago, he co-founded Realtime Robotics, a startup that’s solving the “motion planning problem” for robots.
The company has invented a solution that gives robots the ability to quickly adjust their path to avoid objects as they move to a target.
The Realtime controller is a box that can be connected to a variety of robots and deployed in dynamic environments.
In addition to helping to fulfill the expectations of starry-eyed roboticists, the technology also represents a fundamental advance toward robots that can work effectively in changing environments.
Helping robots get aroundKonidaris was not the first person to get discouraged about the motion planning problem in robotics.",Mit
79,https://news.mit.edu/2020/2020-beaver-works-summer-institute-goes-virtual-0914,"


Despite the Covid-19 emergency, this year’s Beaver Works Summer Institute (BWSI) carried on in a virtual format, offering seven classes to 178 students from 26 states across the country and Canada. 2020 marks the fifth year of the program — a four-week hands-on STEM learning experience for rising high school seniors — which concluded on July 31 with a finale event that showcased student projects, a virtual grand prix, awards, and more. 
“Even with the virtual format, the students were energized, engaged, and truly committed to their projects,” says Jennifer Watson, who is a coordinator for the program and a Lincoln Laboratory staff member. “Many students not only enrolled in BWSI but also challenged themselves by taking on [a new] independent study project. We could see them transform into teams of confident, collegial students who were very proud to showcase their accomplishments during the final event.”
The classes offered this year included Autonomous RACECAR, Autonomous Cognitive Assistant, Data Science for Health and Medicine, Build a CubeSat, Embedded Security and Hardware Hacking, Remote Sensing, and a new course called Serious Game Design and Development with AI. The program also offered its first-ever independent project, called pi-PACT. For this project, 176 students developed solutions for automated contact tracing by using BWSI-provided Raspberry Pis.
Adapting to a virtual environment
Lisa Kelley, also from Lincoln Laboratory, is a manager of the BWSI program. She says that adapting the hands-on courses into an online format was a challenge. The RACECAR (Rapid Autonomous Complex Environment Competing Ackermann steeRing) course, for example, delves into the complex field of autonomous navigation, and challenges teams of students to program and race MIT-designed robots. These robots, however, cost up to $10,000 per vehicle.
“Despite the course going virtual, we wanted to still incorporate as much hands-on learning as possible,” says Eyassu Shimelis, a Lincoln Laboratory staff member who was an instructor for the course. “Thankfully, over the last year we had put effort into designing and building a miniature version of the car at about a tenth of the cost. The newer car, called the RACECAR Model Nano, has all of the same types of sensors and computing capabilities, and we had enough to send one to all 21 students in the course.”
Matthew Calligaro and Emi Suzuki — who worked with Lincoln Laboratory to develop a prototype of the curriculum while they were students at Harvey Mudd College — taught the summer course together.
“Once we learned that the BWSI program would be run virtually this summer, we felt that it was critical to create a virtual environment we could fall back on,” Calligaro says. “This would serve as an alternative if students were experiencing hardware difficulties or did not have the physical space necessary to carry out a lab [experiment].”
Calligaro and Suzuki built this environment, called RacecarSim, using the Unity engine. It mimicked the RACECAR Model Nano hardware so that students could run their code simultaneously within the environment, as well as with the physical cars.
“Going forward, RacecarSim will play a big role in improving the accessibility of our course,” Calligaro says. “Students can now complete the entire curriculum with only a computer — greatly reducing the barrier to entry.”
Jeffrey Liu, also from Lincoln Laboratory, taught the Remote Sensing course in which students explored the intersection of data science and disaster response. Although this course did not require mailing any hardware, there were other logistics, such as a field trip to the Federal Emergency Management Agency (FEMA) Massachusetts Task Force 1 (MA-TF1) Urban Search and Rescue team’s facilities, that had to be rethought.
“The team at MA-TF1 put together an excellent virtual tour jam-packed with experts, including from FEMA headquarters and academic partners at the University of Vermont,” he says. “We were even able to use a 3D lidar scan of the MA-TF1 field site to explore the facilities.”
New courses and offerings
The new Serious Game Design and Development with AI course taught students how to use game design and artificial intelligence to understand and address real-world problems: for example, how a game about zombies could reveal the effects of public health policy on chemical or biological threats. The class was also offered at Lincoln Laboratory’s satellite location on Kwajalein Atoll, where 14 students enrolled.
“During the class, we found that our students had an intuitive grasp of how and why the serious games approach is used, and thus we were able to focus on the experimental design and translating that into code,” says Amna Greaves, a Lincoln Laboratory staff member and an instructor for the course. “Overall, it was a pleasure to be involved with these future engineers and scientists and to see their confidence and teamwork develop. It bodes well for the future of STEM in the U.S. and underscores the need for programs like these to ensure that the next generation is given every advantage to maximize their success.”
Vivian Tao, a student from Noble and Greenough School in Dedham, Massachusetts, who completed the Serious Games course, commented: “What I enjoyed most about the course was working together with a team to create a cohesive final product. I learned a lot about collaborative programming and it was a ton of fun getting to know my teammates and creating a game mod with them. In the future, I will definitely use the project management and teamwork skills I learned from [the course] in all projects that I take on, not only ones that have to do with coding.”
Tao was one of several recipients of a new Dr. Bob Berman Disruptive Innovation Award, which is given to students who make outstanding contributions to their individual teams as well as their class as a whole.
The new independent project option, pi-PACT, was offered to any student who applied to the BWSI program. The project was largely self-directed, and students explored different aspects of the Bluetooth-based Covid-19 contact tracing effort, called the Private Automated Contact Tracing (PACT) application, which was recently developed by staff at the Lincoln Laboratory and from MIT campus.
“Each student was responsible for choosing an aspect of this [application] to explore in-depth and evaluate, for example, collection and characterization of data, in addition to development of algorithms to enable proximity detection between individuals,” says Ramamurthy Bhagavatula, also from Lincoln Laboratory, who was an instructor for the course.
“I really enjoyed the opportunity to have an independent research project that was very relevant to the Covid-19 pandemic,” says Joseph Rosenfeld, a student from Yeshiva University High School for Boys in New York City. “The project has given me confidence that I can complete science experiments from start to finish, including publishing and presenting.""
The BWSI program also hosted several seminars for the students, including a Women’s Network Panel. Seven women from Lincoln Laboratory and from other STEM fields in industry and the U.S. Air Force discussed their career journeys and answered students’ questions.
“Above all, I remember each panelist’s unwavering perseverance and originality,” says Clarise Liu, a student from the Massachusetts Academy of Math and Science in Worcester, Massachusetts, who completed the Data Science for Health and Medicine course. “Nothing held them back from their goals, and no difficulty stopped them from moving forward. I will always remember each of these role models as a reminder that everything life throws is only a new opportunity to grow, improve, and make a meaningful change to our world.""
What’s next?
All in all, the staff describe the program as a success despite the quick adaptation to online learning.
Bob Shin, the director of the BWSI program, says that the push to bring the program online has opened up new avenues for making the course materials available to as many people as possible. The BWSI staff plan to package the lectures into bundles that schools and other programs can use and adapt for their own purposes. In 2021, they intend to run a teacher program to show them how to teach the materials to their own students. 
“Our mission to teach both technical skills as well as a project-based approach to learning made us commit early in the pandemic to run a program this year,” says Joel Grimm, who is from Lincoln Laboratory and is a manager for Beaver Works. “The students’ intensity during BWSI was amazing, and their final presentations demonstrate what they learned as a team and in technology. We now have a wealth of recordings that we can use to expand our online prerequisite courses, create asynchronous courses for learners, and expand the number of learners who can use the BWSI projects.”
""Thanks to our amazing staff, this year's challenges quickly turned into opportunities,” says Professor Sertac Karaman, who is the academic director of the BWSI program and a professor in the Department of Aeronautics and Astronautics. “BWSI will continue to offer high-school courses that are inspired by emerging technologies, such as driverless cars, drones, and 3D printing. Our courses will continue to emphasize hands-on learning, but blending them with a sophisticated virtual experience, using the lessons learned from this year's offerings.”


",2020 Beaver Works Summer Institute goes virtual,2020-09-14,['Anne Mcgovern'],School of Engineering/Aeronautics and Astronautics/Lincoln Laboratory/K-12 education/STEM education/online learning/Covid-19/Pandemic/Contests and academic competitions/Special events and guest speakers,"['institute', 'student', '2020', 'virtual', 'beaver', 'course', 'bwsi', 'goes', 'program', 'laboratory', 'project', 'summer', 'lincoln', 'works', 'staff', 'students']","Despite the Covid-19 emergency, this year’s Beaver Works Summer Institute (BWSI) carried on in a virtual format, offering seven classes to 178 students from 26 states across the country and Canada.
Adapting to a virtual environmentLisa Kelley, also from Lincoln Laboratory, is a manager of the BWSI program.
“Despite the course going virtual, we wanted to still incorporate as much hands-on learning as possible,” says Eyassu Shimelis, a Lincoln Laboratory staff member who was an instructor for the course.
“Once we learned that the BWSI program would be run virtually this summer, we felt that it was critical to create a virtual environment we could fall back on,” Calligaro says.
The BWSI staff plan to package the lectures into bundles that schools and other programs can use and adapt for their own purposes.",Mit
80,https://news.mit.edu/2020/mit-named-no-4-university-us-news-2021,"


U.S. News and World Report has placed MIT fourth in its annual rankings of the nation’s best colleges and universities, which were announced today. The Institute shares the No. 4 spot with Yale University.
MIT’s engineering program continues to top the magazine’s list of undergraduate engineering programs at a doctoral institution. The Institute also placed first in five out of 12 engineering disciplines. No other institution is No. 1 in more than two disciplines.
MIT also remains the No. 2 undergraduate business program. Among business subfields, MIT is ranked first in three specialties.
For the first time, U.S. News has also evaluated undergraduate computer science programs, placing MIT first on the list. The Institute ranks No. 1 in two disciplines in this area.
MIT ranks as the second most innovative national university, according to the U.S. News peer assessment survey of top academics. And it’s fifth on the magazine’s list of national universities that offer students the best value, based on the school’s ranking and the net cost of attendance for a student who received the average level of need-based financial aid, and other variables.
MIT placed first in five engineering specialties: aerospace/aeronautical/astronautical engineering; chemical engineering; electrical/electronic/communication engineering; materials engineering; and mechanical engineering. It placed second in computer engineering.
Other schools in the top five overall for undergraduate engineering programs are Stanford University, the University of California at Berkeley, Georgia Tech, and Caltech.
Among undergraduate business specialties, the MIT Sloan School of Management leads in business analytics, production/operations management, and quantitative analysis/methods. It ranks second in entrepreneurship and in management information systems.
The No. 1-ranked undergraduate business program overall is at the University of Pennsylvania; other schools ranking in the top five include Berkeley, the University of Michigan at Ann Arbor, New York University, and the University of Texas at Austin.
In computer science, MIT placed first in three specialties: biocomputing/bioinformatics/biotechnology; programming languages; and theory. It ranks second in data analytics science, artificial intelligence, and computer systems, and in mobile web applications (shared with Stanford).
Other top-ranking undergraduate computer science programs include Carnegie Mellon, Stanford, and Berkeley.


",MIT named No. 4 university by U.S. News for 2021,2020-09-14,[],"Rankings/Business and management/Education, teaching, academics/Undergraduate/School of Engineering/Sloan School of Management/MIT Schwarzman College of Computing/Computer science and technology","['second', 'named', 'ranks', 'business', 'computer', '2021', 'engineering', 'university', 'placed', 'undergraduate', 'science', 'mit']","MIT ranks as the second most innovative national university, according to the U.S. News peer assessment survey of top academics.
MIT placed first in five engineering specialties: aerospace/aeronautical/astronautical engineering; chemical engineering; electrical/electronic/communication engineering; materials engineering; and mechanical engineering.
Other schools in the top five overall for undergraduate engineering programs are Stanford University, the University of California at Berkeley, Georgia Tech, and Caltech.
1-ranked undergraduate business program overall is at the University of Pennsylvania; other schools ranking in the top five include Berkeley, the University of Michigan at Ann Arbor, New York University, and the University of Texas at Austin.
In computer science, MIT placed first in three specialties: biocomputing/bioinformatics/biotechnology; programming languages; and theory.",Mit
81,https://news.mit.edu/2020/monitoring-sleep-sensors-0911,"


MIT researchers have developed a wireless, private way to monitor a person’s sleep postures — whether snoozing on their back, stomach, or sides — using reflected radio signals from a small device mounted on a bedroom wall.
The device, called BodyCompass, is the first home-ready, radio-frequency-based system to provide accurate sleep data without cameras or sensors attached to the body, according to Shichao Yue, who will introduce the system in a presentation at the UbiComp 2020 conference on Sept. 15. The PhD student has used wireless sensing to study sleep stages and insomnia for several years.
“We thought sleep posture could be another impactful application of our system” for medical monitoring, says Yue, who worked on the project under the supervision of Professor Dina Katabi in the MIT Computer Science and Artificial Intelligence Laboratory. Studies show that stomach sleeping increases the risk of sudden death in people with epilepsy, he notes, and sleep posture could also be used to measure the progression of Parkinson’s disease as the condition robs a person of the ability to turn over in bed.
""Unfortunately, many patients are completely unaware of how they sleep at night or what position they end up in after a seizure,"" says Dong Woo Lee, an epilepsy neurologist at Brigham and Women's Hospital and Harvard Medical School, who was not associated with the study. ""A body monitoring system like BodyCompass would move our field forward in allowing for baseline monitoring of our patients to assess their risk, and when combined with an alerting/intervention system, could save patients from sudden unexpected death in epilepsy.""
In the future, people might also use BodyCompass to keep track of their own sleep habits or to monitor infant sleeping, Yue says: “It can be either a medical device or a consumer product, depending on needs.”
Other authors on the conference paper, published in the Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, include graduate students Yuzhe Yang and Hao Wang, and Katabi Lab affiliate Hariharan Rahul. Katabi is the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science at MIT.
Restful reflections
BodyCompass works by analyzing the reflection of radio signals as they bounce off objects in a room, including the human body. Similar to a Wi-Fi router attached to the bedroom wall, the device sends and collects these signals as they return through multiple paths. The researchers then map the paths of these signals, working backward from the reflections to determine the body’s posture.
For this to work, however, the scientists needed a way to figure out which of the signals were bouncing off the sleeper’s body, and not bouncing off the mattress or a nightstand or an overhead fan. Yue and his colleagues realized that their past work in deciphering breathing patterns from radio signals could solve the problem.
Signals that bounce off a person’s chest and belly are uniquely modulated by breathing, they concluded. Once that breathing signal was identified as a way to “tag” reflections coming from the body, the researchers could analyze those reflections compared to the position of the device to determine how the person was lying in bed. (If a person was lying on her back, for instance, strong radio waves bouncing off her chest would be directed at the ceiling and then to the device on the wall.) “Identifying breathing as coding helped us to separate signals from the body from environmental reflections, allowing us to track where informative reflections are,” Yue says.
Reflections from the body are then analyzed by a customized neural network to infer how the body is angled in sleep. Because the neural network defines sleep postures according to angles, the device can distinguish between a sleeper lying on the right side from one who has merely tilted slightly to the right. This kind of fine-grained analysis would be especially important for epilepsy patients for whom sleeping in a prone position is correlated with sudden unexpected death, Yue says.
Lee says ""it is becoming apparent that patients do not like wearing devices, they forget to wear it, they decrease comfort, battery life is short, and data transfer may be difficult. A non-wearable contactless device like the BodyCompass would overcome these issues.""
BodyCompass has some advantages over other ways of monitoring sleep posture, such as installing cameras in a person’s bedroom or attaching sensors directly to the person or their bed. Sensors can be uncomfortable to sleep with, and cameras reduce a person’s privacy, Yue notes. “Since we will only record essential information for detecting sleep posture, such as a person’s breathing signal during sleep,” he says, “it is nearly impossible for someone to infer other activities of the user from this data.”
An accurate compass
The research team tested BodyCompass’ accuracy over 200 hours of sleep data from 26 healthy people sleeping in their own bedrooms. At the start of the study, the subjects wore two accelerometers (sensors that detect movement) taped to their chest and stomach, to train the device’s neural network with “ground truth” data on their sleeping postures.
BodyCompass was most accurate — predicting the correct body posture 94 percent of the time — when the device was trained on a week’s worth of data. One night’s worth of training data yielded accurate results 87 percent of the time. BodyCompass could achieve 84 percent accuracy with just 16 minutes’ worth of data collected, when sleepers were asked to hold a few usual sleeping postures in front of the wireless sensor.
Along with epilepsy and Parkinson’s disease, BodyCompass could prove useful in treating patients vulnerable to bedsores and sleep apnea, since both conditions can be alleviated by changes in sleeping posture. Yue has his own interest as well: He suffers from migraines that seem to be affected by how he sleeps. “I sleep on my right side to avoid headache the next day,” he says, “but I’m not sure if there really is any correlation between sleep posture and migraines. Maybe this can help me find out if there is any relationship.”
For now, BodyCompass is a monitoring tool, but it may be paired someday with an alert that can prod sleepers to change their posture. “Researchers are working on mattresses that can slowly turn a patient to avoid dangerous sleep positions,” Yue says. “Future work may combine our sleep posture detector with such mattresses to move an epilepsy patient to a safer position if needed.”


",Monitoring sleep positions for a healthy rest,2020-09-11,['Becky Ham'],Research/Electrical Engineering & Computer Science (eecs)/Health/Medicine/Wireless/Computer science and technology/Computer Science and Artificial Intelligence Laboratory (CSAIL)/MIT Schwarzman College of Computing/School of  Engineering,"['sleeping', 'data', 'positions', 'monitoring', 'body', 'yue', 'signals', 'device', 'sleep', 'rest', 'patients', 'bodycompass', 'healthy', 'posture']","“We thought sleep posture could be another impactful application of our system” for medical monitoring, says Yue, who worked on the project under the supervision of Professor Dina Katabi in the MIT Computer Science and Artificial Intelligence Laboratory.
BodyCompass has some advantages over other ways of monitoring sleep posture, such as installing cameras in a person’s bedroom or attaching sensors directly to the person or their bed.
“I sleep on my right side to avoid headache the next day,” he says, “but I’m not sure if there really is any correlation between sleep posture and migraines.
“Researchers are working on mattresses that can slowly turn a patient to avoid dangerous sleep positions,” Yue says.
“Future work may combine our sleep posture detector with such mattresses to move an epilepsy patient to a safer position if needed.”",Mit
82,https://news.mit.edu/2020/sourcetrace-agriculture-0910,"


Millions of cocoa farmers live in poverty across western Africa. Over the years, these farmers have been forced to contend with geopolitical instability, predatory loan practices, and a general lack of information that hampers their ability to maximize yields and sell crops at fair prices. Other problems, such as deforestation and child labor, also plague the cocoa industry.
For the last five years, however, cocoa supply chains in villages around the Ivory Coast, Cameroon, and Ghana have been transformed. A suite of digital solutions has improved profitability for more than 200,000 farmers, encouraged sustainable and ethical production practices, and made cocoa supply chains more traceable and efficient.
The progress was enabled by SourceTrace, a company that helps improve agricultural supply chains around the world. SourceTrace offers tools to help manage and sell crops, buy and track goods, and trace products back to the farms where they were made.
Through partnerships with farmer cooperatives, financial institutions, governments, and consumer brands, SourceTrace has impacted more than 1.2 million farmers across 28 countries.
CEO Venkat Maroju MBA ’07 believes the company’s success comes from the idea that the only way to improve one part of agricultural supply chains is to improve every part.
“The whole idea of our platform is to make the agricultural value chain sustainable, predictable, profitable, equitable, and traceable,” Venkat says.
Illuminating supply chains
Maroju grew up in a rural region of Telangana in southern India in what he describes as “very humble beginnings.” When it rained, his school was cancelled. He also studied in the local language until 12th grade, adding to the difficulty of his college entrance exam.
Through an affirmative action program, Maroju earned admittance to an engineering university, and his English improved over the next four years. He went on to get his master’s degree at the Indian Institute of Science and later came to the United States to pursue his PhD at Old Dominion University in Virginia.
Following his PhD, Maroju stayed in the U.S., but he became active in the politics of his home region of Telangana, including in that area’s push for statehood, which was achieved in 2014.
During that time, Maroju learned a lot about the hardships associated with small plot farming, the main profession for more than 60 percent of the people in the Telangana region. Over the last two decades, such farmers have had to contend with dramatic changes to agricultural policies following the country’s economic liberalization, as well as predatory lending practices that have led to a large number of farmer suicides.
In 2005, Maroju came to MIT for his MBA with the Sloan Fellows Program. As part of his thesis, he studied microfinance in India and considered how the rise of cell phone ownership offered an unprecedented opportunity to help people in rural areas.
“I always had a lot of passion for social issues,” Maroju says. “Coming from a humble background, I’ve seen the struggles of poverty.”
When Maroju finished his thesis in 2007, it caught the attention of Gray Ghost Ventures, an impact-driven investment firm that was working with the newly formed Legatum Center for Development and Entrepreneurship at MIT. Gray Ghost brought Maroju on as an advisor, where he was introduced to a struggling technology company named SourceTrace, which offered branchless or agent banking solutions. Venkat suggested shifting SourceTrace’s focus to agriculture, and the company’s investors liked the idea.
He became CEO of SourceTrace in 2013, setting out to build new solutions to address each step of the agricultural supply chain.
“In agriculture, you can’t do anything in isolation,” Maroju says. “We always viewed it as an entire value chain, from consumer demand to nutrients used to safety of food. It all has an impact. All the players, from input suppliers to extension organizations, to buyers, processors, logistics, there’s a role to play for all of them, and we’ve always thought to make an impact you have to build end to end.”
Accordingly, SourceTrace’s platform includes features for everyone. Supply chain partners can use SourceTrace to buy crops, coordinate and track handoffs, and monitor storage conditions. Consumers can scan an item’s QR code at supermarkets and retail stores and learn about the farm where it came from, including that farm’s production processes.
Of course, the platform offers the most features to farmers, who can use it to get personalized advice on crop management, obtain fair trade and environmental certifications, monitor weather and pest attacks, and sell crops at fair market prices.
“All these solutions are targeted for businesses, governments, farmer cooperatives, financial institutions, so it’s a [business to business] software,” Maroju says. “But the common denominator is these [businesses] are all working with farmers. We’ve always focused on the farmers. I’ve always been passionate about smallholder farmers and we really want to give back.”
Focusing on the farmers
In addition to SourceTrace’s success with cocoa farmers in West Africa, the company has helped rice and maize farmers in Nigeria, grain farmers in Zimbabwe, organic cotton and spice farmers in India, seed producers in Bangladesh, and others. In total, SourceTrace’s platform is being used to improve production practices for 350 different crops around the world.
Maroju, who has been a mentor at the Legatum Center for the last several years, credits the center for helping the company scale across Africa. Today about 60 percent of SourceTrace’s farmers hail from the continent.
Much of the company’s success comes from leveraging the newly ubiquitous connectivity in developing countries and advances in smartphones. The company also uses remote sensing capabilities, artificial intelligence, blockchain, and QR codes to make its platform more effective.
But Maroju says the technologies are a means to an end: The production improvements they unlock must help farmers secure long-term buyers and higher margins. The ultimate goal of the company is transforming the lives of some of the world’s poorest people.
“It’s all about farmer livelihood,” Maroju says. “With all this technology, we enable the farmers to access the best markets wherever globally available. Then we help them optimize their inputs and make procurement processes more reliable and minimize the risk. It all comes back to the farmers.”


",Digitizing supply chains to lift farmers out of poverty,2020-09-10,['Zach Winn'],Startups/Alumni/ae/Farming/Innovation and Entrepreneurship (I&E)/Agriculture/Martin Trust Center for MIT Entrepreneurship/Social entrepreneurship/Developing countries/Poverty/Legatum Center/Sloan School of Management,"['chains', 'farmers', 'digitizing', 'maroju', 'cocoa', 'sourcetraces', 'sourcetrace', 'platform', 'supply', 'poverty', 'company', 'crops', 'agricultural', 'lift']","Millions of cocoa farmers live in poverty across western Africa.
For the last five years, however, cocoa supply chains in villages around the Ivory Coast, Cameroon, and Ghana have been transformed.
A suite of digital solutions has improved profitability for more than 200,000 farmers, encouraged sustainable and ethical production practices, and made cocoa supply chains more traceable and efficient.
The progress was enabled by SourceTrace, a company that helps improve agricultural supply chains around the world.
Today about 60 percent of SourceTrace’s farmers hail from the continent.",Mit
83,https://news.mit.edu/2020/helping-companies-prioritize-cybersecurity-investments-0903,"


One reason that cyberattacks have continued to grow in recent years is that we never actually learn all that much about how they happen. Companies fear that reporting attacks will tarnish their public image, and even those who do report them don’t share many details because they worry that their competitors will gain insight into their security practices. 
“It’s really a nice gift that we’ve given to cyber-criminals,” says Taylor Reynolds, technology policy director at MIT's Internet Policy Research Initiative (IPRI). “In an ideal world, these attacks wouldn’t happen over and over again, because companies would be able to use data from attacks to develop quantitative measurements of the security risk so that we could prevent such incidents in the future.”
In an economy where most industries are tightening their belts, many organizations don’t know which types of attacks lead to the largest financial losses, and therefore how to best deploy scarce security resources. 
But a new platform from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) aims to change that, quantifying companies’ security risk without requiring them to disclose sensitive data about their systems to the research team, much less their competitors.
Developed by Reynolds alongside economist Andrew Lo and cryptographer Vinod Vaikuntanathan, the platform helps companies do multiple things:

quantify how secure they are;
understand how their security compares to peers; and
evaluate whether they’re spending the right amount of money on security, and if and how they should change their particular security priorities.

The team received internal data from seven large companies that averaged 50,000 employees and annual revenues of $24 billion. By securely aggregating 50 different security incidents that took place at the companies, the researchers were able to analyze which specific steps were not taken that could have prevented them. (Their analysis used a well-established set of nearly 200 security actions referred to as the Center for Internet Security Sub-Controls.) 
“We were able to paint a really thorough picture in terms of which security failures were costing companies the most money,” says Reynolds, who co-authored a related paper with professors Lo and Vaikuntanathan, MIT graduate student Leo de Castro, Principal Research Scientist Daniel J. Weitzner, PhD student Fransisca Susan, and graduate student Nicolas Zhang. “If you’re a chief information security officer at one of these organizations, it can be an overwhelming task to try to defend absolutely everything. They need to know where they should direct their attention.”
The team calls their platform “SCRAM,” for “Secure Cyber Risk Aggregation and Measurement.” Among other findings, they determined that the three following security vulnerabilities had the largest total losses, each in excess of $1 million:
Failures in preventing malware attacks
Malware attacks, like the one last month that reportedly forced the wearables company Garmin to pay a $10 million ransom, are still a tried-and-true method of gaining control of valuable consumer data. Reynolds says that companies continue to struggle to prevent such attacks, relying on regularly backing up their data and reminding their employees not to click on suspicious emails. 
Communication over unauthorized ports 
Curiously, the team found that every firm in their study said they had, in fact, implemented the security measure of blocking access to unauthorized ports — the digital equivalent of companies locking all their doors. Even still, attacks that involved gaining access to these ports accounted for a large number of high-cost losses. 
“Losses can arise even when there are defenses that are well-developed and understood,” says Weitzner, who also serves as director of MIT IPRI. “It’s important to recognize that improving common existing defenses should not be neglected in favor of expanding into new areas of defense.”
Failures in log management for security incidents 
Every day companies amass detailed “logs” denoting activity within their systems. Senior security officers often turn to these logs after an attack to audit the incident and see what happened. Reynolds says that there are many ways that companies could be using machine learning and artificial intelligence more efficiently to help understand what’s happening — including, crucially, during or even before a security attack. 
Two other key areas that warrant further analysis include taking inventory of hardware so that only authorized devices are given access, as well as boundary defenses like firewalls and proxies that aim to control the flow of traffic through network borders. 
The team developed their data aggregation platform in conjunction with MIT cryptography experts, using an existing method called multi-party computation (MPC) that allows them to perform calculations on data without themselves being able to read or unlock it. After computing its anonymized findings, the SCRAM system then asks each contributing company to help it unlock only the answer using their own secret cryptographic key.
“The power of this platform is that it allows firms to contribute locked data that would otherwise be too sensitive or risky to share with a third party,” says Reynolds.
As a next step, the researchers plan to expand the pool of participating companies, with representation from a range of different sectors that include electricity, finance, and biotech. Reynolds says that if the team can gather data from upwards of 70 or 80 companies, they’ll be able to do something unprecedented: put an actual dollar figure on the risk of particular defenses failing.
The project was a cross-campus effort involving affiliates at IPRI, CSAIL’s Theory of Computation group, and the MIT Sloan School of Management. It was funded by the Hewlett Foundation and CSAIL’s Financial Technology industry initiative (“FinTech@CSAIL”). 



",Helping companies prioritize their cybersecurity investments,2020-09-03,['Adam Conner-Simons'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical engineering and computer science (EECS)/Sloan School of Management/MIT Schwarzman College of Computing/School of Engineering/Computer science and technology/Security studies and military/Technology and society/Cyber security/Policy/Privacy/Industry/Research,"['helping', 'data', 'cybersecurity', 'security', 'reynolds', 'prioritize', 'attacks', 'risk', 'companies', 'platform', 'investments', 'able', 'mit', 'team']","“It’s really a nice gift that we’ve given to cyber-criminals,” says Taylor Reynolds, technology policy director at MIT's Internet Policy Research Initiative (IPRI).
But a new platform from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) aims to change that, quantifying companies’ security risk without requiring them to disclose sensitive data about their systems to the research team, much less their competitors.
The team received internal data from seven large companies that averaged 50,000 employees and annual revenues of $24 billion.
(Their analysis used a well-established set of nearly 200 security actions referred to as the Center for Internet Security Sub-Controls .)
Reynolds says that companies continue to struggle to prevent such attacks, relying on regularly backing up their data and reminding their employees not to click on suspicious emails.",Mit
84,https://news.mit.edu/2020/mit-hosts-seven-mlk-professors-scholars-0902,"


In light of the Covid-19 pandemic, MIT has been charged with reimagining its campus, classes, and programs, including the Dr. Martin Luther King, Jr. (MLK) Visiting Professors and Scholars Program (VPSP).
Founded in 1990, MLK VPSP honors the life and legacy of Martin Luther King, Jr. by increasing the presence of and recognizing the contributions of scholars from underrepresented groups at MIT. MLK Visiting Professors and Scholars enhance their scholarship through intellectual engagement with the MIT community and enrich the cultural, academic, and professional experience of students. The program hosts between four and eight scholars each year. But what does a virtual year mean for a visiting scholar?
Even with the challenge of remote learning and limited in-person contact, MLK VPSP faculty hosts have articulated innovative ways to engage with the MIT community. Moya Bailey, for instance, will be a content contributor for the Program in Women's and Gender Studies' website and social media accounts. Charles Senteio will continue to collaborate with the Office of Minority Education on curriculum development that reflects a diverse student population with a focus on health and well-being, and he will also explore remote learning and its impact on curriculum.
With Provost Martin Schmidt’s steadfast institutional support, and with active oversight from Institute Community and Equity Officer John Dozier and Associate Provost Tim Jamison, the MLK VPSP continues to honor King’s legacy and be an institutional priority on campus and online. For Academic Year 2020-2021, MIT is hosting seven accomplished scholars representing different areas of interest from all over the United States and Canada.
2020-2021 MLK Visiting Professors and Scholars
Moya Bailey is an assistant professor at Northeastern University in the Department of Cultures, Societies, and Global Studies and in the program in Women’s, Gender, and Sexuality Studies. In 2010, Bailey coined the term “misogynoir,” widely adopted by scholars, which describes the anti-Black racist misogyny that Black women experience. In the spring, she will teach a course in the MIT Program in Women’s and Gender Studies called Black Feminist Health Science Studies. In April 2021, she will organize and host a daylong Black Feminist Health Science symposium.
Jamie Macbeth joins the program for another year in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) as a valuable member of the Genesis group, a research team mainly focused on building computer systems and computational models of human intelligence based on humans’ capability for understanding natural language. One of Macbeth’s research collaborations involves using computer systems in understanding natural language to detect aggressive language on social media with the eventual goal of violence prevention. He will continue to mentor and collaborate with women and underrepresented groups at the undergraduate, MS, and PhD levels.
Ben McDonald is returning for a second year as a postdoc in the Department of Chemistry. His research focuses on developing designer polymers for chemical warfare-responsive membranes and surfactants to control the function of dynamic, complex soft colloids. His role as a mentor will expand to include both undergraduate and graduate students in the Swager Lab. McDonald will continue to collaborate with Chemistry Alliance for Diversity and Inclusion at MIT to organize and host virtual seminars showcasing the work of underrepresented scholars of color in the fields of chemistry and chemical engineering.
Luis Gilberto Murillo-Urrutia, a research fellow hosted by the Environmental Solutions Initiative (ESI), joins us from the Center for Latin America and Latino Studies at American University. His research focuses on the intersection of peace and security with environmental conservation, particularly in Afro-Colombian territories. During his visit, Murillo-Urrutia will hold mentorship sessions at ESI for students conducting research on environmental planning and policy or with a minor in environment and sustainability.
Thomas Searles, recently promoted to associate professor with tenure, is visiting from the Department of Physics at Howard University. While at MIT, he will pursue numerical studies of topological materials for photonic and quantum technological applications. He will mentor students from his lab, the Black Students Union, National Society of Black Engineers, and the Black Graduate Student Association. Searles plans to meet with the MIT physics graduate admissions committee to formulate recruitment strategies with his home and other historically Black colleges and universities.
Charles Senteio joins the program from Rutgers University School of Communication and Information, where he is an assistant professor in library and information science. As a visiting scholar at the MIT Sloan School of Management, he will collaborate with the Operations Management Group to expand on his community health informatics research and investigate health equity barriers. He recently facilitated a workshop, “Healthcare, Technology, and Social Justice Converge — Applied Equity Research and Why It Matters to All of Us” at the MIT Day of Dialogue event in August.
Patricia Saulis is Wolastoqey (Maliseet) from Wolastoq Negotkuk (Tobique First Nation in New Brunswick, Canada). As an MLK Visiting Scholar, Saulis will collaborate with her faculty host, Professor James Paradis from Comparative Media Studies/Writing, on a course titled, “Transmedia Art, Extraction and Environmental Justice” and engage with MIT Center for Environmental Health Sciences on their EPA Superfund-related work in the Northeastern United States. She will work closely with the American Indian Science and Engineering Society (AISES) and the Native American Students Association in raising awareness of the challenges impacting our Indigenous students. Through dialogue and presentations, she will help promote the understanding of Indigenous Peoples’ culture and help identify strategies to create a more inclusive campus for our Indigenous community. 
Community engagement
This year’s scholars are eager to join our community and embark on a mutually rewarding journey of learning and engagement — wherever in the world we may be.  
MIT community members are invited to join the Institute Community and Equity Office in engaging the MLK Professors and Scholars through a signature monthly speaker series, where each scholar will present his or her research and hold discussions via Zoom. The first welcome event will be held on Sept. 16 from 12 to 1 p.m. Contact Rachel Ornitz rornitz@mit.edu for event details.
For more information about this year’s and previous scholars and the program, visit the newly redesigned MLK Visiting Professors and Scholars website.


",MIT hosts seven distinguished MLK Professors and Scholars for 2020-21,2020-09-02,['Beatriz Cantada'],"Awards, honors and fellowships/Diversity and inclusion/Social justice/Women's and Gender Studies/Chemistry/Physics/ESI/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Comparative Media Studies/Writing/Sloan School of Management/School of Science/MIT Schwarzman College of Computing/School of Humanities Arts and Social Sciences","['professors', 'distinguished', 'research', 'community', 'scholars', 'black', 'studies', 'seven', 'hosts', 'mlk', 'visiting', '202021', 'program', 'mit', 'students']","In light of the Covid-19 pandemic, MIT has been charged with reimagining its campus, classes, and programs, including the Dr. Martin Luther King, Jr. (MLK) Visiting Professors and Scholars Program (VPSP).
MLK Visiting Professors and Scholars enhance their scholarship through intellectual engagement with the MIT community and enrich the cultural, academic, and professional experience of students.
Even with the challenge of remote learning and limited in-person contact, MLK VPSP faculty hosts have articulated innovative ways to engage with the MIT community.
He will mentor students from his lab, the Black Students Union, National Society of Black Engineers, and the Black Graduate Student Association.
For more information about this year’s and previous scholars and the program, visit the newly redesigned MLK Visiting Professors and Scholars website.",Mit
85,https://news.mit.edu/2020/strategic-areas-identified-shared-faculty-hiring-college-of-computing-0831,"


Nearly every aspect of the modern world is being transformed by computing. As computing technology continues to revolutionize the way people live, work, learn, and interact, computing research and education are increasingly playing a role in a broad range of academic disciplines, and are in turn being shaped by this expanding breadth.
To connect computing and other disciplines in addressing critical challenges and opportunities facing the world today, the MIT Stephen A. Schwarzman College of Computing is planning to create 25 new faculty positions that will be shared between the college and an MIT department or school. Hiring for these new positions will be focused on six strategic areas of inquiry, to build capacity at MIT in key computing domains that cut across departments and schools. The shared faculty members are expected to engage in research and teaching that contributes to their home department, that is of mutual value to that department and the college, and that helps form and strengthen cross-departmental ties.
“These new shared faculty positions present an unprecedented opportunity to develop crucial areas at MIT which connect computing with other disciplines,” says Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing. “By coordinated hiring between the college and departments and schools, we expect to have significant impact with multiple touch points across MIT.”
The six strategic areas and the schools expected to be involved in hiring for each are as follows:
Social, Economic, and Ethical Implications of Computing and Networks. Associated schools: School of Humanities, Arts and the Social Sciences and MIT Sloan School of Management.
There have been tremendous advances in new digital platforms and algorithms, which have already transformed our economic, social, and even political lives. But the future societal implications of these technologies and the consequences of the use and misuse of massive social data are poorly understood. There are exciting opportunities for building on the growing intellectual connections between computer science, data science, and social science and humanities, in order to bring a better conceptual framework to understand the social and economic implications, ethical dimensions, and regulation of these technologies.
Focusing on the interplay between computing systems and our understanding of individuals and societal institutions, this strategic hiring area will include faculty whose work focuses on the broader consequences of the changing digital and information environment, market design, digital commerce and competition, and economic and social networks. Issues of interest include how computing and AI technologies have shaped and are shaping the work of the future; how social media tools have reshaped political campaigns, changed the nature and organization of mass protests, and spurred governments to either reduce or dramatically enhance censorship and social control; increasing challenges in adjudicating what information is reliable, what is slanted, and what is entirely fake; conceptions of privacy, fairness, and transparency of algorithms; and the effects of new technologies on democratic governance.
Computing and Natural Intelligence: Cognition, Perception, and Language. Associated schools: School of Science; School of Humanities, Arts, and Social Sciences; and School of Architecture and Planning.
Intelligence — what it is, how the brain produces it, and how it can be engineered — is simultaneously one of the greatest open questions in natural sciences and the most important engineering challenges of our time. Significant advances in computing and machine learning have enabled a better understanding of the brain and the mind. Concurrently, neuroscience and cognitive science have started to give meaningful engineering guidance to AI and related computing efforts. Yet, huge gaps remain in connecting the science and engineering of intelligence.
Integrating science, computing, and social sciences and humanities, this strategic hiring area aims to address the gap between science and engineering of intelligence, in order to make transformative advances in AI and deepen our understanding of natural intelligence. Hiring in this area is expected to advance a holistic approach to understanding human perception and cognition through work such as the study of computational properties of language by bridging linguistic theory, cognitive science, and computer science; improving the art of listening by re-engineering music through music classification and machine learning, music cognition, and new interfaces for musical expression; discovering how artificial systems might help explain natural intelligence and vice versa; and seeking ways in which computing can aid in human expression, communications, health, and meaning.
Computing in Health and Life Sciences. Associated schools: School of Engineering; School of Science; and MIT Sloan School of Management.
Computing is increasingly becoming an indispensable tool in the health and life sciences. A key area is facilitating new approaches to identifying molecular and biomolecular agents with desired functions and for discovering new medications and new means of diagnosis. For instance, machine learning provides a unique opportunity in the pursuit of molecular and biomolecular discovery to parameterize and augment physics-based models, or possibly even replace them, and enable a revolution in molecular science and engineering. Another major area is health-care delivery, where novel algorithms, high performance computing, and machine learning offer new possibilities to transform health monitoring and treatment planning, facilitating better patient care, and enabling more effective ways to help prevent disease. In diagnosis, machine learning methods hold the promise of improved detection of diseases, increasing both specificity and sensitivity of imaging and testing.
This strategic area aims to hire faculty who help create transformative new computational methods in health and life sciences, while complementing the considerable existing work at MIT by forging additional connections. The broad scope ranges from computational approaches to fundamental problems in molecular design and synthesis for human health; to reshaping health-care delivery and personalized medicine; to understanding radiation effects and optimizing dose delivery on target cells; to improving tracing, imaging, and diagnosis techniques.
Computing for Health of the Planet. Associated schools: School of Engineering; School of Science; and School of Architecture and Planning.
The health of the planet is one of the most important challenges facing humankind today. Rapid industrialization has led to a number of serious threats to human and ecosystem health, including climate change, unsafe levels of air and water pollution, coastal and agricultural land erosion, and many others. Ensuring the health and safety of our planet necessitates an interdisciplinary approach that connects scientific understanding, engineering solutions, social, economic, and political aspects, with new computational methods to provide data-driven models and solutions for providing clean air, usable water, resilient food, efficient transportation systems, and identifying sustainable sources of energy.
This strategic hiring area will help facilitate such collaborations by bringing together expertise that will enable us to advance physical understanding of low-carbon energy solutions, earth-climate modelling, and urban planning through high performance computing, transformational numerical methods, and/or machine learning techniques.
Computing and Human Experience. Associated schools: School of Humanities, Arts, and Social Sciences and School of Architecture and Planning.
Computing and digital technologies are challenging the very ways in which people understand reality and our role in it. These technologies are embedded in the everyday lives of people around the world, and while frequently highly useful, they can reflect cultural assumptions and technological heritage, even though they are often viewed as being neutral prescriptions for structuring the world. Indeed, as becomes increasingly apparent, these technologies are able to alter individual and societal perceptions and actions, or affect societal institutions, in ways that are not broadly understood or intended. Moreover, although these technologies are conventionally developed for improved efficacy or efficiency, they can also provide opportunities for less utilitarian purposes such as supporting introspection and personal reflection.
This strategic hiring area focuses on growing the set of scholars in the social sciences, humanities, and computing who examine technology designs, systems, policies, and practices that can address the dual challenges of the lack of understanding of these technologies and their implications, including the design of systems that may help ameliorate rather than exacerbate inequalities. It further aims to develop techniques and systems that help people interpret and gain understanding from societal and historical data, including in humanities disciplines such as comparative literature, history, and art and architectural history.
Quantum Computing. Associated schools: School of Engineering and School of Science.
One of the most promising directions for continuing improvements in computing power comes from quantum mechanics. In the coming years, new hardware, algorithms, and discoveries offer the potential to dramatically increase the power of quantum computers far beyond current machines. Achieving these advances poses challenges that span multiple scientific and engineering fields, and from quantum hardware to quantum computing algorithms. Potential quantum computing applications span a broad range of fields, including chemistry, biology, materials science, atmospheric modeling, urban system simulation, nuclear engineering, finance, optimization, and others, requiring a deep understanding of both quantum computing algorithms and the problem space.
This strategic hiring area aims to build on MIT’s rich set of activities in the space to catalyze research and education in quantum computing and quantum information across the Institute, including the study of quantum materials; developing robust controllable quantum devices and networks that can faithfully transmit quantum information; and new algorithms for machine learning, AI, optimization, and data processing to fully leverage the promise of quantum computing.
A coordinated approach
Over the past few months, the MIT Schwarzman College of Computing has undertaken a strategic planning exercise to identify key areas for hiring the new shared faculty. The process has been led by Huttenlocher, together with MIT Provost Martin Schmidt and the deans of the five schools — Anantha Chandrakasan, dean of the School of Engineering; Melissa Nobles, Kenan Sahin Dean of the School of Humanities, Arts, and Social Sciences; Hashim Sarkis, dean of the School of Architecture and Planning; David Schmittlein, John C. Head III Dean of MIT Sloan; and Michael Sipser, dean of the School of Science — beginning with input from departments across the Institute.
This input was in the form of proposals for interdisciplinary computing areas that were solicited from department heads. A total of 29 proposals were received. Over a six-week period, the committee worked with proposing departments to identify strategic hiring themes. The process yielded the six areas that cover several critically important directions. 
“These areas not only bring together computing with numerous departments and schools, but also involve multiple modes of academic inquiry, offering opportunities for new collaborations in research and teaching across a broad range of fields,” says Schmidt. “I’m excited to see us launch this critical part of the college’s mission.”
The college will also coordinate with each of the five schools to ensure that diversity, equity, and inclusion is at the forefront for all of the hiring areas.
Hiring for the 2020-21 academic year
While the number of searches and involved schools will vary from year to year, the plan for the coming academic year is to have five searches, one with each school. These searches will be in three of the six strategic hiring areas as follows:
Social, Economic, and Ethical Implications of Computing and Networks will focus on two searches, one with the Department of Philosophy in the School of Humanities, Arts, and Social Sciences, and one with the MIT Sloan School of Management.
Computing and Natural Intelligence: Cognition, Perception and Language will focus on one search with the Department of Brain and Cognitive Sciences in the School of Science.
Computing for Health of the Planet will focus on two searches, one with the Department of Urban Studies and Planning in the School of Architecture and Planning, and one with a department to be identified in the School of Engineering.


",Six strategic areas identified for shared faculty hiring in computing,2020-08-31,['Terri Park'],"MIT Schwarzman College of Computing/School of Humanities Arts and Social Sciences/Sloan School of Management/School of Science/School of Architecture and Planning/School of Engineering/Philosophy/Brain and cognitive sciences/Urban studies and planning/Faculty/Electrical Engineering & Computer Science (eecs)/Computer science and technology/Computing/Artificial intelligence/Machine learning/Administration/Data/Climate change/Sustainability/Technology and society/Ethics/Quantum computing/Medicine/Disease/Algorithms/Education, teaching, academics","['quantum', 'identified', 'faculty', 'engineering', 'schools', 'computing', 'mit', 'areas', 'strategic', 'hiring', 'shared', 'sciences', 'social', 'school', 'science']","Associated schools: School of Humanities, Arts and the Social Sciences and MIT Sloan School of Management.
Associated schools: School of Science; School of Humanities, Arts, and Social Sciences; and School of Architecture and Planning.
Associated schools: School of Engineering; School of Science; and School of Architecture and Planning.
Quantum Computing.
Achieving these advances poses challenges that span multiple scientific and engineering fields, and from quantum hardware to quantum computing algorithms.",Mit
86,https://news.mit.edu/2020/mit-partners-national-labs-new-national-quantum-information-science-research-centers-0831,"


Early this year, the U.S. Department of Energy sent out a call for proposals as it announced it would award up to $625 million in funding over the next five years to establish multidisciplinary National Quantum Information Science (QIS) Research Centers. These awards would support the National Quantum Initiative Act, passed in 2018 to accelerate the development of quantum science and information technology applications.
Now, MIT is a partner institute on two QIS Research Centers that the Department of Energy has selected for funding.
One of the centers, the Co-design Center for Quantum Advantage (C2QA), will be led by Brookhaven National Laboratory. MIT participation in this center will be coordinated by Professor Isaac Chuang through the Center for Theoretical Physics.
The other center, the Quantum Systems Accelerator (QSA), will be led by Lawrence Berkeley National Laboratory. The Research Laboratory of Electronics (RLE) and MIT Lincoln Laboratory are partners on this center. Professor William Oliver, a Lincoln Laboratory fellow and director of the Center for Quantum Engineering, and Eric Dauler, who leads the Quantum Information and Integrated Nanosystems Group at Lincoln Laboratory, will coordinate MIT research activities with this center.
“Quantum information science and engineering research is a core strength at MIT, ranging broadly from algorithms and molecular chemistry to atomic and superconducting qubits, as well as quantum gravity and the foundations of computer science. This new funding from the Department of Energy will connect ongoing vibrant MIT research in quantum information with teams seeking to harness and discover quantum technologies,” says Chuang.
Devices based on the mysterious phenomena of quantum physics have begun to reshape the technology landscape. In recent years, researchers have been pursuing advanced quantum systems, like those that could lead to tamper-proof communications systems and computers that could tackle problems today's machines would need billions of years to solve.
The foundational expertise, infrastructure, and resources that MIT will bring to both QIS research centers is expected to help accelerate the development of such quantum technologies.
“Much of the theoretical and algorithmic foundation for quantum information science, as well as early experimental implementations, were developed at MIT. The QIS research centers build on this experience and the broader landscape. It is fantastic that MIT is participating with two centers, and this reflects our strength and breadth,” says Oliver.  
Each QIS research center incorporates a collaborative research team spanning multiple scientific and engineering disciplines and multiple institutions. Both centers are focused on pushing quantum computers “beyond-NISQ,” the acronym referring to today's generation of noisy intermediate-scale quantum systems. The long-term goal is to develop a “universal” quantum computer, the kind that can perform computational tasks that would be practically impossible for traditional supercomputers to solve. To get there, researchers face enormous challenges in creating and controlling the perfect conditions for large numbers of quantum bits (qubits) to interact and store information long enough to perform calculations. 
“Unlike most previous efforts, contributors from the algorithm, quantum computing, and quantum engineering areas will all need to work together to achieve the community's acceleration toward this ambitious goal,” says John Chiaverini, a principal investigator in the Quantum Information and Integrated Nanosystems Group.
In their partnership with the QSA, RLE and Lincoln Laboratory researchers will focus their efforts on co-designing fundamental engineering approaches, with the goal of enabling larger programmable quantum systems built from neutral atoms, trapped ions, and superconducting qubits. “Advancing all three hardware approaches to quantum computation within a coordinated, center-scale effort will enable uniquely collaborative development efforts and a deeper understanding of the fundamental quantum engineering constraints,” says Dauler. As larger systems are realized, they will be used by researchers throughout the center to feed quantum science research.  
“We look forward to further strengthening our research collaboration with Lawrence Berkeley National Laboratory, Sandia National Laboratories, and the partner universities to create many advances in quantum information science through the Quantum Systems Accelerator,” says Lincoln Laboratory Director Eric Evans.
At the C2QA, experts in QIS, materials science, computer science, and theory will focus on the superconducting qubit modality and work together to resolve performance issues with quantum computers by simultaneously co-designing software and hardware. Through these parallel efforts, the team will understand and control material properties to extend “coherence” time, or how long the qubits can function; design devices to generate more robust qubits; optimize algorithms to target specific scientific applications; and develop error-correction solutions.
MIT's cutting-edge facilities will bolster these collaborations. Lincoln Laboratory has the Microelectronics Laboratory, an ISO-9001-certified facility for fabricating advanced circuits for superconducting and trapped-ion quantum bit applications, and MIT.nano offers more than 20,000 square feet of clean-room space for making and testing quantum devices.
“I'm excited by the opportunity the research centers offer to collaborate, and to better advance the state of knowledge and technology in the quantum area. Specifically, the collaboration offers a new avenue for the U.S. quantum information science community to access the unique design, fabrication, and testing capabilities at MIT and Lincoln Laboratory, including the Microelectronics Laboratory and numerous laboratories specializing in advanced packaging and testing,” says Robert Atkins, who leads the Advanced Technology Division overseeing quantum computing research at Lincoln Laboratory.
Participation in both centers will complement other major programs that MIT has initiated in recent years, including the MIT-IBM Watson AI Lab, which aims to advance artificial intelligence hardware, software, and algorithms; the MIT Stephen A. Schwarzman College of Computing, which spans all five of MIT's schools; and the most-recently established Center for Quantum Engineering out of RLE and Lincoln Laboratory.
In addition to selecting these two MIT-affiliated centers, the Department of Energy announced funding for three additional QIS research centers. These investments, according to the department, represent a long-term, large-scale commitment of U.S. scientific and technological resources to a highly competitive and promising new area of investigation, with enormous potential to transform science and technology. 
“The QIS research centers will assure that advances in fundamental research in quantum science will progress to practical applications to benefit national security and many other segments of society,” says MIT Vice President for Research Maria Zuber. “The pace of discovery in this field is rapid, and the combined strengths of campus and Lincoln Laboratory are very well-aligned to lead in this area.”


",MIT partners with national labs on two new National Quantum Information Science Research Centers,2020-08-31,"['Kylie Foy', 'Sampson Wilcox']",Lincoln Laboratory/Research Laboratory of Electronics/MIT-IBM Watson AI Lab/Electrical Engineering & Computer Science (eecs)/School of Engineering/Quantum computing/Funding/Department of Energy (DoE)/MIT Schwarzman College of Computing/Computer science and technology,"['research', 'national', 'labs', 'centers', 'partners', 'information', 'laboratory', 'qis', 'quantum', 'lincoln', 'center', 'science', 'mit']","These awards would support the National Quantum Initiative Act, passed in 2018 to accelerate the development of quantum science and information technology applications.
The Research Laboratory of Electronics (RLE) and MIT Lincoln Laboratory are partners on this center.
“Much of the theoretical and algorithmic foundation for quantum information science, as well as early experimental implementations, were developed at MIT.
Each QIS research center incorporates a collaborative research team spanning multiple scientific and engineering disciplines and multiple institutions.
In addition to selecting these two MIT-affiliated centers, the Department of Energy announced funding for three additional QIS research centers.",Mit
87,https://news.mit.edu/2020/toward-machine-learning-that-can-reason-about-everyday-actions-0831,"


The ability to reason abstractly about events as they unfold is a defining feature of human intelligence. We know instinctively that crying and writing are means of communicating, and that a panda falling from a tree and a plane landing are variations on descending. 
Organizing the world into abstract categories does not come easily to computers, but in recent years researchers have inched closer by training machine learning models on words and images infused with structural information about the world, and how objects, animals, and actions relate. In a new study at the European Conference on Computer Vision this month, researchers unveiled a hybrid language-vision model that can compare and contrast a set of dynamic events captured on video to tease out the high-level concepts connecting them. 
Their model did as well as or better than humans at two types of visual reasoning tasks — picking the video that conceptually best completes the set, and picking the video that doesn’t fit. Shown videos of a dog barking and a man howling beside his dog, for example, the model completed the set by picking the crying baby from a set of five videos. Researchers replicated their results on two datasets for training AI systems in action recognition: MIT’s Multi-Moments in Time and DeepMind’s Kinetics.
“We show that you can build abstraction into an AI system to perform ordinary visual reasoning tasks close to a human level,” says the study’s senior author Aude Oliva, a senior research scientist at MIT, co-director of the MIT Quest for Intelligence, and MIT director of the MIT-IBM Watson AI Lab. “A model that can recognize abstract events will give more accurate, logical predictions and be more useful for decision-making.”
As deep neural networks become expert at recognizing objects and actions in photos and video, researchers have set their sights on the next milestone: abstraction, and training models to reason about what they see. In one approach, researchers have merged the pattern-matching power of deep nets with the logic of symbolic programs to teach a model to interpret complex object relationships in a scene. Here, in another approach, researchers capitalize on the relationships embedded in the meanings of words to give their model visual reasoning power.
“Language representations allow us to integrate contextual information learned from text databases into our visual models,” says study co-author Mathew Monfort, a research scientist at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL). “Words like ‘running,’ ‘lifting,’ and ‘boxing’ share some common characteristics that make them more closely related to the concept ‘exercising,’ for example, than ‘driving.’ ”
Using WordNet, a database of word meanings, the researchers mapped the relation of each action-class label in Moments and Kinetics to the other labels in both datasets. Words like “sculpting,” “carving,” and “cutting,” for example, were connected to higher-level concepts like “crafting,” “making art,” and “cooking.” Now when the model recognizes an activity like sculpting, it can pick out conceptually similar activities in the dataset. 
This relational graph of abstract classes is used to train the model to perform two basic tasks. Given a set of videos, the model creates a numerical representation for each video that aligns with the word representations of the actions shown in the video. An abstraction module then combines the representations generated for each video in the set to create a new set representation that is used to identify the abstraction shared by all the videos in the set.
To see how the model would do compared to humans, the researchers asked human subjects to perform the same set of visual reasoning tasks online. To their surprise, the model performed as well as humans in many scenarios, sometimes with unexpected results. In a variation on the set completion task, after watching a video of someone wrapping a gift and covering an item in tape, the model suggested a video of someone at the beach burying someone else in the sand. 
“It’s effectively ‘covering,’ but very different from the visual features of the other clips,” says Camilo Fosco, a PhD student at MIT who is co-first author of the study with PhD student Alex Andonian. “Conceptually it fits, but I had to think about it.”
Limitations of the model include a tendency to overemphasize some features. In one case, it suggested completing a set of sports videos with a video of a baby and a ball, apparently associating balls with exercise and competition.
A deep learning model that can be trained to “think” more abstractly may be capable of learning with fewer data, say researchers. Abstraction also paves the way toward higher-level, more human-like reasoning.
“One hallmark of human cognition is our ability to describe something in relation to something else — to compare and to contrast,” says Oliva. “It’s a rich and efficient way to learn that could eventually lead to machine learning models that can understand analogies and are that much closer to communicating intelligently with us.”

Other authors of the study are Allen Lee from MIT, Rogerio Feris from IBM, and Carl Vondrick from Columbia University.


",Toward a machine learning model that can reason about everyday actions,2020-08-31,['Kim Martineau'],Quest for Intelligence/MIT Schwarzman College of Computing/School of Engineering/Electrical Engineering & Computer Science (eecs)/MIT-IBM Watson AI Lab/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Algorithms/Artificial intelligence/Computer science and technology/Software/Computer vision/Natural language processing,"['everyday', 'tasks', 'model', 'set', 'visual', 'reason', 'learning', 'researchers', 'videos', 'actions', 'words', 'abstraction', 'machine', 'video', 'mit']","Here, in another approach, researchers capitalize on the relationships embedded in the meanings of words to give their model visual reasoning power.
Given a set of videos, the model creates a numerical representation for each video that aligns with the word representations of the actions shown in the video.
To see how the model would do compared to humans, the researchers asked human subjects to perform the same set of visual reasoning tasks online.
“Conceptually it fits, but I had to think about it.”Limitations of the model include a tendency to overemphasize some features.
A deep learning model that can be trained to “think” more abstractly may be capable of learning with fewer data, say researchers.",Mit
88,https://news.mit.edu/2020/nsf-announces-mit-led-institute-artificial-intelligence-fundamental-interactions-0826,"


The U.S. National Science Foundation (NSF) announced today an investment of more than $100 million to establish five artificial intelligence (AI) institutes, each receiving roughly $20 million over five years. One of these, the NSF AI Institute for Artificial Intelligence and Fundamental Interactions (IAIFI), will be led by MIT’s Laboratory for Nuclear Science (LNS) and become the intellectual home of more than 25 physics and AI senior researchers at MIT and Harvard, Northeastern, and Tufts universities. 
By merging research in physics and AI, the IAIFI seeks to tackle some of the most challenging problems in physics, including precision calculations of the structure of matter, gravitational-wave detection of merging black holes, and the extraction of new physical laws from noisy data.
“The goal of the IAIFI is to develop the next generation of AI technologies, based on the transformative idea that artificial intelligence can directly incorporate physics intelligence,” says Jesse Thaler, an associate professor of physics at MIT, LNS researcher, and IAIFI director.  “By fusing the ‘deep learning’ revolution with the time-tested strategies of ‘deep thinking’ in physics, we aim to gain a deeper understanding of our universe and of the principles underlying intelligence.”
IAIFI researchers say their approach will enable making groundbreaking physics discoveries, and advance AI more generally, through the development of novel AI approaches that incorporate first principles from fundamental physics.  
“Invoking the simple principle of translational symmetry — which in nature gives rise to conservation of momentum — led to dramatic improvements in image recognition,” says Mike Williams, an associate professor of physics at MIT, LNS researcher, and IAIFI deputy director. “We believe incorporating more complex physics principles will revolutionize how AI is used to study fundamental interactions, while simultaneously advancing the foundations of AI.”
In addition, a core element of the IAIFI mission is to transfer their technologies to the broader AI community.
“Recognizing the critical role of AI, NSF is investing in collaborative research and education hubs, such as the NSF IAIFI anchored at MIT, which will bring together academia, industry, and government to unearth profound discoveries and develop new capabilities,” says NSF Director Sethuraman Panchanathan. “Just as prior NSF investments enabled the breakthroughs that have given rise to today’s AI revolution, the awards being announced today will drive discovery and innovation that will sustain American leadership and competitiveness in AI for decades to come.”
Research in AI and fundamental interactions
Fundamental interactions are described by two pillars of modern physics: at short distances by the Standard Model of particle physics, and at long distances by the Lambda Cold Dark Matter model of Big Bang cosmology. Both models are based on physical first principles such as causality and space-time symmetries.  An abundance of experimental evidence supports these theories, but also exposes where they are incomplete, most pressingly that the Standard Model does not explain the nature of dark matter, which plays an essential role in cosmology.
AI has the potential to help answer these questions and others in physics.
For many physics problems, the governing equations that encode the fundamental physical laws are known. However, undertaking key calculations within these frameworks, as is essential to test our understanding of the universe and guide physics discovery, can be computationally demanding or even intractable. IAIFI researchers are developing AI for such first-principles theory studies, which naturally require AI approaches that rigorously encode physics knowledge. 
“My group is developing new provably exact algorithms for theoretical nuclear physics,” says Phiala Shanahan, an assistant professor of physics and LNS researcher at MIT. “Our first-principles approach turns out to have applications in other areas of science and even in robotics, leading to exciting collaborations with industry partners.”
Incorporating physics principles into AI could also have a major impact on many experimental applications, such as designing AI methods that are more easily verifiable. IAIFI researchers are working to enhance the scientific potential of various facilities, including the Large Hadron Collider (LHC) and the Laser Interferometer Gravity Wave Observatory (LIGO). 
“Gravitational-wave detectors are among the most sensitive instruments on Earth, but the computational systems used to operate them are mostly based on technology from the previous century,” says Principal Research Scientist Lisa Barsotti of the MIT Kavli Institute for Astrophysics and Space Research. “We have only begun to scratch the surface of what can be done with AI; just enough to see that the IAIFI will be a game-changer.”
The unique features of these physics applications also offer compelling research opportunities in AI more broadly. For example, physics-informed architectures and hardware development could lead to advances in the speed of AI algorithms, and work in statistical physics is providing a theoretical foundation for understanding AI dynamics. 
“Physics has inspired many time-tested ideas in machine learning: maximizing entropy, Boltzmann machines, and variational inference, to name a few,” says Pulkit Agrawal, an assistant professor of electrical engineering and computer science at MIT, and researcher in the Computer Science and Artificial Intelligence Laboratory (CSAIL). “We believe that close interaction between physics and AI researchers will be the catalyst that leads to the next generation of machine learning algorithms.” 
Cultivating early-career talent
AI technologies are advancing rapidly, making it both important and challenging to train junior researchers at the intersection of physics and AI. The IAIFI aims to recruit and train a talented and diverse group of early-career researchers, including at the postdoc level through its IAIFI Fellows Program.  
“By offering our fellows their choice of research problems, and the chance to focus on cutting-edge challenges in physics and AI, we will prepare many talented young scientists to become future leaders in both academia and industry,” says MIT professor of physics Marin Soljacic of the Research Laboratory of Electronics (RLE). 
IAIFI researchers hope these fellows will spark interdisciplinary and multi-investigator collaborations, generate new ideas and approaches, translate physics challenges beyond their native domains, and help develop a common language across disciplines. Applications for the inaugural IAIFI fellows are due in mid-October. 
Another related effort spearheaded by Thaler, Williams, and Alexander Rakhlin, an associate professor of brain and cognitive science at MIT and researcher in the Institute for Data, Systems, and Society (IDSS), is the development of a new interdisciplinary PhD program in physics, statistics, and data science, a collaborative effort between the Department of Physics and the Statistics and Data Science Center.
“Statistics and data science are among the foundational pillars of AI. Physics joining the interdisciplinary doctoral program will bring forth new ideas and areas of exploration, while fostering a new generation of leaders at the intersection of physics, statistics, and AI,"" says Rakhlin.  
Education, outreach, and partnerships 
The IAIFI aims to cultivate “human intelligence” by promoting education and outreach. For example, IAIFI members will contribute to establishing a MicroMasters degree program at MIT for students from non-traditional backgrounds.    
“We will increase the number of students in both physics and AI from underrepresented groups by providing fellowships for the MicroMasters program,” says Isaac Chuang, professor of physics and electrical engineering, senior associate dean for digital learning, and RLE researcher at MIT. “We also plan on working with undergraduate MIT Summer Research Program students, to introduce them to the tools of physics and AI research that they might not have access to at their home institutions.”
The IAIFI plans to expand its impact via numerous outreach efforts, including a K-12 program in which students are given data from the LHC and LIGO and tasked with rediscovering the Higgs boson and gravitational waves. 
“After confirming these recent Nobel Prizes, we can ask the students to find tiny artificial signals embedded in the data using AI and fundamental physics principles,” says assistant professor of physics Phil Harris, an LNS researcher at MIT. “With projects like this, we hope to disseminate knowledge about — and enthusiasm for — physics, AI, and their intersection.”
In addition, the IAIFI will collaborate with industry and government to advance the frontiers of both AI and physics, as well as societal sectors that stand to benefit from AI innovation. IAIFI members already have many active collaborations with industry partners, including DeepMind, Microsoft Research, and Amazon. 
“We will tackle two of the greatest mysteries of science: how our universe works and how intelligence works,” says MIT professor of physics Max Tegmark, an MIT Kavli Institute researcher. “Our key strategy is to link them, using physics to improve AI and AI to improve physics. We're delighted that the NSF is investing the vital seed funding needed to launch this exciting effort.”
Building new connections at MIT and beyond
Leveraging MIT’s culture of collaboration, the IAIFI aims to generate new connections and to strengthen existing ones across MIT and beyond.
Of the 27 current IAIFI senior investigators, 16 are at MIT and members of the LNS, RLE, MIT Kavli Institute, CSAIL, and IDSS. In addition, IAIFI investigators are members of related NSF-supported efforts at MIT, such as the Center for Brains, Minds, and Machines within the McGovern Institute for Brain Research and the MIT-Harvard Center for Ultracold Atoms.  
“We expect a lot of creative synergies as we bring physics and computer science together to study AI,"" says Bill Freeman, the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science and researcher in CSAIL. ""I'm excited to work with my physics colleagues on topics that bridge these fields.""
More broadly, the IAIFI aims to make Cambridge, Massachusetts, and the surrounding Boston area a hub for collaborative efforts to advance both physics and AI. 
“As we teach in 8.01 and 8.02, part of what makes physics so powerful is that it provides a universal language that can be applied to a wide range of scientific problems,” says Thaler. “Through the IAIFI, we will create a common language that transcends the intellectual borders between physics and AI to facilitate groundbreaking discoveries.”


",National Science Foundation announces MIT-led Institute for Artificial Intelligence and Fundamental Interactions,2020-08-26,[],Laboratory for Nuclear Science/Kavli Institute/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Research Laboratory of Electronics/Center for Brains Minds and Machines/Physics/LIGO/Machine learning/Artificial intelligence/National Science Foundation (NSF)/IDSS/McGovern Institute/Electrical Engineering & Computer Science (eecs)/Funding/School of Engineering/School of Science/MIT Schwarzman College of Computing,"['institute', 'mitled', 'physics', 'foundation', 'iaifi', 'nsf', 'artificial', 'science', 'announces', 'interactions', 'researchers', 'professor', 'national', 'fundamental', 'research', 'researcher', 'ai', 'intelligence', 'mit']","The U.S. National Science Foundation (NSF) announced today an investment of more than $100 million to establish five artificial intelligence (AI) institutes, each receiving roughly $20 million over five years.
IAIFI researchers are developing AI for such first-principles theory studies, which naturally require AI approaches that rigorously encode physics knowledge.
IAIFI researchers are working to enhance the scientific potential of various facilities, including the Large Hadron Collider (LHC) and the Laser Interferometer Gravity Wave Observatory (LIGO).
“We will tackle two of the greatest mysteries of science: how our universe works and how intelligence works,” says MIT professor of physics Max Tegmark, an MIT Kavli Institute researcher.
“Our key strategy is to link them, using physics to improve AI and AI to improve physics.",Mit
89,https://news.mit.edu/2020/rewriting-rules-machine-generated-art-0818,"


Horses don’t normally wear hats, and deep generative models, or GANs, don’t normally follow rules laid out by human programmers. But a new tool developed at MIT lets anyone go into a GAN and tell the model, like a coder, to put hats on the heads of the horses it draws. 
In a new study appearing at the European Conference on Computer Vision this month, researchers show that the deep layers of neural networks can be edited, like so many lines of code, to generate surprising images no one has seen before.
“GANs are incredible artists, but they’re confined to imitating the data they see,” says the study’s lead author, David Bau, a PhD student at MIT. “If we can rewrite the rules of a GAN directly, the only limit is human imagination.”
Generative adversarial networks, or GANs, pit two neural networks against each other to create hyper-realistic images and sounds. One neural network, the generator, learns to mimic the faces it sees in photos, or the words it hears spoken. A second network, the discriminator, compares the generator’s outputs to the original. The generator then iteratively builds on the discriminator’s feedback until its fabricated images and sounds are convincing enough to pass for real.
GANs have captivated artificial intelligence researchers for their ability to create representations that are stunningly lifelike and, at times, deeply bizarre, from a receding cat that melts into a pile of fur to a wedding dress standing in a church door as if abandoned by the bride. Like most deep learning models, GANs depend on massive datasets to learn from. The more examples they see, the better they get at mimicking them. 
But the new study suggests that big datasets are not essential. If you understand how a model is wired, says Bau, you can edit the numerical weights in its layers to get the behavior you desire, even if no literal example exists. No dataset? No problem. Just create your own.
“We’re like prisoners to our training data,” he says. “GANs only learn patterns that are already in our data. But here I can manipulate a condition in the model to create horses with hats. It’s like editing a genetic sequence to create something entirely new, like inserting the DNA of a firefly into a plant to make it glow in the dark.”
Bau was a software engineer at Google, and had led the development of Google Hangouts and Google Image Search, when he decided to go back to school. The field of deep learning was exploding and he wanted to pursue foundational questions in computer science. Hoping to learn how to build transparent systems that would empower users, he joined the lab of MIT Professor Antonio Torralba. There, he began probing deep nets and their millions of mathematical operations to understand how they represent the world.
Bau showed that you could slice into a GAN, like layer cake, to isolate the artificial neurons that had learned to draw a particular feature, like a tree, and switch them off to make the tree disappear. With this insight, Bau helped create GANPaint, a tool that lets users add and remove features like doors and clouds from a picture. In the process, he discovered that GANs have a stubborn streak: they wouldn’t let you draw doors in the sky.
“It had some rule that seemed to say, ‘doors don’t go there,’” he says. “That’s fascinating, we thought. It’s like an ‘if’ statement in a program. To me, it was a clear signal that the network had some kind of inner logic.”
Over several sleepless nights, Bau ran experiments and picked through the layers of his models for the equivalent of a conditional statement. Finally, it dawned on him. “The neural network has different memory banks that function as a set of general rules, relating one set of learned patterns to another,” he says. “I realized that if you could identify one line of memory, you could write a new memory into it.” 
In a short version of his ECCV talk, Bau demonstrates how to edit the model and rewrite memories using an intuitive interface he designed. He copies a tree from one image and pastes it into another, placing it, improbably, on a building tower. The model then churns out enough pictures of tree-sprouting towers to fill a family photo album. With a few more clicks, Bau transfers hats from human riders to their horses, and wipes away a reflection of light from a kitchen countertop.
The researchers hypothesize that each layer of a deep net acts as an associative memory, formed after repeated exposure to similar examples. Fed enough pictures of doors and clouds, for example, the model learns that doors are entryways to buildings, and clouds float in the sky. The model effectively memorizes a set of rules for understanding the world.
The effect is especially striking when GANs manipulate light. When GANPaint added windows to a room, for example, the model automatically added nearby reflections. It’s as if the model had an intuitive grasp of physics and how light should behave on object surfaces. “Even this relationship suggests that associations learned from data can be stored as lines of memory, and not only located but reversed,” says Torralba, the study’s senior author. 
GAN editing has its limitations. It’s not easy to identify all of the neurons corresponding to objects and animals the model renders, the researchers say. Some rules also appear edit-proof; some changes the researchers tried to make failed to execute.
Still, the tool has immediate applications in computer graphics, where GANs are widely studied, and in training expert AI systems to recognize rare features and events through data augmentation. The tool also brings researchers closer to understanding how GANs learn visual concepts with minimal human guidance. If the models learn by imitating what they see, forming associations in the process, they may be a springboard for new kinds of machine learning applications. 
The study’s other authors are Steven Liu, Tongzhou Wang, and Jun-Yan Zhu.


",Rewriting the rules of machine-generated art,2020-08-18,['Kim Martineau'],MIT Schwarzman College of Computing/School of Engineering/Electrical engineering and computer science (EECS)/Quest for Intelligence/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Algorithms/Artificial intelligence/Computer science and technology/Machine learning/Software/Computer vision/Arts,"['rewriting', 'create', 'model', 'memory', 'researchers', 'bau', 'deep', 'learn', 'gans', 'doors', 'art', 'rules', 'machinegenerated']","Horses don’t normally wear hats, and deep generative models, or GANs, don’t normally follow rules laid out by human programmers.
“The neural network has different memory banks that function as a set of general rules, relating one set of learned patterns to another,” he says.
It’s not easy to identify all of the neurons corresponding to objects and animals the model renders, the researchers say.
Some rules also appear edit-proof; some changes the researchers tried to make failed to execute.
The tool also brings researchers closer to understanding how GANs learn visual concepts with minimal human guidance.",Mit
90,https://news.mit.edu/2020/mit-data-systems-learn-be-better-tsunami-bao-0810,"


Big data has gotten really, really big: By 2025, all the world’s data will add up to an estimated 175 trillion gigabytes. For a visual, if you stored that amount of data on DVDs, it would stack up tall enough to circle the Earth 222 times. 
One of the biggest challenges in computing is handling this onslaught of information while still being able to efficiently store and process it. A team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) believe that the answer rests with something called “instance-optimized systems.”  
Traditional storage and database systems are designed to work for a wide range of applications because of how long it can take to build them — months or, often, several years. As a result, for any given workload such systems provide performance that is good, but usually not the best. Even worse, they sometimes require administrators to painstakingly tune the system by hand to provide even reasonable performance. 
In contrast, the goal of instance-optimized systems is to build systems that optimize and partially re-organize themselves for the data they store and the workload they serve. 
“It’s like building a database system for every application from scratch, which is not economically feasible with traditional system designs,” says MIT Professor Tim Kraska. 
As a first step toward this vision, Kraska and colleagues developed Tsunami and Bao. Tsunami uses machine learning to automatically re-organize a dataset’s storage layout based on the types of queries that its users make. Tests show that it can run queries up to 10 times faster than state-of-the-art systems. What’s more, its datasets can be organized via a series of ""learned indexes"" that are up to 100 times smaller than the indexes used in traditional systems. 
Kraska has been exploring the topic of learned indexes for several years, going back to his influential work with colleagues at Google in 2017. 
Harvard University Professor Stratos Idreos, who was not involved in the Tsunami project, says that a unique advantage of learned indexes is their small size, which, in addition to space savings, brings substantial performance improvements.
“I think this line of work is a paradigm shift that’s going to impact system design long-term,” says Idreos. “I expect approaches based on models will be one of the core components at the heart of a new wave of adaptive systems.”
Bao, meanwhile, focuses on improving the efficiency of query optimization through machine learning. A query optimizer rewrites a high-level declarative query to a query plan, which can actually be executed over the data to compute the result to the query. However, often there exists more than one query plan to answer any query; picking the wrong one can cause a query to take days to compute the answer, rather than seconds. 
Traditional query optimizers take years to build, are very hard to maintain, and, most importantly, do not learn from their mistakes. Bao is the first learning-based approach to query optimization that has been fully integrated into the popular database management system PostgreSQL. Lead author Ryan Marcus, a postdoc in Kraska’s group, says that Bao produces query plans that run up to 50 percent faster than those created by the PostgreSQL optimizer, meaning that it could help to significantly reduce the cost of cloud services, like Amazon’s Redshift, that are based on PostgreSQL.
By fusing the two systems together, Kraska hopes to build the first instance-optimized database system that can provide the best possible performance for each individual application without any manual tuning. 
The goal is to not only relieve developers from the daunting and laborious process of tuning database systems, but to also provide performance and cost benefits that are not possible with traditional systems.

Traditionally, the systems we use to store data are limited to only a few storage options and, because of it, they cannot provide the best possible performance for a given application. What Tsunami can do is dynamically change the structure of the data storage based on the kinds of queries that it receives and create new ways to store data, which are not feasible with more traditional approaches.
Johannes Gehrke, a managing director at Microsoft Research who also heads up machine learning efforts for Microsoft Teams, says that his work opens up many interesting applications, such as doing so-called “multidimensional queries” in main-memory data warehouses. Harvard’s Idreos also expects the project to spur further work on how to maintain the good performance of such systems when new data and new kinds of queries arrive.
Bao is short for “bandit optimizer,” a play on words related to the so-called “multi-armed bandit” analogy where a gambler tries to maximize their winnings at multiple slot machines that have different rates of return. The multi-armed bandit problem is commonly found in any situation that has tradeoffs between exploring multiple different options, versus exploiting a single option — from risk optimization to A/B testing.
“Query optimizers have been around for years, but they often make mistakes, and usually they don’t learn from them,” says Kraska. “That’s where we feel that our system can make key breakthroughs, as it can quickly learn for the given data and workload what query plans to use and which ones to avoid.”
Kraska says that in contrast to other learning-based approaches to query optimization, Bao learns much faster and can outperform open-source and commercial optimizers with as little as one hour of training time.In the future, his team aims to integrate Bao into cloud systems to improve resource utilization in environments where disk, RAM, and CPU time are scarce resources.
“Our hope is that a system like this will enable much faster query times, and that people will be able to answer questions they hadn’t been able to answer before,” says Kraska.
A related paper about Tsunami was co-written by Kraska, PhD students Jialin Ding and Vikram Nathan, and MIT Professor Mohammad Alizadeh. A paper about Bao was co-written by Kraska, Marcus, PhD students Parimarjan Negi and Hongzi Mao, visiting scientist Nesime Tatbul, and Alizadeh.
The work was done as part of the Data System and AI Lab (DSAIL@CSAIL), which is sponsored by Intel, Google, Microsoft, and the U.S. National Science Foundation. 


",Data systems that learn to be better,2020-08-10,['Adam Conner-Simons'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Algorithms/Research/Machine learning/Artificial intelligence/Data/School of Engineering/Computer science and technology/MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/National Science Foundation (NSF),"['data', 'performance', 'systems', 'provide', 'work', 'query', 'bao', 'queries', 'system', 'learn', 'better', 'tsunami']","Big data has gotten really, really big: By 2025, all the world’s data will add up to an estimated 175 trillion gigabytes.
As a result, for any given workload such systems provide performance that is good, but usually not the best.
In contrast, the goal of instance-optimized systems is to build systems that optimize and partially re-organize themselves for the data they store and the workload they serve.
The goal is to not only relieve developers from the daunting and laborious process of tuning database systems, but to also provide performance and cost benefits that are not possible with traditional systems.
The work was done as part of the Data System and AI Lab (DSAIL@CSAIL), which is sponsored by Intel, Google, Microsoft, and the U.S. National Science Foundation.",Mit
91,https://news.mit.edu/2020/mit-3-questions-john-leonard-future-of-autonomous-vehicles-0804,"


As part of the MIT Task Force on the Work of the Future’s new series of research briefs, professor of mechanical engineering John Leonard teamed with professor of aeronautics and astronautics and the Dibner Professor of the History of Engineering and Manufacturing David Mindell and with doctoral candidate Erik Stayton to explore the future of autonomous vehicles (AV) — an area that could arguably be called the touchstone for the discussion of jobs of the future in recent years. Leonard is the Samuel C. Collins Professor of Mechanical and Ocean Engineering in the Department of Mechanical Engineering, a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), and member of the MIT Task Force on the Work of the Future. His research addresses navigation and mapping for autonomous mobile robots operating in challenging environments. 
Their research brief, “Autonomous Vehicles, Mobility, and Employment Policy: The Roads Ahead,” looks at how the AV transition will affect jobs and explores how sustained investments in workforce training for advanced mobility can help drivers and other mobility workers transition into new careers that support mobility systems and technologies. It also highlights the policies that will greatly ease the integration of automated systems into urban mobility systems, including investing in local and national infrastructure, and forming public-private partnerships. Leonard spoke recently on some of the findings in the brief.
Q: When would you say Level 4 autonomous vehicle systems — those that can operate without active supervision by a human driver — increase their area of operation beyond today’s limited local deployments?
A: The widespread deployment of Level 4 automated vehicles will take much longer than many have predicted — at least a decade for favorable environments, and possibly much longer. Despite substantial recent progress by the community, major challenges remain before we will see the disruptive rollout of fully automated driving systems that have no safety driver onboard over large areas. Expansion will likely be gradual, and will happen region-by-region in specific categories of transportation, resulting in wide variations in availability across the country. The key question is not just “when,” but “where” will the technology be available and profitable?
Driver assistance and active safety systems (known as Level 2 automation) will continue to become more widespread on personal vehicles. These systems, however, will have limited impacts on jobs, since a human driver must be on board and ready to intervene at any moment. Level 3 systems can operate without active engagement by the driver for certain geographic settings, so long as the driver is ready to intervene when requested; however, these systems will likely be restricted to low-speed traffic.
Impacts on trucking are also expected to be less than many have predicted, due to technological challenges and risks that remain, even for more structured highway environments.
Q: In the brief, you make the argument that AV transition, while threatening numerous jobs, will not be “jobless.” Can you explain?  What are the likely impacts to mobility jobs — including transit, vehicle sales, vehicle maintenance, delivery, and other related industries?
A: The longer rollout time for Level 4 autonomy provides time for sustained investments in workforce training that can help drivers and other mobility workers transition into new careers that support mobility systems and technologies. Transitioning from current-day driving jobs to these jobs represents potential pathways for employment, so long as job-training resources are available. Because the geographical rollout of Level 4 automated driving is expected to be slow, human workers will remain essential to the operation of these systems for the foreseeable future, in roles that are both old and new. 
In some cases, Level 4 remote driving systems could move driving jobs from vehicles to fixed-location centers, but these might represent a step down in job quality for many professional drivers. The skills required for these jobs is largely unknown, but they are likely to be a combination of call-center, dispatcher, technician, and maintenance roles with strong language skills. More advanced engineering roles could also be sources of good jobs if automated taxi fleets are deployed at scale, but will require strong technical training that may be out of reach for many. 
Increasing availability of Level 2 and Level 3 systems will result in changes in the nature of work for professional drivers, but do not necessarily impact job numbers to the extent that other systems might, because these systems do not remove drivers from vehicles. 
While the employment implications of widespread Level 4 automation in trucking could eventually be considerable, as with other domains, the rollout is expected to be gradual. Truck drivers do more than just drive, and so human presence within even highly automated trucks would remain valuable for other reasons such as loading, unloading, and maintenance. Human-autonomous truck platooning, in which multiple Level 4 trucks follow a human-driven lead truck, may be more viable than completely operator-free Level 4 operations in the near term.  

Q: How should we prepare policy in the three key areas of infrastructure, jobs, and innovation? 
A: Policymakers can act now to prepare for and minimize disruptions to the millions of jobs in ground transportation and related industries that may come in the future, while also fostering greater economic opportunity and mitigating environmental impacts by building safe and accessible mobility systems. Investing in local and national infrastructure, and forming public-private partnerships, will greatly ease integration of automated systems into urban mobility systems.  
Automated vehicles should be thought of as one element in a mobility mix, and as a potential feeder for public transit rather than a replacement for it, but unintended consequences such as increased congestion remain risks. The crucial role of public transit for connecting workers to workplaces will endure: the future of work depends in large part on how people get to work.
Policy recommendations in the trucking sector include strengthening career pathways for drivers, increasing labor standards and worker protections, advancing public safety, creating good jobs via human-led truck platooning, and promoting safe and electric trucks.


",3 Questions: John Leonard on the future of autonomous vehicles,2020-08-04,[],MIT Task Force on the Work of the Future/Mechanical engineering/Computer Science and Artificial Intelligence Laboratory (CSAIL)/3 Questions/Autonomous vehicles/School of Engineering/Faculty/Policy/MIT Schwarzman College of Computing/History/Aeronautical and astronautical engineering/Jobs/School of Humanities Arts and Social Sciences,"['leonard', 'systems', 'john', 'automated', 'future', 'drivers', 'remain', 'questions', 'autonomous', 'engineering', 'vehicles', 'level', 'mobility', 'jobs']","It also highlights the policies that will greatly ease the integration of automated systems into urban mobility systems, including investing in local and national infrastructure, and forming public-private partnerships.
A: The widespread deployment of Level 4 automated vehicles will take much longer than many have predicted — at least a decade for favorable environments, and possibly much longer.
Driver assistance and active safety systems (known as Level 2 automation) will continue to become more widespread on personal vehicles.
What are the likely impacts to mobility jobs — including transit, vehicle sales, vehicle maintenance, delivery, and other related industries?
Investing in local and national infrastructure, and forming public-private partnerships, will greatly ease integration of automated systems into urban mobility systems.",Mit
92,https://news.mit.edu/2020/machine-learning-health-care-system-understands-when-to-step-in-0731,"


In recent years, entire industries have popped up that rely on the delicate interplay between human workers and automated software. Companies like Facebook work to keep hateful and violent content off their platforms using a combination of automated filtering and human moderators. In the medical field, researchers at MIT and elsewhere have used machine learning to help radiologists better detect different forms of cancer. 
What can be tricky about these hybrid approaches is understanding when to rely on the expertise of people versus programs. This isn’t always merely a question of who does a task “better;” indeed, if a person has limited bandwidth, the system may have to be trained to minimize how often it asks for help.
To tackle this complex issue, researchers from MIT’s Computer Science and Artificial Intelligence Lab (CSAIL) have developed a machine learning system that can either make a prediction about a task, or defer the decision to an expert. Most importantly, it can adapt when and how often it defers to its human collaborator, based on factors such as its teammate’s availability and level of experience.
The team trained the system on multiple tasks, including looking at chest X-rays to diagnose specific conditions such as atelectasis (lung collapse) and cardiomegaly (an enlarged heart). In the case of cardiomegaly, they found that their human-AI hybrid model performed 8 percent better than either could on their own (based on AU-ROC scores).  
“In medical environments where doctors don’t have many extra cycles, it’s not the best use of their time to have them look at every single data point from a given patient’s file,” says PhD student Hussein Mozannar, lead author with David Sontag, the Von Helmholtz Associate Professor of Medical Engineering in the Department of Electrical Engineering and Computer Science, of a new paper about the system that was recently presented at the International Conference of Machine Learning. “In that sort of scenario, it’s important for the system to be especially sensitive to their time and only ask for their help when absolutely necessary.”
The system has two parts: a “classifier” that can predict a certain subset of tasks, and a “rejector” that decides whether a given task should be handled by either its own classifier or the human expert.
Through experiments on tasks in medical diagnosis and text/image classification, the team showed that their approach not only achieves better accuracy than baselines, but does so with a lower computational cost and with far fewer training data samples.
“Our algorithms allow you to optimize for whatever choice you want, whether that’s the specific prediction accuracy or the cost of the expert’s time and effort,” says Sontag, who is also a member of MIT’s Institute for Medical Engineering and Science. “Moreover, by interpreting the learned rejector, the system provides insights into how experts make decisions, and in which settings AI may be more appropriate, or vice-versa.”
The system’s particular ability to help detect offensive text and images could also have interesting implications for content moderation. Mozanner suggests that it could be used at companies like Facebook in conjunction with a team of human moderators. (He is hopeful that such systems could minimize the amount of hateful or traumatic posts that human moderators have to review every day.)
Sontag clarified that the team has not yet tested the system with human experts, but instead developed a series of “synthetic experts” so that they could tweak parameters such as experience and availability. In order to work with a new expert it’s never seen before, the system would need some minimal onboarding to get trained on the person’s particular strengths and weaknesses.
In future work, the team plans to test their approach with real human experts, such as radiologists for X-ray diagnosis. They will also explore how to develop systems that can learn from biased expert data, as well as systems that can work with — and defer to — several experts at once. For example, Sontag imagines a hospital scenario where the system could collaborate with different radiologists who are more experienced with different patient populations.
“There are many obstacles that understandably prohibit full automation in clinical settings, including issues of trust and accountability,” says Sontag. “We hope that our method will inspire machine learning practitioners to get more creative in integrating real-time human expertise into their algorithms.” 
Mozanner is affiliated with both CSAIL and the MIT Institute for Data, Systems and Society (IDSS). The team’s work was supported, in part, by the National Science Foundation.



",An automated health care system that understands when to step in,2020-07-31,['Adam Conner-Simons'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/IDSS/Electrical engineering and computer science (EECS)/School of Engineering/Research/Computer science and technology/Algorithms/Machine learning/Health care/Disease/Medicine/Social media/Institute for Medical Engineering and Science (IMES)/MIT Schwarzman College of Computing,"['medical', 'human', 'systems', 'step', 'automated', 'work', 'care', 'sontag', 'understands', 'system', 'experts', 'machine', 'health', 'science', 'team']","In recent years, entire industries have popped up that rely on the delicate interplay between human workers and automated software.
Companies like Facebook work to keep hateful and violent content off their platforms using a combination of automated filtering and human moderators.
Mozanner suggests that it could be used at companies like Facebook in conjunction with a team of human moderators.
In future work, the team plans to test their approach with real human experts, such as radiologists for X-ray diagnosis.
For example, Sontag imagines a hospital scenario where the system could collaborate with different radiologists who are more experienced with different patient populations.",Mit
93,https://news.mit.edu/2020/school-engineering-first-and-second-quarter-2020-awards-0729,"


Members of the MIT engineering faculty receive many awards in recognition of their scholarship, service, and overall excellence. The School of Engineering periodically recognizes their achievements by highlighting the honors, prizes, and medals won by faculty working in our academic departments, labs, and centers.

Jesús del Alamo of the Department of Electrical Engineering and Computer Science won the Indium Phosphide and Related Materials Award 2020 on May 18.
Saman Amarasinghe of the Department of Electrical Engineering and Computer Science was named an Association for Computing Machinery Fellow on Dec. 12, 2019.
Brian Anthony of the Department of Mechanical Engineering won the Google Faculty Research Award 2019-2020 on Feb. 17.
Robert C. Armstrong of the Department of Chemical Engineering was made member of the American Academy of Arts and Sciences on April 23.
Guy Bresler of the Department of Electrical Engineering and Computer Science won a National Science Foundation CAREER Award on Jan. 24.
Michael Carbin of the Department of Electrical Engineering and Computer Science was awarded a Sloan Research Fellowship in Computer Science on Feb. 12.
Joel Emer of the Department of Electrical Engineering and Computer Science was elected to the National Academy of Engineering on Feb. 6, and won the IEEE/ACM International Symposium on Microarchitecture Best Paper Award in October 2019.
William Freeman of the Department of Electrical Engineering and Computer Science was named a Fellow of the Association for the Advancement of Artificial Intelligence on Feb. 9.
Daniel Frey of the Department of Mechanical Engineering was awarded the Charles M. Manly Memorial Medal on March 20.
Robert Gallager of the Department of Electrical Engineering and Computer Science was named a Japan Prize Laureate by the Japan Prize Foundation on Feb. 7.
Ming Guo of the Department of Mechanical Engineering was named the 2020 Sloan Research Fellow in Physics on Feb. 17.
Song Han of the Department of Electrical Engineering and Computer Science won a National Science Foundation CAREER Award on Jan. 24.
David Hardt of the Department of Mechanical Engineering was nominated to the 2020 Society of Manufacturing Engineers College of Fellows on June 9.
Neville Hogan of the Department of Mechanical Engineering was awarded the Science Foundation Ireland’s St. Patrick’s Day Medal for Academia 2020 on March 20.
Roger Kamm of the Department of Mechanical Engineering won the Shu Chien Achievement Award on Jan. 20.
James LeBeau of the Department of Materials Science and Engineering was awarded the Burton Medal on March 27.
Heather Lechtman of the Department of Materials Science and Engineering won the Pomerance Award for Scientific Contributions to Archaeology on Jan. 20.  
Jae Lim of the Department of Electrical Engineering and Computer Science was named a 2020 Ho-Am Prize laureate on April 8.
Muriel Médard of the Department of Electrical Engineering and Computer Science received an Honorary Doctorate from the Technical University of Munich on Jan. 17, and she became a member of the National Academy of Engineering on Feb. 6.
Stefanie Mueller of the Department of Electrical Engineering and Computer Science won the Sloan Research Fellowship in Computer Science on Feb. 12, and was named a Microsoft Research Faculty Fellow on April 17.
Yury Polyanskiy of the Department of Electrical Engineering and Computer Science won the 2020 James L. Massey Award on June 29.
Wim van Rees of the Department of Mechanical Engineering was named principal investigator for the U.S. Department of Energy's Office of Science Early Career Research Program on June 23.
Alberto Rodriguez of the Department of Mechanical Engineering won the IEEE Early Academic Career Award in Robotics and Automation on Feb. 5, and the Google Faculty Research Award 2019-2020 on Feb. 20.  
Jeffrey H. Shapiro of the Department of Electrical Engineering and Computer Science won the IEEE Signal Processing Society Best Paper Award on May 5.
Collin Stultz of the Department of Electrical Engineering and Computer Science was named a Fellow of the American Institute for Medical and Biological Engineering on March 24.
Vivienne Sze of the Department of Electrical Engineering and Computer Science won the inaugural Rising Star Award from the Association for Computing Machinery Council on Women in Computing on May 29.
Dick K.P. Yue of the Department of Mechanical Engineering was elected to the National Academy of Engineering on Feb. 6.



",School of Engineering first and second quarter 2020 awards,2020-07-29,[],"School of Engineering/Awards, honors and fellowships/Faculty/Electrical engineering and computer science (EECS)/Mechanical engineering/Chemical engineering/DMSE","['awards', 'second', 'scholarship', 'recognition', 'receive', '2020', 'faculty', 'engineering', 'won', 'recognizes', 'school', 'service', 'quarter', 'working']","Members of the MIT engineering faculty receive many awards in recognition of their scholarship, service, and overall excellence.
The School of Engineering periodically recognizes their achievements by highlighting the honors, prizes, and medals won by faculty working in our academic departments, labs, and centers.",Mit
94,https://news.mit.edu/2020/algorithm-finds-hidden-connections-between-paintings-met-museum-0729,"



Art is often heralded as the greatest journey into the past, solidifying a moment in time and space; the beautiful vehicle that lets us momentarily escape the present. 
With the boundless treasure trove of paintings that exist, the connections between these works of art from different periods of time and space can often go overlooked. It’s impossible for even the most knowledgeable of art critics to take in millions of paintings across thousands of years and be able to find unexpected parallels in themes, motifs, and visual styles. 
To streamline this process, a group of researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Microsoft created an algorithm to discover hidden connections between paintings at the Metropolitan Museum of Art (the Met) and Amsterdam’s Rijksmuseum. 
Inspired by a special exhibit “Rembrandt and Velazquez” in the Rijksmuseum, the new “MosAIc” system finds paired or “analogous” works from different cultures, artists, and media by using deep networks to understand how “close” two images are. In that exhibit, the researchers were inspired by an unlikely, yet similar pairing: Francisco de Zurbarán’s “The Martyrdom of Saint Serapion” and Jan Asselijn’s “The Threatened Swan,” two works that portray scenes of profound altruism with an eerie visual resemblance.
“These two artists did not have a correspondence or meet each other during their lives, yet their paintings hinted at a rich, latent structure that underlies both of their works,” says CSAIL PhD student Mark Hamilton, the lead author on a paper about “MosAIc.” 
To find two similar paintings, the team used a new algorithm for image search to unearth the closest match by a particular artist or culture. For example, in response to a query about “which musical instrument is closest to this painting of a blue-and-white dress,” the algorithm retrieves an image of a blue-and-white porcelain violin. These works are not only similar in pattern and form, but also draw their roots from a broader cultural exchange of porcelain between the Dutch and Chinese. 
“Image retrieval systems let users find images that are semantically similar to a query image, serving as the backbone of reverse image search engines and many product recommendation engines,” says Hamilton. “Restricting an image retrieval system to particular subsets of images can yield new insights into relationships in the visual world. We aim to encourage a new level of engagement with creative artifacts.” 
How it works 
For many, art and science are irreconcilable: one grounded in logic, reasoning, and proven truths, and the other motivated by emotion, aesthetics, and beauty. But recently, AI and art took on a new flirtation that, over the past 10 years, developed into something more serious. 
A large branch of this work, for example, has previously focused on generating new art using AI. There was the GauGAN project developed by researchers at MIT, NVIDIA, and the University of California at Berkeley; Hamilton and others’ previous GenStudio project; and even an AI-generated artwork that sold at Sotheby’s for $51,000. 
MosAIc, however, doesn’t aim to create new art so much as help explore existing art. One similar tool, Google’s “X Degrees of Separation,” finds paths of art that connect two works of art, but MosAIc differs in that it only requires a single image. Instead of finding paths, it uncovers connections in whatever culture or media the user is interested in, such as finding the shared artistic form of “Anthropoides paradisea” and “Seth Slaying a Serpent, Temple of Amun at Hibis.” 
Hamilton notes that building out their algorithm was a tricky endeavor, because they wanted to find images that were similar not just in color or style, but in meaning and theme. In other words, they’d want dogs to be close to other dogs, people to be close to other people, and so forth. To achieve this, they probe a deep network’s inner “activations” for each image in the combined open access collections of the Met and the Rijksmuseum. Distance between the “activations” of this deep network, which are commonly called “features,” was how they judged image similarity.
To find analogous images between different cultures, the team used a new image-search data structure called a “conditional KNN tree” that groups similar images together in a tree-like structure. To find a close match, they start at the tree’s “trunk” and follow the most promising “branch” until they are sure they’ve found the closest image. The data structure improves on its predecessors by allowing the tree to quickly “prune” itself to a particular culture, artist, or collection, quickly yielding answers to new types of queries.
What Hamilton and his colleagues found surprising was that this approach could also be applied to helping find problems with existing deep networks, related to the surge of “deepfakes” that have recently cropped up. They applied this data structure to find areas where probabilistic models, such as the generative adversarial networks (GANs) that are often used to create deepfakes, break down. They coined these problematic areas “blind spots,” and note that they give us insight into how GANs can be biased. Such blind spots further show that GANs struggle to represent particular areas of a dataset, even if most of their fakes can fool a human. Testing MosAIc 
The team evaluated MosAIc’s speed, and how closely it aligned with our human intuition about visual analogies.
For the speed tests, they wanted to make sure that their data structure provided value over simply searching through the collection with quick, brute-force search. 
To understand how well the system aligned with human intuitions, they made and released two new datasets for evaluating conditional image retrieval systems. One dataset challenged algorithms to find images with the same content even after they had been “stylized” with a neural style transfer method. The second dataset challenged algorithms to recover English letters across different fonts. A bit less than two-thirds of the time, MosAIc was able to recover the correct image in a single guess from a “haystack” of 5,000 images.
“Going forward, we hope this work inspires others to think about how tools from information retrieval can help other fields like the arts, humanities, social science, and medicine,” says Hamilton. “These fields are rich with information that has never been processed with these techniques and can be a source for great inspiration for both computer scientists and domain experts. This work can be expanded in terms of new datasets, new types of queries, and new ways to understand the connections between works.” 
Hamilton wrote the paper on MosAIc alongside Professor Bill Freeman and MIT undergraduates Stefanie Fu and Mindren Lu. The MosAIc website was built by MIT, Fu, Lu, Zhenbang Chen, Felix Tran, Darius Bopp, Margaret Wang, Marina Rogers, and Johnny Bui, at the Microsoft Garage winter externship program.


",Algorithm finds hidden connections between paintings at the Met,2020-07-29,['Rachel Gordon'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Research/Algorithms/Machine learning/Arts/Design/Electrical Engineering & Computer Science (eecs)/School of Engineering/MIT Schwarzman College of Computing,"['algorithm', 'similar', 'visual', 'finds', 'met', 'connections', 'images', 'hidden', 'paintings', 'image', 'hamilton', 'structure', 'mosaic', 'art', 'works']","With the boundless treasure trove of paintings that exist, the connections between these works of art from different periods of time and space can often go overlooked.
To streamline this process, a group of researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Microsoft created an algorithm to discover hidden connections between paintings at the Metropolitan Museum of Art (the Met) and Amsterdam’s Rijksmuseum.
“Restricting an image retrieval system to particular subsets of images can yield new insights into relationships in the visual world.
One similar tool, Google’s “X Degrees of Separation,” finds paths of art that connect two works of art, but MosAIc differs in that it only requires a single image.
To find analogous images between different cultures, the team used a new image-search data structure called a “conditional KNN tree” that groups similar images together in a tree-like structure.",Mit
95,https://news.mit.edu/2020/looking-black-box-deep-learning-neural-networks-0727,"


Deep learning systems are revolutionizing technology around us, from voice recognition that pairs you with your phone to autonomous vehicles that are increasingly able to see and recognize obstacles ahead. But much of this success involves trial and error when it comes to the deep learning networks themselves. A group of MIT researchers recently reviewed their contributions to a better theoretical understanding of deep learning networks, providing direction for the field moving forward.
“Deep learning was in some ways an accidental discovery,” explains Tommy Poggio, investigator at the McGovern Institute for Brain Research, director of the Center for Brains, Minds, and Machines (CBMM), and the Eugene McDermott Professor in Brain and Cognitive Sciences. “We still do not understand why it works. A theoretical framework is taking form, and I believe that we are now close to a satisfactory theory. It is time to stand back and review recent insights.”
Climbing data mountains
Our current era is marked by a superabundance of data — data from inexpensive sensors of all types, text, the internet, and large amounts of genomic data being generated in the life sciences. Computers nowadays ingest these multidimensional datasets, creating a set of problems dubbed the “curse of dimensionality” by the late mathematician Richard Bellman.
One of these problems is that representing a smooth, high-dimensional function requires an astronomically large number of parameters. We know that deep neural networks are particularly good at learning how to represent, or approximate, such complex data, but why? Understanding why could potentially help advance deep learning applications.










“Deep learning is like electricity after Volta discovered the battery, but before Maxwell,” explains Poggio, who is the founding scientific advisor of The Core, MIT Quest for Intelligence, and an investigator in the Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT. “Useful applications were certainly possible after Volta, but it was Maxwell’s theory of electromagnetism, this deeper understanding that then opened the way to the radio, the TV, the radar, the transistor, the computers, and the internet.”
The theoretical treatment by Poggio, Andrzej Banburski, and Qianli Liao points to why deep learning might overcome data problems such as “the curse of dimensionality.” Their approach starts with the observation that many natural structures are hierarchical. To model the growth and development of a tree doesn’t require that we specify the location of every twig. Instead, a model can use local rules to drive branching hierarchically. The primate visual system appears to do something similar when processing complex data. When we look at natural images — including trees, cats, and faces — the brain successively integrates local image patches, then small collections of patches, and then collections of collections of patches. 
“The physical world is compositional — in other words, composed of many local physical interactions,” explains Qianli Liao, an author of the study, and a graduate student in the Department of Electrical Engineering and Computer Science and a member of the CBMM. “This goes beyond images. Language and our thoughts are compositional, and even our nervous system is compositional in terms of how neurons connect with each other. Our review explains theoretically why deep networks are so good at representing this complexity.”
The intuition is that a hierarchical neural network should be better at approximating a compositional function than a single “layer” of neurons, even if the total number of neurons is the same. The technical part of their work identifies what “better at approximating” means and proves that the intuition is correct.
Generalization puzzle
There is a second puzzle about what is sometimes called the unreasonable effectiveness of deep networks. Deep network models often have far more parameters than data to fit them, despite the mountains of data we produce these days. This situation ought to lead to what is called “overfitting,” where your current data fit the model well, but any new data fit the model terribly. This is dubbed poor generalization in conventional models. The conventional solution is to constrain some aspect of the fitting procedure. However, deep networks do not seem to require this constraint. Poggio and his colleagues prove that, in many cases, the process of training a deep network implicitly “regularizes” the solution, providing constraints.
The work has a number of implications going forward. Though deep learning is actively being applied in the world, this has so far occurred without a comprehensive underlying theory. A theory of deep learning that explains why and how deep networks work, and what their limitations are, will likely allow development of even much more powerful learning approaches.
“In the long term, the ability to develop and build better intelligent machines will be essential to any technology-based economy,” explains Poggio. “After all, even in its current — still highly imperfect — state, deep learning is impacting, or about to impact, just about every aspect of our society and life.”


",Looking into the black box,2020-07-27,['Sabbi Lall'],McGovern Institute/Center for Brains Minds and Machines/Brain and cognitive sciences/Quest for Intelligence/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical engineering and computer science (EECS)/Artificial intelligence/MIT Schwarzman College of Computing/School of Science/School of Engineering/Research,"['looking', 'data', 'compositional', 'poggio', 'model', 'box', 'learning', 'black', 'explains', 'deep', 'networks', 'better', 'theory']","Our current era is marked by a superabundance of data — data from inexpensive sensors of all types, text, the internet, and large amounts of genomic data being generated in the life sciences.
But much of this success involves trial and error when it comes to the deep learning networks themselves.
Generalization puzzleThere is a second puzzle about what is sometimes called the unreasonable effectiveness of deep networks.
Though deep learning is actively being applied in the world, this has so far occurred without a comprehensive underlying theory.
“After all, even in its current — still highly imperfect — state, deep learning is impacting, or about to impact, just about every aspect of our society and life.”",Mit
96,https://news.mit.edu/2020/america-innovate-endless-frontier-0724,"


In July of 1945, in an America just beginning to establish a postwar identity, former MIT vice president Vannevar Bush set forth a vision that guided the country to decades of scientific dominance and economic prosperity. Bush’s report to the president of the United States, “Science: The Endless Frontier,” called on the government to support basic research in university labs. Its ideas, including the creation of the National Science Foundation (NSF), are credited with helping to make U.S. scientific and technological innovation the envy of the world.
Today, America’s lead in science and technology is being challenged as never before, write MIT President L. Rafael Reif and Indiana University President Michael A. McRobbie in an op-ed published today by The Chicago Tribune. They describe a “triple challenge” of bolder foreign competitors, faster technological change, and a merciless race to get from lab to market.
The government’s decision to adopt Bush’s ideas was bold and controversial at the time, and similarly bold action is needed now, they write.
“The U.S. has the fundamental building blocks for success, including many of the world’s top research universities that are at the forefront of the fight against COVID-19,” reads the op-ed. “But without a major, sustained funding commitment, a focus on key technologies and a faster system for transforming discoveries into new businesses, products and quality jobs, in today’s arena, America will not prevail.”
McRobbie and Reif believe a bipartisan bill recently introduced in both chambers of Congress can help America’s innovation ecosystem meet the challenges of the day. Named the “Endless Frontier Act,” the bill would support research focused on advancing key technologies like artificial intelligence and quantum computing. It does not seek to alter or replace the NSF, but to “create new strength in parallel,” they write. 
The bill would also create scholarships, fellowships, and other forms of assistance to help build an American workforce ready to develop and deploy the latest technologies. And, it would facilitate experiments to help commercialize new ideas more quickly.
“Today’s leaders have the opportunity to display the far-sighted vision their predecessors showed after World War II — to expand and shape of our institutions, and to make the investments to adapt to a changing world,” Reif and McRobbie write.

Both university presidents acknowledge that measures such as the Endless Frontier Act require audacious choices. But if leaders take the right steps now, they write, those choices will seem, in retrospect, obvious and wise.
“Now as then, our national prosperity hinges on the next generation of technical triumphs,” Reif and Mcrobbie write. “Now as then, that success is not inevitable, and it will not come by chance. But with focused funding and imaginative policy, we believe it remains in reach.”


",Commentary: America must invest in its ability to innovate,2020-07-24,['Zach Winn'],President L. Rafael Reif/Government/Global/Computer science and technology/Quantum computing/Artificial intelligence/Technology and society/innovation & entrepreneurship (i&e)/Policy/National Science Foundation (NSF)/Research/Funding,"['research', 'reif', 'innovate', 'america', 'technologies', 'ability', 'write', 'university', 'president', 'ideas', 'help', 'commentary', 'mcrobbie', 'invest', 'science']","Bush’s report to the president of the United States, “Science: The Endless Frontier,” called on the government to support basic research in university labs.
Named the “Endless Frontier Act,” the bill would support research focused on advancing key technologies like artificial intelligence and quantum computing.
It does not seek to alter or replace the NSF, but to “create new strength in parallel,” they write.
But if leaders take the right steps now, they write, those choices will seem, in retrospect, obvious and wise.
“Now as then, our national prosperity hinges on the next generation of technical triumphs,” Reif and Mcrobbie write.",Mit
97,https://news.mit.edu/2020/neural-vulnerability-huntingtons-disease-tied-to-mitochondrial-rna-release-0721,"


In the first study to comprehensively track how different types of brain cells respond to the mutation that causes Huntington’s disease (HD), MIT neuroscientists found that a significant cause of death for an especially afflicted kind of neuron might be an immune response to genetic material errantly released by mitochondria, the cellular components that provide cells with energy.
In different cell types at different stages of disease progression, the researchers measured how levels of RNA differed from normal in brain samples from people who died with Huntington’s disease and in mice engineered with various degrees of the genetic mutation. Among several novel observations in both species, one that particularly stood out is that RNA from mitochondria were misplaced within the brain cells, called spiny projection neurons (SPNs), that are ravaged in the disease, contributing to its fatal neurological symptoms. The scientists observed that these stray RNAs, which look different to cells than RNA derived from the cell nucleus, triggered a problematic immune reaction.
“When these RNAs are released from the mitochondria, to the cell they can look just like viral RNAs, and this triggers innate immunity and can lead to cell death,” says study senior author Myriam Heiman, associate professor in MIT’s Department of Brain and Cognitive Sciences, the Picower Institute for Learning and Memory, and the Broad Institute of MIT and Harvard. “We believe this to be part of the pathway that triggers inflammatory signaling, which has been seen in HD before.”
Picower Fellow Hyeseung Lee and former visiting scientist Robert Fenster are co-lead authors of the study published in Neuron.
Mitochondrial mishap
The team’s two different screening methods, “TRAP,” which can be used in mice, and single-nucleus RNA sequencing, which can also be used in mice and humans, not only picked up the presence of mitochondrial RNAs most specifically in the SPNs but also showed a deficit in the expression of genes for a process called oxidative phosphorylation that fuel-hungry neurons employ to make energy. The mouse experiments showed that this downregulation of oxidative phosphorylation and increase in mitochondrial RNA release both occurred very early in disease, before most other gene expression differences were manifest.
Moreover, the researchers found increased expression of an immune system protein called PKR, which has been shown to be a sensor of the released mitochondrial RNA. In fact, the team found that PKR was not only elevated in the neurons, but also activated and bound to mitochondrial RNAs.
The new findings appear to converge with other clinical conditions that, like Huntington’s disease, lead to damage in a brain region called the striatum, Heiman said. In a condition called Aicardi-Goutières syndrome, the same brain region can be damaged because of a misregulated innate immune response. In addition, children with thiamine deficiency suffer mitochondrial dysfunction, and a prior study has shown that mice with thiamine deficiency show PKR activation, much like Heiman’s team found.
“These non-HD human disorders that are characterized by striatal cell death extend the significance of our findings by linking both the oxidative metabolism deficits and autoinflammatory activation phenomena described here directly to human striatal cell death absent the [Huntington’s mutation] context,” they wrote in Neuron.
Other observations
Though the mitochondrial RNA release discovery was the most striking, the study produced several other potentially valuable findings, Heiman says.
One is that the study produced a sweeping catalog of substantial differences in gene expression, including ones related to important neural functions such as their synapse circuit connections and circadian clock function. Another, based on some of the team’s analysis of their results, is that a master regulator of these alterations to gene transcription in neurons may be the retinoic acid receptor b (or “Rarb”) transcription factor. Heiman said that this could be a clinically useful finding because there are drugs that can activate Rarb.
“If we can inhibit transcriptional misregulation, we might be able to alter the outcome of the disease,” Heiman speculates. “It’s an important hypothesis to test.”
Another, more basic, finding in the study is that many of the gene expression differences the researchers saw in neurons in the human brain samples matched well with the changes they saw in mouse neurons, providing additional assurance that mouse models are indeed useful for studying this disease, Heiman says. The question has dogged the field somewhat because mice typically don’t show as much neuron death as people do.
“What we see is that actually the mouse models recapitulate the gene-expression changes that are occurring in these stage HD human neurons very well,” she says. “Interestingly, some of the other, non-neuronal, cell types did not show as much conservation between the human disease and mouse models, information that our team believes will be helpful to other investigators in future studies.”
The single-nucleus RNA sequencing study was part of a longstanding collaboration with Manolis Kellis’s group in MIT’s Computer Science and Artificial Intelligence Laboratory. Together, the two labs hope to expand these studies in the near future to further understand Huntington’s disease mechanisms.
In addition to Heiman, Lee, and Fenster, the paper’s other authors are Sebastian Pineda, Whitney Gibbs, Shahin Mohammadi, Fan Gao, Jose-Davila-Velderrain, Francisco Garcia, Martine Therrien, Hailey Novis, Hilary Wilkinson, Thomas Vogt, Manolis Kellis, and Matthew LaVoie.
The CHDI Foundation, the U.S. National Institutes of Health, Broderick Fund for Phytocannabinoid Research at MIT, and the JPB Foundation funded the study.


",Neural vulnerability in Huntington’s disease tied to release of mitochondrial RNA,2020-07-21,['David Orenstein'],Picower Institute/Brain and cognitive sciences/Broad Institute/Computer Science and Artificial Intelligence Laboratory (CSAIL)/School of Science/Neuroscience/Huntington’s/RNA/Immunology/Cells/Research,"['vulnerability', 'mice', 'disease', 'huntingtons', 'heiman', 'neurons', 'rna', 'mitochondrial', 'brain', 'study', 'cell', 'mouse', 'tied', 'release', 'neural']","The mouse experiments showed that this downregulation of oxidative phosphorylation and increase in mitochondrial RNA release both occurred very early in disease, before most other gene expression differences were manifest.
Moreover, the researchers found increased expression of an immune system protein called PKR, which has been shown to be a sensor of the released mitochondrial RNA.
Other observationsThough the mitochondrial RNA release discovery was the most striking, the study produced several other potentially valuable findings, Heiman says.
“If we can inhibit transcriptional misregulation, we might be able to alter the outcome of the disease,” Heiman speculates.
Together, the two labs hope to expand these studies in the near future to further understand Huntington’s disease mechanisms.",Mit
98,https://news.mit.edu/2020/mit-schwarzman-college-computing-announces-first-named-professorships-0720,"


The MIT Stephen A. Schwarzman College of Computing announced its first two named professorships, beginning July 1, to Frédo Durand and Samuel Madden in the Department of Electrical Engineering and Computer Science (EECS). These named positions recognize the outstanding achievements and future potential of their academic careers.
“I’m thrilled to acknowledge Frédo and Sam for their outstanding contributions in research and education. These named professorships recognize them for their extraordinary achievements,” says Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing.
Frédo Durand, a professor of computer science and engineering in EECS, has been named the inaugural Amar Bose Professor of Computing. The professorship, named after Amar Bose, former longtime member of the MIT faculty and the founder of Bose Corporation, is granted in recognition of the recipient’s excellence in teaching, research, and mentorship in the field of computing. A member of the Computer Science and Artificial Intelligence Laboratory, Durand’s research interests span most aspects of picture generation and creation, including rendering and computational photography. His recent focus includes video magnification for revealing the invisible, differentiable rendering, and compilers for productive high-performance imaging.
He received an inaugural Eurographics Young Researcher Award in 2004; an NSF CAREER Award in 2005; an inaugural Microsoft Research New Faculty Fellowship in 2005; a Sloan Foundation Fellowship in 2006; a Spira Award for distinguished teaching in 2007; and the ACM SIGGRAPH Computer Graphics Achievement Award in 2016.
Samuel Madden has been named the inaugural College of Computing Distinguished Professor of Computing. A professor of electrical engineering and computer science in EECS, Madden is being honored as an outstanding faculty member who is recognized as a leader and innovator. His research is in the area of database systems, focusing on database analytics and query processing, ranging from clouds to sensors to modern high-performance server architectures. He co-directs the Data Systems for AI Lab initiative and the Data Systems Group, investigating issues related to systems and algorithms for data focusing on applying new methodologies for processing data, including applying machine learning methods to data systems and engineering data systems for applying machine learning at scale. 
Madden was named one of MIT Technology Review's ""35 Innovators Under 35"" in 2005, and received an NSF CAREER Award in 2004 and a Sloan Foundation Fellowship in 2007. He has also received several best paper awards in VLDB 2004 and 2007 and in MobiCom 2006. In addition, he was recognized with a ""test of time"" award in SIGMOD 2013 for his work on acquisitional query processing and a 10-year best paper award in VLDB 2015 for his work on the C-Store system.


",MIT Schwarzman College of Computing announces first named professorships,2020-07-20,[],"MIT Schwarzman College of Computing/Electrical engineering and computer science (EECS)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Faculty/Awards, honors and fellowships/Computer science and technology","['professorships', 'data', 'named', 'announces', 'systems', 'research', 'computer', 'computing', 'award', 'professor', 'college', 'schwarzman', 'mit', 'science']","The MIT Stephen A. Schwarzman College of Computing announced its first two named professorships, beginning July 1, to Frédo Durand and Samuel Madden in the Department of Electrical Engineering and Computer Science (EECS).
These named professorships recognize them for their extraordinary achievements,” says Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing.
Frédo Durand, a professor of computer science and engineering in EECS, has been named the inaugural Amar Bose Professor of Computing.
Samuel Madden has been named the inaugural College of Computing Distinguished Professor of Computing.
He co-directs the Data Systems for AI Lab initiative and the Data Systems Group, investigating issues related to systems and algorithms for data focusing on applying new methodologies for processing data, including applying machine learning methods to data systems and engineering data systems for applying machine learning at scale.",Mit
99,https://news.mit.edu/2020/better-simulation-meshes-well-for-design-software-and-more-0720,"


The digital age has spurred the rise of entire industries aimed at simulating our world and the objects in it. Simulation is what helps movies have realistic effects, automakers test cars virtually, and scientists analyze geophysical data.
To simulate physical systems in 3D, researchers often program computers to divide objects into sets of smaller elements, a procedure known as “meshing.” Most meshing approaches tile 2D objects with patterns of triangles or quadrilaterals (quads), and tile 3D objects with patterns of triangular pyramids (tetrahedra) or bent cubes (hexahedra, or “hexes”).
While much progress has been made in the fields of computational geometry and geometry processing, scientists surprisingly still don’t fully understand the math of stacking together cubes when they are allowed to bend or stretch a bit. Many questions remain about the patterns that can be formed by gluing cube-shaped elements together, which relates to an area of math called topology.
New work out of MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) aims to explore several of these questions. Researchers have published a series of papers that address shortcomings of existing meshing tools by seeking out mathematical structure in the problem. In collaboration with scientists at the University of Bern and the University of Texas at Austin, their work shows how areas of math like algebraic geometry, topology, and differential geometry could improve physical simulations used in computer-aided design (CAD), architecture, gaming, and other sectors.
“Simulation tools that are being deployed ‘in the wild’ don’t always fail gracefully,” says MIT Associate Professor Justin Solomon, senior author on the three new meshing-related papers. “If one thing is wrong with the mesh, the simulation might not agree with real-world physics, and you might have to throw the whole thing out.” 
In one paper, a team led by MIT undergraduate Zoë Marschner developed an algorithm to repair issues that can often trip up existing approaches for hex meshing, specifically.
For example, some meshes contain elements that are partially inside-out or that self-intersect in ways that can’t be detected from their outer surfaces. The team’s algorithm works in iterations to repair those meshes in a way that untangles any such inversions while remaining faithful to the original shape.
“Thorny unsolved topology problems show up all over the hex-meshing universe,” says Marschner. “Until we figure them out, our algorithms will often fail in subtle ways.”
Marschner’s algorithm uses a technique called “sum-of-squares (SOS) relaxation” to pinpoint exactly where hex elements are inverted (which researchers describe as being “invalid”). It then moves the vertices of the hex element so that the hex is valid at the point where it was previously most invalid. The algorithm repeats this procedure to repair the hex.
In addition to being published at this week’s Symposium on Geometry Processing, Marschner’s work earned her MIT’s 2020 Anna Pogosyants UROP Award.
A second paper spearheaded by PhD student Paul Zhang improves meshing by incorporating curves, edges, and other features that provide important cues for the human visual system and pattern recognition algorithms. 
It can be difficult for computers to find these features reliably, let alone incorporate them into meshes. By using an existing construction called an “octahedral frame field” that is traditionally used for meshing 3D volumes, Zhang and his team have been able to develop 2D surface meshes without depending on unreliable methods that try to trace out features ahead of time. 
Zhang says that they’ve shown that these so-called “feature-aligned” constructions automatically create visually accurate quad meshes, which are widely used in computer graphics and virtual reality applications.
“As the goal of meshing is to simultaneously simplify the object and maintain accuracy to the original domain, this tool enables a new standard in feature-aligned quad meshing,” says Zhang. 
A third paper led by PhD student David Palmer links Zhang and Marschner’s work, advancing the theory of octahedral fields and showing how better math provides serious practical improvement for hex meshing. 
In physics and geometry, velocities and flows are represented as “vector fields,” which attach an arrow to every point in a region of space. In 3D, these fields can twist, knot around, and cross each other in remarkably complicated ways. Further complicating matters, Palmer’s research studies the structure of “frame fields,” in which more than one arrow appears at each point.
Palmer’s work gives new insight into the ways frames can be described and uses them to design methods for placing frames in 3D space. Building off of existing work, his methods produce smooth, stable fields that can guide the design of high-quality meshes.
Solomon says that his team aims to eventually characterize all the ways that octahedral frames twist and knot around each other to create structures in space. 
“This is a cool area of computational geometry where theory has a real impact on the quality of simulation tools,” says Solomon. 
Palmer cites organizations like Sandia National Labs that conduct complicated physical simulations involving phenomena like nonlinear elasticity and object deformation. He says that, even today, engineering teams often build or repair hex meshes almost completely by hand. 
“Existing software for automatic meshing often fails to produce a complete mesh, even if the frame field guidance ensures that the mesh pieces that are there look good,” Palmer says. “Our approach helps complete the picture.”
Marschner’s paper was co-written by Solomon, Zhang, and Palmer. Zhang’s paper was co-written by Solomon, Josh Vekhter, and Etienne Vouga at the University of Texas at Austin, Professor David Bommes of the University of Bern in Germany, and CSAIL postdoc Edward Chien. Palmer’s paper was co-written by Solomon and Bommes. Zhang and Palmer’s papers will be presented at the SIGGRAPH computer graphics conference later this month.
The projects were supported, in part, by Adobe Systems, the U.S. Air Force Office of Scientific Research, the U.S. Army Research Office, the U.S. Department of Energy, the Fannie and John Hertz Foundation, MathWorks, the MIT-IBM Watson AI Laboratory, the National Science Foundation, the Skoltech-MIT Next Generation program, and the Toyota-CSAIL Joint Research Center.


",Better simulation meshes well for design software (and more),2020-07-20,['Adam Conner-Simons'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/School of Engineering/Research/Algorithms/Computer science and technology/Imaging/Simulation/National Science Foundation (NSF)/Department of Energy (DoE)/MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs),"['simulation', 'fields', '3d', 'hex', 'work', 'meshes', 'software', 'meshing', 'paper', 'better', 'design', 'zhang', 'geometry', 'ways']","Simulation is what helps movies have realistic effects, automakers test cars virtually, and scientists analyze geophysical data.
For example, some meshes contain elements that are partially inside-out or that self-intersect in ways that can’t be detected from their outer surfaces.
Building off of existing work, his methods produce smooth, stable fields that can guide the design of high-quality meshes.
“This is a cool area of computational geometry where theory has a real impact on the quality of simulation tools,” says Solomon.
He says that, even today, engineering teams often build or repair hex meshes almost completely by hand.",Mit
100,https://news.mit.edu/2020/mit-tackles-misinformation-in-event-of-moon-disaster-0720,"


Can you recognize a digitally manipulated video when you see one? It’s harder than most people realize. As the technology to produce realistic “deepfakes” becomes more easily available, distinguishing fact from fiction will only get more challenging. A new digital storytelling project from MIT’s Center for Advanced Virtuality aims to educate the public about the world of deepfakes with “In Event of Moon Disaster.”
This provocative website showcases a “complete” deepfake (manipulated audio and video) of U.S. President Richard M. Nixon delivering the real contingency speech written in 1969 for a scenario in which the Apollo 11 crew were unable to return from the moon. The team worked with a voice actor and a company called Respeecher to produce the synthetic speech using deep learning techniques. They also worked with the company Canny AI to use video dialogue replacement techniques to study and replicate the movement of Nixon’s mouth and lips. Through these sophisticated AI and machine learning technologies, the seven-minute film shows how thoroughly convincing deepfakes can be. 
“Media misinformation is a longstanding phenomenon, but, exacerbated by deepfake technologies and the ease of disseminating content online, it’s become a crucial issue of our time,” says D. Fox Harrell, professor of digital media and of artificial intelligence at MIT and director of the MIT Center for Advanced Virtuality, part of MIT Open Learning. “With this project — and a course curriculum on misinformation being built around it — our powerfully talented XR Creative Director Francesca Panetta is pushing forward one of the center’s broad aims: using AI and technologies of virtuality to support creative expression and truth.”







Play video






Alongside the film, moondisaster.org features an array of interactive and educational resources on deepfakes. Led by Panetta and Halsey Burgund, a fellow at MIT Open Documentary Lab, an interdisciplinary team of artists, journalists, filmmakers, designers, and computer scientists has created a robust, interactive resource site where educators and media consumers can deepen their understanding of deepfakes: how they are made and how they work; their potential use and misuse; what is being done to combat deepfakes; and teaching and learning resources. 
“This alternative history shows how new technologies can obfuscate the truth around us, encouraging our audience to think carefully about the media they encounter daily,” says Panetta.
Also part of the launch is a new documentary, “To Make a Deepfake,” a 30-minute film by Scientific American, that uses “In Event of Moon Disaster” as a jumping-off point to explain the technology behind AI-generated media. The documentary features prominent scholars and thinkers on the state of deepfakes, on the stakes for the spread of misinformation and the twisting of our digital reality, and on the future of truth.
The project is supported by the MIT Open Documentary Lab and the Mozilla Foundation, which awarded “In Event of Moon Disaster” a Creative Media Award last year. These awards are part of Mozilla’s mission to realize more trustworthy AI in consumer technology. The latest cohort of awardees uses art and advocacy to examine AI’s effect on media and truth.
Says J. Bob Alotta, Mozilla’s vice president of global programs: “AI plays a central role in consumer technology today — it curates our news, it recommends who we date, and it targets us with ads. Such a powerful technology should be demonstrably worthy of trust, but often it is not. Mozilla’s Creative Media Awards draw attention to this, and also advocate for more privacy, transparency, and human well-being in AI.” 
“In Event of Moon Disaster” previewed last fall as a physical art installation at the International Documentary Film Festival Amsterdam, where it won the Special Jury Prize for Digital Storytelling; it was selected for the 2020 Tribeca Film Festival and Cannes XR. The new website is the project’s global digital launch, making the film and associated materials available for free to all audiences.
The past few months have seen the world move almost entirely online: schools, talk shows, museums, election campaigns, doctor’s appointments — all have made a rapid transition to virtual. When every interaction we have with the world is seen through a digital filter, it becomes more important than ever to learn how to distinguish between authentic and manipulated media. 
“It’s our hope that this project will encourage the public to understand that manipulated media plays a significant role in our media landscape,” says co-director Burgund, “and that, with further understanding and diligence, we can all reduce the likelihood of being unduly influenced by it.""


",Tackling the misinformation epidemic with “In Event of Moon Disaster”,2020-07-20,[],"Office of Open Learning/Artificial intelligence/Art, Culture and Technology/Arts/Augmented and virtual reality/Computer science and technology/Education, teaching, academics/Film and Television/History/Internet/Machine learning/Media/Computer Science and Artificial Intelligence Laboratory (CSAIL)/School of Engineering/MIT Schwarzman College of Computing/School of Humanities Arts and Social Sciences/Technology and society","['misinformation', 'event', 'technology', 'digital', 'disaster', 'technologies', 'media', 'epidemic', 'ai', 'deepfakes', 'tackling', 'film', 'moon', 'mit', 'documentary']","The team worked with a voice actor and a company called Respeecher to produce the synthetic speech using deep learning techniques.
Through these sophisticated AI and machine learning technologies, the seven-minute film shows how thoroughly convincing deepfakes can be.
As the technology to produce realistic “deepfakes” becomes more easily available, distinguishing fact from fiction will only get more challenging.
The project is supported by the MIT Open Documentary Lab and the Mozilla Foundation, which awarded “In Event of Moon Disaster” a Creative Media Award last year.
The new website is the project’s global digital launch, making the film and associated materials available for free to all audiences.",Mit
101,https://news.mit.edu/2020/faculty-receive-funding-develop-novel-ai-techniques-combat-covid-19-0717,"


Artificial intelligence has the power to help put an end to the Covid-19 pandemic. Not only can techniques of machine learning and natural language processing be used to track and report Covid-19 infection rates, but other AI techniques can also be used to make smarter decisions about everything from when states should reopen to how vaccines are designed. Now, MIT researchers working on seven groundbreaking projects on Covid-19 will be funded to more rapidly develop and apply novel AI techniques to improve medical response and slow the pandemic spread.
Earlier this year, the C3.ai Digital Transformation Institute (C3.ai DTI) formed, with the goal of attracting the world’s leading scientists to join in a coordinated and innovative effort to advance the digital transformation of businesses, governments, and society. The consortium is dedicated to accelerating advances in research and combining machine learning, artificial intelligence, internet of things, ethics, and public policy — for enhancing societal outcomes. MIT, under the auspices of the School of Engineering, joined the C3.ai DTI consortium, along with C3.ai, Microsoft Corporation, the University of Illinois at Urbana-Champaign, the University of California at Berkeley, Princeton University, the University of Chicago, Carnegie Mellon University, and, most recently, Stanford University.The initial call for project proposals aimed to embrace the challenge of abating the spread of Covid-19 and advance the knowledge, science, and technologies for mitigating the impact of pandemics using AI. Out of a total of 200 research proposals, 26 projects were selected and awarded $5.4 million to continue AI research to mitigate the impact of Covid-19 in the areas of medicine, urban planning, and public policy.
The first round of grant recipients was recently announced, and among them are five projects led by MIT researchers from across the Institute: Saurabh Amin, associate professor of civil and environmental engineering; Dimitris Bertsimas, the Boeing Leaders for Global Operations Professor of Management; Munther Dahleh, the William A. Coolidge Professor of Electrical Engineering and Computer Science and director of the MIT Institute for Data, Systems, and Society; David Gifford, professor of biological engineering and of electrical engineering and computer science; and Asu Ozdaglar, the MathWorks Professor of Electrical Engineering and Computer Science, head of the Department of Electrical Engineering and Computer Science, and deputy dean of academics for MIT Schwarzman College of Computing.
“We are proud to be a part of this consortium, and to collaborate with peers across higher education, industry, and health care to collectively combat the current pandemic, and to mitigate risk associated with future pandemics,” says Anantha P. Chandrakasan, dean of the School of Engineering and the Vannevar Bush Professor of Electrical Engineering and Computer Science. “We are so honored to have the opportunity to accelerate critical Covid-19 research through resources and expertise provided by the C3.ai DTI.”
Additionally, three MIT researchers will collaborate with principal investigators from other institutions on projects blending health and machine learning. Regina Barzilay, the Delta Electronics Professor in the Department of Electrical Engineering and Computer Science, and Tommi Jaakkola, the Thomas Siebel Professor of Electrical Engineering and Computer Science, join Ziv Bar-Joseph from Carnegie Mellon University for a project using machine learning to seek treatment for Covid-19. Aleksander Mądry, professor of computer science in the Department of Electrical Engineering and Computer Science, joins Sendhil Mullainathan of the University of Chicago for a project using machine learning to support emergency triage of pulmonary collapse due to Covid-19 on the basis of X-rays.
Bertsimas’s project develops automated, interpretable, and scalable decision-making systems based on machine learning and artificial intelligence to support clinical practices and public policies as they respond to the Covid-19 pandemic. When it comes to reopening the economy while containing the spread of the pandemic, Ozdaglar’s research provides quantitative analyses of targeted interventions for different groups that will guide policies calibrated to different risk levels and interaction patterns. Amin is investigating the design of actionable information and effective intervention strategies to support safe mobilization of economic activity and reopening of mobility services in urban systems. Dahleh’s research innovatively uses machine learning to determine how to safeguard schools and universities against the outbreak. Gifford was awarded funding for his project that uses machine learning to develop more informed vaccine designs with improved population coverage, and to develop models of Covid-19 disease severity using individual genotypes.
“The enthusiastic support of the distinguished MIT research community is making a huge contribution to the rapid start and significant progress of the C3.ai Digital Transformation Institute,” says Thomas Siebel, chair and CEO of C3.ai. “It is a privilege to be working with such an accomplished team.”
The following projects are the MIT recipients of the inaugural C3.ai DTI Awards: 
""Pandemic Resilient Urban Mobility: Learning Spatiotemporal Models for Testing, Contact Tracing, and Reopening Decisions"" — Saurabh Amin, associate professor of civil and environmental engineering; and Patrick Jaillet, the Dugald C. Jackson Professor of Electrical Engineering and Computer Science
""Effective Cocktail Treatments for SARS-CoV-2 Based on Modeling Lung Single Cell Response Data"" — Regina Barzilay, the Delta Electronics Professor in the Department of Electrical Engineering and Computer Science, and Tommi Jaakkola, the Thomas Siebel Professor of Electrical Engineering and Computer Science (Principal investigator: Ziv Bar-Joseph of Carnegie Mellon University)
""Toward Analytics-Based Clinical and Policy Decision Support to Respond to the Covid-19 Pandemic"" — Dimitris Bertsimas, the Boeing Leaders for Global Operations Professor of Management and associate dean for business analytics; and Alexandre Jacquillat, assistant professor of operations research and statistics
""Reinforcement Learning to Safeguard Schools and Universities Against the Covid-19 Outbreak"" — Munther Dahleh, the William A. Coolidge Professor of Electrical Engineering and Computer Science and director of MIT Institute for Data, Systems, and Society; and Peko Hosoi, the Neil and Jane Pappalardo Professor of Mechanical Engineering and associate dean of engineering
""Machine Learning-Based Vaccine Design and HLA Based Risk Prediction for Viral Infections"" — David Gifford, professor of biological engineering and of electrical engineering and computer science
""Machine Learning Support for Emergency Triage of Pulmonary Collapse in Covid-19"" — Aleksander Mądry, professor of computer science in the Department of Electrical Engineering and Computer Science (Principal investigator: Sendhil Mullainathan of the University of Chicago)
""Targeted Interventions in Networked and Multi-Risk SIR Models: How to Unlock the Economy During a Pandemic"" — Asu Ozdaglar, the MathWorks Professor of Electrical Engineering and Computer Science, department head of electrical engineering and computer science, and deputy dean of academics for MIT Schwarzman College of Computing; and Daron Acemoglu, Institute Professor


",Faculty receive funding to develop artificial intelligence techniques to combat Covid-19,2020-07-17,[],School of Engineering/MIT Schwarzman College of Computing/Civil and environmental engineering/Biological engineering/Electrical engineering and computer science (EECS)/Covid-19/Pandemic/Artificial intelligence/Grants/Funding/IDSS/Machine learning/Technology and society/Medicine/Faculty,"['electrical', 'receive', 'develop', 'research', 'faculty', 'computer', 'learning', 'techniques', 'engineering', 'funding', 'combat', 'professor', 'university', 'artificial', 'intelligence', 'covid19', 'science', 'mit']","Artificial intelligence has the power to help put an end to the Covid-19 pandemic.
Now, MIT researchers working on seven groundbreaking projects on Covid-19 will be funded to more rapidly develop and apply novel AI techniques to improve medical response and slow the pandemic spread.
The consortium is dedicated to accelerating advances in research and combining machine learning, artificial intelligence, internet of things, ethics, and public policy — for enhancing societal outcomes.
Gifford was awarded funding for his project that uses machine learning to develop more informed vaccine designs with improved population coverage, and to develop models of Covid-19 disease severity using individual genotypes.
“The enthusiastic support of the distinguished MIT research community is making a huge contribution to the rapid start and significant progress of the C3.ai Digital Transformation Institute,” says Thomas Siebel, chair and CEO of C3.ai.",Mit
102,https://news.mit.edu/2020/bu-law-clinics-0716,"


In 2015, the first of two MIT-Boston University law clinics was formed to provide free legal services to student innovators while giving law students experience working on technology-related legal matters.Several metrics could be used to measure the clinics’ success since then: More than 750 student teams have received support through the program over the course of its lifetime. Those interactions have led to about 50,000 hours of client work performed by BU law students, accounting for around $17.5 million worth of legal services to students from both campuses.The extent to which the clinics have become ingrained into each schools’ operations also underscores their success. In last year’s MIT delta v summer accelerator, two-thirds of the teams benefited from the clinics’ support. On BU’s side, 44 law students worked in the clinics during the last school year and over the summer, accounting for almost one-sixth of the entire law class.The numbers show the importance students place on being on sound legal footing as they bring a disruptive startup or revelatory research paper out into the world. The numbers also made the two schools’ recent decision to renew the clinics’ operations for five more years an easy one.“For a startup or an academic researcher, it’s not knowing [about potential issues] that can be the most paralyzing thing,” says Andy Sellars, a member of BU Law’s faculty who directs the Technology Law Clinic. “What we can give them is the map. We can say here are the legal issues, here’s where the law is pretty settled, here’s where the law is unsettled, here are some things to do to mitigate your risk. And by doing all that we can add some extra confidence and energy to the venture or research project.”Coming into formMIT and BU’s collaboration began in September of 2015, with the launch of the Startup Law Clinic as part of a new Entrepreneurship, Intellectual Property, and Cyberlaw program at BU. A year later, as planned, the Technology Law Clinic was formed.The Startup Law Clinic helps student entrepreneurs navigate issues associated with launching a venture, like establishing a corporation or LLC, securing intellectual property, and hiring employees. The Technology Law Clinic, whose client base includes researchers as well as entrepreneurs, helps ensure students’ work aligns with laws around data collection, privacy, information disclosure, encryption, and more.At the time of their founding, both clinics consisted of a supervising lawyer and eight student advocates.“What we realized pretty much immediately was that wasn’t going to do it — we needed to grow,” Sellars says. “And the major story of the last four years has been figuring out what the needs are and growing to meet those needs.”Today each clinic includes three licensed attorneys, although BU’s law students do most of the work advising and representing clients. The clinics held regular office hours at the Martin Trust Center for MIT Entrepreneurship, the Media Lab, MIT Sandbox, and elsewhere to expand access to clinic services; they have since shifted to virtual office hours due to the Covid-19 pandemic. BU’s law students also write white papers on specific legal areas and conduct presentations at locations around MIT to reach a broader audience.“Our goal is to educate our law students to do the work and maintain the client relationships, although we’re there supervising,” says James Wheaton, who directs the Startup Law Clinic.Since 2017, the collaboration has been bolstered by the Matthew Z. Gomes Fellowships, a program at BU Law that supports students from underrepresented communities in order to foster greater diversity among the next generation of technology and startup lawyers. Four of the seven fellows working for the clinics this summer are Gomes Fellows. “The tech sector has known this about itself for some time: We have a major diversity problem in all corners of tech, including among the lawyers who represent tech companies,” Sellars says. “We wanted to think of some ways to improve diversity in technology by improving the pipeline.”Legal support for impactIn 2014, MIT anthropology PhD candidate Amy Johnson filed a request under the Freedom of Information Act with the CIA, seeking information about the organization’s Twitter account. When the CIA failed to produce any documents, Johnson worked with the Technology Law Clinic to file a lawsuit against the agency, which then sent her 30 documents related to her request. Johnson and her legal team decided that wasn’t enough, and following several more rounds of litigation, she has received around 400 records. That case is ongoing and Johnson is still seeking more documents.“We really kicked in the door by suing the CIA our first year,” Sellars says.The CIA case is one of several high-profile projects the law clinics have been involved with. The Technology Law Clinic also advised MIT researchers who were publishing a study revealing bias in multiple company’s facial-analysis programs. The clinic helped the students ensure the study complied with computer access laws and share the results with the companies in advance of the paper’s publication.  More recently, the clinic helped researchers in MIT’s Computer Science and Artificial Intelligence Laboratory as they published technical papers that exposed security vulnerabilities in a mobile voting application that had been used in the 2018 midterm elections. The vulnerability gave hackers the opportunity to alter, stop, or expose how users voted.“A popular area we work in is computer science, both because of the huge population of CS students at MIT, and also because a lot of advanced computer science research can feel like the sort of ‘hacking’ that is prohibited by laws like the Computer Fraud and Abuse Act,” Sellars says. “So often we’re helping clients stay on the right side of ‘anti-hacking’ laws. Then there are a lot of data related questions … [dealing with] data privacy, access to data, use of data, and web scraping, which is writing a script that systematically gathers info across the web.”Even as the field of computer science accounts for a large portion of the clinics’ work, students from across MIT’s campus have benefited from the clinics’ support, something people familiar with MIT’s innovation ecosystem expected from the start.“I’m not surprised at all that the clinics have supported students from all five of MIT’s schools,” says Michael Cima, MIT’s associate dean for innovation, who also serves on the board of the clinics. “Student-led startups, in particular, are very diverse in their makeup. These include not only for-profit oriented businesses but also sustainable non-profits.”A bright futureThe disruptions caused by Covid-19 have forced everyone to adjust to remote work, but they haven’t slowed the number of innovations coming out of MIT, or the law clinics’ work in support of those innovations. In fact, April 2020 was the busiest month in the clinics’ history, and they’ve continued to see a dramatic increase in work as students pursue ideas to help with the pandemic.Now that the clinics have been renewed for five more years, its directors are brainstorming ways to further expand their services. The pandemic has shown the clinics can work even if members can’t meet their clients in person, and has reinforced the idea that technology can help scale operations.“We know no matter how big we grow, the program will never fully meet the needs of the MIT student body, and because of that we’re trying to think of more ways to have an impact, even if you’re not a client,” Sellars says, noting the clinics have started work on guides and “how to” documents for students that will be offered on the clinics’ websites.Regardless of where the clinics go from here, it’s clear they’ve already blossomed into an integral part of MIT’s innovation ecosystem, which bodes well for the Institute’s next generation of innovators.“The clinics have been successful teaching and learning labs for both MIT and BU students, and have helped our students advance their passion for innovation and entrepreneurship,” says Mark DiVincenzo, vice president and general counsel at MIT. “The issues have been varied, cutting edge in many ways, allowing BU students to assist MIT students in projects that are or will impact the world.”


",MIT-BU law clinics help students bring innovations into the world,2020-07-16,['Zach Winn'],Law/Research/Students/Computer science and technology/Innovation and Entrepreneurship (I&E)/Startups/Digital technology/Technology and society/Martin Trust Center for MIT Entrepreneurship,"['technology', 'mitbu', 'innovations', 'startup', 'clinics', 'sellars', 'work', 'clinic', 'world', 'legal', 'help', 'law', 'bring', 'mit', 'students']","In 2015, the first of two MIT-Boston University law clinics was formed to provide free legal services to student innovators while giving law students experience working on technology-related legal matters.
On BU’s side, 44 law students worked in the clinics during the last school year and over the summer, accounting for almost one-sixth of the entire law class.
A year later, as planned, the Technology Law Clinic was formed.
The CIA case is one of several high-profile projects the law clinics have been involved with.
“The issues have been varied, cutting edge in many ways, allowing BU students to assist MIT students in projects that are or will impact the world.”",Mit
103,https://news.mit.edu/2020/letting-robots-manipulate-cables-0713,"


For humans, it can be challenging to manipulate thin flexible objects like ropes, wires, or cables. But if these problems are hard for humans, they are nearly impossible for robots. As a cable slides between the fingers, its shape is constantly changing, and the robot’s fingers must be constantly sensing and adjusting the cable’s position and motion.
Standard approaches have used a series of slow and incremental deformations, as well as mechanical fixtures, to get the job done. Recently, a group of researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and from the MIT Department of Mechanical Engineering pursued the task from a different angle, in a manner that more closely mimics us humans. The team’s new system uses a pair of soft robotic grippers with high-resolution tactile sensors (and no added mechanical constraints) to successfully manipulate freely moving cables.







Play video






One could imagine using a system like this for both industrial and household tasks, to one day enable robots to help us with things like tying knots, wire shaping, or even surgical suturing. 
The team’s first step was to build a novel two-fingered gripper. The opposing fingers are lightweight and quick moving, allowing nimble, real-time adjustments of force and position. On the tips of the fingers are vision-based “GelSight” sensors, built from soft rubber with embedded cameras. The gripper is mounted on a robot arm, which can move as part of the control system.
The team’s second step was to create a perception-and-control framework to allow cable manipulation. For perception, they used the GelSight sensors to estimate the pose of the cable between the fingers, and to measure the frictional forces as the cable slides. Two controllers run in parallel: one modulates grip strength, while the other adjusts the gripper pose to keep the cable within the gripper.
When mounted on the arm, the gripper could reliably follow a USB cable starting from a random grasp position. Then, in combination with a second gripper, the robot can move the cable “hand over hand” (as a human would) in order to find the end of the cable. It could also adapt to cables of different materials and thicknesses.
As a further demo of its prowess, the robot performed an action that humans routinely do when plugging earbuds into a cell phone. Starting with a free-floating earbud cable, the robot was able to slide the cable between its fingers, stop when it felt the plug touch its fingers, adjust the plug’s pose, and finally insert the plug into the jack. 
“Manipulating soft objects is so common in our daily lives, like cable manipulation, cloth folding, and string knotting,” says Yu She, MIT postdoc and lead author on a new paper about the system. “In many cases, we would like to have robots help humans do this kind of work, especially when the tasks are repetitive, dull, or unsafe.” String me along Cable following is challenging for two reasons. First, it requires controlling the “grasp force” (to enable smooth sliding), and the “grasp pose” (to prevent the cable from falling from the gripper’s fingers).  This information is hard to capture from conventional vision systems during continuous manipulation, because it’s usually occluded, expensive to interpret, and sometimes inaccurate. 
What’s more, this information can’t be directly observed with just vision sensors, hence the team’s use of tactile sensors. The gripper’s joints are also flexible — protecting them from potential impact. 
The algorithms can also be generalized to different cables with various physical properties like material, stiffness, and diameter, and also to those at different speeds. 
When comparing different controllers applied to the team’s gripper, their control policy could retain the cable in hand for longer distances than three others. For example, the “open-loop” controller only followed 36 percent of the total length, the gripper easily lost the cable when it curved, and it needed many regrasps to finish the task. 
Looking ahead 
The team observed that it was difﬁcult to pull the cable back when it reached the edge of the ﬁnger, because of the convex surface of the GelSight sensor. Therefore, they hope to improve the ﬁnger-sensor shape to enhance the overall performance. 
In the future, they plan to study more complex cable manipulation tasks such as cable routing and cable inserting through obstacles, and they want to eventually explore autonomous cable manipulation tasks in the auto industry.
Yu She wrote the paper alongside MIT PhD students Shaoxiong Wang, Siyuan Dong, and Neha Sunil; Alberto Rodriguez, MIT associate professor of mechanical engineering; and Edward Adelson, the John and Dorothy Wilson Professor in the MIT Department of Brain and Cognitive Sciences. 
This work was supported by the Amazon Research Awards, the Toyota Research Institute, and the Office of Naval Research.


",Letting robots manipulate cables,2020-07-13,['Rachel Gordon'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Research/Algorithms/Distributed Robotics Laboratory/Artificial intelligence/Sensors/School of Engineering/Robots/Robotics/MIT Schwarzman College of Computing,"['tasks', 'gripper', 'fingers', 'cables', 'manipulate', 'humans', 'manipulation', 'robots', 'sensors', 'different', 'letting', 'teams', 'cable', 'mit']","The team’s new system uses a pair of soft robotic grippers with high-resolution tactile sensors (and no added mechanical constraints) to successfully manipulate freely moving cables.
For humans, it can be challenging to manipulate thin flexible objects like ropes, wires, or cables.
As a cable slides between the fingers, its shape is constantly changing, and the robot’s fingers must be constantly sensing and adjusting the cable’s position and motion.
The team’s second step was to create a perception-and-control framework to allow cable manipulation.
In the future, they plan to study more complex cable manipulation tasks such as cable routing and cable inserting through obstacles, and they want to eventually explore autonomous cable manipulation tasks in the auto industry.",Mit
104,https://news.mit.edu/2020/share-covid-19-misinformation-0709,"


To stay current about the Covid-19 pandemic, people need to process health information when they read the news. Inevitably, that means people will be exposed to health misinformation, too, in the form of false content, often found online, about the illness.
Now a study co-authored by MIT scholars contains bad news and good news about Covid-19 misinformation — and a new insight that may help reduce the problem.
The bad news is that when people are consuming news on social media, their inclination to share that news with others interferes with their ability to assess its accuracy. The study presented the same false news headlines about Covid-19 to two groups of people: One group was asked if they would share those stories on social media, and the other evaluated their accuracy. The participants were 32.4 percent more likely to say they would share the headlines than they were to say those headlines were accurate.
“There does appear to be a disconnect between accuracy judgments and sharing intentions,” says MIT professor David Rand, co-author of a new paper detailing the findings. “People are much more discerning when you ask them to judge the accuracy, compared to when you ask them whether they would share something or not.”
The good news: A little bit of reflection can go a long way. Participants who were more likely to think critically, or who had more scientific knowledge, were less likely to share misinformation. And when asked directly about accuracy, most participants did reasonably well at telling true news headlines from false ones. 
Moreover, the study offers a solution for over-sharing: When participants were asked to rate the accuracy of a single non-Covid-19 story at the start of their news-viewing sessions, the quality of the Covid-19 news they shared increased significantly.
“The idea is, if you nudge them about accuracy at the outset, people are more likely to be thinking about the concept of accuracy when they later choose what to share. So then they take accuracy into account more when they make their sharing decisions,” explains Rand, who is the Erwin H. Schell Associate Professor with joint appointments at the MIT Sloan School of Management and the Department of Brain and Cognitive Sciences.
The paper, “Fighting COVID-19 misinformation on social media: Experimental evidence for a scalable accuracy nudge intervention,” appears in Psychological Science. Besides Rand, the authors are Gordon Pennycook, an assistant professor of behavioral science at the University of Regina; Jonathan McPhetres, a postdoc at MIT and the University of Regina who is starting a position in August as an assistant professor of psychology at Durham University; Yunhao Zhang, a PhD student at MIT Sloan; and Jackson G. Lu, the Mitsui Career Development Assistant Professor at MIT Sloan.
Thinking, fast and slow
To conduct the study, the researchers conducted two online experiments in March, with a total of roughly 1,700 U.S. participants between them, using the survey platform Lucid. Participants matched the nation’s distribution of age, gender, ethnicity, and geographic region.
The first experiment had 853 participants, and used 15 true and 15 false news headlines about Covid-19, in the style of Facebook posts, with a headline, photo, and initial sentence from a story. The participants were split into two groups. One group was asked if the headlines were accurate; the second group was asked if they would consider sharing the posts on platforms such as Facebook and Twitter.
The first group correctly judged the stories’ accuracy about two-thirds of the time. The second group might therefore be expected to share the stories at a similar rate. However,  the participants in the second group shared about half of the true stories, and just under half of the false stories — meaning their judgment about which stories to share was almost random in regard to accuracy.
The second study, with 856 participants, used the same group of headlines and again split the participants into two groups. The first group simply looked at the headlines and decided whether or not they would share them on social media.
But the second group of participants were asked to evaluate a non-Covid-19 headline before they made decisions about sharing the larger group of Covid-19 headlines. (Both studies were focused on headlines and the single sentence of text, since most people only read headlines on social media.) That extra step, of evaluating one non-Covid-19 headline, made a substantial difference. The “discernment” score of the second group — the gap between the number of accurate and inaccurate stories they shared — was almost three times larger than that of the first group.
The researchers evaluated additional factors that might explain tendencies in the responses of the participants. They gave all participants a six-item Cognitive Reflection Test (CRT), to evaluate their propensity to analyze information, rather than relying on gut instincts; evaluated how much scientific knowledge participants had; and looked at whether respondents were located close to Covid-19 outbreaks, among other things. They found that participants who scored higher on the CRT, and knew more about science, rated headlines more accurately and shared fewer false headlines.
Those findings suggest that the way people assess news stories has less to do with, say, preset partisan views about the news, and a bit more to do with their broader cognitive habits.
“A lot of people have a very cynical take on social media and our moment in history, that we’re post-truth and no one cares about the truth any more,” Pennycook says. “Our evidence suggests it’s not that people don’t care; it’s more that they’re distracted.”
Something systemic about social media
The study follows others Rand and Pennycook have conducted about explicitly political news, which similarly suggest that cognitive habits, more so than partisan views, influence the way people judge the accuracy of news stories and lead to the sharing of misinformation. In this study, the scholars wanted to see if readers analyzed Covid-19 stories, and health information, differently than political information. But the results were generally similar to the political-news experiments the researchers have conducted.
“Our results suggest that the life-and-death stakes of Covid-19 do not make people suddenly take accuracy into [greater] account when they’re deciding what to share,” Lu says.
Indeed, Rand suggests, the very importance of Covid-19 as a subject may interfere with readers’ ability to analyze it.
“Part of the issue with health and this pandemic is that it’s very anxiety-inducing,” Rand says. “Being emotionally aroused is another thing that makes you less likely to stop and think carefully.”
Still, the central explanation, the scholars think, is simply the structure of social media, which encourages rapid browsing of news headlines, elevates splashy news items, and rewards users who post eye-catching news, by tending to give them more followers and retweets, even if those stories happen to be untrue.
“There is just something more systemic and fundamental about the social media context that distracts people from accuracy,” Rand says. “I think part of it is that you’re getting this instantaneous social feedback all the time. Every time you post something, you immediately get to see how many people liked it. And that really focuses your attention on: How many people are going to like this? Which is different from: How true is this?”
The research was supported by the Ethics and Governance of Artificial Intelligence Initiative of the Miami Foundation; the William and Flora Hewlett Foundation; the Omidyar Network; the John Templeton Foundation; the Canadian Institute of Health Research; and the Social Sciences and Humanities Research Council of Canada.


",Our itch to share helps spread Covid-19 misinformation,2020-07-09,['Peter Dizikes'],Sloan School of Management/Social media/Covid-19/Psychology/Internet/Pandemic/Media/Public health/Technology and society,"['misinformation', 'accuracy', 'itch', 'headlines', 'media', 'rand', 'helps', 'share', 'spread', 'participants', 'study', 'covid19', 'social', 'group']","Participants who were more likely to think critically, or who had more scientific knowledge, were less likely to share misinformation.
The paper, “Fighting COVID-19 misinformation on social media: Experimental evidence for a scalable accuracy nudge intervention,” appears in Psychological Science.
The first group simply looked at the headlines and decided whether or not they would share them on social media.
(Both studies were focused on headlines and the single sentence of text, since most people only read headlines on social media.)
“There is just something more systemic and fundamental about the social media context that distracts people from accuracy,” Rand says.",Mit
105,https://news.mit.edu/2020/mit-press-and-uc-berkeley-launch-rapid-reviews-covid-19-0629,"


The MIT Press has announced the launch of Rapid Reviews: COVID-19 (RR:C19), an open access, rapid-review overlay journal that will accelerate peer review of Covid-19-related research and deliver real-time, verified scientific information that policymakers and health leaders can use.
Scientists and researchers are working overtime to understand the SARS-CoV-2 virus and are producing an unprecedented amount of preprint scholarship that is publicly available online but has not been vetted yet by peer review for accuracy. Traditional peer review can take four or more weeks to complete, but RR:C19’s editorial team, led by Editor-in-Chief Stefano M. Bertozzi, professor of health policy and management and dean emeritus of the School of Public Health at the University of California at Berkeley, will produce expert reviews in a matter of days.
Using artificial intelligence tools, a global team will identify promising scholarship in preprint repositories, commission expert peer reviews, and publish the results on an open access platform in a completely transparent process. The journal will strive for disciplinary and geographic breadth, sourcing manuscripts from all regions and across a wide variety of fields, including medicine; public health; the physical, biological, and chemical sciences; the social sciences; and the humanities. RR:C19 will also provide a new publishing option for revised papers that are positively reviewed.
Amy Brand, director of the MIT Press sees the no-cost open access model as a way to increase the impact of global research and disseminate high-quality scholarship. “Offering a peer-reviewed model on top of preprints will bring a level of diligence that clinicians, researchers, and others worldwide rely on to make sound judgments about the current crisis and its amelioration,” says Brand. “The project also aims to provide a proof-of-concept for new models of peer-review and rapid publishing for broader applications.”
Made possible by a $350,000 grant from the Patrick J. McGovern Foundation and hosted on PubPub, an open-source publishing platform from the Knowledge Futures Group for collaboratively editing and publishing journals, monographs, and other open access scholarly content, RR:C19 will limit the spread of misinformation about Covid-19, according to Bertozzi.
“There is an urgent need to validate — or debunk — the rapidly growing volume of Covid-19-related manuscripts on preprint servers,” explains Bertozzi. “I'm excited to be working with the MIT Press, the Patrick J. McGovern Foundation, and the Knowledge Futures Group to create a novel publishing model that has the potential to more efficiently translate important scientific results into action. We are also working with COVIDScholar, an initiative of UC Berkeley and Lawrence Berkeley National Lab, to create unique AI/machine learning tools to support the review of hundreds of preprints per week.”
“This project signals a breakthrough in academic publishing, bringing together urgency and scientific rigor so the world’s researchers can rapidly disseminate new discoveries that we can trust,” says Vilas Dhar, trustee of the Patrick J. McGovern Foundation. “We are confident the RR:C19 journal will quickly become an invaluable resource for researchers, public health officials, and healthcare providers on the frontline of this pandemic. We’re also excited about the potential for a long-term transformation in how we evaluate and share research across all scientific disciplines.”
On the collaboration around this new journal, Travis Rich, executive director of the Knowledge Futures Group notes, “At a moment when credibility is increasingly crucial to the well-being of society, we’re thrilled to be partnering with this innovative journal to expand the idea of reviews as first-class research objects, both on PubPub and as a model for others.
RR:C19 will publish its first reviews in July 2020 and is actively recruiting potential reviewers and contributors. To learn more about this project and its esteemed editorial board, visit rapidreviewscovid19.mitpress.mit.edu.


",The MIT Press and UC Berkeley launch Rapid Reviews: COVID-19,2020-06-29,[],Covid-19/MIT Press/Pandemic/Research/Open access/Science communications/Public health/Technology and society/Artificial intelligence/Digital humanities,"['berkeley', 'research', 'covid19', 'press', 'scientific', 'researchers', 'publishing', 'reviews', 'review', 'launch', 'rrc19', 'rapid', 'journal', 'health', 'uc', 'peer', 'mit']","The MIT Press has announced the launch of Rapid Reviews: COVID-19 (RR:C19), an open access, rapid-review overlay journal that will accelerate peer review of Covid-19-related research and deliver real-time, verified scientific information that policymakers and health leaders can use.
Using artificial intelligence tools, a global team will identify promising scholarship in preprint repositories, commission expert peer reviews, and publish the results on an open access platform in a completely transparent process.
Amy Brand, director of the MIT Press sees the no-cost open access model as a way to increase the impact of global research and disseminate high-quality scholarship.
“We are confident the RR:C19 journal will quickly become an invaluable resource for researchers, public health officials, and healthcare providers on the frontline of this pandemic.
RR:C19 will publish its first reviews in July 2020 and is actively recruiting potential reviewers and contributors.",Mit
106,https://news.mit.edu/2020/csail-robot-disinfects-greater-boston-food-bank-covid-19-0629,"


With every droplet that we can’t see, touch, or feel dispersed into the air, the threat of spreading Covid-19 persists. It’s become increasingly critical to keep these heavy droplets from lingering — especially on surfaces, which are welcoming and generous hosts. 
Thankfully, our chemical cleaning products are effective, but using them to disinfect larger settings can be expensive, dangerous, and time-consuming. Across the globe there are thousands of warehouses, grocery stores, schools, and other spaces where cleaning workers are at risk.
With that in mind, a team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), in collaboration with Ava Robotics and the Greater Boston Food Bank (GBFB), designed a new robotic system that powerfully disinfects surfaces and neutralizes aerosolized forms of the coronavirus.







Play video






The approach uses a custom UV-C light fixture designed at CSAIL that is integrated with Ava Robotics’ mobile robot base. The results were encouraging enough that researchers say that the approach could be useful for autonomous UV disinfection in other environments, such as factories, restaurants, and supermarkets. 
UV-C light has proven to be effective at killing viruses and bacteria on surfaces and aerosols, but it’s unsafe for humans to be exposed. Fortunately, Ava’s telepresence robot doesn’t require any human supervision. Instead of the telepresence top, the team subbed in a UV-C array for disinfecting surfaces. Specifically, the array uses short-wavelength ultraviolet light to kill microorganisms and disrupt their DNA in a process called ultraviolet germicidal irradiation.
The complete robot system is capable of mapping the space — in this case, GBFB’s warehouse — and navigating between waypoints and other specified areas. In testing the system, the team used a UV-C dosimeter, which confirmed that the robot was delivering the expected dosage of UV-C light predicted by the model.
“Food banks provide an essential service to our communities, so it is critical to help keep these operations running,” says Alyssa Pierson, CSAIL research scientist and technical lead of the UV-C lamp assembly. “Here, there was a unique opportunity to provide additional disinfecting power to their current workflow, and help reduce the risks of Covid-19 exposure.” 
Food banks are also facing a particular demand due to the stress of Covid-19. The United Nations projected that, because of the virus, the number of people facing severe food insecurity worldwide could double to 265 million. In the United States alone, the five-week total of job losses has risen to 26 million, potentially pushing millions more into food insecurity. 
During tests at GBFB, the robot was able to drive by the pallets and storage aisles at a speed of roughly 0.22 miles per hour. At this speed, the robot could cover a 4,000-square-foot space in GBFB’s warehouse in just half an hour. The UV-C dosage delivered during this time can neutralize approximately 90 percent of coronaviruses on surfaces. For many surfaces, this dose will be higher, resulting in more of the virus neutralized.
Typically, this method of ultraviolet germicidal irradiation is used largely in hospitals and medical settings, to sterilize patient rooms and stop the spread of microorganisms like methicillin-resistant staphylococcus aureus and Clostridium difficile, and the UV-C light also works against airborne pathogens. While it’s most effective in the direct “line of sight,” it can get to nooks and crannies as the light bounces off surfaces and onto other surfaces. 
""Our 10-year-old warehouse is a relatively new food distribution facility with AIB-certified, state-of-the-art cleanliness and food safety standards,” says Catherine D’Amato, president and CEO of the Greater Boston Food Bank. “Covid-19 is a new pathogen that GBFB, and the rest of the world, was not designed to handle. We are pleased to have this opportunity to work with MIT CSAIL and Ava Robotics to innovate and advance our sanitation techniques to defeat this menace."" 
As a first step, the team teleoperated the robot to teach it the path around the warehouse — meaning it’s equipped with autonomy to move around, without the team needing to navigate it remotely. 
It can go to defined waypoints on its map, such as going to the loading dock, then the warehouse shipping floor, then returning to base. They define those waypoints from the expert human user in teleop mode, and then can add new waypoints to the map as needed. 
Within GBFB, the team identified the warehouse shipping floor as a “high-importance area” for the robot to disinfect. Each day, workers stage aisles of products and arrange them for up to 50 pickups by partners and distribution trucks the next day. By focusing on the shipping area, it prioritizes disinfecting items leaving the warehouse to reduce Covid-19 spread out into the community.
Currently, the team is exploring how to use its onboard sensors to adapt to changes in the environment, such that in new territory, the robot would adjust its speed to ensure the recommended dosage is applied to new objects and surfaces. 
A unique challenge is that the shipping area is constantly changing, so each night, the robot encounters a slightly new environment. When the robot is deployed, it doesn’t necessarily know which of the staging aisles will be occupied, or how full each aisle might be. Therefore, the team notes that they need to teach the robot to differentiate between the occupied and unoccupied aisles, so it can change its planned path accordingly.
As far as production went, “in-house manufacturing” took on a whole new meaning for this prototype and the team. The UV-C lamps were assembled in Pierson's basement, and CSAIL PhD student Jonathan Romanishin crafted a makeshift shop in his apartment for the electronics board assembly. 
“As we drive the robot around the food bank, we are also researching new control policies that will allow the robot to adapt to changes in the environment and ensure all areas receive the proper estimated dosage,” says Pierson. “We are focused on remote operation to minimize  human supervision, and, therefore, the additional risk of spreading Covid-19, while running our system.” 
For immediate next steps, the team is focused on increasing the capabilities of the robot at GBFB, as well as eventually implementing design upgrades. Their broader intention focuses on how to make these systems more capable at adapting to our world: how a robot can dynamically change its plan based on estimated UV-C dosages, how it can work in new environments, and how to coordinate teams of UV-C robots to work together.
“We are excited to see the UV-C disinfecting robot support our community in this time of need,” says CSAIL director and project lead Daniela Rus. “The insights we received from the work at GBFB has highlighted several algorithmic challenges. We plan to tackle these in order to extend the scope of autonomous UV disinfection in complex spaces, including dorms, schools, airplanes, and grocery stores.” 
Currently, the team’s focus is on GBFB, although the algorithms and systems they are developing could be transferred to other use cases in the future, like warehouses, grocery stores, and schools. 
""MIT has been a great partner, and when they came to us, the team was eager to start the integration, which took just four weeks to get up and running,” says Ava Robotics CEO Youssef Saleh. “The opportunity for robots to solve workplace challenges is bigger than ever, and collaborating with MIT to make an impact at the food bank has been a great experience."" 
Pierson and Romanishin worked alongside Hunter Hansen (software capabilities), Bryan Teague of MIT Lincoln Laboratory (who assisted with the UV-C lamp assembly), Igor Gilitschenski and Xiao Li (assisting with future autonomy research), MIT professors Daniela Rus and Saman Amarasinghe, and Ava leads Marcio Macedo and Youssef Saleh. 
This project was supported in part by Ava Robotics, who provided their platform and team support.


















Previous item
Next item

















",CSAIL robot disinfects Greater Boston Food Bank,2020-06-29,['Rachel Gordon'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Lincoln Laboratory/Covid-19/Pandemic/Public health/Disease/Robots/Robotics/Health care/Systems design/Manufacturing/Electrical Engineering & Computer Science (eecs)/School of Engineering,"['light', 'csail', 'team', 'uvc', 'surfaces', 'greater', 'bank', 'food', 'boston', 'gbfb', 'robot', 'covid19', 'warehouse', 'disinfects']","The approach uses a custom UV-C light fixture designed at CSAIL that is integrated with Ava Robotics’ mobile robot base.
UV-C light has proven to be effective at killing viruses and bacteria on surfaces and aerosols, but it’s unsafe for humans to be exposed.
In testing the system, the team used a UV-C dosimeter, which confirmed that the robot was delivering the expected dosage of UV-C light predicted by the model.
""Our 10-year-old warehouse is a relatively new food distribution facility with AIB-certified, state-of-the-art cleanliness and food safety standards,” says Catherine D’Amato, president and CEO of the Greater Boston Food Bank.
Within GBFB, the team identified the warehouse shipping floor as a “high-importance area” for the robot to disinfect.",Mit
107,https://news.mit.edu/2020/macro-eyes-vaccine-chain-health-equity-0626,"


More children are being vaccinated around the world today than ever before, and the prevalence of many vaccine-preventable diseases has dropped over the last decade. Despite these encouraging signs, however, the availability of essential vaccines has stagnated globally in recent years, according the World Health Organization.One problem, particularly in low-resource settings, is the difficulty of predicting how many children will show up for vaccinations at each health clinic. This leads to vaccine shortages, leaving children without critical immunizations, or to surpluses that can’t be used.The startup macro-eyes is seeking to solve that problem with a vaccine forecasting tool that leverages a unique combination of real-time data sources, including new insights from front-line health workers. The company says the tool, named the Connected Health AI Network (CHAIN), was able to reduce vaccine wastage by 96 percent across three regions of Tanzania. Now it is working to scale that success across Tanzania and Mozambique.“Health care is complex, and to be invited to the table, you need to deal with missing data,” says macro-eyes Chief Executive Officer Benjamin Fels, who co-founded the company with Suvrit Sra, the Esther and Harold E. Edgerton Career Development Associate Professor at MIT. “If your system needs age, gender, and weight to make predictions, but for one population you don’t have weight or age, you can’t just say, ‘This system doesn’t work.’ Our feeling is it has to be able to work in any setting.”The company’s approach to prediction is already the basis for another product, the patient scheduling platform Sibyl, which has analyzed over 6 million hospital appointments and reduced wait times by more than 75 percent at one of the largest heart hospitals in the U.S. Sibyl’s predictions work as part of CHAIN’s broader forecasts.Both products represent steps toward macro-eyes’ larger goal of transforming health care through artificial intelligence. And by getting their solutions to work in the regions with the least amount of data, they’re also advancing the field of AI.“The state of the art in machine learning will result from confronting fundamental challenges in the most difficult environments in the world,” Fels says. “Engage where the problems are hardest, and AI too will benefit: [It will become] smarter, faster, cheaper, and more resilient.”Defining an approachSra and Fels first met about 10 years ago when Fels was working as an algorithmic trader for a hedge fund and Sra was a visiting faculty member at the University of California at Berkeley. The pair’s experience crunching numbers in different industries alerted them to a shortcoming in health care.
“A question that became an obsession to me was, ‘Why were financial markets almost entirely determined by machines — by algorithms — and health care the world over is probably the least algorithmic part of anybody’s life?’” Fels recalls. “Why is health care not more data-driven?”
Around 2013, the co-founders began building machine-learning algorithms that measured similarities between patients to better inform treatment plans at Stanford School of Medicine and another large academic medical center in New York. It was during that early work that the founders laid the foundation of the company’s approach.“There are themes we established at Stanford that remain today,” Fels says. “One is [building systems with] humans in the loop: We’re not just learning from the data, we’re also learning from the experts. The other is multidimensionality. We’re not just looking at one type of data; we’re looking at 10 or 15 types, [including] images, time series, information about medication, dosage, financial information, how much it costs the patient or hospital.”Around the time the founders began working with Stanford, Sra joined MIT’s Laboratory for Information and Decision Systems (LIDS) as a principal research scientist. He would go on to become a faculty member in the Department of Electrical Engineering and Computer Science and MIT’s Institute for Data, Systems, and Society (IDSS). The mission of IDSS, to advance fields including data science and to use those advances to improve society, aligned well with Sra’s mission at macro-eyes.“Because of that focus [on impact] within IDSS, I find it my focus to try to do AI for social good,’ Sra says. “The true judgment of success is how many people did we help? How could we improve access to care for people, wherever they may be?”
In 2017, macro-eyes received a small grant from the Bill and Melinda Gates Foundation to explore the possibility of using data from front-line health workers to build a predictive supply chain for vaccines. It was the beginning of a relationship with the Gates Foundation that has steadily expanded as the company has reached new milestones, from building accurate vaccine utilization models in Tanzania and Mozambique to integrating with supply chains to make vaccine supplies more proactive. To help with the latter mission, Prashant Yadav recently joined the board of directors; Yadav worked as a professor of supply chain management with the MIT-Zaragoza International Logistics Program for seven years and is now a senior fellow at the Center for Global Development, a nonprofit thinktank.
In conjunction with their work on CHAIN, the company has deployed another product, Sibyl, which uses machine learning to determine when patients are most likely to show up for appointments, to help front-desk workers at health clinics build schedules. Fels says the system has allowed hospitals to improve the efficiency of their operations so much they’ve reduced the average time patients wait to see a doctor from 55 days to 13 days.
As a part of CHAIN, Sibyl similarly uses a range of data points to optimize schedules, allowing it to accurately predict behavior in environments where other machine learning models might struggle.
The founders are also exploring ways to apply that approach to help direct Covid-19 patients to health clinics with sufficient capacity. That work is being developed with Sierra Leone Chief Innovation Officer David Sengeh SM ’12 PhD ’16.
Pushing frontiers
Building solutions for some of the most underdeveloped health care systems in the world might seem like a difficult way for a young company to establish itself, but the approach is an extension of macro-eyes’ founding mission of building health care solutions that can benefit people around the world equally.“As an organization, we can never assume data will be waiting for us,” Fels says. “We’ve learned that we need to think strategically and be thoughtful about how to access or generate the data we need to fulfill our mandate: Make the delivery of health care predictive, everywhere.”The approach is also a good way to explore innovations in mathematical fields the founders have spent their careers working in.“Necessity is absolutely the mother of invention,” Sra says. “This is innovation driven by need.”And going forward, the company’s work in difficult environments should only make scaling easier.“We think every day about how to make our technology more rapidly deployable, more generalizable, more highly scalable,” Sra says. “How do we get to the immense power of bringing true machine learning to the world’s most important problems without first spending decades and billions of dollars in building digital infrastructure? How do we leap into the future?”


",Improving global health equity by helping clinics do more with less,2020-06-26,['Zach Winn'],Artificial intelligence/Public health/Computer science and technology/Health care/Medicine/Data/Innovation and Entrepreneurship (I&E)/Machine learning/Africa/Vaccines/Laboratory for Information and Decision Systems/IDSS/MIT Schwarzman College of Computing/Startups,"['helping', 'data', 'vaccine', 'global', 'improving', 'macroeyes', 'learning', 'clinics', 'sra', 'work', 'care', 'world', 'equity', 'fels', 'health']","Despite these encouraging signs, however, the availability of essential vaccines has stagnated globally in recent years, according the World Health Organization.
Both products represent steps toward macro-eyes’ larger goal of transforming health care through artificial intelligence.
“The state of the art in machine learning will result from confronting fundamental challenges in the most difficult environments in the world,” Fels says.
The pair’s experience crunching numbers in different industries alerted them to a shortcoming in health care.
The founders are also exploring ways to apply that approach to help direct Covid-19 patients to health clinics with sufficient capacity.",Mit
108,https://news.mit.edu/2020/michael-hawley-former-professor-media-arts-sciences-dies-0625,"


Michael Hawley, a former MIT professor who was recognized globally as a modern-day Renaissance man, died on Wednesday, June 24, at his home in Cambridge, Massachusetts, after battling a long illness. He was 58.Hawley will be remembered for his extraordinary breadth of interests and talents, which spanned the fields of human-computer interfaces and sensing, musical performance and audio signal processing, digital cinema and libraries, documentary photography, exploration, and entrepreneurship. He will also be remembered as a deeply dedicated family man. Parenthood came late to Hawley and his wife, Nina You, who recently wrote: “We spent 15 years trying to have our son, Tycho. Mike’s cancer was discovered on his first Father’s Day last year; Tycho is now 18 months old and was the joy of Mike’s life. He put being a father at the very top of all his contributions to the world.”Hawley participated in some of the great digital breakthroughs of the last 40 years, from writing UNIX code at Bell Labs as a teenager, to his work pioneering digital cinema and sound technology at LucasFilm, to his innovation of large-imprint digital photo systems, which led to the 2003 publication of ""Bhutan: A Visual Odyssey Across the Last Himalayan Kingdom,"" measuring 5’ x 7’ and weighing over 150 pounds, giving it the distinction of being documented as the world’s largest published book in the ""Guinness Book of World Records.""Long before arriving at MIT in 1986 as a graduate student in the Department of Electrical Engineering as well as the newly established Program in Media Arts and Sciences, Hawley had already earned an impressive resume. After receiving undergraduate degrees in music and computer science at Yale University in 1983, his research pursuits took him first to Pierre Boulez’s IRCAM in Paris, where he developed one of the first graphic systems for displaying and editing musical scores, and then to Lucasfilm in San Rafael, California, where he helped to develop the SoundDroid, among other technologies. He then helped Steve Jobs start NeXT, working with Jobs to develop the first generation of digital books, including the writings of Charles Darwin and William Shakespeare, as well as the first digital dictionary (Merriam-Webster). “Central to this digital library was one of the very first search algorithms, conceived by Mike and the team to create a bridge between human curiosity and technology,” says Kate Smith, member of the original digital library team at NeXT.In reviewing Hawley’s application for graduate work at MIT before actually meeting him, Professor Emeritus Nicholas Negroponte, co-founder and former director of the Media Lab, recalls his competing to woo him away from Steve Jobs. “As I recollect,” says Negroponte, “my bait was Marvin Minsky.” 
After completing his doctoral dissertation, “Structure out of Sound,” under the direction of Minsky (for which he created technologies to transform historic early-20th century piano rolls into digital instructions for state-of-the-art robotic pianos), Hawley accepted a faculty appointment in MIT’s Department of Electrical Engineering and Computer Science, where he held the newly endowed Licklider Career Development Professorship, named for J.C.R. Licklider, a pioneer in the field of human-machine interfaces. In 1995, Hawley was named to the Alexander W. Dreyfoos, Jr. '54 Professorship in Media Arts and Sciences as a faculty member in the Program in Media Arts and Sciences.Like Minsky, Hawley shared a passion for music, and was a gifted pianist and organist. Tod Machover, composer and Muriel R. Cooper Professor of Music and Media at the MIT Media Lab, says that “Mike lived and breathed music. He had an encyclopedic knowledge of unusual repertoire from Bach to Bernstein and Bolcom (including little-known Busoni transcriptions of Beethoven symphonies), sported a breathtaking facility at the keyboard which allowed him to sight read just about anything, and communicated through his piano with the same directness, insight, and warmth that made him one of the world’s great public speakers.”
This natural performance talent won Hawley first place (tying with Victoria Bragin) at the Van Cliburn International Piano Competition for Outstanding Amateurs in 2002. He performed numerous solo recitals, chamber concerts, and as a soloist with major orchestras, and had the distinction of accompanying world-renowned cellist Yo-Yo Ma at the wedding of Bill Nye (“The Science Guy”). He was also prominently featured in the 2010 documentary “Bach & Friends.”
At the Media Lab, Hawley headed the Personal Information Architecture research group and was a co-founder of the Toys of Tomorrow and Things That Think consortia, the latter of which provided an early insight into the myriad of ways that digital technologies would soon be seamlessly integrated with the physical atoms of our everyday world. In 1995, Hawley predicted the seismic societal shift that loomed. “This is an absolutely pivotal era,” he said, “We’re moving from a world where none of our everyday things communicate, to one where all of them will.” He imagined a not-too-distant future when we could send bitstreams into almost anything — even into the body.By 1997 Hawley and his students did just that. That year, Hawley (with two of his students) ran his first Boston Marathon, wired “inside and out” with their newly developed microelectronic monitoring devices, to transmit data on their vital signs and position during the race. As with many new experiments, there were some glitches. The data transmission did not work in real-time, and the weight of the devices also took a toll: only Hawley managed to carry the gear all the way to the finish line. But this so-called “Black Box” project was pioneering in its ability to monitor the human body under extreme physical conditions, as well as to provide situational information based on context and location. Building on the Boston Marathon breakthrough, Hawley and his group created one of the first consumer heart-rate monitors, in the form of a $500,000 red-pulsating diamond brooch developed in collaboration with Harry Winston Jewelers.Hawley’s experimentation took him to all corners of the world, most notably leading one of the first major scientific expeditions on Mount Everest, which involved the group developing new systems for monitoring geological and ecological phenomena. Other expeditionary research included dog sledding in Norway, plant biology in Hawaii, and reef ecology in Indonesia, all to study new technological ways to sense the environment, and to better understand human performance in — and effect upon — the world around us.Robert Poor, Hawley’s first PhD student at the Media Lab and a participant on the Everest expedition, says, “Mike crossed and often broke physical and societal rules, playfully disrupting normal life and re-establishing it on a new basis. In doing so, he cajoled many of us students to follow his example: to think bigger and to have the courage to push our own boundaries.”The eclectic nature of Hawley’s interests always went well beyond his academic pursuits. He was a one-time Duncan yo-yo champion, luger, and member of the U.S. Bobsled Federation. At the Media Lab, he also developed software for a computerized sewing machine that allowed him to translate pictures into thread, and then used this technology to make sweatshirts for the Media Lab’s ice hockey team and to wire antennae for the lab’s cyborgs. In 2002, Hawley became director of special projects at the Media Lab, and in 2008 left MIT to become Director of the EG (Entertainment Gathering) conference. Held annually in Monterey, California, EG brings together a broad range of visionaries and inventors, thinkers and makers from essentially every creative field of endeavor, retaining the excitement and profound interdisciplinarity of the Media Lab.   “One of the qualities I treasured most in Mike was the extraordinary breadth of his talents and intellect, which were always on full display at his EG conference,” says Laurene Powell Jobs, founder of the Emerson Collective. “He had this incredible ability to look at the world in all of its complexity, all of its messiness, and make the connections — of people, of ideas — that so many of us might have otherwise missed. He was also fiercely passionate about our democracy, always urging action, always full of ideas for what needed to be done. Mike enriched the lives of everyone he touched.”Architect Moshe Safdie, who developed a close friendship with Hawley, remembers him as deeply humanist and overflowing with compassion. “Michael was always the great connector, bringing us knowledge, understanding and pleasure,” Safdie says.Hawley was honored as the first recipient of the Jack Kilby International Award for innovation in science in 1990. He was a fellow and trustee of Jonathan Edwards College at Yale University and served on the boards of the Rutgers Jazz Institute, the Vanguard Group, and several prominent companies, including Kodak.Artificial intelligence pioneer Danny Hillis remembers Hawley as “a true polymath who constantly sought out beautiful ideas in art, music, culture and science, and shared them joyfully with his friends.” Echoing this sentiment, Robert Millard, chairman of the MIT Corporation, remembers Hawley “as a true artist — not just a craftsman — in everything he touched. He achieved beauty in life by rigorously seeking it, living it, absorbing it, improving it, transforming it. We will miss him, but we will always have him.”In late April, Millard and 16 other colleagues and friends participated in an online festschrift organized by the Media Lab and hosted by NPR’s Peter Sagal to pay tribute to Hawley. In addition to his wife Nina and son Tycho, Hawley is survived by his father, George Hawley, and two brothers, Patrick and Stephen. Michael Hawley’s ideas and communities will be celebrated in various forms and at various times, to be announced.


","Michael Hawley, former professor of media arts and sciences, dies at 58",2020-06-25,"['Ellen Hoffman', 'Tod Machover']",Obituaries/Media Lab/internet of things/Faculty/Electrical Engineering & Computer Science (eecs)/School of Architecture and Planning/School of Engineering/Alumni/ae/Music,"['digital', 'hawley', 'mike', 'dies', 'media', 'music', 'world', 'professor', 'lab', '58', 'sciences', 'arts', 'developed', 'science', 'mit', 'michael']","In 1995, Hawley was named to the Alexander W. Dreyfoos, Jr. '54 Professorship in Media Arts and Sciences as a faculty member in the Program in Media Arts and Sciences.
Tod Machover, composer and Muriel R. Cooper Professor of Music and Media at the MIT Media Lab, says that “Mike lived and breathed music.
In 2002, Hawley became director of special projects at the Media Lab, and in 2008 left MIT to become Director of the EG (Entertainment Gathering) conference.
Held annually in Monterey, California, EG brings together a broad range of visionaries and inventors, thinkers and makers from essentially every creative field of endeavor, retaining the excitement and profound interdisciplinarity of the Media Lab.
In addition to his wife Nina and son Tycho, Hawley is survived by his father, George Hawley, and two brothers, Patrick and Stephen.",Mit
109,https://news.mit.edu/2020/music-gesture-artificial-intelligence-identifies-melody-by-musician-body-language-0625,"


We listen to music with our ears, but also our eyes, watching with appreciation as the pianist’s fingers fly over the keys and the violinist’s bow rocks across the ridge of strings. When the ear fails to tell two instruments apart, the eye often pitches in by matching each musician’s movements to the beat of each part. 
A new artificial intelligence tool developed by the MIT-IBM Watson AI Lab leverages the virtual eyes and ears of a computer to separate similar sounds that are tricky even for humans to differentiate. The tool improves on earlier iterations by matching the movements of individual musicians, via their skeletal keypoints, to the tempo of individual parts, allowing listeners to isolate a single flute or violin among multiple flutes or violins. 
Potential applications for the work range from sound mixing, and turning up the volume of an instrument in a recording, to reducing the confusion that leads people to talk over one another on a video-conference calls. The work will be presented at the virtual Computer Vision Pattern Recognition conference this month.
“Body keypoints provide powerful structural information,” says the study’s lead author, Chuang Gan, an IBM researcher at the lab. “We use that here to improve the AI’s ability to listen and separate sound.” 
In this project, and in others like it, the researchers have capitalized on synchronized audio-video tracks to recreate the way that humans learn. An AI system that learns through multiple sense modalities may be able to learn faster, with fewer data, and without humans having to add pesky labels to each real-world representation. “We learn from all of our senses,” says Antonio Torralba, an MIT professor and co-senior author of the study. “Multi-sensory processing is the precursor to embodied intelligence and AI systems that can perform more complicated tasks.”
The current tool, which uses body gestures to separate sounds, builds on earlier work that harnessed motion cues in sequences of images. Its earliest incarnation, PixelPlayer, let you click on an instrument in a concert video to make it louder or softer. An update to PixelPlayer allowed you to distinguish between two violins in a duet by matching each musician’s movements with the tempo of their part. This newest version adds keypoint data, favored by sports analysts to track athlete performance, to extract finer grained motion data to tell nearly identical sounds apart.
The work highlights the importance of visual cues in training computers to have a better ear, and using sound cues to give them sharper eyes. Just as the current study uses musician pose information to isolate similar-sounding instruments, previous work has leveraged sounds to isolate similar-looking animals and objects. 
Torralba and his colleagues have shown that deep learning models trained on paired audio-video data can learn to recognize natural sounds like birds singing or waves crashing. They can also pinpoint the geographic coordinates of a moving car from the sound of its engine and tires rolling toward, or away from, a microphone. 
The latter study suggests that sound-tracking tools might be a useful addition in self-driving cars, complementing their cameras in poor driving conditions. “Sound trackers could be especially helpful at night, or in bad weather, by helping to flag cars that might otherwise be missed,” says Hang Zhao, PhD ’19, who contributed to both the motion and sound-tracking studies.
Other authors of the CVPR music gesture study are Deng Huang and Joshua Tenenbaum at MIT.


",Identifying a melody by studying a musician’s body language,2020-06-25,['Kim Martineau'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical engineering and computer science (EECS)/Brain and cognitive sciences/School of Engineering/School of Science/Research/Algorithms/Machine learning/Computer vision/Artificial intelligence/Music/Computer science and technology/Quest for Intelligence/MIT Schwarzman College of Computing,"['studying', 'data', 'movements', 'language', 'body', 'tool', 'musicians', 'work', 'sound', 'identifying', 'melody', 'separate', 'learn', 'study', 'sounds']","When the ear fails to tell two instruments apart, the eye often pitches in by matching each musician’s movements to the beat of each part.
“Body keypoints provide powerful structural information,” says the study’s lead author, Chuang Gan, an IBM researcher at the lab.
“We learn from all of our senses,” says Antonio Torralba, an MIT professor and co-senior author of the study.
An update to PixelPlayer allowed you to distinguish between two violins in a duet by matching each musician’s movements with the tempo of their part.
The latter study suggests that sound-tracking tools might be a useful addition in self-driving cars, complementing their cameras in poor driving conditions.",Mit
110,https://news.mit.edu/2020/treasure-map-brain-region-emphasizes-reward-location-0623,"


We are free to wander, but usually when we go somewhere it’s for a reason. In a new study, researchers at the Picower Institute for Learning and Memory at MIT show that as we pursue life’s prizes, a region of the brain tracks our location with an especially strong predilection for the location of the reward. This pragmatic bias of the lateral septum (LS) suggests it’s a linchpin in formulating goal-directed behavior.
“It appears that the lateral septum is, in a sense, ‘prioritizing’ reward-related spatial information,” says Hannah Wirtshafter, lead author of the study in eLife and a former graduate student in the MIT lab of senior author Matthew Wilson, the Sherman Fairchild Professor of Neurobiology at MIT. Wirtshafter is now a postdoc at Northwestern University.
Last year, Wirtshafter and Wilson, who has appointments in the Department of Biology and the Department of Brain and Cognitive Sciences, analyzed measurements of the electrical activity of hundreds of neurons in the LS and the hippocampus, a region known for encoding many forms of memory including spatial maps, as rats navigated a maze toward a reward. In Current Biology they reported that the LS directly encodes information about the speed and acceleration of the rats as they navigated through the environment.
The new study continued this analysis, finding that while the LS dedicates a much smaller proportion of its cells to encoding location than does the hippocampus, a much larger proportion of those cells respond when the rat is proximate to where the reward lies. Moreover, as rats scurried toward the reward point and back again within the H-shaped maze, the pace of their neural activity peaked closest to those reward locations, skewing the curve of their activity in association with where they could find a chocolate treat. Finally, they found that neural activity between the hippocampus and the LS was most highly correlated among cells that represented reward locations.
“Understanding how reward information is linked to memory and space through the hippocampus is crucial for our understanding of how we learn from experience, and this finding points to the role the lateral septum may play in that process,” Wilson says.
Specifically, Wilson and Wirtshafter interpret the results of the two studies to suggest that the LS plays a key role in helping to filter and convert raw information about location, speed, and acceleration coming in from regions such as the hippocampus into more reward-specific output for regions known to guide goal-directed behavior, such as the ventral tegmental area. In the paper, they discuss ways in which the hippocampus and the LS might be wired together to do so. They theorize that the LS might dedicate neurons to receiving reward-related location information from the hippocampus and might blend non-reward location information within neurons also tasked for processing other information, such as motion.
“This is supported by our previous work that shows somewhat overlapping populations of place-encoding and movement-encoding LS cells,” Wirtshafter says.
Though it’s easy for most of us to take the brain’s ability to facilitate navigation for granted, scientists study it for several reasons, Wirtshafter says.
“Elucidating brain mechanisms and circuits involved in navigation, memory, and planning may identify processes underlying impaired cognitive function in motor and memory diseases,” she says. “Additionally, knowledge of the principles of goal-directed behavior can also be used to model context-dependent brain behavior in machine models to further contribute to artificial intelligence development.”
The National Defense Science and Engineering Graduate Fellowship Program and the JPB Foundation provided funding for the study.


","Like a treasure map, brain region emphasizes reward location",2020-06-23,['David Orenstein'],Picower Institute/Biology/Brain and cognitive sciences/School of Science/Neuroscience/Memory/Research/Learning,"['emphasizes', 'wirtshafter', 'memory', 'wilson', 'cells', 'treasure', 'reward', 'information', 'location', 'hippocampus', 'study', 'map', 'ls', 'region', 'brain']","In a new study, researchers at the Picower Institute for Learning and Memory at MIT show that as we pursue life’s prizes, a region of the brain tracks our location with an especially strong predilection for the location of the reward.
Finally, they found that neural activity between the hippocampus and the LS was most highly correlated among cells that represented reward locations.
In the paper, they discuss ways in which the hippocampus and the LS might be wired together to do so.
They theorize that the LS might dedicate neurons to receiving reward-related location information from the hippocampus and might blend non-reward location information within neurons also tasked for processing other information, such as motion.
“This is supported by our previous work that shows somewhat overlapping populations of place-encoding and movement-encoding LS cells,” Wirtshafter says.",Mit
111,https://news.mit.edu/2020/cynthia-breazeal-named-media-lab-associate-director-0619,"


Cynthia Breazeal has been promoted to full professor and named associate director of the Media Lab, joining the two other associate directors: Hiroshi Ishii and Andrew Lippman. Both appointments are effective July 1.In her new associate director role, Breazeal will work with lab faculty and researchers to develop new strategic research initiatives. She will also play a key role in exploring new funding mechanisms to support broad Media Lab needs, including multi-faculty research efforts, collaborations with other labs and departments across the MIT campus, and experimental executive education opportunities. “I am excited that Cynthia will be applying her tremendous energy, creativity, and intellect to rally the community in defining new opportunities for funding and research directions,” says Pattie Maes, chair of the lab’s executive committee. “As a first step, she has already organized a series of informal charrettes, where all members of the lab community can participate in brainstorming collaborations that range from tele-creativity, to resilient communities, to sustainability and climate change.” Most recently, Breazeal has led an MIT collaboration between the Media Lab, MIT Stephen A. Schwarzman College of Computing, and MIT Open Learning to develop aieducation.mit.edu, an online learning site for grades K-12, which shares a variety of online activities for students to learn about artificial intelligence, with a focus on how to design and use AI responsibly. While assuming these new responsibilities, Breazeal will continue to head the lab’s Personal Robots research group, which focuses on developing personal social robots and their potential for meaningful impact on everyday life — from educational aids for children, to pediatric use in hospitals, to at-home assistants for the elderly.Breazeal is globally recognized as a pioneer in human-robot interaction. Her book, “Designing Sociable Robots” (MIT Press, 2002), is considered pivotal in launching the field. In 2019 she was named an AAAI fellow. Previously, she received numerous awards including the National Academy of Engineering’s Gilbreth Lecture Award and MIT Technology Review's TR100/35 Award. Her robot Jibo was on the cover of TIME magazine in its Best Inventions list of 2017, and in 2003 she was a finalist for the National Design Awards in Communications Design. In 2014, Fortune magazine recognized her as one of the Most Promising Women Entrepreneurs. The following year, she was named one of Entrepreneur magazine’s Women to Watch.
Breazeal earned a BS in electrical and computer engineering from the University of California at Santa Barbara, and MS and ScD degrees from MIT in electrical engineering and computer science.








",Cynthia Breazeal named Media Lab associate director,2020-06-19,[],Media Lab/MIT Schwarzman College of Computing/Personal robotics/School of Architecture and Planning/Artificial intelligence/Faculty/Administration/K-12 education/STEM education,"['named', 'cynthia', 'research', 'director', 'labs', 'media', 'robots', 'lab', 'breazeal', 'design', 'associate', 'mit']","Cynthia Breazeal has been promoted to full professor and named associate director of the Media Lab, joining the two other associate directors: Hiroshi Ishii and Andrew Lippman.
In her new associate director role, Breazeal will work with lab faculty and researchers to develop new strategic research initiatives.
She will also play a key role in exploring new funding mechanisms to support broad Media Lab needs, including multi-faculty research efforts, collaborations with other labs and departments across the MIT campus, and experimental executive education opportunities.
Her book, “Designing Sociable Robots” (MIT Press, 2002), is considered pivotal in launching the field.
The following year, she was named one of Entrepreneur magazine’s Women to Watch.",Mit
112,https://news.mit.edu/2020/ionic-device-brain-synapse-0619,"


Teams around the world are building ever more sophisticated artificial intelligence systems of a type called neural networks, designed in some ways to mimic the wiring of the brain, for carrying out tasks such as computer vision and natural language processing.Using state-of-the-art semiconductor circuits to simulate neural networks requires large amounts of memory and high power consumption. Now, an MIT team has made strides toward an alternative system, which uses physical, analog devices that can much more efficiently mimic brain processes.The findings are described in the journal Nature Communications, in a paper by MIT professors Bilge Yildiz, Ju Li, and Jesús del Alamo, and nine others at MIT and Brookhaven National Laboratory. The first author of the paper is Xiahui Yao, a former MIT postdoc now working on energy storage at GRU Energy Lab.Neural networks attempt to simulate the way learning takes place in the brain, which is based on the gradual strengthening or weakening of the connections between neurons, known as synapses. The core component of this physical neural network is the resistive switch, whose electronic conductance can be controlled electrically. This control, or modulation, emulates the strengthening and weakening of synapses in the brain.In neural networks using conventional silicon microchip technology, the simulation of these synapses is a very energy-intensive process. To improve efficiency and enable more ambitious neural network goals, researchers in recent years have been exploring a number of physical devices that could more directly mimic the way synapses gradually strengthen and weaken during learning and forgetting.Most candidate analog resistive devices so far for such simulated synapses have either been very inefficient, in terms of energy use, or performed inconsistently from one device to another or one cycle to the next. The new system, the researchers say, overcomes both of these challenges. “We’re addressing not only the energy challenge, but also the repeatability-related challenge that is pervasive in some of the existing concepts out there,” says Yildiz, who is a professor of nuclear science and engineering and of materials science and engineering.“I think the bottleneck today for building [neural network] applications is energy efficiency. It just takes too much energy to train these systems, particularly for applications on the edge, like autonomous cars,” says del Alamo, who is the Donner Professor in the Department of Electrical Engineering and Computer Science. Many such demanding applications are simply not feasible with today’s technology, he adds.The resistive switch in this work is an electrochemical device, which is made of tungsten trioxide (WO3) and works in a way similar to the charging and discharging of batteries. Ions, in this case protons, can migrate into or out of the crystalline lattice of the material,  explains Yildiz, depending on the polarity and strength of an applied voltage. These changes remain in place until altered by a reverse applied voltage — just as the strengthening or weakening of synapses does.“The mechanism is similar to the doping of semiconductors,” says Li, who is also a professor of nuclear science and engineering and of materials science and engineering. In that process, the conductivity of silicon can be changed by many orders of magnitude by introducing foreign ions into the silicon lattice. “Traditionally those ions were implanted at the factory,” he says, but with the new device, the ions are pumped in and out of the lattice in a dynamic, ongoing process. The researchers can control how much of the “dopant” ions go in or out by controlling the voltage, and “we’ve demonstrated a very good repeatability and energy efficiency,” he says.Yildiz adds that this process is “very similar to how the synapses of the biological brain work. There, we’re not working with protons, but with other ions such as calcium, potassium, magnesium, etc., and by moving those ions you actually change the resistance of the synapses, and that is an element of learning.” The process taking place in the tungsten trioxide in their device is similar to the resistance modulation taking place in biological synapses, she says.“What we have demonstrated here,” Yildiz says, “even though it’s not an optimized device, gets to the order of energy consumption per unit area per unit change in conductance that’s close to that in the brain.” Trying to accomplish the same task with conventional CMOS type semiconductors would take a million times more energy, she says.The materials used in the demonstration of the new device were chosen for their compatibility with present semiconductor manufacturing systems, according to Li. But they include a polymer material that limits the device’s tolerance for heat, so the team is still searching for other variations of the device’s proton-conducting membrane and better ways of encapsulating its hydrogen source for long-term operations.“There’s a lot of fundamental research to be done at the materials level for this device,” Yildiz says. Ongoing research will include “work on how to integrate these devices with existing CMOS transistors” adds del Alamo. “All that takes time,” he says, “and it presents tremendous opportunities for innovation, great opportunities for our students to launch their careers.”The research, which included researchers at Brookhaven National Laboratory as well as MIT, was supported by the Skoltech Program, the MIT Quest for Intelligence, and the U.S. National Science Foundation.


",Engineers design a device that operates like a brain synapse,2020-06-19,['David L. Chandler'],Nuclear science and engineering/DMSE/Materials Science and Engineering/Research/Nanoscience and nanotechnology/Electrical Engineering & Computer Science (eecs)/Quest for Intelligence/School of Engineering/National Science Foundation (NSF),"['process', 'operates', 'ions', 'energy', 'devices', 'device', 'synapse', 'mit', 'engineers', 'yildiz', 'synapses', 'design', 'science', 'brain', 'neural']","Now, an MIT team has made strides toward an alternative system, which uses physical, analog devices that can much more efficiently mimic brain processes.
The first author of the paper is Xiahui Yao, a former MIT postdoc now working on energy storage at GRU Energy Lab.
In neural networks using conventional silicon microchip technology, the simulation of these synapses is a very energy-intensive process.
Yildiz adds that this process is “very similar to how the synapses of the biological brain work.
“There’s a lot of fundamental research to be done at the materials level for this device,” Yildiz says.",Mit
113,https://news.mit.edu/2020/closedloop-ai-predictive-health-care-0619,"


An important aspect of treating patients with conditions like diabetes and heart disease is helping them stay healthy outside of the hospital — before they to return to the doctor’s office with further complications.But reaching the most vulnerable patients at the right time often has more to do with probabilities than clinical assessments. Artificial intelligence (AI) has the potential to help clinicians tackle these types of problems, by analyzing large datasets to identify the patients that would benefit most from preventative measures. However, leveraging AI has often required health care organizations to hire their own data scientists or settle for one-size-fits-all solutions that aren’t optimized for their patients.Now the startup ClosedLoop.ai is helping health care organizations tap into the power of AI with a flexible analytics solution that lets hospitals quickly plug their data into machine learning models and get actionable results.The platform is being used to help hospitals determine which patients are most likely to miss appointments, acquire infections like sepsis, benefit from periodic check ups, and more. Health insurers, in turn, are using ClosedLoop to make population-level predictions around things like patient readmissions and the onset or progression of chronic diseases.“We built a health care data science platform that can take in whatever data an organization has, quickly build models that are specific to [their patients], and deploy those models,” says ClosedLoop co-founder and Chief Technology Officer Dave DeCaprio ’94. “Being able to take somebody’s data the way it lives in their system and convert that into a model that can be readily used is still a problem that requires a lot of [health care] domain knowledge, and that’s a lot of what we bring to the table.”In light of the Covid-19 pandemic, ClosedLoop has also created a model that helps organizations identify the most vulnerable people in their region and prepare for patient surges. The open source tool, called the C-19 Index, has been used to connect high-risk patients with local resources and helped health care systems create risk scores for tens of millions of people overall.The index is just the latest way that ClosedLoop is accelerating the health care industry’s adoption of AI to improve patient health, a goal DeCaprio has worked toward for the better part of his career.Designing a strategyAfter working as a software engineer for several private companies through the internet boom of the early 2000s, DeCaprio was looking to make a career change when he came across a project focused on genome annotation at the Broad Institute of MIT and Harvard.The project was DeCaprio’s first professional exposure to the power of artificial intelligence. It blossomed into a six year stint at the Broad, after which he continued exploring the intersection of big data and health care.“After a year in health care, I realized it was going to be really hard to do anything else,” DeCaprio says. “I’m not going to be able to get excited about selling ads on the internet or anything like that. Once you start dealing with human health, that other stuff just feels insignificant.”In the course of his work, DeCaprio began noticing problems with the ways machine learning and other statistical techniques were making their way into health care, notably in the fact that predictive models were being applied without regard for hospitals’ patient populations.“Someone would say, ‘I know how to predict diabetes’ or ‘I know how to predict readmissions,’ and they’d sell a model,” DeCaprio says. “I knew that wasn’t going to work, because the reason readmissions happen in a low-income population of New York City is very different from the reason readmissions happen in a retirement community in Florida. The important thing wasn’t to build one magic model but to build a system that can quickly take somebody’s data and train a model that’s specific for their problems.”With that approach in mind, DeCaprio joined forces with former co-worker and serial entrepreneur Andrew Eye, and started ClosedLoop in 2017. The startup’s first project involved creating models that predicted patient health outcomes for the Medical Home Network (MHN), a not-for-profit hospital collaboration focused on improving care for Medicaid recipients in Chicago.As the founders created their modeling platform, they had to address many of the most common obstacles that have slowed health care’s adoption of AI solutions.Often the first problems startups run into is making their algorithms work with each health care system’s data. Hospitals vary in the type of data they collect on patients and the way they store that information in their system. Hospitals even store the same types of data in vastly different ways.DeCaprio credits his team’s knowledge of the health care space with helping them craft a solution that allows customers to upload raw data sets into ClosedLoop’s platform and create things like patient risk scores with a few clicks.Another limitation of AI in health care has been the difficulty of understanding how models get to results. With ClosedLoop’s models, users can see the biggest factors contributing to each prediction, giving them more confidence in each output.Overall, to become ingrained in customer’s operations, the founders knew their analytics platform needed to give simple, actionable insights. That has translated into a system that generates lists, risk scores, and rankings that care managers can use when deciding which interventions are most urgent for which patients.“When someone walks into the hospital, it’s already too late [to avoid costly treatments] in many cases,” DeCaprio says. “Most of your best opportunities to lower the cost of care come by keeping them out of the hospital in the first place.”Customers like health insurers also use ClosedLoop’s platform to predict broader trends in disease risk, emergency room over-utilization, and fraud.Stepping up for Covid-19In March, ClosedLoop began exploring ways its platform could help hospitals prepare for and respond to Covid-19. The efforts culminated in a company hackathon over the weekend of March 16. By Monday, ClosedLoop had an open source model on GitHub that assigned Covid-19 risk scores to Medicare patients. By that Friday, it had been used to make predictions on more than 2 million patients.Today, the model works with all patients, not just those on Medicare, and it has been used to assess the vulnerability of communities around the country. Care organizations have used the model to project patient surges and help individuals at the highest risk understand what they can do to prevent infection.“Some of it is just reaching out to people who are socially isolated to see if there’s something they can do,” DeCaprio says. “Someone who is 85 years old and shut in may not know there’s a community based organization that will deliver them groceries.”For DeCaprio, bringing the predictive power of AI to health care has been a rewarding, if humbling, experience.“The magnitude of the problems are so large that no matter what impact you have, you don’t feel like you’ve moved the needle enough,” he says. “At the same time, every time an organization says, ‘This is the primary tool our care managers have been using to figure out who to reach out to,’ it feels great.”


",Bringing the predictive power of artificial intelligence to health care,2020-06-19,['Zach Winn'],Artificial intelligence/Public health/Covid-19/Computer science and technology/Health sciences and technology/Health care/Medicine/Data/Alumni/ae/Broad Institute/Innovation and Entrepreneurship (I&E)/Startups,"['bringing', 'data', 'model', 'models', 'closedloop', 'care', 'decaprio', 'patients', 'patient', 'predictive', 'power', 'artificial', 'intelligence', 'health', 'platform']","It blossomed into a six year stint at the Broad, after which he continued exploring the intersection of big data and health care.
“After a year in health care, I realized it was going to be really hard to do anything else,” DeCaprio says.
Often the first problems startups run into is making their algorithms work with each health care system’s data.
Another limitation of AI in health care has been the difficulty of understanding how models get to results.
“Someone who is 85 years old and shut in may not know there’s a community based organization that will deliver them groceries.”For DeCaprio, bringing the predictive power of AI to health care has been a rewarding, if humbling, experience.",Mit
114,https://news.mit.edu/2020/experts-identify-top-threats-opportunities-global-health-0618,"


Will innovations in health and medicine deliver? This is a question on the top of everyone’s mind as Covid-19 tests the resiliency of global medical supply chains. 
Over 100 experts recently participated in Trust CoLab, an innovative online exercise that developed a set of alternative scenarios about the future of medicine and health care. The exercise identified one potential development that quickly became quite salient: the prospect of global pandemic leading to drastic changes in health-care practices. This possibility is spelled out in “Scaling the Tried and True,” one of the four scenarios developed in the exercise.   
The MIT Center for Collective Intelligence and U.S. Pharmacopeia has now released a report, Trust or Consequences 2040: Will innovations in health and medicine deliver?, based on the Trust CoLab effort. The report is the product of a four-week process that elicited ideas from experts affiliated with leading organizations, including the Bill and Melinda Gates Foundation, the American Association of Pharmacy, and Harvard Medical School. Participants were asked to consider what developments might shape people’s health between now and 2040 and what impact these developments could have on the evolution of trust in medicine and health care.
The report details four potential future scenarios: 


 Scaling the tried and true: In response to a series of major health crises, key actors in health care cooperate across all sectors to build institutions with global scope, designed to deliver basic, proven cures to all. 


 Dangerous uncertainty: Problems with big data/AI and gene modification trigger devastating medical failures. Disparities in access continue. In response, the wealthy depend on the latest, science-based medicine, the middle class turns to trusted local caregivers, and the less-privileged rely on folk medicines and food-based cures. 


A world of difference: Rapid advances occur by pairing genetic information with big data and artificial intelligence, but inequality creates a “haves” versus “have-nots” dynamic. Those who cannot access the latest therapies mistrust the overall health care system. 


Solving tomorrow’s problems: Smart and deliberate innovation is distributed broadly. With diseases more predictable, the focus of health care shifts to prevention. Innovation leads to remarkable new discoveries and also curbs increases in health-care costs. 


As leaders around the world think about how to respond to the Covid-19 outbreak, considering potential fault lines that could increase risks in health care and medicine will become even more important. This prospect highlights the importance of experts coming together across disciplines, as they did in Trust Colab, to identify potential threats and opportunities to enhance the system’s resilience.



",Trust or consequences,2020-06-18,['Annalyn Bachmann'],Center for Collective Intelligence/Health care/Medicine/Pandemic/Covid-19/Global,"['medical', 'deliver', 'global', 'trust', 'medicine', 'care', 'consequences', 'potential', 'experts', 'report', 'health']","This is a question on the top of everyone’s mind as Covid-19 tests the resiliency of global medical supply chains.
Over 100 experts recently participated in Trust CoLab, an innovative online exercise that developed a set of alternative scenarios about the future of medicine and health care.
The MIT Center for Collective Intelligence and U.S. Pharmacopeia has now released a report, Trust or Consequences 2040: Will innovations in health and medicine deliver?, based on the Trust CoLab effort.
Participants were asked to consider what developments might shape people’s health between now and 2040 and what impact these developments could have on the evolution of trust in medicine and health care.
Those who cannot access the latest therapies mistrust the overall health care system.",Mit
115,https://news.mit.edu/2020/mit-takeda-program-launches-research-ai-and-human-health-0618,"


In February, researchers from MIT and Takeda Pharmaceuticals joined together to celebrate the official launch of the MIT-Takeda Program. The MIT-Takeda Program aims to fuel the development and application of artificial intelligence (AI) capabilities to benefit human health and drug development. Centered within the Abdul Latif Jameel Clinic for Machine Learning in Health (Jameel Clinic), the program brings together the MIT School of Engineering and Takeda Pharmaceuticals, to combine knowledge and address challenges of mutual interest.   
Following a competitive proposal process, nine inaugural research projects were selected. The program’s flagship research projects include principal investigators from departments and labs spanning the School of Engineering and the Institute. Research includes diagnosis of diseases, prediction of treatment response, development of novel biomarkers, process control and improvement, drug discovery, and clinical trial optimization.
“We were truly impressed by the creativity and breadth of the proposals we received,” says Anantha P. Chandrakasan, dean of the School of Engineering, Vannevar Bush Professor of Electrical Engineering and Computer Science, and co-chair of the MIT-Takeda Program Steering Committee.
Engaging with researchers and industry experts from Takeda, each project team will bring together different disciplines, merging theory and practical implementation, while combining algorithm and platform innovations.
“This is an incredible opportunity to merge the cross-disciplinary and cross-functional expertise of both MIT and Takeda researchers,” says Chandrakasan. “This particular collaboration between academia and industry is of great significance as our world faces enormous challenges pertaining to human health. I look forward to witnessing the evolution of the program and the impact its research aims to have on our society.” 
“The shared enthusiasm and combined efforts of researchers from across MIT and Takeda have the opportunity to shape the future of health care,” says Anne Heatherington, senior vice president and head of Data Sciences Institute (DSI) at Takeda, and co-chair of the MIT-Takeda Program Steering Committee. “Together we are building capabilities and addressing challenges through interrogation of multiple data types that we have not been able to solve with the power of humans alone that have the potential to benefit both patients and the greater community.”
The following are the inaugural projects of the MIT-Takeda Program. Included are the MIT teams collaborating with Takeda researchers, who are leveraging AI to positively impact human health.
""AI-enabled, automated inspection of lyophilized products in sterile pharmaceutical manufacturing"": Duane Boning, the Clarence J. LeBel Professor of Electrical Engineering and faculty co-director of the Leaders for Global Operations program; Luca Daniel, professor of electrical engineering and computer science; Sanjay Sarma, the Fred Fort Flowers and Daniel Fort Flowers Professor of Mechanical Engineering and vice president for open learning; and Brian Subirana, research scientist and director MIT Auto-ID Laboratory within the Department of Mechanical Engineering.
""Automating adverse effect assessments and scientific literature review"": Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science and Jameel Clinic faculty co-lead; Tommi Jaakkola, the Thomas Siebel Professor of Electrical Engineering and Computer Science and the Institute for Data, Systems, and Society; and Jacob Andreas, assistant professor of electrical engineering and computer science.
""Automated analysis of speech and language deficits for frontotemporal dementia"": James Glass, senior research scientist in the MIT Computer Science and Artificial Intelligence Laboratory; Sanjay Sarma, the Fred Fort Flowers and Daniel Fort Flowers Professor of Mechanical Engineering and vice president for open learning; and Brian Subirana, research scientist and director of the MIT Auto-ID Laboratory within the Department of Mechanical Engineering.
""Discovering human-microbiome protein interactions with continuous distributed representation"": Jim Collins, the Termeer Professor of Medical Engineering and Science in MIT’s Institute for Medical Engineering and Science and Department of Biological Engineering, Jameel Clinic faculty co-lead, and MIT-Takeda Program faculty lead; and Timothy Lu, associate professor of electrical engineering and computer science and of biological engineering.
""Machine learning for early diagnosis, progression risk estimation, and identification of non-responders to conventional therapy for inflammatory bowel disease"": Peter Szolovits, professor of computer science and engineering, and David Sontag, associate professor of electrical engineering and computer science.
""Machine learning for image-based liver phenotyping and drug discovery"": Polina Golland, professor of electrical engineering and computer science; Brian W. Anthony, principal research scientist in the Department of Mechanical Engineering; and Peter Szolovits, professor of computer science and engineering.
""Predictive in silico models for cell culture process development for biologics manufacturing"": Connor W. Coley, assistant professor of chemical engineering, and J. Christopher Love, the Raymond A. (1921) and Helen E. St. Laurent Professor of Chemical Engineering.
""Automated data quality monitoring for clinical trial oversight via probabilistic programming"": Vikash Mansinghka, principal research scientist in the Department of Brain and Cognitive Sciences; Tamara Broderick, associate professor of electrical engineering and computer science; David Sontag, associate professor of electrical engineering and computer science; Ulrich Schaechtle, research scientist in the Department of Brain and Cognitive Sciences; and Veronica Weiner, director of special projects for the MIT Probabilistic Computing Project.
""Time series analysis from video data for optimizing and controlling unit operations in production and manufacturing"": Allan S. Myerson, professor of chemical engineering; George Barbastathis, professor of mechanical engineering; Richard Braatz, the Edwin R. Gilliland Professor of Chemical Engineering; and Bernhardt Trout, the Raymond F. Baddour, ScD, (1949) Professor of Chemical Engineering.
“The flagship research projects of the MIT-Takeda Program offer real promise to the ways we can impact human health,” says Jim Collins. “We are delighted to have the opportunity to collaborate with Takeda researchers on advances that leverage AI and aim to shape health care around the globe.”


",MIT-Takeda program launches,2020-06-18,[],"School of Engineering/J-Clinic/Electrical engineering and computer science (EECS)/Mechanical engineering/Institute for Data, Systems, and Society/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Institute for Medical Engineering and Science (IMES)/Biological engineering/Brain and cognitive sciences/Artificial intelligence/Medicine/Collaboration/Funding/Industry/Research/MIT Schwarzman College of Computing","['research', 'electrical', 'computer', 'engineering', 'launches', 'professor', 'science', 'takeda', 'program', 'mit', 'mittakeda']","In February, researchers from MIT and Takeda Pharmaceuticals joined together to celebrate the official launch of the MIT-Takeda Program.
The MIT-Takeda Program aims to fuel the development and application of artificial intelligence (AI) capabilities to benefit human health and drug development.
“We were truly impressed by the creativity and breadth of the proposals we received,” says Anantha P. Chandrakasan, dean of the School of Engineering, Vannevar Bush Professor of Electrical Engineering and Computer Science, and co-chair of the MIT-Takeda Program Steering Committee.
“Together we are building capabilities and addressing challenges through interrogation of multiple data types that we have not been able to solve with the power of humans alone that have the potential to benefit both patients and the greater community.”The following are the inaugural projects of the MIT-Takeda Program.
“The flagship research projects of the MIT-Takeda Program offer real promise to the ways we can impact human health,” says Jim Collins.",Mit
116,https://news.mit.edu/2020/what-jumps-out-photo-changes-longer-we-look-0617,"


What seizes your attention at first glance might change with a closer look. That elephant dressed in red wallpaper might initially grab your eye until your gaze moves to the woman on the living room couch and the surprising realization that the pair appear to be sharing a quiet moment together.
In a study being presented at the virtual Computer Vision and Pattern Recognition conference this week, researchers show that our attention moves in distinctive ways the longer we stare at an image, and that these viewing patterns can be replicated by artificial intelligence models. The work suggests immediate ways of improving how visual content is teased and eventually displayed online. For example, an automated cropping tool might zoom in on the elephant for a thumbnail preview or zoom out to include the intriguing details that become visible once a reader clicks on the story.
“In the real world, we look at the scenes around us and our attention also moves,” says Anelise Newman, the study’s co-lead author and a master's student at MIT. “What captures our interest over time varies.” The study’s senior authors are Zoya Bylinskii PhD ’18, a research scientist at Adobe Research, and Aude Oliva, co-director of the MIT Quest for Intelligence and a senior research scientist at MIT’s Computer Science and Artificial Intelligence Laboratory.










What researchers know about saliency, and how humans perceive images, comes from experiments in which participants are shown pictures for a fixed period of time. But in the real world, human attention often shifts abruptly. To simulate this variability, the researchers used a crowdsourcing user interface called CodeCharts to show participants photos at three durations — half a second, 3 seconds, and 5 seconds — in a set of online experiments. 
When the image disappeared, participants were asked to report where they had last looked by typing in a three-digit code on a gridded map corresponding to the image. In the end, the researchers were able to gather heat maps of where in a given image participants had collectively focused their gaze at different moments in time. 
At the split-second interval, viewers focused on faces or a visually dominant animal or object. By 3 seconds, their gaze had shifted to action-oriented features, like a dog on a leash, an archery target, or an airborne frisbee. At 5 seconds, their gaze either shot back, boomerang-like, to the main subject, or it lingered on the suggestive details. 
“We were surprised at just how consistent these viewing patterns were at different durations,” says the study’s other lead author, Camilo Fosco, a PhD student at MIT.
With real-world data in hand, the researchers next trained a deep learning model to predict the focal points of images it had never seen before, at different viewing durations. To reduce the size of their model, they included a recurrent module that works on compressed representations of the input image, mimicking the human gaze as it explores an image at varying durations. When tested, their model outperformed the state of the art at predicting saliency across viewing durations.
The model has potential applications for editing and rendering compressed images and even improving the accuracy of automated image captioning. In addition to guiding an editing tool to crop an image for shorter or longer viewing durations, it could prioritize which elements in a compressed image to render first for viewers. By clearing away the visual clutter in a scene, it could improve the overall accuracy of current photo-captioning techniques. It could also generate captions for images meant for split-second viewing only. 
“The content that you consider most important depends on the time you have to look at it,” says Bylinskii. “If you see the full image at once, you may not have time to absorb it all.”
As more images and videos are shared online, the need for better tools to find and make sense of relevant content is growing. Research on human attention offers insights for technologists. Just as computers and camera-equipped mobile phones helped create the data overload, they are also giving researchers new platforms for studying human attention and designing better tools to help us cut through the noise.
In a related study accepted to the ACM Conference on Human Factors in Computing Systems, researchers outline the relative benefits of four web-based user interfaces, including CodeCharts, for gathering human attention data at scale. All four tools capture attention without relying on traditional eye-tracking hardware in a lab, either by collecting self-reported gaze data, as CodeCharts does, or by recording where subjects click their mouse or zoom in on an image.
“There's no one-size-fits-all interface that works for all use cases, and our paper focuses on teasing apart these trade-offs,” says Newman, lead author of the study.
By making it faster and cheaper to gather human attention data, the platforms may help to generate new knowledge on human vision and cognition. “The more we learn about how humans see and understand the world, the more we can build these insights into our AI tools to make them more useful,” says Oliva.
Other authors of the CVPR paper are Pat Sukhum, Yun Bin Zhang, and Nanxuan Zhao. The research was supported by the Vannevar Bush Faculty Fellowship program, an Ignite grant from the SystemsThatLearn@CSAIL, and cloud computing services from MIT Quest.


",What jumps out in a photo changes the longer we look,2020-06-17,['Kim Martineau'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical engineering and computer science (EECS)/School of Engineering/School of Science/Algorithms/Artificial intelligence/Computer science and technology/Machine learning/Software/Computer vision/Quest for Intelligence/MIT Schwarzman College of Computing,"['longer', 'data', 'human', 'research', 'researchers', 'changes', 'attention', 'gaze', 'look', 'images', 'image', 'viewing', 'durations', 'jumps']","But in the real world, human attention often shifts abruptly.
When tested, their model outperformed the state of the art at predicting saliency across viewing durations.
In addition to guiding an editing tool to crop an image for shorter or longer viewing durations, it could prioritize which elements in a compressed image to render first for viewers.
Research on human attention offers insights for technologists.
By making it faster and cheaper to gather human attention data, the platforms may help to generate new knowledge on human vision and cognition.",Mit
117,https://news.mit.edu/2020/seven-from-mit-win-2020-hertz-fellowships-0611,"


The Fannie and John Hertz Foundation announced that it has awarded seven of its 16 graduate fellowships to MIT students this year — six recent graduates and one graduate student. These prestigious awards provide each student with five years of doctoral-level research funding with “the freedom to pursue innovative ideas, wherever they may lead,” as well as lifelong professional support from previous Hertz Fellows recipients.
This year’s winners are Alexander Alabugin '20, Alyssa Dayan '18, Marisa Gaetz '20, Isaac Metcalf '20, Nolan Peard '20, Maya Sankar '20, and Constantine Tzouanas.
The newly minted MIT Hertz Fellows were selected from a pool of over 800 students, representing 24 universities around the United States.
“The pursuits of our 2020 Hertz Fellows embody the type of bold, risk-taking research that the Hertz Foundation has supported for almost six decades,” says Robbee Baker Kosak, the Fannie and John Hertz Foundation president. “By funding innovative thinkers and connecting visionary researchers across generations, geography, and disciplines, we create the conditions for our fellows to have an exponential impact on the most pressing problems facing our nation and world.”
To date, there are 423 MIT alumni among the 1,200 Hertz Fellows named since the foundation’s inception in 1963, and 299 completed their doctorates at MIT. In fact, more Hertz Fellows have chosen to pursue their PhDs at MIT than any other university.
In addition to the seven fellows from MIT, three Hertz Fellows from other undergraduate institutions will soon join the MIT community as PhD students: Hannah Lawrence (computer science), Vikram Sundar (computational and systems biology), and Nico Valdes Meller (physics).
This year’s MIT recipients represent a broad spectrum of research interests and potential applications, from creating new materials to engineering biological systems.
Alexander Alabugin '20 plans to pursue chemistry further at the Caltech. Using a range of techniques such as electron resonance, Mössbauer spectroscopy, and X-ray absorption, he aims to better understand inorganic reaction mechanisms. Alabugin is also a fellow of the National Science Foundation.
Alyssa Dayan '18 completed her bachelor’s degree in mathematics and computer science. She is currently working on simulation and prediction for autonomous vehicles at Uber Advanced Technologies Group. Dayan will begin her PhD this fall at the University of California at Berkeley, focusing on machine learning and artificial intelligence.
Marisa Gaetz '20 graduated with a major in mathematics and a minor in philosophy. She has been deeply involved in MIT’s Prison Education Initiative and in efforts to improve diversity and inclusivity within the math community. She will remain at the Institute to begin her doctorate in mathematics, building on her work to date on the numerous connections between physics and representation theory.
Isaac Metcalf '20 graduated as a double major in materials science and physics. He plans to attend Rice University to pursue a PhD in materials science, focusing on increasing the efficiency and stability of two-dimensional perovskite photovoltaics. He’s the co-inventor of a patent-pending design of an electrochemical flow reactor for the carbon-neutral synthesis of calcium hydroxide.
Nolan Peard '20 completed dual degrees in physics and music in May and will begin his doctorate in applied physics at Stanford University this fall. He is especially interested exploring the use of optical techniques to control quantum states of molecules and their interactions, with the goal of creating materials and molecules with new capabilities.
Maya Sankar '20 graduated with bachelor’s degrees in mathematics and computer science, and a minor in music. Her passion for mathematics began at an early age, doing math problems with her dad, and she went on to participate in prestigious math competitions in high school. She will continue her studies at Stanford University, with a particular focus on combinatorics.
Constantine Tzouanas is currently a National Science Foundation Graduate Research Fellow in the Harvard-MIT Program in Health Sciences and Technology. He plans pursue a PhD in medical engineering and medical physics at MIT. Ultimately, he hopes to engineer biological systems with applications such as environmentally responsible chemical production and organ transplants.


",Seven from MIT win 2020 Hertz Fellowships,2020-06-11,[],"Vice Chancellor/Harvard-MIT Health Sciences and Technology/School of Science/School of Engineering/School of Humanities Arts and Social Sciences/Awards, honors and fellowships/Students/Graduate, postdoctoral/Alumni/ae/Undergraduate/Chemistry/Mathematics/Philosophy/DMSE/Physics/Music","['fellows', 'research', 'hertz', 'mathematics', '2020', '20', 'seven', 'win', 'fellowships', 'university', 'pursue', 'physics', 'science', 'mit']","The Fannie and John Hertz Foundation announced that it has awarded seven of its 16 graduate fellowships to MIT students this year — six recent graduates and one graduate student.
This year’s winners are Alexander Alabugin '20, Alyssa Dayan '18, Marisa Gaetz '20, Isaac Metcalf '20, Nolan Peard '20, Maya Sankar '20, and Constantine Tzouanas.
The newly minted MIT Hertz Fellows were selected from a pool of over 800 students, representing 24 universities around the United States.
“The pursuits of our 2020 Hertz Fellows embody the type of bold, risk-taking research that the Hertz Foundation has supported for almost six decades,” says Robbee Baker Kosak, the Fannie and John Hertz Foundation president.
In fact, more Hertz Fellows have chosen to pursue their PhDs at MIT than any other university.",Mit
118,https://news.mit.edu/2020/student-geeticka-chauhan-0609,"


In March, as her friends and neighbors were scrambling to pack up and leave campus due to the Covid-19 pandemic, Geeticka Chauhan found her world upended in yet another way. Just weeks earlier, she had been elected council president of MIT’s largest graduate residence, Sidney-Pacific. Suddenly the fourth-year PhD student was plunged into rounds of emergency meetings with MIT administrators.From her apartment in Sidney-Pacific, where she has stayed put due to travel restrictions in her home country of India, Chauhan is still learning the ropes of her new position. With others, she has been busy preparing to meet the future challenge of safely redensifying the living space of more than 1,000 people: how to regulate high-density common areas, handle noise complaints as people spend more time in their rooms, and care for the mental and physical well-being of a community that can only congregate virtually. “It’s just such a crazy time,” she says.She’s prepared for the challenge. During her time at MIT, while pursuing her research using artificial intelligence to understand human language, Chauhan has worked to strengthen the bonds of her community in numerous ways, often drawing on her experience as an international student to do so.Adventures in brunchingWhen Chauhan first came to MIT in 2017, she quickly fell in love with Sidney-Pacific’s thriving and freewheeling “helper culture.” “These are all researchers, but they’re maybe making brownies, doing crazy experiments that they would do in lab, except in the kitchen,” she says. “That was my first introduction to the MIT spirit.”Next thing she knew, she was teaching Budokon yoga, mashing chickpeas into guacamole, and immersing herself in the complex operations of a monthly brunch attended by hundreds of graduate students, many of whom came to MIT from outside the U.S. In addition to the genuine thrill of cracking 300 eggs in 30 minutes, working on the brunches kept her grounded in a place thousands of miles from her home in New Delhi. “It gave me a sense of community and made me feel like I have a family here,” she says.Chauhan has found additional ways to address the particular difficulties that international students face. As a member of the Presidential Advisory Council this year, she gathered international student testimonies on visa difficulties and presented them to MIT’s president and the director of the International Students Office. And when a friend from mainland China had to self-quarantine on Valentine’s Day, Chauhan knew she had to act. As brunch chair, she organized food delivery, complete with chocolates and notes, for Sidney-Pacific residents who couldn’t make it to the monthly event. “Initially when you come back to the U.S. from your home country, you really miss your family,” she says. “I thought self-quarantining students should feel their MIT community cares for them.”Culture shockGrowing up in New Delhi, math was initially one of her weaknesses, Chauhan says, and she was scared and confused by her early introduction to coding. Her mother and grandmother, with stern kindness and chocolates, encouraged her to face these fears. “My mom used to teach me that with hard work, you can make your biggest weakness your biggest strength,” she explains. She soon set her sights on a future in computer science.However, as Chauhan found her life increasingly dominated by the high-pressure culture of preparing for college, she began to long for a feeling of wholeness, and for the person she left behind on the way. “I used to have a lot of artistic interests but didn’t get to explore them,” she says. She quit her weekend engineering classes, enrolled in a black and white photography class, and after learning about the extracurricular options at American universities, landed a full scholarship to attend Florida International University.It was a culture shock. She didn’t know many Indian students in Miami and felt herself struggling to reconcile the individualistic mindset around her with the community and family-centered life at home. She says the people she met got her through, including Mark Finlayson, a professor studying the science of narrative from the viewpoint of natural language processing. Under Finlayson’s guidance she developed a fascination with the way AI techniques could be used to better understand the patterns and structures in human narratives. She learned that studying AI wasn’t just a way of imitating human thinking, but rather an approach for deepening our understanding of ourselves as reflected by our language. “It was due to Mark’s mentorship that I got involved in research” and applied to MIT, she says.The holistic researcherChauan now works in the Clinical Decision Making Group led by Peter Szolovits at the Computer Science and Artificial Intelligence Laboratory, where she is focusing on the ways natural language processing can address health care problems. For her master’s project, she worked on the problem of relation extraction and built a tool to digest clinical literature that would, for example, help pharamacologists easily assess negative drug interactions. Now, she’s finishing up a project integrating visual analysis of chest radiographs and textual analysis of radiology reports for quantifying pulmonary edema, to help clinicians manage the fluid status of their patients who have suffered acute heart failure.“In routine clinical practice, patient care is interweaved with a lot of bureaucratic work,” she says. “The goal of my lab is to assist with clinical decision making and give clinicians the full freedom and time to devote to patient care.”It’s an exciting moment for Chauhan, who recently submitted a paper she co-first authored with another grad student, and is starting to think about her next project: interpretability, or how to elucidate a decision-making model’s “thought process” by highlighting the data from which it draws its conclusions. She continues to find the intersection of computer vision and natural language processing an exciting area of research. But there have been challenges along the way.After the initial flurry of excitement her first year, personal and faculty expectations of students’ independence and publishing success grew, and she began to experience uncertainty and imposter syndrome. “I didn’t know what I was capable of,” she says. “That initial period of convincing yourself that you belong is difficult. I am fortunate to have a supportive advisor that understands that.”Finally, one of her first-year projects showed promise, and she came up with a master’s thesis plan in a month and submitted the project that semester. To get through, she says, she drew on her “survival skills”: allowing herself to be a full person beyond her work as a researcher so that one setback didn’t become a sense of complete failure. For Chauhan, that meant working as a teaching assistant, drawing henna designs, singing, enjoying yoga, and staying involved in student government. “I used to try to separate that part of myself with my work side,” she says. “I needed to give myself some space to learn and grow, rather than compare myself to others.”Citing a study showing that women are more likely to drop out of STEM disciplines when they receive a B grade in a challenging course, Chauhan says she wishes she could tell her younger self not to compare herself with an ideal version of herself. Dismantling imposter syndrome requires an understanding that qualification and success can come from a broad range of experiences, she says: It’s about “seeing people for who they are holistically, rather than what is seen on the resume.”


",Learning the ropes and throwing lifelines,2020-06-09,['Sofia Tong'],"Profile/Students/Administration/Covid-19/Pandemic/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/School of Engineering/Health care/Health science and technology/Medicine/Artificial intelligence/Women in STEM/MIT Schwarzman College of Computing/Graduate, postdoctoral","['lifelines', 'student', 'chauhan', 'language', 'learning', 'community', 'mit', 'work', 'used', 'way', 'ropes', 'international', 'throwing', 'students']","From her apartment in Sidney-Pacific, where she has stayed put due to travel restrictions in her home country of India, Chauhan is still learning the ropes of her new position.
“It gave me a sense of community and made me feel like I have a family here,” she says.
Chauhan has found additional ways to address the particular difficulties that international students face.
As a member of the Presidential Advisory Council this year, she gathered international student testimonies on visa difficulties and presented them to MIT’s president and the director of the International Students Office.
For Chauhan, that meant working as a teaching assistant, drawing henna designs, singing, enjoying yoga, and staying involved in student government.",Mit
119,https://news.mit.edu/2020/social-life-of-data-0608,"


On a typical day in our data-saturated world, Facebook announces plans to encrypt its Messenger data, prompting uproar from child welfare activists who fear privacy will come at the cost of online safety. A new company called Tillable, an AirBnB for farmers, makes headlines for allowing the public to rent farmland while collecting and tracking massive swathes of data on land use and profitability. Tesla comes under fire for concealing autopilot data, while the U.S. Federal Trade Commission announces that 2019 was a record year in protecting consumer privacy.
Given the daily avalanche of news in the contemporary tug of war between privacy and safety, Data and Society (STS 11.155J/STS.005J) always begins with a discussion of current events.
One of 36 classes in the new Computing and Society concentration in MIT's School of Humanities, Arts, and Social Sciences, Data and Society focuses on two linked concepts: the process of data creation and analysis, and the ethical quandaries and policy vacuums surrounding how those data impact society.
A gestalt approach to data

“The purpose of this class is to engage MIT students in thinking about data — data creation, data analysis — in ways that are not only technical but are also societal,” says Eden Medina, associate professor of science, technology, and society, who co-taught the class this spring with Sarah Williams, an associate professor of technology and urban planning.
Medina is particularly well-versed in the social, historical, and ethical aspects of computing, and Williams brings expertise as a practicing data scientist. Their multi-layered course is designed to “train practitioners who think about the ethics of the work that they’re doing” and who know how to use data in responsible ways.
Medina and Williams crafted the inaugural semester of Data and Society around the life-cycle stages of a normal data science project, guiding students to consider project facts such as who is collecting the data, how is the data created, and how it is analyzed. Students then explore broader questions, including: How can power intersect with the way those data are created? What is the role of bias in data creation? What is informed consent and what role might it play into the way that datasets are generated and then eventually used and reused?
 
Impacts of data collection in daily life

As the course continues, students begin to discover the fine threads of cause and effect that can often slip under a purely technical radar. Bias in data collection, for instance, can have subtle and insidious effects on how the world is constructed around us; for instance, the way in which data are collected could further pre-existing bias rooted in social inequality. Practices of data collection, aggregation, and reuse can also present challenges for ethical practices such as informed consent. How can we make an informed decision without fully understanding how our data might be used in the future and the ramifications of that use?  
“I have worked a lot on the technical side with data both in my computer science classes, and with work experiences and my UROP [undergraduate research project],” says Darian Bhathena '20, a recent graduate whose studies span computer science and engineering, biomedical engineering, and urban studies and planning. “As engineering students, we sometimes forget that, to be useful and applicable, all the technical material we’re learning has to fit within society as a whole.”
The intricate impacts of data collection in the students’ daily lives — from what they see in their Twitter feeds to how they interact with health-tracking apps — are front and center in the class, making the curriculum material and its implications personal.
A challenge at the core of a data-driven society
For one assignment, students created visualizations from data they collected, endeavoring to be as neutral as possible, then wrote about the decisions they made, including non-technical decisions, to build the dataset and use it for analysis.
One student downloaded all her text messages for a week, trying to track a correlation between weather and texting patterns. Another tried to determine which MIT dorm was the healthiest, entering diet data into a program they designed. Another student tried to track her own water usage against self-reported norms across the Cambridge, Massachusetts, area. All of the students ran into assumptions in their data models — for instance, about how much water is used to wash hands, or how diets change over time. One by one, the students faced a series of built-in human decisions that prevented their data from being truly neutral.
The exercise illustrated the challenge at the core of our data-driven society: data are easy to gather, but their implications are far less easy to discern and manage. “A lot of decisions around data in the world are ours to make,” says Williams. “Technology moves much more quickly than regulation can.”
 
Fluency in the ethics of technology

The new Computing and Society concentration, of which Data and Society is a core course, is part of a larger push across the Institute, echoed in the mission of the new MIT Schwarzman College of Computing, to enable a holistic view of how technology both shapes, and is shaped by, the nuances of the world, and to develop Institute-wide fluency in the ethics of technology.   
Zach Johnson, a rising junior majoring in computer science and engineering, is also pursuing the new Computing and Society concentration. He says his experience in simultaneous technical and humanistic instruction has been eye-opening. “I get to see all the application of what I am learning in the real world and get to learn the ethics behind what I am doing,” he explains. “While I am learning how to write the code in my Course 6 classes, this class is showing me how that code is used to do incredible good or incredible harm for the world.”
In the current public health crisis, Johnson is eager to apply his new insights to this unprecedented moment in the course’s final project. The assignment: study how another country is using data to address the coronavirus pandemic and identify which aspects of this approach, if any, the United States should adopt.
Johnson says, “While all the topics of this course are interesting, it is particularly fascinating to be able to apply what is happening in the world during a time of crisis to my study of data science.”
Does tech provide more objective decisions?

Medina, herself a 2005 doctoral graduate of the MIT STS program, joined the faculty last July. Her current research centers on technology and human rights, with a focus on Chile. Much of her previous and current scholarship relates to how people use data to bring certainty to highly uncertain situations, and how our increased trust in technology and its capabilities echo through social realities.
“I see [this research] as very relevant to emerging issues in artificial intelligence and machine learning — because we are now putting our faith in new technological systems that are built on large repositories of data and whose decision-making processes are often not transparent. We are trusting them to give us a more objective decision — often without having the means to consider how flawed that 'objective' decision might be. What harms can result from such practices?”
Williams' Civic Data Design Lab is immersed in questions of how data can be used to expose and inform urban policies. In one example from her book, “Data Action,” she created a model to identify cities in China that were built but never inhabited. The model was based on the idea that thriving communities need amenities (grocery stores and schools) — analysis of Chinese social media data showed that in many Chinese cities these basic resources did not exist, and therefore they were “ghost cities.” Williams lab went further to visualize the data to “ground truth” the results with Chinese officials. The approach allowed more candid conversations with the government and a more accurate model for understanding the phenomenon of China’s vacant cities.
“We hear a lot about how data can be used for bad things, which is true, but it also can be used for good,” reflects Williams. “Like anything in the world, data is a tool, and that tool can be used to improve society, rather than cause harm.”
Based on the inaugural class, Williams thinks Data and Society is exactly the kind of rigorous, thoughtful environment that will empower MIT graduates, helping them develop the awareness, analytical/ethical framework, and skills needed to act consciously as data practitioners in the field. “Engaging students across disciplines — that’s how innovation happens,” she says.

Story prepared by MIT SHASS Communications
Editorial and Design Director: Emily Hiestand
Writer: Alison Lanier, Senior Communications Associate



",The social life of data,2020-06-08,[],School of Humanities Arts and Social Sciences/MIT Schwarzman College of Computing/Computing/Data/Ethics/Technology and society/Social sciences/Urban studies and planning/School of Architecture and Planning/Program in STS,"['data', 'technology', 'technical', 'life', 'mit', 'world', 'society', 'williams', 'social', 'used', 'students']","One of 36 classes in the new Computing and Society concentration in MIT's School of Humanities, Arts, and Social Sciences, Data and Society focuses on two linked concepts: the process of data creation and analysis, and the ethical quandaries and policy vacuums surrounding how those data impact society.
Medina is particularly well-versed in the social, historical, and ethical aspects of computing, and Williams brings expertise as a practicing data scientist.
Medina and Williams crafted the inaugural semester of Data and Society around the life-cycle stages of a normal data science project, guiding students to consider project facts such as who is collecting the data, how is the data created, and how it is analyzed.
The exercise illustrated the challenge at the core of our data-driven society: data are easy to gather, but their implications are far less easy to discern and manage.
What harms can result from such practices?”Williams' Civic Data Design Lab is immersed in questions of how data can be used to expose and inform urban policies.",Mit
120,https://news.mit.edu/2020/thousands-artificial-brain-synapses-single-chip-0608,"


MIT engineers have designed a “brain-on-a-chip,” smaller than a piece of confetti, that is made from tens of thousands of artificial brain synapses known as memristors — silicon-based components that mimic the information-transmitting synapses in the human brain.The researchers borrowed from principles of metallurgy to fabricate each memristor from alloys of silver and copper, along with silicon. When they ran the chip through several visual tasks, the chip was able to “remember” stored images and reproduce them many times over, in versions that were crisper and cleaner compared with existing memristor designs made with unalloyed elements.Their results, published today in the journal Nature Nanotechnology, demonstrate a promising new memristor design for neuromorphic devices — electronics that are based on a new type of circuit that processes information in a way that mimics the brain’s neural architecture. Such brain-inspired circuits could be built into small, portable devices, and would carry out complex computational tasks that only today’s supercomputers can handle.“So far, artificial synapse networks exist as software. We’re trying to build real neural network hardware for portable artificial intelligence systems,” says Jeehwan Kim, associate professor of mechanical engineering at MIT. “Imagine connecting a neuromorphic device to a camera on your car, and having it recognize lights and objects and make a decision immediately, without having to connect to the internet. We hope to use energy-efficient memristors to do those tasks on-site, in real-time.”Wandering ionsMemristors, or memory transistors, are an essential element in neuromorphic computing. In a neuromorphic device, a memristor would serve as the transistor in a circuit, though its workings would more closely resemble a brain synapse — the junction between two neurons. The synapse receives signals from one neuron, in the form of ions, and sends a corresponding signal to the next neuron.A transistor in a conventional circuit transmits information by switching between one of only two values, 0 and 1, and doing so only when the signal it receives, in the form of an electric current, is of a particular strength. In contrast, a memristor would work along a gradient, much like a synapse in the brain. The signal it produces would vary depending on the strength of the signal that it receives. This would enable a single memristor to have many values, and therefore carry out a far wider range of operations than binary transistors.Like a brain synapse, a memristor would also be able to “remember” the value associated with a given current strength, and produce the exact same signal the next time it receives a similar current. This could ensure that the answer to a complex equation, or the visual classification of an object, is reliable — a feat that normally involves multiple transistors and capacitors.Ultimately, scientists envision that memristors would require far less chip real estate than conventional transistors, enabling powerful, portable computing devices that do not rely on supercomputers, or even connections to the Internet.Existing memristor designs, however, are limited in their performance. A single memristor is made of a positive and negative electrode, separated by a “switching medium,” or space between the electrodes. When a voltage is applied to one electrode, ions from that electrode flow through the medium, forming a “conduction channel” to the other electrode. The received ions make up the electrical signal that the memristor transmits through the circuit. The size of the ion channel (and the signal that the memristor ultimately produces) should be proportional to the strength of the stimulating voltage.Kim says that existing memristor designs work pretty well in cases where voltage stimulates a large conduction channel, or a heavy flow of ions from one electrode to the other. But these designs are less reliable when memristors need to generate subtler signals, via thinner conduction channels.The thinner a conduction channel, and the lighter the flow of ions from one electrode to the other, the harder it is for individual ions to stay together. Instead, they tend to wander from the group, disbanding within the medium. As a result, it’s difficult for the receiving electrode to reliably capture the same number of ions, and therefore transmit the same signal, when stimulated with a certain low range of current.Borrowing from metallurgyKim and his colleagues found a way around this limitation by borrowing a technique from metallurgy, the science of melding metals into alloys and studying their combined properties.“Traditionally, metallurgists try to add different atoms into a bulk matrix to strengthen materials, and we thought, why not tweak the atomic interactions in our memristor, and add some alloying element to control the movement of ions in our medium,” Kim says.Engineers typically use silver as the material for a memristor’s positive electrode. Kim’s team looked through the literature to find an element that they could combine with silver to effectively hold silver ions together, while allowing them to flow quickly through to the other electrode.The team landed on copper as the ideal alloying element, as it is able to bind both with silver, and with silicon.“It acts as a sort of bridge, and stabilizes the silver-silicon interface,” Kim says.To make memristors using their new alloy, the group first fabricated a negative electrode out of silicon, then made a positive electrode by depositing a slight amount of copper, followed by a layer of silver. They sandwiched the two electrodes around an amorphous silicon medium. In this way, they patterned a millimeter-square silicon chip with tens of thousands of memristors.As a first test of the chip, they recreated a gray-scale image of the Captain America shield. They equated each pixel in the image to a corresponding memristor in the chip. They then modulated the conductance of each memristor that was relative in strength to the color in the corresponding pixel.The chip produced the same crisp image of the shield, and was able to “remember” the image and reproduce it many times, compared with chips made of other materials.The team also ran the chip through an image processing task, programming the memristors to alter an image, in this case of MIT’s Killian Court, in several specific ways, including sharpening and blurring the original image. Again, their design produced the reprogrammed images more reliably than existing memristor designs.“We’re using artificial synapses to do real inference tests,” Kim says. “We would like to develop this technology further to have larger-scale arrays to do image recognition tasks. And some day, you might be able to carry around artificial brains to do these kinds of tasks, without connecting to supercomputers, the internet, or the cloud.”This research was funded, in part, by the MIT Research Support Committee funds, the MIT-IBM Watson AI Lab, Samsung Global Research Laboratory, and the National Science Foundation.


",Engineers put tens of thousands of artificial brain synapses on a single chip,2020-06-08,['Jennifer Chu'],electronics/Mechanical engineering/Research/School of Engineering/National Science Foundation (NSF)/Materials Science and Engineering/Computer science and technology/MIT-IBM Watson AI Lab/Artificial intelligence,"['tasks', 'silver', 'ions', 'memristor', 'chip', 'memristors', 'tens', 'synapse', 'engineers', 'signal', 'image', 'thousands', 'artificial', 'synapses', 'single', 'electrode', 'brain']","MIT engineers have designed a “brain-on-a-chip,” smaller than a piece of confetti, that is made from tens of thousands of artificial brain synapses known as memristors — silicon-based components that mimic the information-transmitting synapses in the human brain.
The researchers borrowed from principles of metallurgy to fabricate each memristor from alloys of silver and copper, along with silicon.
When a voltage is applied to one electrode, ions from that electrode flow through the medium, forming a “conduction channel” to the other electrode.
In this way, they patterned a millimeter-square silicon chip with tens of thousands of memristors.
“We would like to develop this technology further to have larger-scale arrays to do image recognition tasks.",Mit
121,https://news.mit.edu/2020/mit-csail-computing-technology-after-moores-law-0605,"


In 1965, Intel co-founder Gordon Moore predicted that the number of transistors that could fit on a computer chip would grow exponentially — and they did, doubling about every two years. For half a century, Moore’s Law has endured: Computers have gotten smaller, faster, cheaper, and more efficient, enabling the rapid worldwide adoption of PCs, smartphones, high-speed internet, and more.
This miniaturization trend has led to silicon chips today that have almost unimaginably small circuitry. Transistors, the tiny switches that implement computer microprocessors, are so small that 1,000 of them laid end-to-end are no wider than a human hair. And for a long time, the smaller the transistors were, the faster they could switch. But today, we’re approaching the limit of how small transistors can get. As a result, over the past decade researchers have been scratching their heads to find other ways to improve performance so that the computer industry can continue to innovate.
While we wait for the maturation of new computing technologies like quantum, carbon nanotubes, or photonics (which may take a while), other approaches will be needed to get performance as Moore’s Law comes to an end. In a recent journal article published in Science, a team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) identifies three key areas to prioritize to continue to deliver computing speed-ups: better software, new algorithms, and more streamlined hardware.
Senior author Charles E. Leiserson says that the performance benefits from miniaturization have been so great that, for decades, programmers have been able to prioritize making code-writing easier rather than making the code itself run faster. The inefficiency that this tendency introduces has been acceptable, because faster computer chips have always been able to pick up the slack.
“But nowadays, being able to make further advances in fields like machine learning, robotics, and virtual reality will require huge amounts of computational power that miniaturization can no longer provide,” says Leiserson, the Edwin Sibley Webster Professor in MIT's Department of Electrical Engineering and Computer Science. “If we want to harness the full potential of these technologies, we must change our approach to computing.”
Leiserson co-wrote the paper, published this week, with Research Scientist Neil Thompson, professors Daniel Sanchez and Joel Emer, Adjunct Professor Butler Lampson, and research scientists Bradley Kuszmaul and Tao Schardl.
No more Moore
The authors make recommendations about three areas of computing: software, algorithms, and hardware architecture.
With software, they say that programmers’ previous prioritization of productivity over performance has led to problematic strategies like “reduction”: taking code that worked on problem A and using it to solve problem B. For example, if someone has to create a system to recognize yes-or-no voice commands, but doesn’t want to code a whole new custom program, they could take an existing program that recognizes a wide range of words and tweak it to respond only to yes-or-no answers.
While this approach reduces coding time, the inefficiencies it creates quickly compound: if a single reduction is 80 percent as efficient as a custom solution, and you then add 20 layers of reduction, the code will ultimately be 100 times less efficient than it could be.
“These are the kinds of strategies that programmers have to rethink as hardware improvements slow down,” says Thompson. “We can’t keep doing ‘business as usual’ if we want to continue to get the speed-ups we’ve grown accustomed to.”
Instead, the researchers recommend techniques like parallelizing code. Much existing software has been designed using ancient assumptions that processors can only do only one operation at a time. But in recent years multicore technology has enabled complex tasks to be completed thousands of times faster and in a much more energy-efficient way. 
“Since Moore's Law will not be handing us improved performance on a silver platter, we will have to deliver performance the hard way,” says Moshe Vardi, a professor in computational engineering at Rice University. “This is a great opportunity for computing research, and the [MIT CSAIL] report provides a road map for such research.” 
As for algorithms, the team suggests a three-pronged approach that includes exploring new problem areas, addressing concerns about how algorithms scale, and tailoring them to better take advantage of modern hardware.
Lastly, in terms of hardware architecture, the team advocates that hardware be streamlined so that problems can be solved with fewer transistors and less silicon. Streamlining includes using simpler processors and creating hardware tailored to specific applications, like the graphics-processing unit is tailored for computer graphics. 
“Hardware customized for particular domains can be much more efficient and use far fewer transistors, enabling applications to run tens to hundreds of times faster,” says Schardl. “More generally, hardware streamlining would further encourage parallel programming, creating additional chip area to be used for more circuitry that can operate in parallel.”
While these approaches may be the best path forward, the researchers say that it won’t always be an easy one. Organizations that use such techniques may not know the benefits of their efforts until after they’ve invested a lot of engineering time. Plus, the speed-ups won’t be as consistent as they were with Moore’s Law: they may be dramatic at first, and then require large amounts of effort for smaller improvements. 
Certain companies have already gotten the memo.
“For tech giants like Google and Amazon, the huge scale of their data centers means that even small improvements in software performance can result in large financial returns,” says Thompson.  “But while these firms may be leading the charge, many others will need to take these issues seriously if they want to stay competitive.”
Getting improvements in the areas identified by the team will also require building up the infrastructure and workforce that make them possible.  
“Performance growth will require new tools, programming languages, and hardware to facilitate more and better performance engineering,” says Leiserson. “It also means computer scientists being better educated about how we can make software, algorithms, and hardware work together, instead of putting them in different silos.”
This work was supported, in part, by the National Science Foundation.



","If transistors can’t get smaller, then coders have to get smarter",2020-06-05,['Adam Conner-Simons'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical engineering and computer science (EECS)/School of Engineering/Computer science and technology/Technology and society/Software/Algorithms/Data/internet of things/MIT Schwarzman College of Computing/National Science Foundation (NSF),"['cant', 'performance', 'smaller', 'transistors', 'code', 'small', 'computer', 'smarter', 'algorithms', 'coders', 'faster', 'hardware', 'software', 'team']","Transistors, the tiny switches that implement computer microprocessors, are so small that 1,000 of them laid end-to-end are no wider than a human hair.
And for a long time, the smaller the transistors were, the faster they could switch.
But today, we’re approaching the limit of how small transistors can get.
The inefficiency that this tendency introduces has been acceptable, because faster computer chips have always been able to pick up the slack.
No more MooreThe authors make recommendations about three areas of computing: software, algorithms, and hardware architecture.",Mit
122,https://news.mit.edu/2020/giving-soft-robots-senses-0601,"


One of the hottest topics in robotics is the field of soft robots, which utilizes squishy and flexible materials rather than traditional rigid materials. But soft robots have been limited due to their lack of good sensing. A good robotic gripper needs to feel what it is touching (tactile sensing), and it needs to sense the positions of its fingers (proprioception). Such sensing has been missing from most soft robots.
In a new pair of papers, researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) came up with new tools to let robots better perceive what they’re interacting with: the ability to see and classify items, and a softer, delicate touch. 
“We wish to enable seeing the world by feeling the world. Soft robot hands have sensorized skins that allow them to pick up a range of objects, from delicate, such as potato chips, to heavy, such as milk bottles,” says CSAIL Director Daniela Rus, the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science and the deputy dean of research for the MIT Stephen A. Schwarzman College of Computing. 
This work was supported by the Toyota Research Institute.







Play video






One paper builds off last year’s research from MIT and Harvard University, where a team developed a soft and strong robotic gripper in the form of a cone-shaped origami structure. It collapses in on objects much like a Venus' flytrap, to pick up items that are as much as 100 times its weight. 
To get that newfound versatility and adaptability even closer to that of a human hand, a new team came up with a sensible addition: tactile sensors, made from latex “bladders” (balloons) connected to pressure transducers. The new sensors let the gripper not only pick up objects as delicate as potato chips, but it also classifies them — letting the robot better understand what it’s picking up, while also exhibiting that light touch. 
When classifying objects, the sensors correctly identified 10 objects with over 90 percent accuracy, even when an object slipped out of grip.
“Unlike many other soft tactile sensors, ours can be rapidly fabricated, retrofitted into grippers, and show sensitivity and reliability,” says MIT postdoc Josie Hughes, the lead author on a new paper about the sensors. “We hope they provide a new method of soft sensing that can be applied to a wide range of different applications in manufacturing settings, like packing and lifting.” 
In a second paper, a group of researchers created a soft robotic finger called “GelFlex” that uses embedded cameras and deep learning to enable high-resolution tactile sensing and “proprioception” (awareness of positions and movements of the body). 
The gripper, which looks much like a two-finger cup gripper you might see at a soda station, uses a tendon-driven mechanism to actuate the fingers. When tested on metal objects of various shapes, the system had over 96 percent recognition accuracy. 
“Our soft finger can provide high accuracy on proprioception and accurately predict grasped objects, and also withstand considerable impact without harming the interacted environment and itself,” says Yu She, lead author on a new paper on GelFlex. “By constraining soft fingers with a flexible exoskeleton, and performing high-resolution sensing with embedded cameras, we open up a large range of capabilities for soft manipulators.” 
Magic ball senses 
The magic ball gripper is made from a soft origami structure, encased by a soft balloon. When a vacuum is applied to the balloon, the origami structure closes around the object, and the gripper deforms to its structure. 
While this motion lets the gripper grasp a much wider range of objects than ever before, such as soup cans, hammers, wine glasses, drones, and even a single broccoli floret, the greater intricacies of delicacy and understanding were still out of reach — until they added the sensors.  
When the sensors experience force or strain, the internal pressure changes, and the team can measure this change in pressure to identify when it will feel that again. 
In addition to the latex sensor, the team also developed an algorithm which uses feedback to let the gripper possess a human-like duality of being both strong and precise — and 80 percent of the tested objects were successfully grasped without damage. 
The team tested the gripper-sensors on a variety of household items, ranging from heavy bottles to small, delicate objects, including cans, apples, a toothbrush, a water bottle, and a bag of cookies. 
Going forward, the team hopes to make the methodology scalable, using computational design and reconstruction methods to improve the resolution and coverage using this new sensor technology. Eventually, they imagine using the new sensors to create a fluidic sensing skin that shows scalability and sensitivity. 
Hughes co-wrote the new paper with Rus, which they will present virtually at the 2020 International Conference on Robotics and Automation. 
GelFlex
In the second paper, a CSAIL team looked at giving a soft robotic gripper more nuanced, human-like senses. Soft fingers allow a wide range of deformations, but to be used in a controlled way there must be rich tactile and proprioceptive sensing. The team used embedded cameras with wide-angle “fisheye” lenses that capture the finger’s deformations in great detail.
To create GelFlex, the team used silicone material to fabricate the soft and transparent finger, and put one camera near the fingertip and the other in the middle of the finger. Then, they painted reflective ink on the front and side surface of the finger, and added LED lights on the back. This allows the internal fish-eye camera to observe the status of the front and side surface of the finger. 
The team trained neural networks to extract key information from the internal cameras for feedback. One neural net was trained to predict the bending angle of GelFlex, and the other was trained to estimate the shape and size of the objects being grabbed. The gripper could then pick up a variety of items such as a Rubik’s cube, a DVD case, or a block of aluminum. 
During testing, the average positional error while gripping was less than 0.77 millimeter, which is better than that of a human finger. In a second set of tests, the gripper was challenged with grasping and recognizing cylinders and boxes of various sizes. Out of 80 trials, only three were classified incorrectly. 
In the future, the team hopes to improve the proprioception and tactile sensing algorithms, and utilize vision-based sensors to estimate more complex finger configurations, such as twisting or lateral bending, which are challenging for common sensors, but should be attainable with embedded cameras.
Yu She co-wrote the GelFlex paper with MIT graduate student Sandra Q. Liu, Peiyu Yu of Tsinghua University, and MIT Professor Edward Adelson. They will present the paper virtually at the 2020 International Conference on Robotics and Automation.



",Giving soft robots feeling,2020-06-01,['Rachel Gordon'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Robotics/Electrical engineering and computer science (EECS)/Research/Algorithms/Distributed Robotics Laboratory/School of Engineering/Artificial intelligence/Sensors/Robots/MIT Schwarzman College of Computing,"['feeling', 'gripper', 'range', 'tactile', 'giving', 'robots', 'sensors', 'soft', 'finger', 'paper', 'objects', 'sensing', 'team']","One of the hottest topics in robotics is the field of soft robots, which utilizes squishy and flexible materials rather than traditional rigid materials.
But soft robots have been limited due to their lack of good sensing.
A good robotic gripper needs to feel what it is touching (tactile sensing), and it needs to sense the positions of its fingers (proprioception).
Such sensing has been missing from most soft robots.
GelFlexIn the second paper, a CSAIL team looked at giving a soft robotic gripper more nuanced, human-like senses.",Mit
123,https://news.mit.edu/2020/algorithm-simulates-roll-loaded-dice-0528,"



The fast and efficient generation of random numbers has long been an important challenge. For centuries, games of chance have relied on the roll of a die, the flip of a coin, or the shuffling of cards to bring some randomness into the proceedings. In the second half of the 20th century, computers started taking over that role, for applications in cryptography, statistics, and artificial intelligence, as well as for various simulations — climatic, epidemiological, financial, and so forth.
MIT researchers have now developed a computer algorithm that might, at least for some tasks, churn out random numbers with the best combination of speed, accuracy, and low memory requirements available today. The algorithm, called the Fast Loaded Dice Roller (FLDR), was created by MIT graduate student Feras Saad, Research Scientist Cameron Freer, Professor Martin Rinard, and Principal Research Scientist Vikash Mansinghka, and it will be presented next week at the 23rd International Conference on Artificial Intelligence and Statistics. 
Simply put, FLDR is a computer program that simulates the roll of dice to produce random integers. The dice can have any number of sides, and they are “loaded,” or weighted, to make some sides more likely to come up than others. A loaded die can still yield random numbers — as one cannot predict in advance which side will turn up — but the randomness is constrained to meet a preset probability distribution. One might, for instance, use loaded dice to simulate the outcome of a baseball game; while the superior team is more likely to win, on a given day either team could end up on top.
With FLDR, the dice are “perfectly” loaded, which means they exactly achieve the specified probabilities. With a four-sided die, for example, one could arrange things so that the numbers 1,2,3, and 4 turn up exactly 23 percent, 34 percent, 17 percent, and 26 percent of the time, respectively.
To simulate the roll of loaded dice that have a large number of sides, the MIT team first had to draw on a simpler source of randomness — that being a computerized (binary) version of a coin toss, yielding either a 0 or a 1, each with 50 percent probability. The efficiency of their method, a key design criterion, depends on the number of times they have to tap into this random source — the number of “coin tosses,” in other words — to simulate each dice roll. 
In a landmark 1976 paper, the computer scientists Donald Knuth and Andrew Yao devised an algorithm that could simulate the roll of loaded dice with the maximum efficiency theoretically attainable. “While their algorithm was optimally efficient with respect to time,” Saad explains, meaning that literally nothing could be faster, “it is inefficient in terms of the space, or computer memory, needed to store that information.” In fact, the amount of memory required grows exponentially, depending on the number of sides on the dice and other factors. That renders the Knuth-Yao method impractical, he says, except for special cases, despite its theoretical importance.
FLDR was designed for greater utility. “We are almost as time efficient,” Saad says, “but orders of magnitude better in terms of memory efficiency.” FLDR can use up to 10,000 times less memory storage space than the Knuth-Yao approach, while taking no more than 1.5 times longer per operation.
For now, FLDR’s main competitor is the Alias method, which has been the field’s dominant technology for decades. When analyzed theoretically, according to Freer, FLDR has one clear-cut advantage over Alias: It makes more efficient use of the random source — the “coin tosses,” to continue with that metaphor — than Alias. In certain cases, moreover, FLDR is also faster than Alias in generating rolls of loaded dice.
FLDR, of course, is still brand new and has not yet seen widespread use. But its developers are already thinking of ways to improve its effectiveness through both software and hardware engineering. They also have specific applications in mind, apart from the general, ever-present need for random numbers. Where FLDR can help most, Mansinghka suggests, is by making so-called Monte Carlo simulations and Monte Carlo inference techniques more efficient. Just as FLDR uses coin flips to simulate the more complicated roll of weighted, many-sided dice, Monte Carlo simulations use a dice roll to generate more complex patterns of random numbers. 
The United Nations, for instance, runs simulations of seismic activity that show when and where earthquakes, tremors, or nuclear tests are happening on the globe. The United Nations also carries out Monte Carlo inference: running random simulations that generate possible explanations for actual seismic data. This works by conducting a second series of Monte Carlo simulations, which randomly test out alternative parameters for an underlying seismic simulation to find the parameter values most likely to reproduce the observed data. These parameters contain information about when and where earthquakes and nuclear tests might actually have occurred. 
“Monte Carlo inference can require hundreds of thousands of times more random numbers than Monte Carlo simulations,” Mansinghka says. “That’s one big bottleneck where FLDR could really help. Monte Carlo simulation and inference algorithms are also central to probabilistic programming, an emerging area of AI with broad applications.” 
Ryan Rifkin, Director of Research at Google, sees great potential for FLDR in this regard. “Monte Carlo inference algorithms are central to modern AI engineering … and to large-scale statistical modeling,” says Rifkin, who was not involved in the study. “FLDR is an extremely promising development that may lead to ways to speed up the fundamental building blocks of random number generation, and might help Google make Monte Carlo inference significantly faster and more energy efficient.”
Despite its seemingly bright future, FLDR almost did not come to light. Hints of it first emerged from a previous paper the same four MIT researchers published at a symposium in January, which introduced a separate algorithm. In that work, the authors showed that if a predetermined amount of memory were allocated for a computer program to simulate the roll of loaded dice, their algorithm could determine the minimum amount of “error” possible — that is, how close one comes toward meeting the designated probabilities for each side of the dice. 
If one doesn’t limit the memory in advance, the error can be reduced to zero, but Saad noticed a variant with zero error that used substantially less memory and was nearly as fast. At first he thought the result might be too trivial to bother with. But he mentioned it to Freer who assured Saad that this avenue was worth pursuing. FLDR, which is error-free in this same respect, arose from those humble origins and now has a chance of becoming a leading technology in the realm of random number generation. That’s no trivial matter given that we live in a world that’s governed, to a large extent, by random processes — a principle that applies to the distribution of galaxies in the universe, as well as to the outcome of a spirited game of craps.



",Algorithm quickly simulates a roll of loaded dice,2020-05-28,['Steve Nadis'],Research/Computer Science and Artificial Intelligence Laboratory/Electrical Engineering & Computer Science (eecs)/Mathematics/Brain and cognitive sciences/School of Engineering/School of Science/Algorithms/Data/MIT Schwarzman College of Computing/Cyber security,"['simulates', 'algorithm', 'simulations', 'memory', 'loaded', 'fldr', 'monte', 'number', 'roll', 'quickly', 'random', 'dice', 'carlo']","Simply put, FLDR is a computer program that simulates the roll of dice to produce random integers.
In a landmark 1976 paper, the computer scientists Donald Knuth and Andrew Yao devised an algorithm that could simulate the roll of loaded dice with the maximum efficiency theoretically attainable.
Where FLDR can help most, Mansinghka suggests, is by making so-called Monte Carlo simulations and Monte Carlo inference techniques more efficient.
Just as FLDR uses coin flips to simulate the more complicated roll of weighted, many-sided dice, Monte Carlo simulations use a dice roll to generate more complex patterns of random numbers.
“Monte Carlo inference can require hundreds of thousands of times more random numbers than Monte Carlo simulations,” Mansinghka says.",Mit
124,https://news.mit.edu/2020/making-nuclear-energy-cost-competitive-0527,"


Nuclear energy is a low-carbon energy source that is vital to decreasing carbon emissions. A critical factor in its continued viability as a future energy source is finding novel and innovative ways to improve operations and maintenance (O&M) costs in the next generation of advanced reactors. The U.S. Department of Energy’s Advanced Research Projects Agency-Energy (ARPA-E) established the Generating Electricity Managed by Intelligent Nuclear Assets (GEMINA) program to do exactly this. Through $27 million in funding, GEMINA is accelerating research, discovery, and development of new digital technologies that would produce effective and sustainable reductions in O&M costs.
Three MIT research teams have received APRA-E GEMINA awards to generate critical data and strategies to reduce O&M costs for the next generation of nuclear power plants to make them more economical, flexible, and efficient. The MIT teams include researchers from Department of Nuclear Science and Engineering (NSE), the Department of Civil and Environmental Engineering, and the MIT Nuclear Reactor Laboratory. By leveraging state-of-art in high-fidelity simulations and unique MIT research reactor capabilities, the MIT-led teams will collaborate with leading industry partners with practical O&M experience and automation to support the development of digital twins. Digital twins are virtual replicas of physical systems that are programmed to have the same properties, specifications, and behavioral characteristics as actual systems. The goal is to apply artificial intelligence, advanced control systems, predictive maintenance, and model-based fault detection within the digital twins to inform the design of O&M frameworks for advanced nuclear power plants.
In a project focused on developing high-fidelity digital twins for the critical systems in advanced nuclear reactors, NSE professors Emilio Baglietto and Koroush Shirvan will collaborate with researchers from GE Research and GE Hitachi. The GE Hitachi BWRX-300, a small modular reactor designed to provide flexible energy generation, will serve as a reference design. BWRX-300 is a promising small modular reactor concept that aims to be competitive with natural gas to realize market penetration in the United States. The team will assemble, validate, and exercise high-fidelity digital twins of the BWRX-300 systems. Digital twins address mechanical and thermal fatigue failure modes that drive O&M activities well beyond selected BWRX-300 components and extend to all advanced reactors where a flowing fluid is present. The role of high-fidelity resolution is central to the approach, as it addresses the unique challenges of the nuclear industry.
NSE will leverage the tremendous advancements they have achieved in recent years to accelerate the transition of the nuclear industry toward high-fidelity simulations in the form of computational fluid dynamics. The high spatial and time resolution accuracy of the simulations, combined with the AI-enabled digital twins, offer the opportunity to deliver predictive maintenance approaches that can greatly reduce the operating cost of nuclear stations. GE Research represents an ideal partner, given their tremendous experience in developing digital twins and close link to GE Hitachi and BWRX-300 design team. This team is particularly well position to tackle regulatory challenges of applying digital twins to safety-grade components through explicit characterization of uncertainties. This three-year MIT-led project is supported by an award of $1,787,065.
MIT Principal Research Engineer and Interim Director of the Nuclear Reactor Lab Gordon Kohse will lead a collaboration with MPR Associates to generate critical irradiation data to be used in digital twinning of molten-salt reactors (MSRs). MSRs produce radioactive materials when nuclear fuel is dissolved in a molten salt at high temperature and undergoes fission as it flows through the reactor core. Understanding the behavior of these radioactive materials is important for MSR design and for predicting and reducing O&M costs — a vital step in bringing safe, clean, next-generation nuclear power to market. The MIT-led team will use the MIT nuclear research reactor's unique capability to provide data to determine how radioactive materials are generated and transported in MSR components. Digital twins of MSRs will require this critical data, which is currently unavailable. The MIT team will monitor radioactivity during and after irradiation of molten salts containing fuel in materials that will be used in MSR construction. Along with Kohse, the MIT research team includes David Carpenter and Kaichao Sun from the MIT Nuclear Reactor Laboratory, and Charles Forsberg and Professor Mingda Li from NSE. Storm Kauffman and the MPR Associates team bring a wealth of nuclear industry experience to the project and will ensure that the data generated aligns with the needs of reactor developers. This two-year project is supported by an award of $899,825.
In addition to these two MIT-led projects, a third MIT team will work closely with the Electric Power Research Institute (EPRI) on a new paradigm for reducing advanced reactor O&M. This is a proof-of-concept study that will explore how to move away from the traditional maintenance and repair approach. The EPRI-led project will examine a “replace and refurbish” model in which components are intentionally designed and tested for shorter and more predictable lifetimes with the potential for game-changing O&M cost savings. This approach is similar to that adopted by the commercial airline industry, in which multiple refurbishments — including engine replacement — can keep a jet aircraft flying economically over many decades. The study will evaluate several advanced reactor designs with respect to cost savings and other important economic benefits, such as increased sustainability for suppliers. The MIT team brings together Jeremy Gregory from the Department of Civil and Environmental Engineering, Lance Snead from the Nuclear Reactor Laboratory, and professors Jacopo Buongiorno and Koroush Shirvan from NSE. 
“This collaborative project will take a fresh look at reducing the operation and maintenance cost by allowing nuclear technology to better adapt to the ever-changing energy market conditions. MIT's role is to identify cost-reducing pathways that would be applicable across a range of promising advanced reactor technologies. Particularly, we need to incorporate latest advancements in material science and engineering along with civil structures in our strategies,"" says MIT project lead Shirvan.
The advances by these three MIT teams, along with the six other awardees in the GEMINA program, will provide a framework for more streamlined O&M costs for next-generation advanced nuclear reactors — a critical factor to being competitive with alternative energy sources.


",Making nuclear energy cost-competitive,2020-05-27,[],Nuclear science and engineering/Civil and environmental engineering/School of Engineering/Nuclear power and reactors/Research/Energy/Climate change/Sustainability/Funding,"['reactor', 'digital', 'research', 'om', 'energy', 'twins', 'making', 'nuclear', 'costcompetitive', 'advanced', 'project', 'mit', 'team']","Nuclear energy is a low-carbon energy source that is vital to decreasing carbon emissions.
The MIT teams include researchers from Department of Nuclear Science and Engineering (NSE), the Department of Civil and Environmental Engineering, and the MIT Nuclear Reactor Laboratory.
The MIT-led team will use the MIT nuclear research reactor's unique capability to provide data to determine how radioactive materials are generated and transported in MSR components.
Along with Kohse, the MIT research team includes David Carpenter and Kaichao Sun from the MIT Nuclear Reactor Laboratory, and Charles Forsberg and Professor Mingda Li from NSE.
MIT's role is to identify cost-reducing pathways that would be applicable across a range of promising advanced reactor technologies.",Mit
125,https://news.mit.edu/2020/undergraduates-develop-next-generation-intelligence-tools-0526,"


The coronavirus pandemic has driven us apart physically while reminding us of the power of technology to connect. When MIT shut its doors in March, much of campus moved online, to virtual classes, labs, and chatrooms. Among those making the pivot were students engaged in independent research under MIT’s Undergraduate Research Opportunities Program (UROP). 
With regular check-ins with their advisors via Slack and Zoom, many students succeeded in pushing through to the end. One even carried on his experiments from his bedroom, after schlepping his Sphero Bolt robots home in a backpack. “I’ve been so impressed by their resilience and dedication,” says Katherine Gallagher, one of three artificial intelligence engineers at MIT Quest for Intelligence who works with students each semester on intelligence-related applications. “There was that initial week of craziness and then they were right back to work.” Four projects from this spring are highlighted below.
Learning to explore the world with open eyes and ears
Robots rely heavily on images beamed through their built-in cameras, or surrogate “eyes,” to get around. MIT senior Alon Kosowsky-Sachs thinks they could do a lot more if they also used their microphone “ears.” 
From his home in Sharon, Massachusetts, where he retreated after MIT closed in March, Kosowsky-Sachs is training four baseball-sized Sphero Bolt robots to roll around a homemade arena. His goal is to teach the robots to pair sights with sounds, and to exploit this information to build better representations of their environment. He's working with Pulkit Agrawal, an assistant professor in MIT’s Department of Electrical Engineering and Computer Science, who is interested in designing algorithms with human-like curiosity.
While Kosowsky-Sachs sleeps, his robots putter away, gliding through an object-strewn rink he built for them from two-by-fours. Each burst of movement becomes a pair of one-second video and audio clips. By day, Kosowsky-Sachs trains a “curiosity” model aimed at pushing the robots to become bolder, and more skillful, at navigating their obstacle course.
“I want them to see something through their camera, and hear something from their microphone, and know that these two things happen together,” he says. “As humans, we combine a lot of sensory information to get added insight about the world. If we hear a thunder clap, we don’t need to see lightning to know that a storm has arrived. Our hypothesis is that robots with a better model of the world will be able to accomplish more difficult tasks.”
Training a robot agent to design a more efficient nuclear reactor 
One important factor driving the cost of nuclear power is the layout of its reactor core. If fuel rods are arranged in an optimal fashion, reactions last longer, burn less fuel, and need less maintenance. As engineers look for ways to bring down the cost of nuclear energy, they are eying the redesign of the reactor core.
“Nuclear power emits very little carbon and is surprisingly safe compared to other energy sources, even solar or wind,” says third-year student Isaac Wolverton. “We wanted to see if we could use AI to make it more efficient.” 
In a project with Josh Joseph, an AI engineer at the MIT Quest, and Koroush Shirvan, an assistant professor in MIT’s Department of Nuclear Science and Engineering, Wolverton spent the year training a reinforcement learning agent to find the best way to lay out fuel rods in a reactor core. To simulate the process, he turned the problem into a game, borrowing a machine learning technique for producing agents with superhuman abilities at chess and Go.
He started by training his agent on a simpler problem: arranging colored tiles on a grid so that as few tiles as possible of the same color would touch. As Wolverton increased the number of options, from two colors to five, and four tiles to 225, he grew excited as the agent continued to find the best strategy. “It gave us hope we could teach it to swap the cores into an optimal arrangement,” he says.
Eventually, Wolverton moved to an environment meant to simulate a 36-rod reactor core, with two enrichment levels and 2.1 million possible core configurations. With input from researchers in Shirvan’s lab, Wolverton trained an agent that arrived at the optimal solution.
The lab is now building on Wolverton's code to try to train an agent in a life-sized 100-rod environment with 19 enrichment levels. “There’s no breakthrough at this point,” he says. “But we think it’s possible, if we can find enough compute resources.”
Making more livers available to patients who need them
About 8,000 patients in the United States receive liver transplants each year, but that’s only half the number who need one. Many more livers might be made available if hospitals had a faster way to screen them, researchers say. In a collaboration with Massachusetts General Hospital, MIT Quest is evaluating whether automation could help to boost the nation’s supply of viable livers.  
In approving a liver for transplant, pathologists estimate its fat content from a slice of tissue. If it’s low enough, the liver is deemed ready for transplant. But there are often not enough qualified doctors to review tissue samples on the tight timeline needed to match livers with recipients. A shortage of doctors, coupled with the subjective nature of analyzing tissue, means that viable livers are inevitably discarded.
This loss represents a huge opportunity for machine learning, says third-year student Kuan Wei Huang, who joined the project to explore AI applications in health care. The project involves training a deep neural network to pick out globules of fat on liver tissue slides to estimate the liver’s overall fat content.
One challenge, says Huang, has been figuring out how to handle variations in how various pathologists classify fat globules. “This makes it harder to tell whether I’ve created the appropriate masks to feed into the neural net,” he says. “However, after meeting with experts in the field, I received clarifications and was able to continue working.”
Trained on images labeled by pathologists, the model will eventually learn to isolate fat globules in unlabeled images on its own. The final output will be a fat content estimate with pictures of highlighted fat globules showing how the model arrived at its final count. “That’s the easy part — we just count up the pixels in the highlighted globules as a percentage of the overall biopsy and we have our fat content estimate,” says the Quest’s Gallagher, who is leading the project.
Huang says he’s excited by the project’s potential to help people. “Using machine learning to address medical problems is one of the best ways that a computer scientist can impact the world.”
Exposing the hidden constraints of what we mean in what we say
Language shapes our understanding of the world in subtle ways, with slight variations in the words we use conveying sharply different meanings. The sentence, “Elephants live in Africa and Asia,” looks a lot like the sentence “Elephants eat twigs and leaves.” But most readers will conclude that the elephants in the first sentence are split into distinct groups living on separate continents but not apply the same reasoning to the second sentence, because eating twigs and eating leaves can both be true of the same elephant in a way that living on different continents cannot.
Karen Gu is a senior majoring in computer science and molecular biology, but instead of putting cells under a microscope for her SuperUROP project, she chose to look at sentences like the ones above. “I’m fascinated by the complex and subtle things that we do to constrain language understanding, almost all of it subconsciously,” she says.
Working with Roger Levy, a professor in MIT’s Department of Brain and Cognitive Sciences, and postdoc MH Tessler, Gu explored how prior knowledge guides our interpretation of syntax and ultimately, meaning. In the sentences above, prior knowledge about geography and mutual exclusivity interact with syntax to produce different meanings.
After steeping herself in linguistics theory, Gu built a model to explain how, word by word, a given sentence produces meaning. She then ran a set of online experiments to see how human subjects would interpret analogous sentences in a story. Her experiments, she says, largely validated intuitions from linguistic theory.
One challenge, she says, was having to reconcile two approaches for studying language. “I had to figure out how to combine formal linguistics, which applies an almost mathematical approach to understanding how words combine, and probabilistic semantics-pragmatics, which has focused more on how people interpret whole utterances.’ ""
After MIT closed in March, she was able to finish the project from her parents’ home in East Hanover, New Jersey. “Regular meetings with my advisor have been really helpful in keeping me motivated and on track,” she says. She says she also got to improve her web-development skills, which will come in handy when she starts work at Benchling, a San Francisco-based software company, this summer.
Spring semester Quest UROP projects were funded, in part, by the MIT-IBM Watson AI Lab and Eric Schmidt, technical advisor to Alphabet Inc., and his wife, Wendy.


",Undergraduates develop next-generation intelligence tools,2020-05-26,['Kim Martineau'],Quest for Intelligence/Undergraduate Research Opportunities Program (UROP)/Electrical engineering and computer science (EECS)/Nuclear science and engineering/SuperUROP/Brain and cognitive sciences/School of Engineering/School of Science/Artificial intelligence/Algorithms/Computer science and technology/Machine learning/Students/Undergraduate/MIT-IBM Watson AI Lab/MIT Schwarzman College of Computing/Pandemic,"['develop', 'model', 'livers', 'tools', 'agent', 'robots', 'undergraduates', 'fat', 'sentence', 'globules', 'intelligence', 'nextgeneration', 'wolverton', 'project', 'mit']","One even carried on his experiments from his bedroom, after schlepping his Sphero Bolt robots home in a backpack.
“I’ve been so impressed by their resilience and dedication,” says Katherine Gallagher, one of three artificial intelligence engineers at MIT Quest for Intelligence who works with students each semester on intelligence-related applications.
The project involves training a deep neural network to pick out globules of fat on liver tissue slides to estimate the liver’s overall fat content.
One challenge, says Huang, has been figuring out how to handle variations in how various pathologists classify fat globules.
The final output will be a fat content estimate with pictures of highlighted fat globules showing how the model arrived at its final count.",Mit
126,https://news.mit.edu/2020/microsoft-president-brad-smith-talks-data-covid-19-why-we-should-worry-about-digital-9-11-0521,"


In a virtual discussion hosted by MIT last week, viewers learned that there are many problems that concern Microsoft President Brad Smith: things like climate change, Covid-19, and the work of the future.
Attendees also learned how seriously he takes the issue of computer security: 45 minutes into the event, his Windows system automatically rebooted for a lightning-quick software update.
“There are a lot of benefits to working from home,” he said with a laugh after rejoining, “but it certainly also adds a level of unpredictability.”
Smith’s conversation with MIT Professor Daniela Rus on May 14 spanned a wide range of topics, from the challenges of Covid-19 to the security of online voting. The fireside chat was held as part of MIT’s “Hot Topics in Computing” series, founded by the Computer Science and Artificial Intelligence Laboratory (CSAIL). The series is now an Institute-wide effort being co-presented with the MIT Stephen A. Schwarzman College of Computing (SCC). SCC Dean Dan Huttenlocher opened the event with a welcome to the audience and introduced Smith and Rus.







Play video






Having worked at Microsoft for 25 years and served as its president since 2015, Smith gave an inside look at what it’s been like to be there in recent months — and what the tech company has done to try to help curb the spread of Covid-19. 
In March, for example, Microsoft developed a chatbot to help people determine if they might need a Covid-19 test. Within weeks of deploying the app at a hospital in Seattle, Washington, the company started rolling it out more broadly. The chatbot was ultimately used 190 million times in April, and is now available at 1,500 institutions across 23 countries. 
Smith also discussed the promising work being done with Bluetooth-based contact tracing, but expressed skepticism that it could be adopted at a meaningful scale.
“Not everyone is going to walk around with an app on their phone,” he said. “I think we should recognize that it is a tool, and not a panacea.”
Smith and his colleague Carol Ann Browne recently co-wrote the book “Tools and Weapons,"" which examines the promise and peril of technology for both good and bad. The authors draw on lessons in history, from Edward Snowden’s revelations about government surveillance to the Cambridge Analytica scandal, to explore the future of technology and how it needs to be managed. 
While he agreed with Rus’ assessment of the book as one “where the geeks are the heroes,” Smith also warned of the dangers of a future “digital 9/11” with respect to the electric grid and future presidential elections. 
“You don’t need to be a PhD in computer science to have an important role to play,” he said. “As consumers, citizens, and voters, we’re at a point of time where we’d all benefit from being better informed.” 
The long-time sustainability advocate also spoke about Microsoft’s ambitious goals to not just be carbon-negative by 2030, but to remove all of the carbon that the company has emitted since 1975 in the next 30 years. At a higher level, Smith advocated for making what he calls “the biggest R&D investment of our century” to develop new techniques to remove carbon from the environment. 
“We’re going to need huge breakthroughs in the next three decades if we are going to achieve this fundamental goal of protecting the planet the way we need to,” he said.
In the hourlong conversation, Smith often implored his audience of computer scientists and technologists to recognize the responsibility they have to solve tangible real-world problems. He pointed out that, for all the buzz about big data over the last 20 years, the Covid-19 pandemic has actually led to government officials making decisions directly informed by data.
“Data is running the economy and deciding who can leave their house and who needs to stay home,” Smith said. “The world will need the kind of technology that we can create … [and] you all have an opportunity to make a more positive impact in the world than perhaps any generation of MIT students has had before. How can that not get you excited about getting up in the morning?”


","Microsoft President Brad Smith talks data, Covid-19, and a potential “digital 9/11”",2020-05-21,['Adam Conner-Simons'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/MIT Schwarzman College of Computing/Special events and guest speakers/Technology and society/Computer science and technology/School of Engineering/Electrical engineering and computer science (EECS)/Covid-19/Pandemic/Cyber security/Public health/Industry,"['company', 'data', 'technology', 'digital', 'talks', 'smith', 'future', 'computer', 'need', '911', 'potential', 'brad', 'president', 'microsoft', 'covid19', 'mit', 'going']","The series is now an Institute-wide effort being co-presented with the MIT Stephen A. Schwarzman College of Computing (SCC).
SCC Dean Dan Huttenlocher opened the event with a welcome to the audience and introduced Smith and Rus.
In a virtual discussion hosted by MIT last week, viewers learned that there are many problems that concern Microsoft President Brad Smith: things like climate change, Covid-19, and the work of the future.
In March, for example, Microsoft developed a chatbot to help people determine if they might need a Covid-19 test.
“Data is running the economy and deciding who can leave their house and who needs to stay home,” Smith said.",Mit
127,https://news.mit.edu/2020/fireflies-meetings-0521,"


Many decisions are made and details sorted out in a productive business meeting. But in order for that meeting to translate into results, participants have to remember all those details, understand their assignments, and follow through on commitments.The startup Fireflies.ai is helping people get the most out of their meetings with a note-taking, information-organizing virtual assistant named Fred. Fred transcribes every word of meetings and then uses artificial intelligence to help people sort and share that information later on.“There’s a tremendous amount of data generated in meetings that can help your team stay on the same page,” says Sam Udotong ’16, who founded the company with Krish Ramineni in 2016. “We let people capture that data, search through it, and then share it to the places that matter most.”The tool integrates with popular meeting and scheduling software like Zoom and Google Calendar so users can quickly add Fred to calls. It also works with collaboration platforms like Slack and customer management software like Salesforce to help ensure plans turn into coordinated action.Fireflies is used by people working in roles including sales, recruiting, and product management. They can use the service to automate project management tasks, screen candidates, and manage internal team communications.In the last few months, driven in part by the Covid-19 pandemic, Fred has sat through millions of minutes of meetings involving more than half a million people. And the founders believe Fred can do more than simply help people adjust to remote work; it can also help them collaborate more effectively than ever before.“[Fred] is giving you perfect memory,” says Udotong, who serves as Firelies’ chief technology officer. “The dream is for everyone to have perfect recall and make all their decisions based on the right information. So being able to search back to exact points in conversation and remember that is powerful. People have told us it makes them look smarter in front of clients.”Taking the leapUdotong was introduced to the power of machine learning in his first year at MIT while working on a project in which students built a drone that could lead people on campus tours. Later, during his first MIT hackathon, he sought to use machine learning in a cryptography solution. That’s when he met Ramineni, who was a student at the University of Pennsylvania. That's also when Fireflies was born — although the founders would go on to change everything about the company besides its name as they sought to use artificial intelligence to improve efficiency in a range of fields.“We ended up building six iterations of Fireflies before this current meeting assistant,” Udotong remembers. “And every time we would build a different iteration, we would tell our friends, ‘Download it, use it, and get back to us next week, we’ll grab coffee.’ We were making all these agreements and promises, and it became really challenging to keep track of all the conversations we were having to get our products out there. We thought, ‘What if we just had an AI that could keep track of conversations for us?’”The founders’ initial note-taking solution, built in short bursts between classes and homework, tracked action items written in messages, sending reminders to users later on.Following Udotong’s graduation with a degree in aeronautics and astronautics in 2016, the founders decided to use a $25,000 stipend they received from Rough Draft Ventures, along with $5,000 from the MIT Sandbox Innovation Fund, to work on Fireflies through the summer.The plan was to work on Fireflies for another short burst: Ramineni was already making plans to attend Cambridge University for his master’s degree in the fall, and Udotong was weighing acceptance letters from graduate schools as well as job offers. By July, however, the founders had changed their plans.“I think deciding [on a career path] is really hard these days, even if you identify your passion,” Udotong says. “The easy path for someone in tech is to follow the money and go work for Google or Facebook. We decided to go a different route and take the risk.”They moved to Ramineni’s hometown of San Francisco to officially launch the company. Udotong remembers getting to San Francisco with $100 dollars in his bank account.The founders had fully committed themselves to Fireflies, but it didn’t make starting the company any easier. They decided not to raise venture capital in the company’s early years, and Ramineni admits to questioning whether going all in on Fireflies was the right decision as recently at 2018.The founders also weren’t sure a radically new software category would be embraced so readily by businesses. They continued to invest in the voice AI space, as they believed that the need for their technology was growing and the timing was right.“We realized that there’s a ton of data generated every day through speech, either in meetings like Zoom or in person,” Ramineni says. “Today, two hours after your meeting, unless you’re taking good notes or recording, you’re not going to be able to recall everything. You might not even remember what action items you agreed to a few hours ago. It’s such a common problem that people don’t even know it’s an issue. You have meetings and you expect things to slip through the cracks.”Illuminating conversationsToday the Fireflies solution shows little trace of the arduous journey the founders took to get to this point. In fact, building simplicity into the tool has been a major focus for the founders.Fred can join calendar events automatically or be added to meetings using the fred@fireflies.ai address. Fred joins Zoom, Google Meet, Skype, or Microsoft calls as a participant, silently transcribing and generating notes from the meeting. After the meeting, the AI assistant sends a full transcript to whomever the organizer chooses, allowing users to click on sections of the transcript to hear that part of the meeting audio. Users can also search the transcript and go through an hourlong meeting in five minutes, according to the company. The transcript can also surface action items, tasks, metrics, pricing, and other topics of interest.After each meeting, Fireflies can automatically sync all this meeting data into apps from companies like Slack, Salesforce, and Hubspot.“Fireflies is like a personal assistant that helps connect your systems of communication with your systems of record,” Udotong says. “If you’re having these meetings over Zoom and Google Meet every day, and you’re interacting with Slack or Trello, Fireflies is that middle router that can bring synchronicity to your work life.”In the midst of the Covid-19 pandemic, millions of companies have been forced to operate remotely, and the founders think the impact of that response will be felt for far longer than the virus.“I think the world’s now realizing that people can be fully distributed,” says Ramineni, who notes Fireflies’ team has been remote since he and Udotong began working together in college hackathons from different campuses in 2014.And as the company has grown, customers have begun using Fred for use cases the founders hadn’t even considered, like sending Fred to meetings that they can’t attend and reviewing the notes later on. Customers, the founders believe, are realizing that being able to quickly search, sort, and otherwise collaborate across audio data unlocks a world of new possibilities.“It’s kind of like what Google did with search,” Udotong says. “There was five to 10 years of web data building up, and there was no way for people to find what they were looking for. The same thing is true today of audio and meeting data. It’s out there, but there’s no way to actually find what you’re looking for because it’s never even stored in the first place.”


",Fireflies helps companies get more out of meetings,2020-05-21,['Zach Winn'],Startups/Alumni/ae/Innovation and Entrepreneurship (I&E)/Machine learning/Artificial intelligence/Business and management/Human-computer interaction/Assistive technology,"['fireflies', 'udotong', 'data', 'founders', 'helps', 'ramineni', 'companies', 'youre', 'meetings', 'fred', 'company', 'meeting']","The startup Fireflies.ai is helping people get the most out of their meetings with a note-taking, information-organizing virtual assistant named Fred.
Fred transcribes every word of meetings and then uses artificial intelligence to help people sort and share that information later on.
After each meeting, Fireflies can automatically sync all this meeting data into apps from companies like Slack, Salesforce, and Hubspot.
“Fireflies is like a personal assistant that helps connect your systems of communication with your systems of record,” Udotong says.
The same thing is true today of audio and meeting data.",Mit
128,https://news.mit.edu/2020/machine-learning-develop-materials-0520,"


For engineers developing new materials or protective coatings, there are billions of different possibilities to sort through. Lab tests or even detailed computer simulations to determine their exact properties, such as toughness, can take hours, days, or more for each variation. Now, a new artificial intelligence-based approach developed at MIT could reduce that to a matter of milliseconds, making it practical to screen vast arrays of candidate materials.The system, which MIT researchers hope could be used to develop stronger protective coatings or structural materials — for example, to protect aircraft or spacecraft from impacts — is described in a paper in the journal Matter, by MIT postdoc Chi-Hua Yu, civil and environmental engineering professor and department head Markus J. Buehler, and Yu-Chuan Hsu at the National Taiwan University.The focus of this work was on predicting the way a material would break or fracture, by analyzing the propagation of cracks through the material’s molecular structure. Buehler and his colleagues have spent many years studying fractures and other failure modes in great detail, since understanding failure processes is key to developing robust, reliable materials. “One of the specialties of my lab is to use what we call molecular dynamics simulations, or basically atom-by-atom simulations” of such processes, Buehler says.These simulations provide a chemically accurate description of how fracturing happens, he says. But it’s slow, because it requires solving equations of motion for every single atom. “It takes a lot of time to simulate these processes,” he says. The team decided to explore ways of streamlining that process, using a machine-learning system.“We’re kind of taking a detour,” he says. “We’ve been asking, what if you had just the observation of how fracturing happens [in a given material], and let computers learn this relationship itself?” To do that, artificial intelligence (AI) systems need a variety of examples to use as a training set, to learn about the correlations between the material’s characteristics and its performance.In this case, they were looking at a variety of composite, layered coatings made of crystalline materials. The variables included the composition of the layers and the relative orientations of their orderly crystal structures, and the way those materials each responded to fracturing, based on the molecular dynamics simulations. “We basically simulate, atom by atom, how materials break, and we record that information,” Buehler says.

The team used atom-by-atom simulations to determine how cracks propagate through different materials. This animation shows one such simulation, in which the crack propagates all the way through.They painstakingly generated hundreds of such simulations, with a wide variety of structures, and subjected each one to many different simulated fractures. Then they fed large amounts of data about all these simulations into their AI system, to see if it could discover the underlying physical principles and predict the performance of a new material that was not part of the training set.And it did. “That’s the really exciting thing,” Buehler says, “because the computer simulation through AI can do what normally takes a very long time using molecular dynamics, or using finite element simulations, which are another way that engineers solve this problem, and it’s very slow as well. So, this is a whole new way of simulating how materials fail.”How materials fail is crucial information for any engineering project, Buehler emphasizes. Materials failures such as fractures are “one of the biggest reasons for losses in any industry. For inspecting planes or trains or cars, or for roads or infrastructure, or concrete, or steel corrosion, or to understand the fracture of biological tissues such as bone, the ability to simulate fracturing with AI, and doing that quickly and very efficiently, is a real game changer.”The improvement in speed produced by using this method is remarkable. Hsu explains that “for single simulations in molecular dynamics, it has taken several hours to run the simulations, but in this artificial intelligence prediction, it only takes 10 milliseconds to go through all the predictions from the patterns, and show how a crack forms step by step.”
""Over the past 30 years or so there have been multiple approaches to model crack propagation in solids, but it remains a formidable and computationally expensive problem,"" says Pradeep Guduru, a professor of engineering at Brown University, who was not involved in this work. ""By shifting the computational expense to training a robust machine-learning algorithm, this new approach can potentially result in a quick and computationally inexpensive design tool, which is always desirable for practical applications.""The method they developed is quite generalizable, Buehler says. “Even though in our paper we only applied it to one material with different crystal orientations, you can apply this methodology to much more complex materials.” And while they used data from atomistic simulations, the system could also be used to make predictions on the basis of experimental data such as images of a material undergoing fracturing.“If we had a new material that we’ve never simulated before,” he says, “if we have a lot of images of the fracturing process, we can feed that data into the machine-learning model as well.” Whatever the input, simulated or experimental, the AI system essentially goes through the evolving process frame by frame, noting how each image differs from the one before in order to learn the underlying dynamics.For example, as researchers make use of the new facilities in MIT.nano, the Institute’s facility dedicated to fabricating and testing materials at the nanoscale, vast amounts of new data about a variety of synthesized materials will be generated.“As we have more and more high-throughput experimental techniques that can produce a lot of images very quickly, in an automated way, these kind of data sources can immediately be fed into the machine-learning model,” Buehler says. “We really think that the future will be one where we have a lot more integration between experiment and simulation, much more than we have in the past.”The system could be applied not just to fracturing, as the team did in this initial demonstration, but to a wide variety of processes unfolding over time, he says, such as diffusion of one material into another, or corrosion processes. “Anytime where you have evolutions of physical fields, and we want to know how these fields evolve as a function of the microstructure,” he says, this method could be a boon.The research was supported by the U.S. Office of Naval Research and the Army Research Office.


",Machine-learning tool could help develop tougher materials,2020-05-20,['David L. Chandler'],Research/Biomaterials/Civil and environmental engineering/Materials Science and Engineering/Artificial intelligence/Machine learning/School of Engineering,"['data', 'simulations', 'materials', 'tougher', 'develop', 'processes', 'tool', 'fracturing', 'material', 'variety', 'way', 'help', 'buehler', 'system', 'machinelearning']","For engineers developing new materials or protective coatings, there are billions of different possibilities to sort through.
The focus of this work was on predicting the way a material would break or fracture, by analyzing the propagation of cracks through the material’s molecular structure.
“One of the specialties of my lab is to use what we call molecular dynamics simulations, or basically atom-by-atom simulations” of such processes, Buehler says.
In this case, they were looking at a variety of composite, layered coatings made of crystalline materials.
So, this is a whole new way of simulating how materials fail.”How materials fail is crucial information for any engineering project, Buehler emphasizes.",Mit
129,https://news.mit.edu/2020/mit-scientist-turns-to-entrepreneurship-pablo-ducru-0520,"


Like the atomic particles he studies, Pablo Ducru seems constantly on the move, vibrating with energy. But if he sometimes appears to be headed in an unexpected direction, Ducru, a doctoral candidate in nuclear science and computational engineering, knows exactly where he is going: “My goal is to address climate change as an innovator and creator, whether by pushing the boundaries of science” through research, says Ducru, or pursuing a zero-carbon future as an entrepreneur.
It can be hard catching up with Ducru. In January, he returned to Cambridge, Massachusetts, from Beijing, where he was spending a year earning a master’s degree in global affairs as a Schwarzman Scholar at Tsinghua University. He flew out just days before a travel crackdown in response to Covid-19.
“This year has been intense, juggling my PhD work and the master’s overseas,” he says. “But I needed to do it, to get a 360-degree understanding of the problem of climate change, which isn’t just a technological problem, but also one involving economics, trade, policy, and finance.”
Schwarzman Scholars, an international cohort selected on the basis of academic excellence and leadership potential, among other criteria, focus on critical challenges of the 21st century. While all the students must learn the basics of international relations and China’s role in the world economy, they can tailor their studies according to their interests.
Ducru is incorporating nuclear science into his master’s program. “It is at the core of many of the world’s key problems, from climate change to arms controls, and it also impacts artificial intelligence by advancing high-performance computing,” he says.
A Franco-Mexican raised in Paris, Ducru arrived at nuclear science by way of France’s selective academic system. He excelled in math, history, and English during his high school years. “I realized technology is what drives history,” he says. “I thought that if I wanted to make history, I needed to make technology.” He graduated from Ecole Polytechnique specializing in physics and applied mathematics, and with a major in energies of the 21st century.
Creating computational shortcuts
Today, as a member of MIT’s Computational Reactor Physics Group (CRPG), Ducru is deploying his expertise in singular ways to help solve some of the toughest problems in nuclear science.
Nuclear engineers, hoping to optimize efficiency and safety in current and next-generation reactor designs, are on a quest for high-fidelity nuclear simulations. At such fine-grained levels of modeling, the behavior of subatomic particles is sensitive to minute uncertainties in temperature change, or differences in reactor core geometry, for instance. To quantify such uncertainties, researchers currently need countless costly hours of supercomputer time to simulate the behaviors of billions of neutrons under varying conditions, estimating and then averaging outcomes.
“But with some problems, more computing won’t make a difference,” notes Ducru. “We have to help computers do the work in smarter ways.” To accomplish this task, he has developed new formulations for characterizing basic nuclear physics that make it much easier for a computer to solve problems: “I dig into the fundamental properties of physics to give nuclear engineers new mathematical algorithms that outperform thousands of times over the old ways of computing.”
With his novel statistical methods and algorithms, developed with CRPG colleagues and during summer stints at Los Alamos and Oak Ridge National Laboratories, Ducru offers “new ways of looking at problems that allow us to infer trends from uncertain inputs, such as physics, geometries, or temperatures,” he says.  
These innovative tools accommodate other kinds of problems that involve computing average behaviors from billions of individual occurrences, such as bubbles forming in a turbulent flow of reactor coolant. “My solutions are quite fundamental and problem-agnostic — applicable to the design of new reactors, to nuclear imaging systems for tumor detection, or to the plutonium battery of a Mars rover,” he says. “They will be useful anywhere scientists need to lower costs of high-fidelity nuclear simulations.""
But Ducru won’t be among the scientists deploying these computational advances. “I think we’ve done a good job, and others will continue in this area of research,” he says. “After six years of delving deep into quantum physics and statistics, I felt my next step should be a startup.”
Scaling up with shrimp
As he pivots away from academia and nuclear science, Ducru remains constant to his mission of addressing the climate problem. The result is Torana, a company Ducru and a partner started in 2018 to develop the financial products and services aquaculture needs to sustainably feed the world.
“I thought we could develop a scalable zero-carbon food,” he says. “The world needs high-nutrition proteins to feed growing populations in a climate-friendly way, especially in developing nations.” 
Land-based protein sources such as livestock can take a heavy toll on the environment. But shrimp, on the other hand, are “very efficient machines, scavenging crud at the bottom of the ocean and converting it into high-quality protein,” notes Ducru, who received the 2018 MIT Water Innovation Prize and the 2019 Rabobank-MIT Food and Agribusiness Prize, and support from MIT Sandbox to help develop his aquaculture startup (then called Velaron).
Torana is still in early stages, and Ducru hopes to apply his modeling expertise to build a global system of sustainable shrimp farming. His Schwarzman master thesis studies the role of aquaculture in our future global food system, with a focus on the shrimp supply chain.
In response to the Covid-19 pandemic, Ducru relocated to the family farm in southern France, which he helps run while continuing to follow the Tsinghua masters online and work on his MIT PhD. He is tweaking his business plans, and putting the final touches on his PhD research, including submitting several articles for publication. While it’s been challenging keeping all these balls in the air, he has supportive mentors — “Benoit Forget [CRPG director] has backed almost all my crazy ideas,” says Ducru. “People like him make MIT the best university on Earth.”
Ducru is already mapping out his next decade or so: grow his startup, and perhaps create a green fund that could underwrite zero-carbon projects, including nuclear ones. “I don’t have Facebook and don’t watch online series or TV, because I prefer being an actor, creating things through my work,” he says. “I’m a scientific entrepreneur, and will continue to innovate across different realms.”


",A scientist turns to entrepreneurship,2020-05-20,['Leda Zimmerman'],"Nuclear science and engineering/School of Engineering/Research/Graduate, postdoctoral/Energy/Startups/Innovation and Entrepreneurship (I&E)/Profile/Nuclear power and reactors/Sustainability/Climate change","['reactor', 'ducru', 'ways', 'entrepreneurship', 'work', 'scientist', 'nuclear', 'turns', 'problems', 'physics', 'science', 'mit', 'masters']","“This year has been intense, juggling my PhD work and the master’s overseas,” he says.
Ducru is incorporating nuclear science into his master’s program.
A Franco-Mexican raised in Paris, Ducru arrived at nuclear science by way of France’s selective academic system.
Creating computational shortcutsToday, as a member of MIT’s Computational Reactor Physics Group (CRPG), Ducru is deploying his expertise in singular ways to help solve some of the toughest problems in nuclear science.
Nuclear engineers, hoping to optimize efficiency and safety in current and next-generation reactor designs, are on a quest for high-fidelity nuclear simulations.",Mit
130,https://news.mit.edu/2020/mit-marshaling-artificial-intelligence-fight-against-covid-19-0519,"


Artificial intelligence could play a decisive role in stopping the Covid-19 pandemic. To give the technology a push, the MIT-IBM Watson AI Lab is funding 10 projects at MIT aimed at advancing AI’s transformative potential for society. The research will target the immediate public health and economic challenges of this moment. But it could have a lasting impact on how we evaluate and respond to risk long after the crisis has passed. The 10 research projects are highlighted below.
Early detection of sepsis in Covid-19 patients 
Sepsis is a deadly complication of Covid-19, the disease caused by the new coronavirus SARS-CoV-2. About 10 percent of Covid-19 patients get sick with sepsis within a week of showing symptoms, but only about half survive.
Identifying patients at risk for sepsis can lead to earlier, more aggressive treatment and a better chance of survival. Early detection can also help hospitals prioritize intensive-care resources for their sickest patients. In a project led by MIT Professor Daniela Rus, researchers will develop a machine learning system to analyze images of patients’ white blood cells for signs of an activated immune response against sepsis.
Designing proteins to block SARS-CoV-2
Proteins are the basic building blocks of life, and with AI, researchers can explore and manipulate their structures to address longstanding problems. Take perishable food: The MIT-IBM Watson AI Lab recently used AI to discover that a silk protein made by honeybees could double as a coating for quick-to-rot foods to extend their shelf life.
In a related project led by MIT professors Benedetto Marelli and Markus Buehler, researchers will enlist the protein-folding method used in their honeybee-silk discovery to try to defeat the new coronavirus. Their goal is to design proteins able to block the virus from binding to human cells, and to synthesize and test their unique protein creations in the lab.
Saving lives while restarting the U.S. economy
Some states are reopening for business even as questions remain about how to protect those most vulnerable to the coronavirus. In a project led by MIT professors Daron Acemoglu, Simon Johnson and Asu Ozdaglar will model the effects of targeted lockdowns on the economy and public health.
In a recent working paper co-authored by Acemoglu, Victor Chernozhukov, Ivan Werning, and Michael Whinston, MIT economists analyzed the relative risk of infection, hospitalization, and death for different age groups. When they compared uniform lockdown policies against those targeted to protect seniors, they found that a targeted approach could save more lives. Building on this work, researchers will consider how antigen tests and contact tracing apps can further reduce public health risks.
Which materials make the best face masks?
Massachusetts and six other states have ordered residents to wear face masks in public to limit the spread of coronavirus. But apart from the coveted N95 mask, which traps 95 percent of airborne particles 300 nanometers or larger, the effectiveness of many masks remains unclear due to a lack of standardized methods to evaluate them.
In a project led by MIT Associate Professor Lydia Bourouiba, researchers are developing a rigorous set of methods to measure how well homemade and medical-grade masks do at blocking the tiny droplets of saliva and mucus expelled during normal breathing, coughs, or sneezes. The researchers will test materials worn alone and together, and in a variety of configurations and environmental conditions. Their methods and measurements will determine how well materials protect mask wearers and the people around them.
Treating Covid-19 with repurposed drugs
As Covid-19’s global death toll mounts, researchers are racing to find a cure among already-approved drugs. Machine learning can expedite screening by letting researchers quickly predict if promising candidates can hit their target.
In a project led by MIT Assistant Professor Rafael Gomez-Bombarelli, researchers will represent molecules in three dimensions to see if this added spatial information can help to identify drugs most likely to be effective against the disease. They will use NASA’s Ames and U.S. Department of Energy’s NSERC supercomputers to further speed the screening process.
A privacy-first approach to automated contact tracing
Smartphone data can help limit the spread of Covid-19 by identifying people who have come into contact with someone infected with the virus, and thus may have caught the infection themselves. But automated contact tracing also carries serious privacy risks.
In collaboration with MIT Lincoln Laboratory and others, MIT researchers Ronald Rivest and Daniel Weitzner will use encrypted Bluetooth data to ensure personally identifiable information remains anonymous and secure.
Overcoming manufacturing and supply hurdles to provide global access to a coronavirus vaccine
A vaccine against SARS-CoV-2 would be a crucial turning point in the fight against Covid-19. Yet, its potential impact will be determined by the ability to rapidly and equitably distribute billions of doses globally. This is an unprecedented challenge in biomanufacturing. 
In a project led by MIT professors Anthony Sinskey and Stacy Springs, researchers will build data-driven statistical models to evaluate tradeoffs in scaling the manufacture and supply of vaccine candidates. Questions include how much production capacity will need to be added, the impact of centralized versus distributed operations, and how to design strategies for fair vaccine distribution. The goal is to give decision-makers the evidence needed to cost-effectively achieve global access.
Leveraging electronic medical records to find a treatment for Covid-19
Developed as a treatment for Ebola, the anti-viral drug remdesivir is now in clinical trials in the United States as a treatment for Covid-19. Similar efforts to repurpose already-approved drugs to treat or prevent the disease are underway.
In a project led by MIT professors Roy Welsch and Stan Finkelstein, researchers will use statistics, machine learning, and simulated clinical drug trials to find and test already-approved drugs as potential therapeutics against Covid-19. Researchers will sift through millions of electronic health records and medical claims for signals indicating that drugs used to fight chronic conditions like hypertension, diabetes, and gastric influx might also work against Covid-19 and other diseases.
Finding better ways to treat Covid-19 patients on ventilators 
Troubled breathing from acute respiratory distress syndrome is one of the complications that brings Covid-19 patients to the ICU. There, life-saving machines help patients breathe by mechanically pumping oxygen into the lungs. But even as towns and cities lower their Covid-19 infections through social distancing, there remains a national shortage of mechanical ventilators and serious health risks of ventilation itself.
In collaboration with IBM researchers Zach Shahn and Daby Sow, MIT researchers Li-Wei Lehman and Roger Mark will develop an AI tool to help doctors find better ventilator settings for Covid-19 patients and decide how long to keep them on a machine. Shortened ventilator use can limit lung damage while freeing up machines for others. To build their models, researchers will draw on data from intensive-care patients with acute respiratory distress syndrome, as well as Covid-19 patients at a local Boston hospital.Returning to normal via targeted lockdowns, personalized treatments, and mass testing
In a few short months, Covid-19 has devastated towns and cities around the world. Researchers are now piecing together the data to understand how government policies can limit new infections and deaths and how targeted policies might protect the most vulnerable.
In a project led by MIT Professor Dimitris Bertsimas, researchers will study the effects of lockdowns and other measures meant to reduce new infections and deaths and prevent the health-care system from being swamped. In a second phase of the project, they will develop machine learning models to predict how vulnerable a given patient is to Covid-19, and what personalized treatments might be most effective. They will also develop an inexpensive, spectroscopy-based test for Covid-19 that can deliver results in minutes and pave the way for mass testing. The project will draw on clinical data from four hospitals in the United States and Europe, including Codogno Hospital, which reported Italy’s first infection.


",Marshaling artificial intelligence in the fight against Covid-19,2020-05-19,['Kim Martineau'],Quest for Intelligence/MIT-IBM Watson AI Lab/Lincoln Laboratory/Covid-19/Pandemic/Disease/Research/Artificial intelligence/Machine learning/School of Engineering/School of Science/Sloan School of Management/Medicine/Health care/Economics/Biology/Electrical engineering and computer science (EECS)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/DMSE/Materials Science and Engineering/Funding/MIT Schwarzman College of Computing,"['targeted', 'machine', 'researchers', 'marshaling', 'drugs', 'led', 'patients', 'help', 'artificial', 'fight', 'intelligence', 'covid19', 'project', 'mit']","Artificial intelligence could play a decisive role in stopping the Covid-19 pandemic.
Early detection of sepsis in Covid-19 patientsSepsis is a deadly complication of Covid-19, the disease caused by the new coronavirus SARS-CoV-2.
About 10 percent of Covid-19 patients get sick with sepsis within a week of showing symptoms, but only about half survive.
Finding better ways to treat Covid-19 patients on ventilatorsTroubled breathing from acute respiratory distress syndrome is one of the complications that brings Covid-19 patients to the ICU.
There, life-saving machines help patients breathe by mechanically pumping oxygen into the lungs.",Mit
131,https://news.mit.edu/2020/what-can-your-microwave-tell-you-about-your-health-mit-sapple-0518,"


For many of us, our microwaves and dishwashers aren’t the first thing that come to mind when trying to glean health information, beyond that we should (maybe) lay off the Hot Pockets and empty the dishes in a timely way.
But we may soon be rethinking that, thanks to new research from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL). The system, called “Sapple,” analyzes in-home appliance usage to better understand our health patterns, using just radio signals and a smart electricity meter.
Taking information from two in-home sensors, the new machine learning model examines use of everyday items like microwaves, stoves, and even hair dryers, and can detect where and when a particular appliance is being used.
For example, for an elderly person living alone, learning appliance usage patterns could help their health-care professionals understand their ability to perform various activities of daily living, with the goal of eventually helping advise on healthy patterns. These can include personal hygiene, dressing, eating, maintaining continence, and mobility.
“This system uses passive sensing data, and does not require people to change the way they live,” says MIT PhD student Chen-Yu Hsu, the lead author on a new paper about Sapple. “It has potential to improve things like energy saving and efficiency, give us a better understanding of the daily activities of seniors living alone, and provide insight into the behavioral analytics for smart environments.”
Of the two sensors, the “location sensor” uses radio signals to sense placement, and covers around 40 feet, or enough to cover a typical one-bedroom apartment. A user can walk around their apartment to set up the sensor, which allows it to understand the physical boundaries, and then the sensor can limit itself to that specified area.










The team says the system could potentially be useful during the Covid-19 pandemic, where there’s an increasing interest in contactless sensing of health and behaviors. They can imagine using passive sensor data to free up the need for caregivers to visit higher-risk populations and minimize overall in-person contact.
Sapple comes from the team’s growing body of research focused on using wireless sensing to better understand our complex human bodies — such as an in-body “GPS” sensor with the goal of tracking tumors or dispensing drugs, a wireless smart-home system for monitoring diseases and helping the elderly “age in place,” and another system for measuring gait to help monitor and diagnose various ailments.
Previous work in learning appliance usage has looked at using energy data from a utility meter. But this approach makes it challenging to tease out details, as the energy data is a mix of multiple appliances’ patterns all added together.
Unsupervised approaches — those in which training data aren’t labeled — assume patterns of individual appliances are unknown. However, since the utility meter measures the total energy used by the home, it’s really hard to learn individual appliances or detect them effectively.
Sapple stays in the unsupervised realm: It doesn’t assume we know the patterns of individual appliances, but instead uses data from a second sensor to help learn appliance usage patterns with self-supervision. For example, the location sensor captures a person's motion as they approach a microwave, put food in it, and turn it on. The model then analyzes the data, and learns when specific appliances are turned on, and what their locations are in a home.
In addition to health, Sapple could potentially help reduce our heavy imprint on the natural world. By analyzing appliance usage patterns within homes, the system could be used to encourage energy-saving behaviors and improve forecasting and delivery for utility companies.
The team notes that their system’s approach solves some of the issues that can be tricky for in-home sensors. For example, using the location data doesn’t always imply appliance usage, as people can be next to an appliance without using it. Also, many appliances like refrigerators cycle their power and create ""background events,” and there could be location data from multiple people in a home, but not all of them are related to appliance usage. Sapple solves these problems by learning when the two sensor streams become related, and uses that to discover when appliances are turned on, and their locations.
“As indoor location-sensing starts to potentially become as common as Wi-Fi in the future, the hope is that our technology can be effortlessly applied to all places with utility meters,” says Hsu. “This could enable new applications for passive health sensing in the homes. Utility companies, for example, could reduce peak demands by providing personalized feedback, optimize energy generation and delivery, and ultimately improve energy efficiency.”
Hsu wrote the paper alongside CSAIL PhD students Abbas Zeitoun and Guang-He Lee, as well as MIT professors Dina Katabi and Tommi Jaakkola. They presented the paper virtually at the International Conference on Learning Representations.


",What can your microwave tell you about your health?,2020-05-18,['Rachel Gordon'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Computer science and technology/Electrical engineering and computer science (EECS)/Covid-19/Wireless/Health sciences and technology/Sensors/Research/internet of things/Behavior/Machine learning/School of Engineering/MIT Schwarzman College of Computing,"['data', 'appliance', 'using', 'energy', 'usage', 'patterns', 'sensor', 'system', 'tell', 'microwave', 'health', 'utility', 'appliances']","The system, called “Sapple,” analyzes in-home appliance usage to better understand our health patterns, using just radio signals and a smart electricity meter.
They can imagine using passive sensor data to free up the need for caregivers to visit higher-risk populations and minimize overall in-person contact.
Previous work in learning appliance usage has looked at using energy data from a utility meter.
But this approach makes it challenging to tease out details, as the energy data is a mix of multiple appliances’ patterns all added together.
For example, using the location data doesn’t always imply appliance usage, as people can be next to an appliance without using it.",Mit
132,https://news.mit.edu/2020/smart-fabrics-future-0508,"


In an opinion piece published in the journal Matter, members of the Fibers@MIT research group recently laid out a detailed vision for how the rapidly growing field of  advanced fibers and fabrics could transform many aspects of our lives. For example, “smart clothing” might continuously monitor temperature, heart rate, and other vital signs, then analyze the data and give warnings of potential health conditions. Headed by Professor Yoel Fink, the group is developing fibers and fabrics with advanced computational properties. MIT News asked PhD student Gabriel Loke, who was the article’s lead author, along with Fink and six others, to elaborate on the team’s outlook.Q: The paper you just published describes a vision for a fabric computer. Could such computers help to address a pandemic situation like the one we confront now?A: The current pandemic has revealed the need for new paradigms to assess the health of large populations in real-time. Current approaches of symptom-driven tests are lagging indicators, and can be likened to driving just with your rear-view mirror, as far as the spread of Covid-19 is concerned. So how do we create systems that are predictive, forward-looking and can deliver leading indicators? What if you had a way to access your vital signs on a continuous basis? Could subtle, imperceptibly small changes become early warning signs for the health issues of an individual? What if you could correlate in space and time these changes for a large population, and do so in real time, to identify the spread of disease?No human-made objects are more ubiquitous or exposed to more vital data than the clothes we all wear.  Wouldn’t it be great if we could somhow teach our fabrics to sense, store, analyze, extract, and communicate this potentially useful information?In this piece, I describe the four principles for this new computer. First, the capabilities of a single strand of fiber will advance rapidly over time through new material designs and scalable fiber fabrication approaches. The second step is the synergistic assembly of these fibers into a fabric uniquely positioned to capture, store, and process vast amounts of data released by our bodies. The third is the development of artificially intelligent fabrics, where specially architected machine-learning algorithms programmed into the fabrics could uncover and gain new insights into hidden bodily patterns. Fourth, fabrics become sophisticated platforms for value-added services catering to a large population.Q: You describe a potential “Moore's Law,” which originally described a doubling of computing capacity every 18 months, for the development of computational fabrics. Could you describe what you mean by that?A: For a Moore’s Law for fibers to emerge, fibers have to be made up of multiple materials, precisely arranged within a single fiber cross-section to produce devices of varying functionalities including computation. The field of multimaterial fibers is young, relative to that of thin-film technology for microchip devices. But what we are seeing now in papers and research is a large growth in the number of functions that a fiber can exhibit.For example, in the past few years, the fabrication method called thermal fiber drawing has resulted in a variety of material combinations and functions including heart-rate monitoring and optical communication. With a Moore’s Law for fibers, we imagine a future where computational fabrics will be consistently updated with new functions and capabilities, similar to how we are always updating software in our computers. Q: You’ve laid out a long-term vision and blueprint for the future of computational fabrics. What do you see as the most significant near-term steps in that direction that we can expect to see in the next few years?A: The most important thing is to make sure that people, in particular students, realize what is happening in fabrics and how capable they will soon become. In our group, a host of students from different disciplines are working on creating fabric computers as we speak. Similar to the personal computer evolution, there are vast opportunities for new companies and innovation in this space. I anticipate fibers entering the digital domain and the introduction of fiber input and output. Modern computers are made up of millions of logic gates, so incorporating digital circuits and gates into a fiber represents the first of many steps toward achieving full computing capabilities in fibers and fabrics.  Second, for the realization of a fabric computer, the significant near-term step will be the development of fabric architectures that allow fibers to communicate with each other while retaining the conventional qualities of fabrics.Finally, to enable fabrics with artificial intelligence capabilities, training useful networks for accurate predictions requires large data sets. This requires the collection of large amounts of data from our body. It is then necessary for sensors in fabrics to be as seamless and resistant as possible so that these sensors can be worn for prolonged durations. Work on these fronts such as improving the flexibility, washability, and power requirements of fiber sensors will bring us a step ahead into the pervasive sampling of human body data.


",3 Questions: The rapidly unfolding future of smart fabrics,2020-05-08,['David L. Chandler'],Materials Science and Engineering/DMSE/electronics/Innovation and Entrepreneurship (I&E)/School of Engineering/Electrical Engineering & Computer Science (eecs)/3 Questions/Research,"['data', 'vital', 'future', 'computer', 'fibers', 'fabrics', 'unfolding', 'smart', 'questions', 'fabric', 'capabilities', 'fiber', 'large', 'computational', 'rapidly']","Q: The paper you just published describes a vision for a fabric computer.
No human-made objects are more ubiquitous or exposed to more vital data than the clothes we all wear.
Q: You describe a potential “Moore's Law,” which originally described a doubling of computing capacity every 18 months, for the development of computational fabrics.
Q: You’ve laid out a long-term vision and blueprint for the future of computational fabrics.
Finally, to enable fabrics with artificial intelligence capabilities, training useful networks for accurate predictions requires large data sets.",Mit
133,https://news.mit.edu/2020/visualizing-the-world-beyond-the-frame-0506,"


Most firetrucks come in red, but it’s not hard to picture one in blue. Computers aren’t nearly as creative.
Their understanding of the world is colored, often literally, by the data they’ve trained on. If all they’ve ever seen are pictures of red fire trucks, they have trouble drawing anything else. 
To give computer vision models a fuller, more imaginative view of the world, researchers have tried feeding them more varied images. Some have tried shooting objects from odd angles, and in unusual positions, to better convey their real-world complexity. Others have asked the models to generate pictures of their own, using a form of artificial intelligence called GANs, or generative adversarial networks. In both cases, the aim is to fill in the gaps of image datasets to better reflect the three-dimensional world and make face- and object-recognition models less biased.
In a new study at the International Conference on Learning Representations, MIT researchers propose a kind of creativity test to see how far GANs can go in riffing on a given image. They “steer” the model into the subject of the photo and ask it to draw objects and animals close up, in bright light, rotated in space, or in different colors.
The model’s creations vary in subtle, sometimes surprising ways. And those variations, it turns out, closely track how creative human photographers were in framing the scenes in front of their lens. Those biases are baked into the underlying dataset, and the steering method proposed in the study is meant to make those limitations visible. 
“Latent space is where the DNA of an image lies,” says study co-author Ali Jahanian, a research scientist at MIT. “We show that you can steer into this abstract space and control what properties you want the GAN to express — up to a point. We find that a GAN’s creativity is limited by the diversity of images it learns from.” Jahanian is joined on the study by co-author Lucy Chai, a PhD student at MIT, and senior author Phillip Isola, the Bonnie and Marty (1964) Tenenbaum CD Assistant Professor of Electrical Engineering and Computer Science.
The researchers applied their method to GANs that had already been trained on ImageNet’s 14 million photos. They then measured how far the models could go in transforming different classes of animals, objects, and scenes. The level of artistic risk-taking, they found, varied widely by the type of subject the GAN was trying to manipulate. 
For example, a rising hot air balloon generated more striking poses than, say, a rotated pizza. The same was true for zooming out on a Persian cat rather than a robin, with the cat melting into a pile of fur the farther it recedes from the viewer while the bird stays virtually unchanged. The model happily turned a car blue, and a jellyfish red, they found, but it refused to draw a goldfinch or firetruck in anything but their standard-issue colors. 
The GANs also seemed astonishingly attuned to some landscapes. When the researchers bumped up the brightness on a set of mountain photos, the model whimsically added fiery eruptions to the volcano, but not a geologically older, dormant relative in the Alps. It’s as if the GANs picked up on the lighting changes as day slips into night, but seemed to understand that only volcanos grow brighter at night.
The study is a reminder of just how deeply the outputs of deep learning models hinge on their data inputs, researchers say. GANs have caught the attention of intelligence researchers for their ability to extrapolate from data, and visualize the world in new and inventive ways. 
They can take a headshot and transform it into a Renaissance-style portrait or favorite celebrity. But though GANs are capable of learning surprising details on their own, like how to divide a landscape into clouds and trees, or generate images that stick in people’s minds, they are still mostly slaves to data. Their creations reflect the biases of thousands of photographers, both in what they’ve chosen to shoot and how they framed their subject.
“What I like about this work is it’s poking at representations the GAN has learned, and pushing it to reveal why it made those decisions,” says Jaakko Lehtinen, a professor at Finland’s Aaalto University and a research scientist at NVIDIA who was not involved in the study. “GANs are incredible, and can learn all kinds of things about the physical world, but they still can’t represent images in physically meaningful ways, as humans can.”


",Visualizing the world beyond the frame,2020-05-06,['Kim Martineau'],Electrical engineering and computer science (EECS)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/School of Engineering/Algorithms/Artificial intelligence/Computer science and technology/Machine learning/Software/Computer vision/Quest for Intelligence/MIT Schwarzman College of Computing,"['space', 'data', 'visualizing', 'researchers', 'models', 'red', 'theyve', 'world', 'frame', 'gans', 'images', 'study']","Their understanding of the world is colored, often literally, by the data they’ve trained on.
To give computer vision models a fuller, more imaginative view of the world, researchers have tried feeding them more varied images.
In both cases, the aim is to fill in the gaps of image datasets to better reflect the three-dimensional world and make face- and object-recognition models less biased.
GANs have caught the attention of intelligence researchers for their ability to extrapolate from data, and visualize the world in new and inventive ways.
“GANs are incredible, and can learn all kinds of things about the physical world, but they still can’t represent images in physically meaningful ways, as humans can.”",Mit
134,https://news.mit.edu/2020/siranush-babakhanova-michal-gala-named-knight-hennessy-scholars-0505,"


Two MIT seniors, Siranush Babakhanova and Michal Gala, have been awarded Knight-Hennessy Scholarships. The prestigious fellowship attracts thousands of applicants from around the world and provides full funding for graduate studies in any field at Stanford University. Knight-Hennessy scholars also receive leadership development training, mentorship, and experiential learning opportunities.
The Knight-Hennessy Scholars program aims to address the world’s challenges through innovation and collaboration by developing a community of emerging leaders equipped to work across disciplines and cultures. Up to 90 scholars are selected each year.
Citizens of all countries are eligible to apply. In accordance with its goal of global impact, the program seeks to select two-thirds of its scholars from outside the United States. In addition to academic excellence, selection criteria include independence of thought, purposeful leadership, and civic mindset.
“We are very proud that Michal and Siranush will represent MIT in the Knight Hennessy community,” says Benard. “They both are exceptional examples of creative researchers, who are working on issues that will improve our world.”
Siranush Babakhanova, from Yerevan, Armenia, will graduate this May with a BS in physics. At Stanford University, she will pursue a PhD in biophysics at Stanford School of Humanities and Sciences.
Babakhanova aspires to apply physical and computational sciences to build tools to read, manipulate, and augment complex biological systems. She has worked on exo-atmosphere-wide simulations at NASA, and designed tools for deep brain imaging and in situ proteomics with Professor Ed Boyden in the Synthetic Neurobiology Group at the MIT Media Lab. She co-founded Xapiens, MIT’s first human augmentation and brain-machine interface club, served as president of the MIT Armenian Society, and advised on projects for MIT’s Students for the Exploration and Development of Space (SEDS).
Babakhanova was the first Armenian woman to win each of six International Olympiad awards. Her teams earned second place in the NASA BIG Idea Challenge and she was a grant recipient for the Council for Arts at MIT; a program fellow for NSF GRFP, Interact and ADVANCE; and a Hertz Foundation Fellowship finalist.
Michal Gala, from Gliwice, Poland, will graduate in May with a BS in chemical engineering. As a Knight-Hennessy scholar, he will embark on a PhD in chemical engineering at Stanford School of Engineering.
Gala aspires to engineer novel clean processes and technologies that will create the chemical industry of the future and help solve the problem of climate change. At MIT, he tackled similar problems experimentally and computationally.
Gala has interned at Shell in India and Schlumberger developing artificial intelligence tools for accelerated materials discovery and property prediction. Throughout his time at MIT, he worked on electrochemical ammonia synthesis, elucidating key parts of this promising technology, for which he was recognized as one of 25 under 25 in Science by Polish Forbes.
Gala served as vice-president of the MIT chapter of the Tau Beta Pi engineering honor society and was actively involved with the MIT Gordon Engineering Leadership Program.
MIT students interested in applying for the Knight-Hennessy Scholarship may contact Kim Benard, assistant dean of distinguished fellowships in Career Advising and Professional Development. The deadline to apply for the program’s 2021 cohort is Oct. 10.



",Siranush Babakhanova and Michal Gala named 2020 Knight-Hennessy Scholars,2020-05-05,['Julia Mongo'],"Chemical engineering/Physics/Undergraduate/Awards, honors and fellowships/Students/School of Engineering/School of Science/Media Lab/School of Architecture and Planning","['graduate', 'named', 'stanford', 'knighthennessy', 'gala', 'leadership', '2020', 'michal', 'scholars', 'engineering', 'siranush', 'tools', 'babakhanova', 'development', 'program', 'mit']","Two MIT seniors, Siranush Babakhanova and Michal Gala, have been awarded Knight-Hennessy Scholarships.
Knight-Hennessy scholars also receive leadership development training, mentorship, and experiential learning opportunities.
The Knight-Hennessy Scholars program aims to address the world’s challenges through innovation and collaboration by developing a community of emerging leaders equipped to work across disciplines and cultures.
As a Knight-Hennessy scholar, he will embark on a PhD in chemical engineering at Stanford School of Engineering.
Gala served as vice-president of the MIT chapter of the Tau Beta Pi engineering honor society and was actively involved with the MIT Gordon Engineering Leadership Program.",Mit
135,https://news.mit.edu/2020/three-mit-elected-national-academy-sciences-0501,"


On April 27, the National Academy of Sciences elected 120 new members and 26 international associates, including three professors from MIT — Abhijit Banerjee, Bonnie Berger, and Roger Summons — recognizing their “distinguished and continuing achievements in original research.” Current membership totals 2,403 active members and 501 international associates, including 190 Nobel Prize recipients.
The National Academy of Sciences is a private, nonprofit institution for scientific advancement established in 1863 by congressional charter and signed into law by President Abraham Lincoln. Together, with the National Academy of Engineering and the National Academy of Medicine, the 157-year-old society provides science, engineering, and health policy advice to the federal government and other organizations.
Abhijit Banerjee is the Ford Foundation International Professor of Economics, and in 2003 cofounded, with Esther Duflo and Sendhil Mullainathan, the Abdul Latif Jameel Poverty Action Lab (J-PAL). Banerjee’s groundbreaking research focuses on development economics and the alleviation of global poverty, work for which he shared the 2019 Nobel Prize in Economic Sciences.
He continues to serve as a director of J-PAL; he is also a past president of the Bureau for Research and Economic Analysis of Development, a research associate of the National Bureau of Economic Research, a Center for Economic and Policy Research research fellow, an international research fellow of the Kiel Institute, and a fellow of the American Academy of Arts and Sciences and the Econometric Society. He has been a Guggenheim fellow, an Alfred P. Sloan fellow, and a winner of the Infosys Prize.   
Banerjee’s scholarship, in collaboration with fellow NAS member and MIT Professor Esther Duflo, emphasizes the importance of field work in antipoverty initiatives, in order to recreate the precision of randomized controlled trials (RCTs) and laboratory-style data within the complexity of ever-evolving social realities. The resulting RCT evidence reveals which poverty interventions really work, enabling governments, non-governmental organizations, donors, and the private sector to plan effective programs and policies for poverty alleviation. When Banerjee began his career, development economics was considered marginal in economic studies, a view that Banerjee’s work and high-profile achievements have helped to correct.
Bonnie Berger is the Simons Professor of Mathematics and holds a joint appointment in the Department of Electrical Engineering and Computer Science. She is the head of the Computation and Biology group at MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL). She is also a faculty member of the Harvard-MIT Program in Health Sciences and Technology and an associate member of the Broad Institute of MIT and Harvard .
After beginning her career working in algorithms at MIT, Berger was one of the pioneer researchers in the area of computational molecular biology and, together with the many students she has mentored, has been instrumental in defining the field. Her work addresses biological and biomedical questions by using computation in support of or in place of laboratory procedures, with a goal being to get more accurate answers at a greatly reduced cost. Combining genomic and health-related data from millions of patients will empower unprecedented insights into human health and disease risk. Berger transforms and creates techniques from algorithmic thinking to provide novel computational methods and software to enable biomedical data sharing and analysis at scale.
Berger is an elected fellow of the American Academy of Arts and Sciences, Association for Computing Machinery, International Society for Computational Biology (ISCB), American Institute of Medical and Biological Engineering, and American Mathematical Society. Recently she was recognized by ISCB with their Accomplishments by a Senior Scientist Award. She received the NIH Margaret Pittman Director's Award, the SIAM Sonya Kovalevsky Lecture Prize, and an honorary doctorate from EPFL. Earlier in her career, she received an NSF Career Award, the Biophysical Society's Dayhoff Award, and recognition as MIT Technology Review magazine's inaugural TR100 top young innovators. She serves as vice president of ISCB, head of the steering committee for Research in Computational Molecular Biology, and member-at-large of the Section on Mathematics at American Association for the Advancement of Science (AAAS), as well as on multiple advisory committees and editorial boards.
Roger Summons is the Schlumberger Professor of Geobiology in the Department of Earth, Atmospheric and Planetary Sciences (EAPS) at MIT.  
Working at the intersection of biogeochemistry, geobiology, and astrobiology, Summons’ work examines the origins and co-evolution of Earth’s early life and the environment, beginning with the first geological and geochemical records and microbially dominated ecosystems. As an investigator in the Simons Collaboration on the Origins of Life, he’s particularly focused on lipid chemistry of microbes important to understating Earth through deep time, organic and isotopic indicators of climate change, and biomarkers in sediments and petroleum. 
Summons applies findings from this research to understanding life on Earth and the search for it elsewhere in the universe, recently on Mars. As such, he has served on three committees of the National Research Council: Committee on Origin and Evolution of Life, the Committee on Limits of Life, and the Committee on Mars Astrobiology. As an emeritus member of the NASA Astrobiology Institute (NAI) Executive Council and the head of the MIT team of NAI called the Foundations of Complex Life: Evolution, Preservation, and Detection on Earth and Beyond, Summons helped integrate this research with international science communities. Here, his group investigated factors that led to the evolution of complex life by examining processes and conditions that preserve biological signatures. More recently, Summons has contributed to Mars rover missions Curiosity and Perseverance, providing expertise on the preservation of organic matter from different environments on Earth and the red planet.


",Three from MIT elected to the National Academy of Sciences for 2020,2020-05-01,['Sandi Miller'],"Mathematics/Abdul Latif Jameel Poverty Action Lab (J-PAL)/Economics/Electrical engineering and computer science (EECS)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Harvard-MIT Health Sciences and Technology/Broad Institute/EAPS/Awards, honors and fellowships/Faculty/School of Science/School of Engineering","['summons', 'national', 'research', 'academy', '2020', 'life', 'work', 'elected', 'fellow', 'sciences', 'international', 'mit']","The National Academy of Sciences is a private, nonprofit institution for scientific advancement established in 1863 by congressional charter and signed into law by President Abraham Lincoln.
Together, with the National Academy of Engineering and the National Academy of Medicine, the 157-year-old society provides science, engineering, and health policy advice to the federal government and other organizations.
Banerjee’s groundbreaking research focuses on development economics and the alleviation of global poverty, work for which he shared the 2019 Nobel Prize in Economic Sciences.
Roger Summons is the Schlumberger Professor of Geobiology in the Department of Earth, Atmospheric and Planetary Sciences (EAPS) at MIT.
As such, he has served on three committees of the National Research Council: Committee on Origin and Evolution of Life, the Committee on Limits of Life, and the Committee on Mars Astrobiology.",Mit
136,https://news.mit.edu/2020/mit-ideas-celebrates-social-innovation-0501,"


Members of the MIT community from around the world gathered virtually on Sunday, April 26 to celebrate the 19th annual IDEAS Awards presented by the PKG Center for Public Service. IDEAS is MIT’s social innovation challenge and has been bringing MIT students together with mentors from industry, academia, and community organizations for nearly 20 years to tackle pressing social and environmental issues through innovation. 
Due to the outbreak of Covid-19, the IDEAS Showcase (which typically takes place in-person at MIT), was virtualized this year. To share their work with the public, the 20 finalist teams posted 3-minute pitch videos in early April to an online platform hosted in partnership with Solve. More than 2,700 individuals around the world cast votes for their favorite project and three teams received “crowd favorite” awards of $2,500 each during the IDEAS Awards broadcast. They were:


Earned Credit Project, a tool that combats U.S. income inequality by combining big data analytics with free remote tax filing to help Americans claim the Earned Income Tax Credit (EITC);


MyPath Global,  a social venture dedicated to preparing and linking young women in Kenya to successful careers and entrepreneurship opportunities through upskilling and coaching; and


SpeakEasy, a platform that empowers workers to build a safer, more just workplace community using the power of social media to spark collective action.


Four teams were awarded juried grants during Sunday’s live IDEAS Awards broadcast to continue or launch their social innovation:


Greensource, a social venture committed to advancing the livelihood of smallholder palm oil producers across Central and West Africa by providing them with the resources to create palm oil that is fair trade and sustainably made by building out a network of mobile presses ($20,000);


Insightiv, a distributed diagnostics platform that leverages connectivity and artificial intelligence to optimize the workflow for doctors, radiographers, and radiologists in Rwanda, allowing them to effectively serve more patients from all over the country in a short amount of time ($15,000);


TILT, a low-cost wheelchair attachment that allows assistants to pull wheelchair users and their wheelchairs up and down stairs in areas that lack accessibility infrastructure ($10,000); and


Out of the Box, a mobile, modular classroom-in-a-box with an accompanying digital platform to enable preschool learning to occur anywhere ($7,500).












“It has been a humbling experience to participate in IDEAS,” says Celi Lindiwe Khanyile-Lynch, an MIT Sloan MBA student and Greensource team member. “It is programs like IDEAS that differentiate MIT from its peers. I am truly inspired by all the work my classmates are doing to make an impact in communities beyond MIT.”
Grantees were selected by a group of 20 judges who reviewed written proposals and engaged with the teams virtually prior to the IDEAS Awards. The judging pool included MIT alumni, industry experts, and community leaders.
“We joined the IDEAS social innovation challenge because it's such an amazing opportunity for mentorship, learning from other phenomenal cohort members, and being a part of a movement at MIT for social innovation,” says sophomore Smita Bhattacharjee, of TILT. “We knew that through IDEAS we would have the support to make our vision a reality, and for that, we are extremely grateful.” 
More than 70 teams entered the IDEAS challenge in the 2019-20 cycle addressing issues across topics ranging from health to sustainability to finance. Each finalist team received a seed grant of a $1,000 to further their efforts. Thirteen of the 20 final teams were women-led. IDEAS participants work throughout the year to develop and improve their projects through mentoring and several rounds of formal feedback from industry and community experts. All IDEAS teams are led by at least one full-time MIT undergraduate or graduate student, though teams may include members outside the MIT community. 
“Beyond the numbers, the IDEAS teams are a group of kind and generous individuals who have powerful stories that can move some of you to tears,” says Rebecca Obounou, the PKG Center assistant dean who oversees IDEAS. “They exemplify what MIT means by ‘Building a Better World’. I have had the great privilege of witnessing their generous acts of kindness from voting for each other’s projects online, sharing resources, to immersing themselves in the Covid-19 pandemic response efforts, all while fulfilling their personal, academic, and IDEAS commitments.” 
Over 50 percent of IDEAS projects are still active today, including teams like 2019 grantee Mantle Biotech, 2017 grantees BioBot Analytics and Nesterly, and 2003 grantee Design That Matters, all of which are supporting innovative efforts to combat Covid-19.
IDEAS is currently recruiting volunteers for the 2020-21 program cycle.



",MIT IDEAS celebrates social innovation at the Institute,2020-05-01,[],"IDEAS competition/Solve/Sloan School of Management/Mechanical engineering/Social entrepreneurship/Public service/Awards, honors and fellowships/Special events and guest speakers","['awards', 'innovation', 'institute', 'celebrates', 'community', 'work', '20', 'world', 'ideas', 'platform', 'teams', 'social', 'mit']","They were:Members of the MIT community from around the world gathered virtually on Sunday, April 26 to celebrate the 19th annual IDEAS Awards presented by the PKG Center for Public Service.
IDEAS is MIT’s social innovation challenge and has been bringing MIT students together with mentors from industry, academia, and community organizations for nearly 20 years to tackle pressing social and environmental issues through innovation.
“We joined the IDEAS social innovation challenge because it's such an amazing opportunity for mentorship, learning from other phenomenal cohort members, and being a part of a movement at MIT for social innovation,” says sophomore Smita Bhattacharjee, of TILT.
All IDEAS teams are led by at least one full-time MIT undergraduate or graduate student, though teams may include members outside the MIT community.
“Beyond the numbers, the IDEAS teams are a group of kind and generous individuals who have powerful stories that can move some of you to tears,” says Rebecca Obounou, the PKG Center assistant dean who oversees IDEAS.",Mit
137,https://news.mit.edu/2020/foolproof-way-shrink-deep-learning-models-0430,"


As more artificial intelligence applications move to smartphones, deep learning models are getting smaller to allow apps to run faster and save battery power. Now, MIT researchers have a new and better way to compress models. 
It’s so simple that they unveiled it in a tweet last month: Train the model, prune its weakest connections, retrain the model at its fast, early training rate, and repeat, until the model is as tiny as you want. 
“That’s it,” says Alex Renda, a PhD student at MIT. “The standard things people do to prune their models are crazy complicated.” 
Renda discussed the technique when the International Conference of Learning Representations (ICLR) convened remotely this month. Renda is a co-author of the work with Jonathan Frankle, a fellow PhD student in MIT’s Department of Electrical Engineering and Computer Science (EECS), and Michael Carbin, an assistant professor of electrical engineering and computer science — all members of the Computer Science and Artificial Science Laboratory.  
The search for a better compression technique grew out of Frankle and Carbin’s award-winning Lottery Ticket Hypothesis paper at ICLR last year. They showed that a deep neural network could perform with only one-tenth the number of connections if the right subnetwork was found early in training. Their revelation came as demand for computing power and energy to train ever larger deep learning models was increasing exponentially, a trend that continues to this day. Costs of that growth include a rise in planet-warming carbon emissions and a potential drop in innovation as researchers not affiliated with big tech companies compete for scarce computing resources. Everyday users are affected, too. Big AI models eat up mobile-phone bandwidth and battery power.
But at a colleague’s suggestion, Frankle decided to see what lessons it might hold for pruning, a set of techniques for reducing the size of a neural network by removing unnecessary connections or neurons. Pruning algorithms had been around for decades, but the field saw a resurgence after the breakout success of neural networks at classifying images in the ImageNet competition. As models got bigger, with researchers adding on layers of artificial neurons to boost performance, others proposed techniques for whittling them down. 
Song Han, now an assistant professor at MIT, was one pioneer. Building on a series of influential papers, Han unveiled a pruning algorithm he called AMC, or AutoML for model compression, that’s still the industry standard. Under Han’s technique, redundant neurons and connections are automatically removed, and the model is retrained to restore its initial accuracy. 
In response to Han’s work, Frankle recently suggested in an unpublished paper that results could be further improved by rewinding the smaller, pruned model to its initial parameters, or weights, and retraining the smaller model at its faster, initial rate. 
In the current ICLR study, the researchers realized that the model could simply be rewound to its early training rate without fiddling with any parameters. In any pruning regimen, the tinier a model gets, the less accurate it becomes. But when the researchers compared this new method to Han’s AMC or Frankle’s weight-rewinding methods, it performed better no matter how much the model shrank. 
It’s unclear why the pruning technique works as well as it does. The researchers say they will leave that question for others to answer. As for those who wish to try it, the algorithm is as easy to implement as other pruning methods, without time-consuming tuning, the researchers say. 
“It’s the pruning algorithm from the ‘Book,’” says Frankle. “It’s clear, generic, and drop-dead simple.”
Han, for his part, has now partly shifted focus from compression AI models to channeling AI to design small, efficient models from the start. His newest method, Once for All, also debuts at ICLR. Of the new learning rate method, he says: “I’m happy to see new pruning and retraining techniques evolve, giving more people access to high-performing AI applications.” 
Support for the study came from the Defense Advanced Research Projects Agency, Google, MIT-IBM Watson AI Lab, MIT Quest for Intelligence, and the U.S. Office of Naval Research.


",A foolproof way to shrink deep learning models,2020-04-30,['Kim Martineau'],Electrical engineering and computer science (EECS)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/MIT-IBM Watson AI Lab/School of Engineering/Computer science and technology/Algorithms/Machine learning/Artificial intelligence/Mobile devices/Quest for Intelligence/MIT Schwarzman College of Computing,"['model', 'foolproof', 'learning', 'shrink', 'models', 'researchers', 'pruning', 'frankle', 'ai', 'way', 'deep', 'technique', 'science', 'mit']","As more artificial intelligence applications move to smartphones, deep learning models are getting smaller to allow apps to run faster and save battery power.
Now, MIT researchers have a new and better way to compress models.
Their revelation came as demand for computing power and energy to train ever larger deep learning models was increasing exponentially, a trend that continues to this day.
Big AI models eat up mobile-phone bandwidth and battery power.
“It’s clear, generic, and drop-dead simple.”Han, for his part, has now partly shifted focus from compression AI models to channeling AI to design small, efficient models from the start.",Mit
138,https://news.mit.edu/2020/automating-search-entirely-new-curiosity-algorithms-0428,"


Driven by an innate curiosity, children pick up new skills as they explore the world and learn from their experience. Computers, by contrast, often get stuck when thrown into new environments.
To get around this, engineers have tried encoding simple forms of curiosity into their algorithms with the hope that an agent pushed to explore will learn about its environment more effectively. An agent with a child’s curiosity might go from learning to pick up, manipulate, and throw objects to understanding the pull of gravity, a realization that could dramatically accelerate its ability to learn many other things. 
Engineers have discovered many ways of encoding curious exploration into machine learning algorithms. A research team at MIT wondered if a computer could do better, based on a long history of enlisting computers in the search for new algorithms. 
In recent years, the design of deep neural networks, algorithms that search for solutions by adjusting numeric parameters, has been automated with software like Google’s AutoML and auto-sklearn in Python. That’s made it easier for non-experts to develop AI applications. But while deep nets excel at specific tasks, they have trouble generalizing to new situations. Algorithms expressed in code, in a high-level programming language, by contrast, have the capacity to transfer knowledge across different tasks and environments. 
“Algorithms designed by humans are very general,” says study co-author Ferran Alet, a graduate student in MIT’s Department of Electrical Engineering and Computer Science and Computer Science and Artificial Intelligence Laboratory (CSAIL). “We were inspired to use AI to find algorithms with curiosity strategies that can adapt to a range of environments.”
The researchers created a “meta-learning” algorithm that generated 52,000 exploration algorithms. They found that the top two were entirely new — seemingly too obvious or counterintuitive for a human to have proposed. Both algorithms generated exploration behavior that substantially improved learning in a range of simulated tasks, from navigating a two-dimensional grid based on images to making a robotic ant walk. Because the meta-learning process generates high-level computer code as output, both algorithms can be dissected to peer inside their decision-making processes.
The paper’s senior authors are Leslie Kaelbling and Tomás Lozano-Pérez, both professors of computer science and electrical engineering at MIT. The work will be presented at the virtual International Conference on Learning Representations later this month. 







Play video






The paper received praise from researchers not involved in the work. “The use of program search to discover a better intrinsic reward is very creative,” says Quoc Le, a principal scientist at Google who has helped pioneer computer-aided design of deep learning models. “I like this idea a lot, especially since the programs are interpretable.”
The researchers compare their automated algorithm design process to writing sentences with a limited number of words. They started by choosing a set of basic building blocks to define their exploration algorithms. After studying other curiosity algorithms for inspiration, they picked nearly three dozen high-level operations, including basic programs and deep learning models, to guide the agent to do things like remember previous inputs, compare current and past inputs, and use learning methods to change its own modules. The computer then combined up to seven operations at a time to create computation graphs describing 52,000 algorithms. 
Even with a fast computer, testing them all would have taken decades. So, instead, the researchers limited their search by first ruling out algorithms predicted to perform poorly, based on their code structure alone. Then, they tested their most promising candidates on a basic grid-navigation task requiring substantial exploration but minimal computation. If the candidate did well, its performance became the new benchmark, eliminating even more candidates. 
Four machines searched over 10 hours to find the best algorithms. More than 99 percent were junk, but about a hundred were sensible, high-performing algorithms. Remarkably, the top 16 were both novel and useful, performing as well as, or better than, human-designed algorithms at a range of other virtual tasks, from landing a moon rover to raising a robotic arm and moving an ant-like robot in a physical simulation. 
All 16 algorithms shared two basic exploration functions. 
In the first, the agent is rewarded for visiting new places where it has a greater chance of making a new kind of move. In the second, the agent is also rewarded for visiting new places, but in a more nuanced way: One neural network learns to predict the future state while a second recalls the past, and then tries to predict the present by predicting the past from the future. If this prediction is erroneous it rewards itself, as it is a sign that it discovered something it didn't know before. The second algorithm was so counterintuitive it took the researchers time to figure out. 
“Our biases often prevent us from trying very novel ideas,” says Alet. “But computers don’t care. They try, and see what works, and sometimes we get great unexpected results.”More researchers are turning to machine learning to design better machine learning algorithms, a field known as AutoML. At Google, Le and his colleagues recently unveiled a new algorithm-discovery tool called Auto-ML Zero. (Its name is a play on Google’s AutoML software for customizing deep net architectures for a given application, and Google DeepMind’s Alpha Zero, the program that can learn to play different board games by playing millions of games against itself.) Their method searches through a space of algorithms made up of simpler primitive operations. But rather than look for an exploration strategy, their goal is to discover algorithms for classifying images. Both studies show the potential for humans to use machine-learning methods themselves to create novel, high-performing machine-learning algorithms.“The algorithms we generated could be read and interpreted by humans, but to actually understand the code we had to reason through each variable and operation and how they evolve with time,” says study co-author Martin Schneider, a graduate student at MIT. “It’s an interesting open challenge to design algorithms and workflows that leverage the computer’s ability to evaluate lots of algorithms and our human ability to explain and improve on those ideas.” 
The research received support from the U.S. National Science Foundation, Air Force Office of Scientific Research, Office of Naval Research, Honda Research Institute, SUTD Temasek Laboratories, and MIT Quest for Intelligence.


",Automating the search for entirely new “curiosity” algorithms,2020-04-28,['Kim Martineau'],Quest for Intelligence/Artificial intelligence/Algorithms/Machine learning/School of Engineering/MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL),"['curiosity', 'automating', 'exploration', 'research', 'entirely', 'computer', 'learning', 'researchers', 'algorithms', 'search', 'agent', 'deep', 'design']","The researchers created a “meta-learning” algorithm that generated 52,000 exploration algorithms.
Because the meta-learning process generates high-level computer code as output, both algorithms can be dissected to peer inside their decision-making processes.
Engineers have discovered many ways of encoding curious exploration into machine learning algorithms.
They started by choosing a set of basic building blocks to define their exploration algorithms.
So, instead, the researchers limited their search by first ruling out algorithms predicted to perform poorly, based on their code structure alone.",Mit
139,https://news.mit.edu/2020/3-questions-tom-leighton-managing-covid-19-internet-traffic-surge-0427,"


With various physical distancing guidelines in place throughout the world as a means to curb the spread of Covid-19, the internet has experienced a dramatic spike in overall traffic. MIT Professor Tom Leighton is chief executive officer and co-founder of Akamai Technologies, a global content delivery network, cybersecurity, and cloud service company that provides web and internet security services. At MIT he specializes in applied mathematics in the Department of Mathematics and is a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL). The Department of Mathematics Communications spoke to Leighton about his company’s response to the world’s increased reliance on the internet during the Covid-19 pandemic.
Q: How is the pandemic changing the way people use the internet?
A: The internet has become our lifeline as we face the challenges of working remotely, distance learning, and sheltering in place. Everything has moved online: religious services, movie premieres, commerce of all kinds, and even gatherings of friends for a cup of coffee. We’ve already been doing many of these things online for years — the big difference now is that we are suddenly only doing them online.
When we’ve emerged from the pandemic, it seems quite possible that our usage of the internet for nearly every facet of our lives will have increased permanently. Many more people may be working remotely even when offices reopen; the shift to virtual meetings may become the norm even when we can travel again; a much greater share of commerce may be conducted online even when we can return to shopping malls; and our usage of social media and video streaming could well be greater than ever before, even when it’s OK to meet others in person.
Q: How much more use is the internet seeing as a result of the pandemic?
A: Akamai operates a globally distributed intelligent edge platform with more than 270,000 servers in 4,000 locations across 137 countries. From our vantage point, we can see that global internet traffic increased by about 30 percent during the past month. That's about 10 times normal, and it means we've seen an entire year's worth of growth in internet traffic in just the past few weeks. And that's without any live sports streaming, like the usual March Madness college basketball tournament in the United States.
Just a few weeks ago, we set a new peak record of traffic on the Akamai edge platform of 167 terabits per second. That’s more than double the peak we saw one year before. These are truly unprecedented times. The internet is being used at a scale that the world has never experienced.
Q: Can the internet keep up with the surge in traffic?
A: The answer is yes, but with many more caveats now.
Around the world, some regulators, major carriers, and content providers are taking steps to reduce load during peak traffic times in an effort to avert online gridlock. For example, European regulators have asked telecom providers and streaming platforms to switch to standard definition video during periods of peak demand. And Akamai is working with leading companies such as Microsoft and Sony to deliver software updates for e-gaming at off-peak traffic times. The typical software update uses as much traffic as about 30,000 web pages, so this makes a big difference when it comes to managing congestion.
In addition, Akamai's intelligent edge network architecture is designed to mitigate and minimize network congestion. Because we’ve deployed our infrastructure deep into carrier networks, we can help those networks avoid overload by diverting traffic away from areas experiencing high levels of congestion.
Overall, we fully expect to maintain the integrity and reliability of website and mobile application delivery, as well as security services, for all of our customers during this time. In particular, Akamai customers across sectors such as government, health care, financial services, commerce, manufacturing, and business services should not experience any change in the performance of their services. We will continue working with governments, network operators, and our customers to minimize stress on the system. At the same time, we’ll do our best to make sure that everyone who is relying on the internet for their work, studies, news, and entertainment continues to have a high-quality, positive experience.


",3 Questions: Tom Leighton on the major surge in internet traffic triggered by physical distancing,2020-04-27,['Sandi Miller'],Mathematics/3 Questions/Covid-19/Internet/School of Science/Pandemic/Industry/Computer science and technology/Technology and society/Faculty/Information Systems and Technology/Computer Science and Artificial Intelligence Laboratory (CSAIL),"['distancing', 'traffic', 'peak', 'services', 'triggered', 'questions', 'akamai', 'times', 'tom', 'online', 'network', 'major', 'weve', 'physical', 'surge', 'leighton', 'internet', 'working']","A: The internet has become our lifeline as we face the challenges of working remotely, distance learning, and sheltering in place.
From our vantage point, we can see that global internet traffic increased by about 30 percent during the past month.
That's about 10 times normal, and it means we've seen an entire year's worth of growth in internet traffic in just the past few weeks.
Around the world, some regulators, major carriers, and content providers are taking steps to reduce load during peak traffic times in an effort to avert online gridlock.
And Akamai is working with leading companies such as Microsoft and Sony to deliver software updates for e-gaming at off-peak traffic times.",Mit
140,https://news.mit.edu/2020/mit-conference-reveals-power-using-artificial-intelligence-discover-new-drugs-0427,"


Developing drugs to combat Covid-19 is a global priority, requiring communities to come together to fight the spread of infection. At MIT, researchers with backgrounds in machine learning and life sciences are collaborating, sharing datasets and tools to develop machine learning methods that can identify novel cures for Covid-19.
This research is an extension of a community effort launched earlier this year. In February, before the Institute de-densified as a result of the pandemic, the first-ever AI Powered Drug Discovery and Manufacturing Conference, conceived and hosted by the Abdul Latif Jameel Clinic for Machine Learning in Health, drew attendees including pharmaceutical industry researchers, government regulators, venture capitalists, and pioneering drug researchers. More than 180 health care companies and 29 universities developing new artificial intelligence methods used in pharmaceuticals got involved, making the conference a singular event designed to lift the mask and reveal what goes on in the process of drug discovery.
As secretive as Silicon Valley seems, computer science and engineering students typically know what a job looks like when aspiring to join companies like Facebook or Tesla. But the global head of research and development for Janssen — the innovative pharmaceutical company owned by Johnson & Johnson — said it’s often much harder for students to grasp how their work fits into drug discovery.
“That’s a problem at the moment,” Mathai Mammen says, after addressing attendees, including MIT graduate students and postdocs, who gathered in the Samberg Conference Center in part to get a glimpse behind the scenes of companies currently working on bold ideas blending artificial intelligence with health care. Mathai, who is a graduate of the Harvard-MIT Program in Health Sciences and Technology and whose work at Theravance has brought to market five new medicines and many more on their way, is here to be part of the answer to that problem. “What the industry needs to do, is talk to students and postdocs about the sorts of interesting scientific and medical problems whose solutions can directly and profoundly benefit the health of people everywhere” he says.
“The conference brought together research communities that rarely overlap at technical conferences,” says Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science, Jameel Clinic faculty co-lead, and one of the conference organizers. “This blend enables us to better understand open problems and opportunities in the intersection. The exciting piece for MIT students, especially for computer science and engineering students, is to see where the industry is moving and to understand how they can contribute to this changing industry, which will happen when they graduate.”
Over two days, conference attendees snapped photographs through a packed schedule of research presentations, technical sessions, and expert panels, covering everything from discovering new therapeutic molecules with machine learning to funding AI research. Carefully curated, the conference provided a roadmap of bold tech ideas at work in health care now and traced the path to show how those tech solutions get implemented.
At the conference, Barzilay and Jim Collins, the Termeer Professor of Medical Engineering and Science in MIT’s Institute for Medical Engineering and Science (IMES) and Department of Biological Engineering, and Jameel Clinic faculty co-lead, presented research from a study published in Cell where they used machine learning to help identify a new drug that can target antibiotic-resistant bacteria. Together with MIT researchers Tommi Jaakkola, Kevin Yang, Kyle Swanson, and the first author Jonathan Stokes, they demonstrated how blending their backgrounds can yield potential answers to combat the growing antibiotic resistance crisis.
Collins saw the conference as an opportunity to inspire interest in antibiotic research, hoping to get the top young minds involved in battling resistance to antibiotics built up over decades of overuse and misuse, an urgent predicament in medicine that computer science students might not understand their role in solving. “I think we should take advantage of the innovation ecosystem at MIT and the fact that there are many experts here at MIT who are willing to step outside their comfort zone and get engaged in a new problem,” Collins says. “Certainly in this case, the development and discovery of novel antibiotics, is critically needed around the globe.”
AIDM showed the power of collaboration, inviting experts from major health-care companies and relevant organizations like Merck, Bayer, Darpa, Google, Pfizer, Novartis, Amgen, the U.S. Food and Drug Administration, and Janssen. Reaching capacity for conference attendees, it also showed people are ready to pull together to get on the same page. “I think the time is right and I think the place is right,” Collins says. “I think MIT is well-positioned to be a national, if not an international leader in this space, given the excitement and engagement of our students and our position in Kendall Square.”
A biotech hub for decades, Kendall Square has come a long way since big data came to Cambridge, Massachusetts, forever changing life science companies based here. AIDM kicked off with Institute Professor and Professor of Biology Phillip Sharp walking attendees through a brief history of AI in health care in the area. He was perhaps the person at the conference most excited for others to see the potential, as through his long career, he’s watched firsthand the history of innovation that led to this conference.
“The bigger picture, which this conference is a major part of, is this bringing together of the life science — biologists and chemists with machine learning and artificial intelligence — it’s the future of life science,” Sharp says. “It’s clear. It will reshape how we talk about our science, how we think about solving problems, how we deal with the other parts of the process of taking insights to benefit society.”


",MIT conference reveals the power of using artificial intelligence to discover new drugs,2020-04-27,['Ashley Belanger'],School of Engineering/Harvard-MIT Health Sciences and Technology/Institute for Medical Engineering and Science (IMES)/Biological engineering/Artificial intelligence/MIT Schwarzman College of Computing/Covid-19/Special events and guest speakers/J-Clinic/Medicine/Drug discovery/Disease,"['drug', 'research', 'reveals', 'using', 'learning', 'engineering', 'drugs', 'discover', 'artificial', 'power', 'machine', 'intelligence', 'health', 'students', 'conference', 'mit', 'science']","Developing drugs to combat Covid-19 is a global priority, requiring communities to come together to fight the spread of infection.
At MIT, researchers with backgrounds in machine learning and life sciences are collaborating, sharing datasets and tools to develop machine learning methods that can identify novel cures for Covid-19.
As secretive as Silicon Valley seems, computer science and engineering students typically know what a job looks like when aspiring to join companies like Facebook or Tesla.
Reaching capacity for conference attendees, it also showed people are ready to pull together to get on the same page.
“The bigger picture, which this conference is a major part of, is this bringing together of the life science — biologists and chemists with machine learning and artificial intelligence — it’s the future of life science,” Sharp says.",Mit
141,https://news.mit.edu/2020/conduct-a-bot-muscle-signals-can-pilot-robot-mit-csail-0427,"


Albert Einstein famously postulated that “the only real valuable thing is intuition,” arguably one of the most important keys to understanding intention and communication. 
But intuitiveness is hard to teach — especially to a machine. Looking to improve this, a team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) came up with a method that dials us closer to more seamless human-robot collaboration. The system, called “Conduct-A-Bot,” uses human muscle signals from wearable sensors to pilot a robot’s movement. 
“We envision a world in which machines help people with cognitive and physical work, and to do so, they adapt to people rather than the other way around,” says Professor Daniela Rus, director of CSAIL, deputy dean of research for the MIT Stephen A. Schwarzman College of Computing, and co-author on a paper about the system. 
To enable seamless teamwork between people and machines, electromyography and motion sensors are worn on the biceps, triceps, and forearms to measure muscle signals and movement. Algorithms then process the signals to detect gestures in real time, without any offline calibration or per-user training data. The system uses just two or three wearable sensors, and nothing in the environment — largely reducing the barrier to casual users interacting with robots.
This work was funded, in part, by the Boeing Company. 







Play video






While Conduct-A-Bot could potentially be used for various scenarios, including navigating menus on electronic devices or supervising autonomous robots, for this research the team used a Parrot Bebop 2 drone, although any commercial drone could be used.
By detecting actions like rotational gestures, clenched fists, tensed arms, and activated forearms, Conduct-A-Bot can move the drone left, right, up, down, and forward, as well as allow it to rotate and stop. 
If you gestured toward the right to your friend, they could likely interpret that they should move in that direction. Similarly, if you waved your hand to the left, for example, the drone would follow suit and make a left turn. 
In tests, the drone correctly responded to 82 percent of over 1,500 human gestures when it was remotely controlled to fly through hoops. The system also correctly identified approximately 94 percent of cued gestures when the drone was not being controlled.
“Understanding our gestures could help robots interpret more of the nonverbal cues that we naturally use in everyday life,” says Joseph DelPreto, lead author on the new paper. “This type of system could help make interacting with a robot more similar to interacting with another person, and make it easier for someone to start using robots without prior experience or external sensors.” 
This type of system could eventually target a range of applications for human-robot collaboration, including remote exploration, assistive personal robots, or manufacturing tasks like delivering objects or lifting materials. 
These intelligent tools are also consistent with social distancing — and could potentially open up a realm of future contactless work. For example, you can imagine machines being controlled by humans to safely clean a hospital room, or drop off medications, while letting us humans stay a safe distance.
Muscle signals can often provide information about states that are hard to observe from vision, such as joint stiffness or fatigue.    
For example, if you watch a video of someone holding a large box, you might have difficulty guessing how much effort or force was needed — and a machine would also have difficulty gauging that from vision alone. Using muscle sensors opens up possibilities to estimate not only motion, but also the force and torque required to execute that physical trajectory.
For the gesture vocabulary currently used to control the robot, the movements were detected as follows: 


stiffening the upper arm to stop the robot (similar to briefly cringing when seeing something going wrong): biceps and triceps muscle signals;


waving the hand left/right and up/down to move the robot sideways or vertically: forearm muscle signals (with the forearm accelerometer indicating hand orientation);


fist clenching to move the robot forward: forearm muscle signals; and


rotating clockwise/counterclockwise to turn the robot: forearm gyroscope.


Machine learning classifiers detected the gestures using the wearable sensors. Unsupervised classifiers processed the muscle and motion data and clustered it in real time to learn how to separate gestures from other motions. A neural network also predicted wrist flexion or extension from forearm muscle signals.  
The system essentially calibrates itself to each person's signals while they're making gestures that control the robot, making it faster and easier for casual users to start interacting with robots.
In the future, the team hopes to expand the tests to include more subjects. And while the movements for Conduct-A-Bot cover common gestures for robot motion, the researchers want to extend the vocabulary to include more continuous or user-defined gestures. Eventually, the hope is to have the robots learn from these interactions to better understand the tasks and provide more predictive assistance or increase their autonomy. 
“This system moves one step closer to letting us work seamlessly with robots so they can become more effective and intelligent tools for everyday tasks,” says DelPreto. “As such collaborations continue to become more accessible and pervasive, the possibilities for synergistic benefit continue to deepen.” 
DelPreto and Rus presented the paper virtually earlier this month at the ACM/IEEE International Conference on Human Robot Interaction.


",Muscle signals can pilot a robot,2020-04-27,['Rachel Gordon'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Robotics/Robots/Research/Wearable sensors/Neuroscience/Brain and cognitive sciences/Algorithms/Distributed Robotics Laboratory/School of Engineering/Artificial intelligence/Muscles/Human-computer interaction/MIT Schwarzman College of Computing/Electrical Engineering & Computer Science (eecs),"['drone', 'gestures', 'signals', 'pilot', 'work', 'forearm', 'robots', 'sensors', 'muscle', 'system', 'robot']","To enable seamless teamwork between people and machines, electromyography and motion sensors are worn on the biceps, triceps, and forearms to measure muscle signals and movement.
The system, called “Conduct-A-Bot,” uses human muscle signals from wearable sensors to pilot a robot’s movement.
Muscle signals can often provide information about states that are hard to observe from vision, such as joint stiffness or fatigue.
For the gesture vocabulary currently used to control the robot, the movements were detected as follows:stiffening the upper arm to stop the robot (similar to briefly cringing when seeing something going wrong): biceps and triceps muscle signals;waving the hand left/right and up/down to move the robot sideways or vertically: forearm muscle signals (with the forearm accelerometer indicating hand orientation);fist clenching to move the robot forward: forearm muscle signals; androtating clockwise/counterclockwise to turn the robot: forearm gyroscope.
A neural network also predicted wrist flexion or extension from forearm muscle signals.",Mit
142,https://news.mit.edu/2020/artificial-intelligence-ai-carbon-footprint-0423,"


Artificial intelligence has become a focus of certain ethical concerns, but it also has some major sustainability issues. 
Last June, researchers at the University of Massachusetts at Amherst released a startling report estimating that the amount of power required for training and searching a certain neural network architecture involves the emissions of roughly 626,000 pounds of carbon dioxide. That’s equivalent to nearly five times the lifetime emissions of the average U.S. car, including its manufacturing.
This issue gets even more severe in the model deployment phase, where deep neural networks need to be deployed on diverse hardware platforms, each with different properties and computational resources. 
MIT researchers have developed a new automated AI system for training and running certain neural networks. Results indicate that, by improving the computational efficiency of the system in some key ways, the system can cut down the pounds of carbon emissions involved — in some cases, down to low triple digits. 
The researchers’ system, which they call a once-for-all network, trains one large neural network comprising many pretrained subnetworks of different sizes that can be tailored to diverse hardware platforms without retraining. This dramatically reduces the energy usually required to train each specialized neural network for new platforms — which can include billions of internet of things (IoT) devices. Using the system to train a computer-vision model, they estimated that the process required roughly 1/1,300 the carbon emissions compared to today’s state-of-the-art neural architecture search approaches, while reducing the inference time by 1.5-2.6 times. 
“The aim is smaller, greener neural networks,” says Song Han, an assistant professor in the Department of Electrical Engineering and Computer Science. “Searching efficient neural network architectures has until now had a huge carbon footprint. But we reduced that footprint by orders of magnitude with these new methods.”
The work was carried out on Satori, an efficient computing cluster donated to MIT by IBM that is capable of performing 2 quadrillion calculations per second. The paper is being presented next week at the International Conference on Learning Representations. Joining Han on the paper are four undergraduate and graduate students from EECS, MIT-IBM Watson AI Lab, and Shanghai Jiao Tong University. 
Creating a “once-for-all” network
The researchers built the system on a recent AI advance called AutoML (for automatic machine learning), which eliminates manual network design. Neural networks automatically search massive design spaces for network architectures tailored, for instance, to specific hardware platforms. But there’s still a training efficiency issue: Each model has to be selected then trained from scratch for its platform architecture. 
“How do we train all those networks efficiently for such a broad spectrum of devices — from a $10 IoT device to a $600 smartphone? Given the diversity of IoT devices, the computation cost of neural architecture search will explode,” Han says.   
The researchers invented an AutoML system that trains only a single, large “once-for-all” (OFA) network that serves as a “mother” network, nesting an extremely high number of subnetworks that are sparsely activated from the mother network. OFA shares all its learned weights with all subnetworks — meaning they come essentially pretrained. Thus, each subnetwork can operate independently at inference time without retraining. 
The team trained an OFA convolutional neural network (CNN) — commonly used for image-processing tasks — with versatile architectural configurations, including different numbers of layers and “neurons,” diverse filter sizes, and diverse input image resolutions. Given a specific platform, the system uses the OFA as the search space to find the best subnetwork based on the accuracy and latency tradeoffs that correlate to the platform’s power and speed limits. For an IoT device, for instance, the system will find a smaller subnetwork. For smartphones, it will select larger subnetworks, but with different structures depending on individual battery lifetimes and computation resources. OFA decouples model training and architecture search, and spreads the one-time training cost across many inference hardware platforms and resource constraints. 
This relies on a “progressive shrinking” algorithm that efficiently trains the OFA network to support all of the subnetworks simultaneously. It starts with training the full network with the maximum size, then progressively shrinks the sizes of the network to include smaller subnetworks. Smaller subnetworks are trained with the help of large subnetworks to grow together. In the end, all of the subnetworks with different sizes are supported, allowing fast specialization based on the platform’s power and speed limits. It supports many hardware devices with zero training cost when adding a new device.
 
In total, one OFA, the researchers found, can comprise more than 10 quintillion — that’s a 1 followed by 19 zeroes — architectural settings, covering probably all platforms ever needed. But training the OFA and searching it ends up being far more efficient than spending hours training each neural network per platform. Moreover, OFA does not compromise accuracy or inference efficiency. Instead, it provides state-of-the-art ImageNet accuracy on mobile devices. And, compared with state-of-the-art industry-leading CNN models , the researchers say OFA provides 1.5-2.6 times speedup, with superior accuracy. 
    
“That’s a breakthrough technology,” Han says. “If we want to run powerful AI on consumer devices, we have to figure out how to shrink AI down to size.”
“The model is really compact. I am very excited to see OFA can keep pushing the boundary of efficient deep learning on edge devices,” says Chuang Gan, a researcher at the MIT-IBM Watson AI Lab and co-author of the paper.
“If rapid progress in AI is to continue, we need to reduce its environmental impact,” says John Cohn, an IBM fellow and member of the MIT-IBM Watson AI Lab. “The upside of developing methods to make AI models smaller and more efficient is that the models may also perform better.”


",Reducing the carbon footprint of artificial intelligence,2020-04-23,['Rob Matheson'],Research/Computer science and technology/Algorithms/Microsystems Technology Laboratories/Electrical Engineering & Computer Science (eecs)/School of Engineering/Artificial intelligence/Machine learning/MIT-IBM Watson AI Lab/MIT Schwarzman College of Computing,"['carbon', 'subnetworks', 'researchers', 'devices', 'reducing', 'network', 'ai', 'system', 'artificial', 'footprint', 'platforms', 'intelligence', 'training', 'ofa', 'neural']","Artificial intelligence has become a focus of certain ethical concerns, but it also has some major sustainability issues.
MIT researchers have developed a new automated AI system for training and running certain neural networks.
“Searching efficient neural network architectures has until now had a huge carbon footprint.
This relies on a “progressive shrinking” algorithm that efficiently trains the OFA network to support all of the subnetworks simultaneously.
But training the OFA and searching it ends up being far more efficient than spending hours training each neural network per platform.",Mit
143,https://news.mit.edu/2020/jim-collins-receives-funding-harness-ai-drug-discovery-0422,"


Housed at TED and supported by leading social impact advisor The Bridgespan Group, The Audacious Project is a collaborative funding initiative that’s catalyzing social impact on a grand scale by convening funders and social entrepreneurs, with the goal of supporting bold solutions to the world’s most urgent challenges.

Among this year’s carefully selected change-makers is Jim Collins and a team at MIT’s Abdul Latif Jameel Clinic for Machine Learning in Health (Jameel Clinic), including co-principal investigator Regina Barzilay. The funding provided through The Audacious Project will support the response to the antibiotic resistance crisis through the development of new classes of antibiotics to protect patients against some of the world’s deadliest bacterial pathogens.
“The work of Jim Collins and his colleagues is more relevant now than ever before,” says Anantha P. Chandrakasan, dean of the MIT School of Engineering and the Vannevar Bush Professor of Electrical Engineering and Computer Science. “We are grateful for the commitment from The Audacious Project and its contributors, to both support and foster the research around AI and drug discovery, and to join our efforts in the School of Engineering to realize the potential global impact of this incredible work.” 
Collins’ and Barzilay’s Antibiotics-AI Project seeks to produce the first new classes of antibiotics society has seen in three decades, by calling in an interdisciplinary team of world-class bioengineers, microbiologists, computer scientists, and chemists.
Collins is the Termeer Professor of Medical Engineering and Science in MIT’s Institute for Medical Engineering and Science (IMES) and the Department of Biological Engineering, faculty co-lead of Jameel Clinic, faculty lead of the MIT-Takeda Program, and a member of the Harvard-MIT Health Sciences and Technology faculty. He is also a core founding faculty member of the Wyss Institute for Biologically Inspired Engineering at Harvard University and an Institute member of the Broad Institute of MIT and Harvard.
Barzilay is the Delta Electronics Professor in MIT’s Department of Electrical Engineering and Computer Science, faculty co-lead of Jameel Clinic, and a member of the Computer Science and Artificial Intelligence Laboratory at MIT.
Earlier this year, Collins and Barzilay along with Tommi Jaakkola, Thomas Siebel Professor of Electrical Engineering and Computer Science and the Institute for Data, Systems, and Society, and postdoc Jonathan Stokes were part of a research team that successfully used a deep-learning model to identify a new antibiotic. Over the next seven years, The Audacious Project’s commitment will support Collins and Barzilay as they continue to use the same process to rapidly explore over a billion molecules to identify and design novel antibiotics.


",Jim Collins receives funding to harness AI for drug discovery,2020-04-23,['Lori Loturco'],"School of Engineering/Institute for Medical Engineering and Science (IMES)/Biological engineering/Harvard-MIT Health Sciences and Technology/Broad Institute/Electrical engineering and computer science (EECS)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Institute for Data, Systems, and Society/Drug development/Antibiotics/Funding","['drug', 'harness', 'institute', 'faculty', 'computer', 'jameel', 'engineering', 'receives', 'discovery', 'funding', 'ai', 'professor', 'jim', 'collins', 'project', 'member', 'science']","Among this year’s carefully selected change-makers is Jim Collins and a team at MIT’s Abdul Latif Jameel Clinic for Machine Learning in Health (Jameel Clinic), including co-principal investigator Regina Barzilay.
“The work of Jim Collins and his colleagues is more relevant now than ever before,” says Anantha P. Chandrakasan, dean of the MIT School of Engineering and the Vannevar Bush Professor of Electrical Engineering and Computer Science.
He is also a core founding faculty member of the Wyss Institute for Biologically Inspired Engineering at Harvard University and an Institute member of the Broad Institute of MIT and Harvard.
Barzilay is the Delta Electronics Professor in MIT’s Department of Electrical Engineering and Computer Science, faculty co-lead of Jameel Clinic, and a member of the Computer Science and Artificial Intelligence Laboratory at MIT.
Over the next seven years, The Audacious Project’s commitment will support Collins and Barzilay as they continue to use the same process to rapidly explore over a billion molecules to identify and design novel antibiotics.",Mit
144,https://news.mit.edu/2020/lidar-and-ai-road-status-clears-after-disaster-0415,"


Consider the days after a hurricane strikes. Trees and debris are blocking roads, bridges are destroyed, and sections of roadway are washed out. Emergency managers soon face a bevy of questions: How can supplies get delivered to certain areas? What's the best route for evacuating survivors? Which roads are too damaged to remain open?
Without concrete data on the state of the road network, emergency managers often have to base their answers on incomplete information. The Humanitarian Assistance and Disaster Relief Systems Group at MIT Lincoln Laboratory hopes to use its airborne lidar platform, paired with artificial intelligence (AI) algorithms, to fill this information gap.  
""For a truly large-scale catastrophe, understanding the state of the transportation system as early as possible is critical,"" says Chad Council, a researcher in the group. ""With our particular approach, you can determine road viability, do optimal routing, and also get quantified road damage. You fly it, you run it, you've got everything.""
Since the 2017 hurricane season, the team has been flying its advanced lidar platform over stricken cities and towns. Lidar works by pulsing photons down over an area and measuring the time it takes for each photon to bounce back to the sensor. These time-of-arrival data points paint a 3D ""point cloud"" map of the landscape — every road, tree, and building — to within about a foot of accuracy.
To date, they've mapped huge swaths of the Carolinas, Florida, Texas, and all of Puerto Rico. In the immediate aftermath of hurricanes in those areas, the team manually sifted through the data to help the Federal Emergency Management Agency (FEMA) find and quantify damage to roads, among other tasks. The team's focus now is on developing AI algorithms that can automate these processes and find ways to route around damage.
What's the road status?
Information about the road network after a disaster comes to emergency managers in a ""mosaic of different information streams,"" Council says, namely satellite images, aerial photographs taken by the Civil Air Patrol, and crowdsourcing from vetted sources.
""These various efforts for acquiring data are important because every situation is different. There might be cases when crowdsourcing is fastest, and it's good to have redundancy. But when you consider the scale of disasters like Hurricane Maria on Puerto Rico, these various streams can be overwhelming, incomplete, and difficult to coalesce,"" he says.
During these times, lidar can act as an all-seeing eye, providing a big-picture map of an area and also granular details on road features. The laboratory's platform is especially advanced because it uses Geiger-mode lidar, which is sensitive to a single photon. As such, its sensor can collect each of the millions of photons that trickle through openings in foliage as the system is flown overhead. This foliage can then be filtered out of the lidar map, revealing roads that would otherwise be hidden from aerial view.
To provide the status of the road network, the lidar map is first run through a neural network. This neural network is trained to find and extract the roads, and to determine their widths. Then, AI algorithms search these roads and flag anomalies that indicate the roads are impassable. For example, a cluster of lidar points extending up and across a road is likely a downed tree. A sudden drop in the elevation is likely a hole or washed out area in a road.
The extracted road network, with its flagged anomalies, is then merged with an OpenStreetMap of the area (an open-access map similar to Google Maps). Emergency managers can use this system to plan routes, or in other cases to identify isolated communities — those that are cut off from the road network. The system will show them the most efficient route between two specified locations, finding detours around impassable roads. Users can also specify how important it is to stay on the road; on the basis of that input, the system provides routes through parking lots or fields.  
This process, from extracting roads to finding damage to planning routes, can be applied to the data at the scale of a single neighborhood or across an entire city.
How fast and how accurate?
To gain an idea of how fast this system works, consider that in a recent test, the team flew the lidar platform, processed the data, and got AI-based analytics in 36 hours. That sortie covered an area of 250 square miles, an area about the size of Chicago, Illinois.
But accuracy is equally as important as speed. ""As we incorporate AI techniques into decision support, we're developing metrics to characterize an algorithm's performance,"" Council says.
For finding roads, the algorithm determines if a point in the lidar point cloud is ""road"" or ""not road."" The team ran a performance evaluation of the algorithm against 50,000 square meters of suburban data, and the resulting ROC curve indicated that the current algorithm provided an 87 percent true positive rate (that is, correctly labeled a point as ""road""), with a 20 percent false positive rate (that is, labeling a point as ""road"" that may not be road). The false positives are typically areas that geometrically look like a road but aren't.
""Because we have another data source for identifying the general location of roads, OpenStreetMaps, these false positives can be excluded, resulting in a highly accurate 3D point cloud representation of the road network,"" says Dieter Schuldt, who has been leading the algorithm-testing efforts.
For the algorithm that detects road damage, the team is in the process of further aggregating ground truth data to evaluate its performance. In the meantime, preliminary results have been promising. Their damage-finding algorithm recently flagged for review a potentially blocked road in Bedford, Massachusetts, which appeared to be a hole measuring 10 meters wide by 7 meters long by 1 meter deep. The town's public works department and a site visit confirmed that construction blocked the road.
""We actually didn't go in expecting that this particular sortie would capture examples of blocked roads, and it was an interesting find,"" says Bhavani Ananthabhotla, a contributor to this work. ""With additional ground truth annotations, we hope to not only evaluate and improve performance, but also to better tailor future models to regional emergency management needs, including informing route planning and repair cost estimation.""
The team is continuing to test, train, and tweak their algorithms to improve accuracy. Their hope is that these techniques may soon be deployed to help answer important questions during disaster recovery.
""We picture lidar as a 3D scaffold that other data can be draped over and that can be trusted,"" Council says. ""The more trust, the more likely an emergency manager, and a community in general, will use it to make the best decisions they can.""


","With lidar and artificial intelligence, road status clears up after a disaster",2020-04-21,['Kylie Foy'],Lincoln Laboratory/Natural disasters/Imaging/Sensors/Weather/Artificial intelligence/Algorithms/Research/Disaster response/Technology and society/Radar,"['lidar', 'data', 'disaster', 'point', 'emergency', 'area', 'roads', 'clears', 'road', 'network', 'system', 'artificial', 'intelligence', 'status', 'team']","Without concrete data on the state of the road network, emergency managers often have to base their answers on incomplete information.
""With our particular approach, you can determine road viability, do optimal routing, and also get quantified road damage.
To provide the status of the road network, the lidar map is first run through a neural network.
The extracted road network, with its flagged anomalies, is then merged with an OpenStreetMap of the area (an open-access map similar to Google Maps).
For finding roads, the algorithm determines if a point in the lidar point cloud is ""road"" or ""not road.""",Mit
145,https://news.mit.edu/2020/mit-professor-daniela-rus-white-house-science-council-0421,"


This week the White House announced that MIT Professor Daniela Rus, director of MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), has been selected to serve on the President’s Council of Advisors on Science and Technology (PCAST).
The council provides advice to the White House on topics critical to U.S. security and the economy, including policy recommendations on the future of work, American leadership in science and technology, and the support of U.S. research and development. 
PCAST operates under the aegis of the White House Office of Science and Technology Policy (OSTP), which was established in law in 1976. However, the council has existed more informally going back to Franklin Roosevelt’s Science Advisory Board in 1933.
“I’m grateful to be able to add my perspective as a computer scientist to this group at a time when so many issues involving AI and other aspects of computing raise important scientific and policy questions for the nation and the world,” says Rus.
 
Rus is the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science at MIT and the deputy dean of research for the MIT Stephen A. Schwarzman College of Computing. Her research in robotics, artificial intelligence, and data science focuses primarily on developing the science and engineering of autonomy, with the long-term objective of enabling a future where machines are integrated into daily life to support both cognitive and physical tasks. The applications of her work are broad and include transportation, manufacturing, medicine, and urban planning. 
 
More than a dozen MIT faculty and alumni have served on PCAST during past presidential administrations. These include former MIT president Charles Vest; Institute Professors Phillip Sharp and John Deutch; Ernest Moniz, professor of physics and former U.S. Secretary of Energy; and Eric Lander, director of the Broad Institute of MIT and Harvard and professor of biology, who co-chaired PCAST during the Obama administration. Previous councils have offered advice on topics ranging from data privacy and nanotechnology to job training and STEM education.


",Professor Daniela Rus named to White House science council,2020-04-21,['Adam Conner-Simons'],Industry/National relations and service/Policy/Government/Public service/Technology and society/Faculty/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/School of Engineering/MIT Schwarzman College of Computing/Artificial intelligence,"['technology', 'named', 'research', 'house', 'computer', 'daniela', 'policy', 'professor', 'white', 'rus', 'council', 'science', 'mit']","This week the White House announced that MIT Professor Daniela Rus, director of MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL), has been selected to serve on the President’s Council of Advisors on Science and Technology (PCAST).
The council provides advice to the White House on topics critical to U.S. security and the economy, including policy recommendations on the future of work, American leadership in science and technology, and the support of U.S. research and development.
PCAST operates under the aegis of the White House Office of Science and Technology Policy (OSTP), which was established in law in 1976.
However, the council has existed more informally going back to Franklin Roosevelt’s Science Advisory Board in 1933.
Rus is the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science at MIT and the deputy dean of research for the MIT Stephen A. Schwarzman College of Computing.",Mit
146,https://news.mit.edu/2020/e-vent-covid-19-ventilator-shortage-0420,"


It was clear early on in the unfolding Covid-19 pandemic that a critical need in the coming weeks and months would be for ventilators, the potentially life-saving devices that keep air flowing into a patient whose ability to breathe is failing.Seeing a potential shortfall of hundreds of thousands of such units, professor of mechanical engineering Alex Slocum Sr. and other engineers at MIT swung into action, rapidly pulling together a team of volunteers with expertise in mechanical design, electronics, and controls, and a team of doctors with clinical experience in treating respiratory conditions. They started working together nonstop to develop an inexpensive alternative and share what they learned along the way. The goal was a design that could be produced quickly enough, potentially worldwide, to make a real difference in the immediate crisis.In a very short time, they succeeded.Just four weeks since the team convened, production of the first devices based directly on its work has begun in New York City. A group including 10XBeta, Boyce Technologies, and Newlab has begun production of a version called Spiro Wave, in close collaboration with the MIT team. The consortium expects to quickly deliver hundreds of units to meet the immediate needs of hospitals in New York and, eventually, other hospitals around the country.Meanwhile, the team, called MIT E-Vent, has continued their research to develop the design further. The next iteration will be more compact, have a slightly different drive system, and add a key respiratory function. Their overarching goal is to focus on safety and straightforward functionality and fabrication. 10XBeta, in both New York and Johannesburg, along with Vecna Technologies and NN Life Sciences in the Boston Area, are participating in this effort. 10XBeta was founded by MIT alumnus Marcel Botha SM ’06.

One version of the MIT E-Vent team's emergency ventilator design undergoes testing in their lab. Courtesy of MIT E-Vent TeamA complex design challengeAlexander Slocum Jr. SB ’08, SM ’10, PhD ’13, a mechanical engineer who is now a surgical resident at the Medical College of Wisconsin, worked closely with his father, Slocum Sr., and MIT research scientist Nevan Hanumara MS ’06, PhD ’12 to help lead the initial ramp up.“The numbers are frightening, to put it bluntly,” Slocum Jr. says. “This project started around the time of news reports from Italy describing ventilators being rationed due to shortages, and available data at that time suggested about 10 percent of Covid patients would require an ICU.” One of his first tasks was to estimate the potential ventilator shortage, using resources like the CDC’s Pandemic Response Plan, and literature on critical care resource utilization. “We estimated a shortage of around 100,000 to 200,000 ventilators was possible by April or May,” he says.Hanumara, who is one of the project leads for the E-Vent team, says the team intends to offer open-source guidelines, rather than detailed plans or kits, that will serve as resources to enable skilled teams around the country and world — such as hospital-based engineering groups, biomedical device manufacturing companies, and industry groups — to develop their own specific versions, taking into account local supply chains.“There's a reason we don't have a single exact plan [on the website],” Hanumara says. “We have information and reference designs, because this isn't something a home hobbyist should be making. We want to emphasize that it’s not trivial to create a system that can provide ventilation safely.”“We saw all these designs being posted online, which is awesome that so many people wanted to help,” says Slocum Jr. “We thought the best first step would be to identify the minimum clinical functional requirements for safe ventilation, compare that to reported methods for managing ventilated patients with Covid, and use that to help us choose a design.”The principle behind the existing device is certainly simple enough: Take an emergency resuscitator bag (Ambu is a common brand), which hospitals already have in large numbers and which is designed to be squeezed by hand. Automating the squeezing — using a pair of curved paddles driven by a motor — would allow rapid scale-up. But there’s a lot more to it, Hanumara says: “The controls are really tricky, and they have required many iterations as our understanding of the clinical and safety challenge grew.”Slocum Jr. adds, “Covid patients often require ventilation for a week or more, and in longer cases that would mean about a million breaths. The paddles are specifically designed to encourage rolling contact in order to minimize wear on the bag.”The starting point was a design developed a decade ago as a student team project in MIT class 2.75 (Medical Device Design), taught by Slocum Sr. and Hanumara. The team’s paper gave the new project a significant head start in tackling the design problem now, as they make rapid progress in close consultation with clinical practitioners.That integral involvement of clinicians “is one key difference between us and a lot of the others” working on this engineering problem, says Kimberly Jung, an MIT master’s student in mechanical engineering.Jung — who previously served five years in the U.S. Army, earned an MBA at Harvard University, and started a spice business that is currently the largest employer of women in Afghanistan — has been acting as the team’s executive officer as well as part of the engineering team. She says “there's a lot of individuals and many small companies who are trying to make solutions for low-cost ventilators. The problem is that they just haven't adhered to clinical guidelines, such as the tidal volume, inspiration-to-expiration ratio, breath per minute rate, maximum pressures, and key monitoring for safety. Developing these clinical requirements and translating them into engineering design requirements takes a lot of time and effort. This is a year-long research and development process that has been condensed into several weeks.”A team assemblesOthers got pulled into the team as the project ramped up. Coby Unger, an industrial designer and instructor at the MIT Hobby Shop, started building the first prototypes in the machine shop. Jung recruited her classmate and neighbor, Shakti Shaligram SM ’19, to help with machining, and also brought in Michael Detienne, an electrical engineer and member of the MITERS makerspace. Two students at the MIT Maker Workshop helped with initial fabrication with stock borrowed from the MIT Laboratory for Manufacturing and Productivity shop. Looking for pressure sensors, Hanumara reached out to David Hagan PhD ’20, CEO of an MIT spinoff company called QuantAQ, and he joined the team. The website was rapidly deployed by Eric Norman, a communications expert who had worked with Hanumara on another MIT project.Realizing that feedback and control systems were crucial to the device’s safe operation, the team early on decided they needed help from specialists in that area. Daniela Rus, head of MIT’s Computer Science and Artificial Intelligence Laboratory, joined the team and took responsibility for the control system along with several members of her research group. Rus also suggested research scientist Murad Abu-Khalaf and graduate students Teddy Ort and Brandon Araki join the volunteer team. They eagerly accepted the invictation. Ort’s roommate, Amado Antonini SM ’18, also joined the team to assist with motor controls.Meanwhile, alumnus Albert Kwon SB ’08, HST ’13, an anesthesiologist at Westchester Medical Center and assistant professor of anesthesiology at New York Medical College, was recruited by Slocum Jr. to join the project early on. Kwon was granted leave from his job at Westchester to devote time to the project, providing clinical guidance on the kinds of controls and safety systems needed for the device to work safely. “Westchester Medical Center gave him up, which is very special, and he's been working to translate the technical to clinical, and explain the scenarios that fit with a stripped-down system like this,” Hanumara says. Jay Connor, a surgeon at Mt. Auburn Hospital and part of the Medical Device Design course teaching team, Christoph Nabzdyk, a cardiothoracic anesthesiologist and critical care physician at Mayo Clinic and long-time colleague of Kwon, and Dirk Varelmann, another anesthesiologist from Brigham and Women’s Hospital, and many other clinicians advised the MIT E-Vent team.A spark to help others fill the gap“While our design cannot replace a full featured ventilator,” Hanumara stresses, “it does provide key ventilation functions that will allow health care facilities under pressure to better ration their ICU ventilators and human resources, in a bad scenario.”In a way, he says, “we're turning the clock back, going back to the core parameters of ventilation.” Before today’s electronic sensors and controls were available, “doctors were trained to adjust the ventilators based on looking directly at physiological responses of the patient. So, we know that’s doable. … The patient himself is a reasonable sensor.”While the federal government has now established contracts with large manufacturing companies to start producing ventilators to help meet the urgent need, that process will take time, Jung says, leaving a significant gap for something to meet the need in the meantime. “The fastest these large manufacturers can spin up is about two months,” she says.“This need will probably be even more pronounced in the emerging markets,” Hanumara adds.The team doesn’t plan to directly launch their own production, or even to provide a single, detailed set of plans. “Our goal is to put out a really solid reference design,” Hanumara says “and to a limited extent help big groups scale it. We have shared great learnings with our local industry collaborators.” It will be up to local teams to adapt the design to the materials and parts that they can reliably obtain and the particular needs of their hospitals.He says “your mechanical and electrical engineering team will have to inquire as to what's in their supply chain and what fabrication methods they have easily available to them and adapt the design. The base designs are intended to be really adaptable, but it may require modifications. What motors can they source? What motor drivers and controllers do electrical team need to look at? What level of controls and safeties do their clinicians require for their patient population and how should this be reflected in the code? So, we can't put out an exact kit,” says Hanumara.The hope is to provide a spark to start teams everywhere to further develop and adapt the concept, Hanumara says. “Provided clinical safety is shown, we'll probably see many of these around the world, with some shared DNA from us, as well as local flavors. And I think that will be beautiful, because it will mean that people all over are working hard to help their communities.”“I'm super proud of the team,” Jung says, “for how each of us has stepped up to the plate and stuck with it despite the internal and external challenges. All of us have one mission in mind, which is to save lives, and that's what has kept us together and turned us into a quirky MIT family.”


",MIT team races to fill Covid-19-related ventilator shortage,2020-04-20,['David L. Chandler'],Mechanical engineering/School of Engineering/Electrical Engineering & Computer Science (eecs)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Covid-19/Health science and technology/Medicine/Pandemic,"['covid19related', 'ventilator', 'teams', 'slocum', 'shortage', 'ventilators', 'clinical', 'races', 'help', 'hanumara', 'design', 'project', 'mit', 'team']","A group including 10XBeta, Boyce Technologies, and Newlab has begun production of a version called Spiro Wave, in close collaboration with the MIT team.
Meanwhile, the team, called MIT E-Vent, has continued their research to develop the design further.
One version of the MIT E-Vent team's emergency ventilator design undergoes testing in their lab.
The website was rapidly deployed by Eric Norman, a communications expert who had worked with Hanumara on another MIT project.
“Our goal is to put out a really solid reference design,” Hanumara says “and to a limited extent help big groups scale it.",Mit
147,https://news.mit.edu/2020/posh-chatbots-0417,"


The comedian Bill Burr has said he refuses to call into automated customer service lines for fear that, years later on his death bed, all he’ll be able to think about are the moments he wasted dealing with chatbots.Indeed, the frustrating experience of trying to complete even the most straightforward task through an automated customer service line is enough to make anyone question the purpose of life.Now the startup Posh is trying to make conversations with chatbots more natural and less maddening. It’s accomplishing this with an artificial intelligence-powered system that uses “conversational memory” to help users complete tasks.“We noticed bots in general would take what the user said at face value, without connecting the dots of what was said before in the conversation,” says Posh co-founder and CEO Karan Kashyap ’17, SM ’17. “If you think about your conversations with humans, especially in places like banks with tellers or in customer service, what you said in the past is very important, so we focused on making bots more humanlike by giving them the ability to remember historical information in a conversation.”Posh’s chatbots are currently used by over a dozen credit unions across voice- and text-based channels. The well-defined customer base has allowed the company to train its system on only the most relevant data, improving performance.The founders plan to gradually partner with companies in other sectors to gather industry-specific data and expand the use of their system without compromising performance. Down the line, Kashyap and Posh co-founder and CTO Matt McEachern ’17, SM ’18 plan to provide their chatbots as a platform for developers to build on.The expansion plans should attract businesses in a variety of sectors: Kashyap says some credit unions have successfully resolved more than 90 percent of customer calls with Posh’s platform. The company’s expansion may also help alleviate the mind-numbing experience of calling into traditional customer service lines.“When we deploy our telephone product, there’s no notion of ‘Press one or press two,’” Kashyap explains. “There’s no dial tone menu. We just say, ‘Welcome to whatever credit union, how can I help you today?’ In a few words, you let us know. We prompt users to describe their problems via natural speech instead of waiting for menu options to be read out.”Bootstrapping better botsKashyap and McEachern became friends while pursuing their degrees in MIT’s Department of Electrical Engineering and Computer Science. They also worked together in the same research lab at the Computer Science and Artificial Intelligence Laboratory (CSAIL).But their relationship quickly grew outside of MIT. In 2016, the students began software consulting, in part designing chatbots for companies to handle customer inquiries around medical devices, flight booking, personal fitness, and more. Kashyap says they used their time consulting to learn about and take business risks.“That was a great learning experience, because we got real-world experience in designing these bots using the tools that were available,” Kashyap says. “We saw the market need for a bot platform and for better bot experiences.”From the start, the founders executed a lean business strategy that made it clear the engineering undergrads were thinking long term. Upon graduation, the founders used their savings from consulting to fund Posh’s early operations, giving themselves salaries and even hiring some contacts from MIT.It also helped that they were accepted into the delta v accelerator, run by the Martin Trust Center for MIT Entrepreneurship, which gave them a summer of guidance and free rent. Following delta v, Posh was accepted into the DCU Fintech Innovation Center, connecting it with one of the largest credit unions in the country and netting the company another 12 months of free rent.
 
With DCU serving as a pilot customer, the founders got a “crash course” in the credit union industry, Kashyap says. From there they began a calculated expansion to ensure they didn’t grow faster than Posh’s revenue allowed, freeing them from having to raise venture capital.The disciplined growth strategy at times forced Posh to get creative. Last year, as the founders were looking to build out new features and grow their team, they secured about $1.5 million in prepayments from eight credit unions in exchange for discounts on their service along with a peer-driven profit-sharing incentive. In total, the company has raised $2.5 million using that strategy.Now on more secure financial footing, the founders are poised to accelerate Posh’s growth.Pushing the boundariesEven referring to today’s automated messaging platforms as chatbots seems generous. Most of the ones on the market today are only designed to understand what a user is asking for, something known as intent recognition.The result is that many of the virtual agents in our lives, from the robotic telecom operator to Amazon’s Alexa to the remote control, take directions but struggle to hold a conversation. Posh’s chatbots go beyond intent recognition, using what Kashyap calls context understanding to figure out what users are saying based on the history of the conversation. The founders have a patent pending for the approach.“[Context understanding] allows us to more intelligently understand user inputs and handle things like changes in topics without having the bots break,” Kashyap says. “One of our biggest pet peeves was, in order to have a successful interaction with a bot, you as a user have to be very unnatural sometimes to convey what you want to convey or the bot won’t understand you.”Kashyap says context understanding is a lot easier to accomplish when designing bots for specific industries. That’s why Posh’s founders decided to start by focusing on credit unions.“The platforms on the market today are almost spreading themselves too thin to make a deep impact in a particular vertical,” Kashyap says. “If you have banks and telecos and health care companies all using the same [chatbot] service, it’s as if they’re all sharing the same customer service rep. It’s difficult to have one person trained across all of these domains meaningfully.”To onboard a new credit union, Posh uses the customer’s conversational data to train its deep learning model.“The bots continue to train even after they go live and have actual conversations,” Kashyap says. “We’re always improving it; I don’t think we’ll ever deploy a bot and say it’s done.”Customers can use Posh’s bots for online chats, voice calls, SMS messaging, and through third party channels like Slack, WhatsApp, and Amazon Echo. Posh also offers an analytics platform to help customers analyze what users are calling about.For now, Kashyap says he’s focused on quadrupling the number of credit unions using Posh over the next year. Then again, the founders’ have never let short term business goals cloud their larger vision for the company.“Our perspective has always been that [the robot assistant] Jarvis from ‘Iron Man’ and the AI from the movie ‘Her’ are going to be reality sometime soon,” Kashyap says. “Someone has to pioneer the ability for bots to have contextual awareness and memory persistence. I think there’s a lot more that needs to go into bots overall, but we felt by pushing the boundaries a little bit, we’d succeed where other bots would fail, and ultimately people would like to use our bots more than others.”


",Deploying more conversational chatbots,2020-04-17,['Zach Winn'],Innovation and Entrepreneurship (I&E)/Startups/Business and management/Alumni/ae/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical engineering and computer science (EECS)/Machine learning/Artificial intelligence/School of Engineering,"['deploying', 'bots', 'customer', 'credit', 'using', 'founders', 'conversational', 'posh', 'poshs', 'service', 'kashyap', 'chatbots']","Indeed, the frustrating experience of trying to complete even the most straightforward task through an automated customer service line is enough to make anyone question the purpose of life.
Down the line, Kashyap and Posh co-founder and CTO Matt McEachern ’17, SM ’18 plan to provide their chatbots as a platform for developers to build on.
The company’s expansion may also help alleviate the mind-numbing experience of calling into traditional customer service lines.
Posh’s chatbots go beyond intent recognition, using what Kashyap calls context understanding to figure out what users are saying based on the history of the conversation.
For now, Kashyap says he’s focused on quadrupling the number of credit unions using Posh over the next year.",Mit
148,https://news.mit.edu/2020/three-mit-students-named-2020-goldwater-scholars-0409,"


MIT students Katie Collins, Vaishnavi Phadnis, and Vaibhavi Shah have  been selected to receive a Barry Goldwater Scholarship for the 2020-21 academic year. Over 5,000 college students from across the United States were nominated for the scholarships, from which only 396 recipients were selected based on academic merit. 
The Goldwater scholarships have been conferred since 1989 by the Barry Goldwater Scholarship and Excellence in Education Foundation. These scholarships have supported undergraduates who go on to become leading scientists, engineers, and mathematicians in their respective fields. All of the 2020-21 Goldwater Scholars intend to obtain a doctorate in their area of research, including the three MIT recipients. 
Katie Collins, a third-year majoring in brain and cognitive sciences with minors in computer science and biomedical engineering, got involved with research in high school, when she worked on computational models of metabolic networks and synthetic gene networks in the lab of Department of Electrical Engineering and Computer Science Professor Timothy Lu at MIT. It was this project that led her to realize how challenging it is to model and analyze complex biological networks. She also learned that machine learning can provide a path for exploring these networks and understanding human diseases. This realization has coursed a scientific path for Collins that is equally steeped in computer science and human biology.
Over the past few years, Collins has become increasingly interested in the human brain, particularly what machine learning can learn from human common-sense reasoning and the way brains process sparse, noisy data. “I aim to develop novel computational algorithms to analyze complex, high-dimensional data in biomedicine, as well as advance modelling paradigms to improve our understanding of human cognition,” explains Collins. In his letter of recommendation, Professor Tomaso Poggio, the Eugene McDermott Professor in the Department of Brain and Cognitive Sciences and one of Collins’ mentors, wrote, “It is very difficult to imagine a better candidate for the Goldwater fellowship.” Collins plans to pursue a PhD studying machine learning or computational neuroscience and to one day run her own lab. “I hope to become a professor, leading a research program at the interface of computer science and cognitive neuroscience.”
Vaishnavi Phadnis, a second-year majoring in computer science and molecular biology, sees molecular and cellular biology as the bridge between chemistry and life, and she’s been enthralled with understanding that bridge since 7th grade, when she learned about the chemical basis of the cell. Phadnis spent two years working in a cancer research lab while still in high school, an experience which convinced her that research was not just her passion but also her future. “In my first week at MIT, I approached Professor Robert Weinberg, and I’ve been grateful to do research in his lab ever since,” she says. 
“Vaishnavi’s exuberance makes her a joy to have in the lab,” wrote Weinberg, who is the Daniel Ludwig Professor in the Department of Biology. Phadnis is investigating ferroptosis, a recently discovered, iron-dependent form of cell death that may be relevant in neurodegeneration and also a potential strategy for targeting highly aggressive cancer cells. “She is a phenomenon who has vastly exceeded our expectations of the powers of someone her age,” Weinberg says. Phadnis is thankful to Weinberg and all the scientific mentors, both past and present, that have inspired her along her research path. Deciphering the mechanisms behind fundamental cellular processes and exploring their application in human diseases is something Phadnis plans to continue doing in her future as a physician-scientist after pursuing an MD/PhD. “I hope to devote most of my time to leading my own research group, while also practicing medicine,” she says. 
Vaibhavi Shah, a third-year studying biological engineering with a minor in science, technology and society, spent a lot of time in high school theorizing ways to tackle major shortcomings in medicine and science with the help of technology. “When I came to college, I was able to bring some of these ideas to fruition,” she says, working with both the Big Data in Radiology Group at the University of California at San Francisco and the lab of Professor Mriganka Sur, the Newton Professor of Neuroscience in the Department of Brain and Cognitive Sciences. 
Shah is particularly interested in integrating innovative research findings with traditional clinical practices. According to her, technology, like computer vision algorithms, can be adopted to diagnose diseases such as Alzheimer’s, allowing patients to start appropriate treatments earlier. “This is often harder to do at smaller, rural institutions that may not always have a specialist present,” says Shah, and algorithms can help fill that gap. One of aims of Shah’s research is to improve the efficiency and equitability of physician decision-making. “My ultimate goal is to improve patient outcomes, and I aim to do this by tackling emerging scientific questions in machine learning and artificial intelligence at the forefront of neurology,” she says. The clinic is a place Shah expects to be in the future after obtaining her physician-scientist training, saying, “I hope to a practicing neurosurgeon and clinical investigator.”
The Barry Goldwater Scholarship and Excellence in Education Program was established by Congress in 1986 to honor Senator Barry Goldwater, a soldier and statesman who served the country for 56 years. Awardees receive scholarships of up to $7,500 a year to cover costs related to tuition, room and board, fees, and books.


","Katie Collins, Vaishnavi Phadnis, and Vaibhavi Shah named 2020-21 Goldwater Scholars",2020-04-09,['Fernanda Ferreira'],"School of Science/Brain and cognitive sciences/Electrical engineering and computer science (EECS)/Biology/Biological engineering/School of Engineering/Students/Undergraduate/Awards, honors and fellowships/Technology and society","['named', 'shah', 'goldwater', 'research', 'katie', 'human', 'computer', 'scholars', 'vaishnavi', 'professor', 'lab', 'collins', 'weinberg', 'vaibhavi', '202021', 'science', 'phadnis']","MIT students Katie Collins, Vaishnavi Phadnis, and Vaibhavi Shah have been selected to receive a Barry Goldwater Scholarship for the 2020-21 academic year.
The Goldwater scholarships have been conferred since 1989 by the Barry Goldwater Scholarship and Excellence in Education Foundation.
All of the 2020-21 Goldwater Scholars intend to obtain a doctorate in their area of research, including the three MIT recipients.
This realization has coursed a scientific path for Collins that is equally steeped in computer science and human biology.
Phadnis spent two years working in a cancer research lab while still in high school, an experience which convinced her that research was not just her passion but also her future.",Mit
149,https://news.mit.edu/2020/bluetooth-covid-19-contact-tracing-0409,"


Imagine you’ve been diagnosed as Covid-19 positive. Health officials begin contact tracing to contain infections, asking you to identify people with whom you’ve been in close contact. The obvious people come to mind — your family, your coworkers. But what about the woman ahead of you in line last week at the pharmacy, or the man bagging your groceries? Or any of the other strangers you may have come close to in the past 14 days?
A team led by MIT researchers and including experts from many institutions is developing a system that augments “manual” contact tracing by public health officials, while preserving the privacy of all individuals. The system relies on short-range Bluetooth signals emitted from people’s smartphones. These signals represent random strings of numbers, likened to “chirps” that other nearby smartphones can remember hearing.
If a person tests positive, they can upload the list of chirps their phone has put out in the past 14 days to a database. Other people can then scan the database to see if any of those chirps match the ones picked up by their phones. If there’s a match, a notification will inform that person that they may have been exposed to the virus, and will include information from public health authorities on next steps to take. Vitally, this entire process is done while maintaining the privacy of those who are Covid-19 positive and those wishing to check if they have been in contact with an infected person.
“I keep track of what I’ve broadcasted, and you keep track of what you’ve heard, and this will allow us to tell if someone was in close proximity to an infected person,” says Ron Rivest, MIT Institute Professor and principal investigator of the project. “But for these broadcasts, we’re using cryptographic techniques to generate random, rotating numbers that are not just anonymous, but pseudonymous, constantly changing their ‘ID,’ and that can’t be traced back to an individual.”
This approach to private, automated contact tracing will be available in a number of ways, including through the privacy-first effort launched at MIT in response to Covid-19 called SafePaths. This broad set of mobile apps is under development by a team led by Ramesh Raskar of the MIT Media Lab. The design of the new Bluetooth-based system has benefited from SafePaths’ early work in this area.







Play video






Bluetooth exchanges
Smartphones already have the ability to advertise their presence to other devices via Bluetooth. Apple’s “Find My” feature, for example, uses chirps from a lost iPhone or MacBook to catch the attention of other Apple devices, helping the owner of the lost device to eventually find it. 
“Find My inspired this system. If my phone is lost, it can start broadcasting a Bluetooth signal that’s just a random number; it’s like being in the middle of the ocean and waving a light. If someone walks by with Bluetooth enabled, their phone doesn’t know anything about me; it will just tell Apple, ‘Hey, I saw this light,’” says Marc Zissman, the associate head of MIT Lincoln Laboratory’s Cyber Security and Information Science Division and co-principal investigator of the project.
With their system, the team is essentially asking a phone to send out this kind of random signal all the time and to keep a log of these signals. At the same time, the phone detects chirps it has picked up from other phones, and only logs chirps that would be medically significant for contact tracing — those emitted from within an approximate 6-foot radius and picked up for a certain duration of time, say 10 minutes.
Phone owners would get involved by downloading an app that enables this system. After a positive diagnosis, a person would receive a QR code from a health official. By scanning the code through that app, that person can upload their log to the cloud. Anyone with the app could then initiate their phones to scan these logs. A notification, if there’s a match, could tell a user how long they were near an infected person and the approximate distance.  
Privacy-preserving technology
Some countries most successful at containing the spread of Covid-19 have been using smartphone-based approaches to conduct contact tracing, yet the researchers note these approaches have not always protected individual’s privacy. South Korea, for example, has implemented apps that notify officials if a diagnosed person has left their home, and can tap into people’s GPS data to pinpoint exactly where they’ve been.
“We’re not tracking location, not using GPS, not attaching your personal ID or phone number to any of these random numbers your phone is emitting,” says Daniel Weitzner, a principal research scientist in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and co-principal investigator of this effort. “What we want is to enable everyone to participate in a shared process of seeing if you might have been in contact, without revealing, or forcing anyone to reveal, anything.”
Choice is key. Weitzner sees the system as a virtual knock on the door that preserves people’s right to not answer it. The hope, though, is that everyone who can opt in would do so to help contain the spread of Covid-19. “We need a large percentage of the population to opt in for this system to really work. We care about every single Bluetooth device out there; it’s really critical to make this a whole ecosystem,” he says.
Public health impact
Throughout the development process, the researchers have worked closely with a medical advisory team to ensure that this system would contribute effectively to contact tracing efforts. This team is led by Louise Ivers, who is an infectious disease expert, associate professor at Harvard Medical School, and executive director of the Massachusetts General Hospital Center for Global Health.
“In order for the U.S. to really contain this epidemic, we need to have a much more proactive approach that allows us to trace more widely contacts for confirmed cases. This automated and privacy-protecting approach could really transform our ability to get the epidemic under control here and could be adapted to have use in other global settings,” Ivers says. “What’s also great is that the technology can be flexible to how public health officials want to manage contacts with exposed cases in their specific region, which may change over time.”
For example, the system could notify someone that they should self-isolate, or it could request that they check in through the app to connect with specialists regarding daily symptoms and well-being. In other circumstances, public health officials could request that this person get tested if they were noticing a cluster of cases.
The ability to conduct contact tracing quickly and at a large scale can be effective not only in flattening the curve of the outbreak, but also for enabling people to safely enter public life once a community is on the downward side of the curve. “We want to be able to let people carefully get back to normal life while also having this ability to carefully quarantine and identify certain vectors of an outbreak,” Rivest says.
Toward implementation
Lincoln Laboratory engineers have led the prototyping of the system. One of the hardest technical challenges has been achieving interoperability, that is, making it possible for a chirp from an iPhone to be picked up by an Android device and vice versa. A test at the laboratory late last week proved that they achieved this capability, and that chirps could be picked up by other phones of various makes and models.
A vital next step toward implementation is engaging with the smartphone manufacturers and software developers — Apple, Google, and Microsoft. “They have a critical role here. The aim of the prototype is to prove to these developers that this is feasible for them to implement,” Rivest says. As those collaborations are forming, the team is also demonstrating its prototype system to state and federal government agencies.
Rivest emphasizes that collaboration has made this project possible. These collaborators include the Massachusetts General Hospital Center for Global Health, CSAIL, MIT Lincoln Laboratory, Boston University, Brown University, MIT Media Lab, The Weizmann Institute of Science, and SRI International.
The team also aims to play a central, coordinating role with other efforts around the country and in Europe to develop similar, privacy-preserving contact-tracing systems.
“This project is being done in true academic style. It’s not a contest; it’s a collective effort on the part of many, many people to get a system working,” Rivest says.  


",Bluetooth signals from your smartphone could automate Covid-19 contact tracing while preserving privacy,2020-04-09,['Kylie Foy'],Covid-19/Research/Lincoln Laboratory/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/Apps/Disease/Public health/Pandemic/Cyber security/Technology and society/Media Lab/School of Engineering/School of Architecture and Planning,"['smartphone', 'phone', 'contact', 'signals', 'chirps', 'privacy', 'bluetooth', 'system', 'automate', 'person', 'tracing', 'random', 'health', 'covid19', 'preserving', 'mit', 'team']","This approach to private, automated contact tracing will be available in a number of ways, including through the privacy-first effort launched at MIT in response to Covid-19 called SafePaths .
A team led by MIT researchers and including experts from many institutions is developing a system that augments “manual” contact tracing by public health officials, while preserving the privacy of all individuals.
Health officials begin contact tracing to contain infections, asking you to identify people with whom you’ve been in close contact.
Privacy-preserving technologySome countries most successful at containing the spread of Covid-19 have been using smartphone-based approaches to conduct contact tracing, yet the researchers note these approaches have not always protected individual’s privacy.
Public health impactThroughout the development process, the researchers have worked closely with a medical advisory team to ensure that this system would contribute effectively to contact tracing efforts.",Mit
150,https://news.mit.edu/2020/mit-csail-sprayabletech-sprayable-user-interfaces-0408,"


For decades researchers have envisioned a world where digital user interfaces are seamlessly integrated with the physical environment, until the two are virtually indistinguishable from one another. 
This vision, though, is held up by a few boundaries. First, it’s difficult to integrate sensors and display elements into our tangible world due to various design constraints. Second, most methods to do so are limited to smaller scales, bound by the size of the fabricating device. 
Recently, a group of researchers from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) came up with SprayableTech, a system that lets users create room-sized interactive surfaces with sensors and displays. The system, which uses airbrushing of functional inks, enables various displays, like interactive sofas with embedded sensors to control your television, and sensors for adjusting lighting and temperature through your walls.
The Engineering and Physical Sciences Research Council partially funded this work.







Play video






SprayableTech lets users channel their inner Picassos: After designing your interactive artwork in the 3D editor, it automatically generates stencils for airbrushing the layout onto a surface. Once they’ve created the stencils from cardboard, a user can then add sensors to the desired surface, whether it’s a sofa, a wall, or even a building, to control various appliances like your lamp or television. (An alternate option to stenciling is projecting them digitally.)
“Since SprayableTech is so flexible in its application, you can imagine using this type of system beyond walls and surfaces to power larger-scale entities like interactive smart cities and interactive architecture in public places,” says Michael Wessely, postdoc in CSAIL and lead author on a new paper about SprayableTech. “We view this as a tool that will allow humans to interact with and use their environment in newfound ways.”
The race for the smartest home has now been in the works for some time, with a large interest in sensor technology. It’s a big advance from the enormous glass wall displays with quick-shifting images and screens we’ve seen in countless dystopian films. 
The MIT researchers’ approach is focusing on scale, and creative expression. By using the airbrush technology, they’re no longer limited to the size of the printer, the area of the screen-printing net, or the size of the hydrographic bath — and there’s thousands of possible design options. 
Let’s say a user wanted to design a tree symbol on their wall to control the ambient light in the room. To start the process, they would use a toolkit in a 3D editor to design their digital object, and customize for things like proximity sensors, touch buttons, sliders, and electroluminescent displays. 
Then, the toolkit would output the choice of stencils: fabricated stencils cut from cardboard, which are great for high-precision spraying on simple, flat, surfaces, or projected stencils, which are less precise, but better for doubly-curved surfaces. 
Designers can then spray on the functional ink, which is ink with electrically functional elements, using an airbrush. As a final step to get the system going, a microcontroller is attached that connects the interface to the board that runs the code for sensing and visual output.
The team tested the system on a variety of items, including:

a musical interface on a concrete pillar;
an interactive sofa that’s connected to a television;
a wall display for controlling light; and
a street post with a touchable display that provides audible information on subway stations and local attractions.

Since the stencils need to be created in advance via the digital editor, it reduces the opportunity for spontaneous exploration. Looking forward, the team wants to explore so-called “modular” stencils that create touch buttons of different sizes, as well as shape-changing stencils that adjust themselves based on a desired user interface shape. 
“In the future, we aim to collaborate with graffiti artists and architects to explore the future potential for large-scale user interfaces in enabling the internet of things for smart cities and interactive homes,” says Wessely.
Wessely wrote the paper alongside MIT PhD student Ticha Sethapakdi, MIT undergraduate students Carlos Castillo and Jackson C. Snowden, MIT postdoc Isabel P.S. Qamar, MIT Professor Stefanie Mueller, University of Bristol PhD student Ollie Hanton, University of Bristol Professor Mike Fraser, and University of Bristol Associate Professor Anne Roudaut. 


",Sprayable user interfaces,2020-04-08,['Rachel Gordon'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Computer science and technology/3-D printing/Design/Manufacturing/electronics/Computer graphics/Electrical engineering and computer science (EECS)/School of Engineering/MIT Schwarzman College of Computing,"['stencils', 'wall', 'using', 'interfaces', 'various', 'sensors', 'user', 'system', 'sprayable', 'design', 'interactive', 'mit']","First, it’s difficult to integrate sensors and display elements into our tangible world due to various design constraints.
For decades researchers have envisioned a world where digital user interfaces are seamlessly integrated with the physical environment, until the two are virtually indistinguishable from one another.
Let’s say a user wanted to design a tree symbol on their wall to control the ambient light in the room.
“In the future, we aim to collaborate with graffiti artists and architects to explore the future potential for large-scale user interfaces in enabling the internet of things for smart cities and interactive homes,” says Wessely.
Wessely wrote the paper alongside MIT PhD student Ticha Sethapakdi, MIT undergraduate students Carlos Castillo and Jackson C. Snowden, MIT postdoc Isabel P.S.",Mit
151,https://news.mit.edu/2020/learning-about-artificial-intelligence-hub-of-mit-resources-k-12-students-0407,"


In light of the recent events surrounding Covid-19, learning for grades K-12 looks very different than it did a month ago. Parents and educators may be feeling overwhelmed about turning their homes into classrooms. 
With that in mind, a team led by Media Lab Associate Professor Cynthia Breazeal has launched aieducation.mit.edu to share a variety of online activities for K-12 students to learn about artificial intelligence, with a focus on how to design and use it responsibly. Learning resources provided on this website can help to address the needs of the millions of children, parents, and educators worldwide who are staying at home due to school closures caused by Covid-19, and are looking for free educational activities that support project-based STEM learning in an exciting and innovative area. 
The website is a collaboration between the Media Lab, MIT Stephen A. Schwarzman College of Computing, and MIT Open Learning, serving as a hub to highlight diverse work by faculty, staff, and students across the MIT community at the intersection of AI, learning, and education. 
“MIT is the birthplace of Constructionism under Seymour Papert. MIT has revolutionized how children learn computational thinking with hugely successful platforms such as Scratch and App Inventor. Now, we are bringing this rich tradition and deep expertise to how children learn about AI through project-based learning that dovetails technical concepts with ethical design and responsible use,” says Breazeal. 
The website will serve as a hub for MIT’s latest work in innovating learning and education in the era of AI. In addition to highlighting research, it also features up-to-date project-based activities, learning units, child-friendly software tools, digital interactives, and other supporting materials, highlighting a variety of MIT-developed educational research and collaborative outreach efforts across and beyond MIT. The site is intended for use by students, parents, teachers, and lifelong learners alike, with resources for children and adults at all learning levels, and with varying levels of comfort with technology, for a range of artificial intelligence topics. The team has also gathered a variety of external resources to explore, such as Teachable Machines by Google, a browser-based platform that lets users train classifiers for their own image-recognition algorithms in a user-friendly way.
In the spirit of “mens et manus” — the MIT motto, meaning “mind and hand” — the vision of technology for learning at MIT is about empowering and inspiring learners of all ages in the pursuit of creative endeavors. The activities highlighted on the new website are designed in the tradition of constructionism: learning through project-based experiences in which learners build and share their work. The approach is also inspired by the idea of computational action, where children can design AI-enabled technologies to help others in their community.
“MIT has been a world leader in AI since the 1960s,” says MIT professor of computer science and engineering Hal Abelson, who has long been involved in MIT’s AI research and educational technology. “MIT’s approach to making machines intelligent has always been strongly linked with our work in K-12 education. That work is aimed at empowering young people through computational ideas that help them understand the world and computational actions that empower them to improve life for themselves and their communities.”
Research in computer science education and AI education highlights the importance of having a mix of plugged and unplugged learning approaches. Unplugged activities include kinesthetic or discussion-based activities developed to introduce children to concepts in AI and its societal impact without using a computer. Unplugged approaches to learning AI are found to be especially helpful for young children. Moreover, these approaches can also be accessible to learning environments (classrooms and homes) that have limited access to technology. 
As computers continue to automate more and more routine tasks, inequity of education remains a key barrier to future opportunities, where success depends increasingly on intellect, creativity, social skills, and having specific skills and knowledge. This accelerating change raises the critical question of how to best prepare students, from children to lifelong learners, to be successful and to flourish in the era of AI.
It is important to help prepare a diverse and inclusive citizenry to be responsible designers and conscientious users of AI. In that spirit, the activities on aieducation.mit.edu range from hands-on programming to paper prototyping, to Socratic seminars, and even creative writing about speculative fiction. The learning units and project-based activities are designed to be accessible to a wide audience with different backgrounds and comfort levels with technology. A number of these activities leverage learning about AI as a way to connect to the arts, humanities, and social sciences, too, offering a holistic view of how AI intersects with different interests and endeavors. 
The rising ubiquity of AI affects us all, but today a disproportionately small slice of the population has the skills or power to decide how AI is designed or implemented; worrying consequences have been seen in algorithmic bias and perpetuation of unjust systems. Democratizing AI through education, starting in K-12, will help to make it more accessible and diverse at all levels, ultimately helping to create a more inclusive, fair, and equitable future.


",Learning about artificial intelligence: A hub of MIT resources for K-12 students,2020-04-07,[],"Media Lab/online learning/K-12 education/School of Architecture and Planning/Artificial intelligence/Education, teaching, academics/STEM education/MIT Schwarzman College of Computing","['education', 'resources', 'hub', 'projectbased', 'children', 'learning', 'website', 'work', 'ai', 'help', 'k12', 'artificial', 'activities', 'intelligence', 'mit', 'students']","In light of the recent events surrounding Covid-19, learning for grades K-12 looks very different than it did a month ago.
The website is a collaboration between the Media Lab, MIT Stephen A. Schwarzman College of Computing, and MIT Open Learning, serving as a hub to highlight diverse work by faculty, staff, and students across the MIT community at the intersection of AI, learning, and education.
The website will serve as a hub for MIT’s latest work in innovating learning and education in the era of AI.
Unplugged approaches to learning AI are found to be especially helpful for young children.
The learning units and project-based activities are designed to be accessible to a wide audience with different backgrounds and comfort levels with technology.",Mit
152,https://news.mit.edu/2020/computational-thinking-class-enables-students-engage-covid-19-response-0407,"


When an introductory computational science class, which is open to the general public, was repurposed to study the Covid-19 pandemic this spring, the instructors saw student registration rise from 20 students to nearly 300.
Introduction to Computational Thinking (6.S083/18.S190), which applies data science, artificial intelligence, and mathematical models using the Julia programming language developed at MIT, was introduced in the fall as a pilot half-semester class. It was launched as part of the MIT Stephen A. Schwarzman College of Computing’s computational thinking program and spearheaded by Department of Mathematics Professor Alan Edelman and Visiting Professor David P. Sanders. They very quickly were able to fast-track the curriculum to focus on applications to Covid-19 responses; students were equally fast in jumping on board.
“Everyone at MIT wants to contribute,” says Edelman. “While we at the Julia Lab are doing research in building tools for scientists, Dave and I thought it would be valuable to teach the students about some of the fundamentals related to computation for drug development, disease models, and such.” 
The course is offered through MIT’s Department of Electronic Engineering and Computer Science and the Department of Mathematics. “This course opens a trove of opportunities to use computation to better understand and contain the Covid-19 pandemic,” says MIT Computer Science and Artificial Intelligence Laboratory Director Daniela Rus.
The fall version of the class had a maximum enrollment of 20 students, but the spring class has ballooned to nearly 300 students in one weekend, almost all from MIT. “We’ve had a tremendous response,” Edelman says. “This definitely stressed the MIT sign-up systems in ways that I could not have imagined.”
Sophomore Shinjini Ghosh, majoring in computer science and linguistics, says she was initially drawn to the class to learn Julia, “but also to develop the skills to do further computational modeling and conduct research on the spread and possible control of Covid-19.”
""There's been a lot of misinformation about the epidemiology and statistical modeling of the coronavirus,” adds sophomore Raj Movva, a computer science and biology major. “I think this class will help clarify some details, and give us a taste of how one might actually make predictions about the course of a pandemic."" 
Edelman says that he has always dreamed of an interdisciplinary modern class that would combine the machine learning and AI of a “data-driven” world, the modern software and systems possibilities that Julia allows, and the physical models, differential equations, and  scientific machine learning of the “physical world.” 
He calls this class “a natural outgrowth of Julia Lab's research, and that of the general cooperative open-source Julia community.” For years, this online community collaborates to create tools to speed up the drug approval process, aid in scientific machine learning and differential equations, and predict infectious disease transmission. “The lectures are open to the world, following the great MIT tradition of MIT open courses,” says Edelman.
So when MIT turned to virtual learning to de-densify campus, the transition to an online, remotely taught version of the class was not too difficult for Edelman and Sanders.
""Even though we have run open remote learning courses before, it's never the same as being able to see the live audience in front of you,” says Edelman. “However, MIT students ask such great questions in the Zoom chat, so that it remains as intellectually invigorating as ever.""
Sanders, a Marcos Moshinsky research fellow currently on leave as a professor at the National University of Mexico, is working on techniques for accelerating global optimization. Involved with the Julia Lab since 2014, Sanders has worked with Edelman on various teaching, research, and outreach projects related to Julia, and his YouTube tutorials have reached over 100,000 views. “His videos have often been referred to as the best way to learn the Julia language,” says Edelman.
Edelman will also be enlisting some help from Philip, his family’s Corgi who until recently had been a frequent wanderer of MIT’s halls and classrooms. “Philip is a well-known Julia expert whose image has been classified many times by Julia’s AI Systems,” says Edelman. “Students are always happy when Philip participates in the online classes.”


",Computational thinking class enables students to engage in Covid-19 response,2020-04-07,['Sandi Miller'],"Mathematics/MIT Schwarzman College of Computing/Electrical engineering and computer science (EECS)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Covid-19/online learning/Artificial intelligence/School of Science/Classes and programs/Education, teaching, academics/Computer science and technology/Students/School of Engineering","['engage', 'thinking', 'julia', 'edelman', 'research', 'computer', 'learning', 'class', 'response', 'science', 'computational', 'open', 'covid19', 'enables', 'mit', 'students']","When an introductory computational science class, which is open to the general public, was repurposed to study the Covid-19 pandemic this spring, the instructors saw student registration rise from 20 students to nearly 300.
“This course opens a trove of opportunities to use computation to better understand and contain the Covid-19 pandemic,” says MIT Computer Science and Artificial Intelligence Laboratory Director Daniela Rus.
The fall version of the class had a maximum enrollment of 20 students, but the spring class has ballooned to nearly 300 students in one weekend, almost all from MIT.
“The lectures are open to the world, following the great MIT tradition of MIT open courses,” says Edelman.
“However, MIT students ask such great questions in the Zoom chat, so that it remains as intellectually invigorating as ever.""",Mit
153,https://news.mit.edu/2020/researching-from-home-picower-science-stays-strong-even-at-distance-0407,"


With all but a skeleton crew staying home from each lab to minimize the spread of Covid-19, scores of Picower Institute researchers are immersing themselves in the considerable amount of scientific work that can done away from the bench. With piles of data to analyze; plenty of manuscripts to write; new skills to acquire; and fresh ideas to conceive, share, and refine for the future, neuroscientists have full plates, even when they are away from their, well, plates. They are proving that science can remain social, even if socially distant.
Ever since the mandatory ramp down of on-campus research took hold March 20, for example, teams of researchers in the lab of Troy Littleton, the Menicon Professor of Neuroscience, have sharpened their focus on two data-analysis projects that are every bit as essential to their science as acquiring the data in the lab in the first place. Research scientist Yulia Akbergenova and graduate student Karen Cunningham, for example, are poring over a huge amount of imaging data showing how the strength of connections between neurons, or synapses, mature and how that depends on the molecular components at the site. Another team, comprised of Picower postdoc Suresh Jetti and graduate students Andres Crane and Nicole Aponte-Santiago, is analyzing another large dataset, this time of gene transcription, to learn what distinguishes two subclasses of motor neurons that form synapses of characteristically different strength.
Work is similarly continuing among researchers in the lab of Elly Nedivi, the William R. (1964) and Linda R. Young Professor of Neuroscience. Since heading home, Senior Research Support Associate Kendyll Burnell has been looking at microscope images tracking how inhibitory interneurons innervate the visual cortex of mice throughout their development. By studying the maturation of inhibition, the lab hopes to improve understanding of the role of inhibitory circuitry in the experience-dependent changes, or plasticity, and development of the visual cortex, she says. As she’s worked, her poodle Soma (named for the central body structure of a neuron) has been by her side.
Despite extra time with comforts of home, though, it’s clear that nobody wanted this current mode of socially distant science. For every lab, it’s tremendously disruptive and costly. But labs are finding many ways to make progress nonetheless.
“Although we are certainly hurting because our lab work is at a standstill, the Miller lab is fortunate to have a large library of multiple-electrode neurophysiological data,” says Picower Professor Earl Miller. “The datasets are very rich. As our hypotheses and analytical tools develop, we can keep going back to old data to ask new questions. We are taking advantage of the wet lab downtime to analyze data and write papers. We have three under review and are writing at least three more right now.”
Miller is inviting new collaborations regardless of the physical impediment of social distancing. A recent lab meeting held via the videoconferencing app Zoom included MIT Department of Brain and Cognitive Sciences Associate Professor Ila Fiete and her graduate student, Mikail Khona. The Miller lab has begun studying how neural rhythms move around the cortex and what that means for brain function. Khona presented models of how timing relationships affect those waves. While this kind of an interaction between labs of the Picower Institute and the McGovern Institute for Brain Research would normally have taken place in person in MIT’s Building 46, neither lab let the pandemic get in the way.
Similarly, the lab of Li-Huei Tsai, Picower Professor and director of the Picower Institute, has teamed up with that of Manolis Kellis, professor in the MIT Computer Science and Artificial Intelligence Laboratory. They’re forming several small squads of experimenters and computational experts to launch analyses of gene expression and other data to illuminate the fate of individual cell types like interneurons or microglia in the context of the Alzheimer’s disease-afflicted brain. Other teams are focusing on analyses of questions such as how pathology varies in brain samples carrying different degrees of genetic risk factors. These analyses will prove useful for stages all along the scientific process, Tsai says, from forming new hypotheses to wrapping up papers that are well underway.
Remote collaboration and communication are proving crucial to researchers in other ways, too, proving that online interactions, though distant, can be quite personally fulfilling.
Nicholas DiNapoli, a research engineer in the lab of Associate Professor Kwanghun Chung, is making the best of time away from the bench by learning about the lab’s computational pipeline for processing the enormous amounts of imaging data it generates. He’s also taking advantage of a new program within the lab in which Senior Computer Scientist Lee Kamentsky is teaching Python computer programming principles to anyone in the lab who wants to learn. The training occurs via Zoom two days a week.
As part of a crowded calendar of Zoom meetings, or “Zeetings” as the lab has begun to call them, Newton Professor Mriganka Sur says he makes sure to have one-to-one meetings with everyone in the lab. The team also has organized into small subgroups around different themes of the lab’s research.
But also, the lab has continued to maintain its cohesion by banding together informally creating novel work and social experiences.
Graduate student Ning Leow, for example, used Zoom to create a co-working session in which participants kept a video connection open for hours at a time, just to be in each other’s virtual presence while they worked. Among a group of Sur lab friends, she read a paper related to her thesis and did a substantial amount of data analysis. She also advised a colleague on an analysis technique via the connection.
“I’ve got to say that it worked out really well for me personally because I managed to get whatever I wanted to complete on my list done,” she says, “and there was also a sense of healthy accountability along with the sense of community.”
Whether in person or via an officially imposed distance, science is social. In that spirit, graduate student K. Guadalupe ""Lupe"" Cruz organized a collaborative art event via Zoom for female scientists in brain and cognitive sciences at MIT. She took a photo of Rosalind Franklin, the scientist whose work was essential for resolving the structure of DNA, and divided it into nine squares to distribute to the event attendees. Without knowing the full picture, everyone drew just their section, talking all the while about how the strange circumstances of Covid-19 have changed their lives. At the end, they stitched their squares together to reconstruct the image.
Examples abound of how Picower scientists, though mostly separate and apart, are still coming together to advance their research and to maintain the fabric of their shared experiences.


","Researching from home: Science stays social, even at a distance",2020-04-07,['David Orenstein'],Picower Institute/Brain and cognitive sciences/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Biology/School of Science/Covid-19/Neuroscience/Women in STEM/Community/Pandemic,"['data', 'picower', 'research', 'researching', 'student', 'work', 'lab', 'professor', 'zoom', 'stays', 'distance', 'social', 'brain', 'science']","They are proving that science can remain social, even if socially distant.
Despite extra time with comforts of home, though, it’s clear that nobody wanted this current mode of socially distant science.
“Although we are certainly hurting because our lab work is at a standstill, the Miller lab is fortunate to have a large library of multiple-electrode neurophysiological data,” says Picower Professor Earl Miller.
Similarly, the lab of Li-Huei Tsai, Picower Professor and director of the Picower Institute, has teamed up with that of Manolis Kellis, professor in the MIT Computer Science and Artificial Intelligence Laboratory.
But also, the lab has continued to maintain its cohesion by banding together informally creating novel work and social experiences.",Mit
154,https://news.mit.edu/2020/qa-markus-buehler-setting-coronavirus-and-ai-inspired-proteins-to-music-0402,"


The proteins that make up all living things are alive with music. Just ask Markus Buehler: The musician and MIT professor develops artificial intelligence models to design new proteins, sometimes by translating them into sound. His goal is to create new biological materials for sustainable, non-toxic applications. In a project with the MIT-IBM Watson AI Lab, Buehler is searching for a protein to extend the shelf-life of perishable food. In a new study in Extreme Mechanics Letters, he and his colleagues offer a promising candidate: a silk protein made by honeybees for use in hive building. 
In another recent study, in APL Bioengineering, he went a step further and used AI discover an entirely new protein. As both studies went to print, the Covid-19 outbreak was surging in the United States, and Buehler turned his attention to the spike protein of SARS-CoV-2, the appendage that makes the novel coronavirus so contagious. He and his colleagues are trying to unpack its vibrational properties through molecular-based sound spectra, which could hold one key to stopping the virus. Buehler recently sat down to discuss the art and science of his work. 
Q: Your work focuses on the alpha helix proteins found in skin and hair. Why makes this protein so intriguing? 
A: Proteins are the bricks and mortar that make up our cells, organs, and body. Alpha helix proteins are especially important. Their spring-like structure gives them elasticity and resilience, which is why skin, hair, feathers, hooves, and even cell membranes are so durable. But they’re not just tough mechanically, they have built-in antimicrobial properties. With IBM, we’re trying to harness this biochemical trait to create a protein coating that can slow the spoilage of quick-to-rot foods like strawberries.
Q: How did you enlist AI to produce this silk protein?
A: We trained a deep learning model on the Protein Data Bank, which contains the amino acid sequences and three-dimensional shapes of about 120,000 proteins. We then fed the model a snippet of an amino acid chain for honeybee silk and asked it to predict the protein’s shape, atom-by-atom. We validated our work by synthesizing the protein for the first time in a lab — a first step toward developing a thin antimicrobial, structurally-durable coating that can be applied to food. My colleague, Benedetto Marelli, specializes in this part of the process. We also used the platform to predict the structure of proteins that don’t yet exist in nature. That’s how we designed our entirely new protein in the APL Bioengineering study. 
Q: How does your model improve on other protein prediction methods? 
A: We use end-to-end prediction. The model builds the protein’s structure directly from its sequence, translating amino acid patterns into three-dimensional geometries. It’s like translating a set of IKEA instructions into a built bookshelf, minus the frustration. Through this approach, the model effectively learns how to build a protein from the protein itself, via the language of its amino acids. Remarkably, our method can accurately predict protein structure without a template. It outperforms other folding methods and is significantly faster than physics-based modeling. Because the Protein Data Bank is limited to proteins found in nature, we needed a way to visualize new structures to make new proteins from scratch.
Q: How could the model be used to design an actual protein?
A: We can build atom-by-atom models for sequences found in nature that haven’t yet been studied, as we did in the APL Bioengineering study using a different method. We can visualize the protein’s structure and use other computational methods to assess its function by analyzing its stablity and the other proteins it binds to in cells. Our model could be used in drug design or to interfere with protein-mediated biochemical pathways in infectious disease.
Q: What’s the benefit of translating proteins into sound?
A: Our brains are great at processing sound! In one sweep, our ears pick up all of its hierarchical features: pitch, timbre, volume, melody, rhythm, and chords. We would need a high-powered microscope to see the equivalent detail in an image, and we could never see it all at once. Sound is such an elegant way to access the information stored in a protein. 
Typically, sound is made from vibrating a material, like a guitar string, and music is made by arranging sounds in hierarchical patterns. With AI we can combine these concepts, and use molecular vibrations and neural networks to construct new musical forms. We’ve been working on methods to turn protein structures into audible representations, and translate these representations into new materials. 
Q: What can the sonification of SARS-CoV-2's ""spike"" protein tell us?
A: Its protein spike contains three protein chains folded into an intriguing pattern. These structures are too small for the eye to see, but they can be heard. We represented the physical protein structure, with its entangled chains, as interwoven melodies that form a multi-layered composition. The spike protein’s amino acid sequence, its secondary structure patterns, and its intricate three-dimensional folds are all featured. The resulting piece is a form of counterpoint music, in which notes are played against notes. Like a symphony, the musical patterns reflect the protein’s intersecting geometry realized by materializing its DNA code.
Q: What did you learn?
A: The virus has an uncanny ability to deceive and exploit the host for its own multiplication. Its genome hijacks the host cell’s protein manufacturing machinery, and forces it to replicate the viral genome and produce viral proteins to make new viruses. As you listen, you may be surprised by the pleasant, even relaxing, tone of the music. But it tricks our ear in the same way the virus tricks our cells. It’s an invader disguised as a friendly visitor. Through music, we can see the SARS-CoV-2 spike from a new angle, and appreciate the urgent need to learn the language of proteins.  
Q: Can any of this address Covid-19, and the virus that causes it?
A: In the longer term, yes. Translating proteins into sound gives scientists another tool to understand and design proteins. Even a small mutation can limit or enhance the pathogenic power of SARS-CoV-2. Through sonification, we can also compare the biochemical processes of its spike protein with previous coronaviruses, like SARS or MERS. 
In the music we created, we analyzed the vibrational structure of the spike protein that infects the host. Understanding these vibrational patterns is critical for drug design and much more. Vibrations may change as temperatures warm, for example, and they may also tell us why the SARS-CoV-2 spike gravitates toward human cells more than other viruses. We’re exploring these questions in current, ongoing research with my graduate students. 
We might also use a compositional approach to design drugs to attack the virus. We could search for a new protein that matches the melody and rhythm of an antibody capable of binding to the spike protein, interfering with its ability to infect.
Q: How can music aid protein design?
A: You can think of music as an algorithmic reflection of structure. Bach’s Goldberg Variations, for example, are a brilliant realization of counterpoint, a principle we’ve also found in proteins. We can now hear this concept as nature composed it, and compare it to ideas in our imagination, or use AI to speak the language of protein design and let it imagine new structures. We believe that the analysis of sound and music can help us understand the material world better. Artistic expression is, after all, just a model of the world within us and around us.  
Co-authors of the study in Extreme Mechanics Letters are: Zhao Qin, Hui Sun, Eugene Lim and Benedetto Marelli at MIT; and Lingfei Wu, Siyu Huo, Tengfei Ma and Pin-Yu Chen at IBM Research. Co-author of the study in APL Bioengineering is Chi-Hua Yu. Buehler’s sonification work is supported by MIT’s Center for Art, Science and Technology (CAST) and the Mellon Foundation. 


",Q&A: Markus Buehler on setting coronavirus and AI-inspired proteins to music,2020-04-02,['Kim Martineau'],Quest for Intelligence/MIT-IBM Watson AI Lab/Nanoscience and nanotechnology/Proteins/Civil and environmental engineering/School of Engineering/Artificial intelligence/Machine learning/Music/Arts/Covid-19/Research/Biology,"['protein', 'model', 'aiinspired', 'spike', 'music', 'proteins', 'coronavirus', 'sound', 'virus', 'setting', 'markus', 'buehler', 'structure', 'design', 'qa', 'translating']","Just ask Markus Buehler: The musician and MIT professor develops artificial intelligence models to design new proteins, sometimes by translating them into sound.
We can visualize the protein’s structure and use other computational methods to assess its function by analyzing its stablity and the other proteins it binds to in cells.
We represented the physical protein structure, with its entangled chains, as interwoven melodies that form a multi-layered composition.
Translating proteins into sound gives scientists another tool to understand and design proteins.
Through sonification, we can also compare the biochemical processes of its spike protein with previous coronaviruses, like SARS or MERS.",Mit
155,https://news.mit.edu/2020/system-trains-driverless-cars-simulations-0323,"


A simulation system invented at MIT to train driverless cars creates a photorealistic world with infinite steering possibilities, helping the cars learn to navigate a host of worse-case scenarios before cruising down real streets.  Control systems, or “controllers,” for autonomous vehicles largely rely on real-world datasets of driving trajectories from human drivers. From these data, they learn how to emulate safe steering controls in a variety of situations. But real-world data from hazardous “edge cases,” such as nearly crashing or being forced off the road or into other lanes, are — fortunately — rare.Some computer programs, called “simulation engines,” aim to imitate these situations by rendering detailed virtual roads to help train the controllers to recover. But the learned control from simulation has never been shown to transfer to reality on a full-scale vehicle.The MIT researchers tackle the problem with their photorealistic simulator, called Virtual Image Synthesis and Transformation for Autonomy (VISTA). It uses only a small dataset, captured by humans driving on a road, to synthesize a practically infinite number of new viewpoints from trajectories that the vehicle could take in the real world. The controller is rewarded for the distance it travels without crashing, so it must learn by itself how to reach a destination safely. In doing so, the vehicle learns to safely navigate any situation it encounters, including regaining control after swerving between lanes or recovering from near-crashes.  In tests, a controller trained within the VISTA simulator safely was able to be safely deployed onto a full-scale driverless car and to navigate through previously unseen streets. In positioning the car at off-road orientations that mimicked various near-crash situations, the controller was also able to successfully recover the car back into a safe driving trajectory within a few seconds. A paper describing the system has been published in IEEE Robotics and Automation Letters and will be presented at the upcoming ICRA conference in May.“It’s tough to collect data in these edge cases that humans don’t experience on the road,” says first author Alexander Amini, a PhD student in the Computer Science and Artificial Intelligence Laboratory (CSAIL). “In our simulation, however, control systems can experience those situations, learn for themselves to recover from them, and remain robust when deployed onto vehicles in the real world.”The work was done in collaboration with the Toyota Research Institute. Joining Amini on the paper are Igor Gilitschenski, a postdoc in CSAIL; Jacob Phillips, Julia Moseyko, and Rohan Banerjee, all undergraduates in CSAIL and the Department of Electrical Engineering and Computer Science; Sertac Karaman, an associate professor of aeronautics and astronautics; and Daniela Rus, director of CSAIL and the Andrew and Erna Viterbi Professor of Electrical Engineering and Computer Science.Data-driven simulationHistorically, building simulation engines for training and testing autonomous vehicles has been largely a manual task. Companies and universities often employ teams of artists and engineers to sketch virtual environments, with accurate road markings, lanes, and even detailed leaves on trees. Some engines may also incorporate the physics of a car’s interaction with its environment, based on complex mathematical models.But since there are so many different things to consider in complex real-world environments, it’s practically impossible to incorporate everything into the simulator. For that reason, there’s usually a mismatch between what controllers learn in simulation and how they operate in the real world.Instead, the MIT researchers created what they call a “data-driven” simulation engine that synthesizes, from real data, new trajectories consistent with road appearance, as well as the distance and motion of all objects in the scene.They first collect video data from a human driving down a few roads and feed that into the engine. For each frame, the engine projects every pixel into a type of 3D point cloud. Then, they place a virtual vehicle inside that world. When the vehicle makes a steering command, the engine synthesizes a new trajectory through the point cloud, based on the steering curve and the vehicle’s orientation and velocity.Then, the engine uses that new trajectory to render a photorealistic scene. To do so, it uses a convolutional neural network — commonly used for image-processing tasks — to estimate a depth map, which contains information relating to the distance of objects from the controller’s viewpoint. It then combines the depth map with a technique that estimates the camera’s orientation within a 3D scene. That all helps pinpoint the vehicle’s location and relative distance from everything within the virtual simulator.Based on that information, it reorients the original pixels to recreate a 3D representation of the world from the vehicle’s new viewpoint. It also tracks the motion of the pixels to capture the movement of the cars and people, and other moving objects, in the scene. “This is equivalent to providing the vehicle with an infinite number of possible trajectories,” Rus says. “Because when we collect physical data, we get data from the specific trajectory the car will follow. But we can modify that trajectory to cover all possible ways of and environments of driving. That’s really powerful.”Reinforcement learning from scratchTraditionally, researchers have been training autonomous vehicles by either following human defined rules of driving or by trying to imitate human drivers. But the researchers make their controller learn entirely from scratch under an “end-to-end” framework, meaning it takes as input only raw sensor data — such as visual observations of the road — and, from that data, predicts steering commands at outputs.“We basically say, ‘Here’s an environment. You can do whatever you want. Just don’t crash into vehicles, and stay inside the lanes,’” Amini says.This requires “reinforcement learning” (RL), a trial-and-error machine-learning technique that provides feedback signals whenever the car makes an error. In the researchers’ simulation engine, the controller begins by knowing nothing about how  to drive, what a lane marker is, or even other vehicles look like, so it starts executing random steering angles. It gets a feedback signal only when it crashes. At that point, it gets teleported to a new simulated location and has to execute a better set of steering angles to avoid crashing again. Over 10 to 15 hours of training, it uses these sparse feedback signals to learn to travel greater and greater distances without crashing.After successfully driving 10,000 kilometers in simulation, the authors apply that learned controller onto their full-scale autonomous vehicle in the real world. The researchers say this is the first time a controller trained using end-to-end reinforcement learning in simulation has successful been deployed onto a full-scale autonomous car. “That was surprising to us. Not only has the controller never been on a real car before, but it’s also never even seen the roads before and has no prior knowledge on how humans drive,” Amini says.Forcing the controller to run through all types of driving scenarios enabled it to regain control from disorienting positions — such as being half off the road or into another lane — and steer back into the correct lane within several seconds. “And other state-of-the-art controllers all tragically failed at that, because they never saw any data like this in training,” Amini says.Next, the researchers hope to simulate all types of road conditions from a single driving trajectory, such as night and day, and sunny and rainy weather. They also hope to simulate more complex interactions with other vehicles on the road. “What if other cars start moving and jump in front of the vehicle?” Rus says. “Those are complex, real-world interactions we want to start testing.”


",System trains driverless cars in simulation before they hit the road,2020-03-23,['Rob Matheson'],Research/Computer science and technology/Algorithms/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/School of Engineering,"['driving', 'data', 'simulation', 'trains', 'hit', 'vehicle', 'researchers', 'cars', 'road', 'steering', 'system', 'vehicles', 'controller', 'driverless', 'real']","A simulation system invented at MIT to train driverless cars creates a photorealistic world with infinite steering possibilities, helping the cars learn to navigate a host of worse-case scenarios before cruising down real streets.
But the learned control from simulation has never been shown to transfer to reality on a full-scale vehicle.
For that reason, there’s usually a mismatch between what controllers learn in simulation and how they operate in the real world.
After successfully driving 10,000 kilometers in simulation, the authors apply that learned controller onto their full-scale autonomous vehicle in the real world.
They also hope to simulate more complex interactions with other vehicles on the road.",Mit
156,https://news.mit.edu/2020/accessible-designs-data%20visualization-0313,"


Academic researchers and others have long struggled with making data visualizations accessible to people who are blind. One technological approach has been 3D printing tactile representations of data, in the form of raised bar graphs and line charts. But, often, the intended users have little say in the actual design process, and the end result isn’t as effective as planned. 
A team of MIT researchers hopes to fix that. They used a collaborative project with staff and students at the Perkins School for the Blind as a case study of the accessible design process, and generated a list of “sociotechnical” considerations to guide researchers in similar work. A paper detailing the work appears in the journal IEEE Transactions on Visualization and Computer Graphics. Co-authors Alan Lundgard, a graduate student in the Department of Electrical Engineering and Computer Science (EECS); Crystal Lee, a graduate student in the Program in Science, Technology, and Society; and EECS and Computer Science and Artificial Intelligence Laboratory professor Arvind Satyanarayan spoke with MIT News about the case study and their findings.
Q: How did you land on this idea to record “sociotechnical considerations,” and what are some notable examples?
Lundgard: Crystal and I met during an intersession workshop in participatory design, where researchers collaboratively designed products with and for particular communities. We worked with the Perkins School to co-design a 3D-printed visualization of a time-series chart for people who are blind. Coming from MIT, there was this idea that we’d come up with a high-tech, flashy solution — but, it turns out, that wasn’t really the best approach. In that regard, I think a first-order sociotechnical consideration is, what degree of technological intervention is necessary, if any? Could the intervention take a more social approach without the need for a fancy technological design? Would a low-tech solution meet the needs of the community better than a high-tech solution?
Another big consideration is planning and communicating the extent of the collaboration, which is especially important when collaborating with marginalized communities. That means researchers clearly communicating their intentions and goals. As researchers, are we aiming to produce academic research, or a design solution that is immediately adoptable within the community? What is the duration of the project and what are the available resources? Failing to communicate clearly can leave community collaborators out of the loop in ways that are actively harmful.
Lee: We realized there were tons of intermediate steps before you start to even design a product. What does collaboration actually mean and what does participatory design look like? We got frustrated at certain junctures thinking about what product to make. While we talked to teachers, occupational therapists, and the Perkins School staff, we’d come up with a prototype and realize it was an idea that didn’t actually meet the needs of the community. Thinking through these tensions helped us come up with a list of sociotechnical considerations for other researchers and collaborators who may feel these same frustrations when working on co-design projects.
One notable consideration from our case study: As researchers, don’t assume that your resources are the same as the community’s resources. For example, don’t make something for a small school if it requires a $300,000 3D printer that only MIT can afford. In our 3D-printed visualization, we at first tried to use a cheap and accessible 3D printer that’s often available in libraries. But, this affordability imposed other constraints. For example, using the inexpensive printer, it was hard to actually make something legible in braille, because the resolution is too low to be useful. It can’t capture the detail you need to accurately represent the data. So, using the affordable printer, our graph failed to meet certain accessibility guidelines. On the other hand, MIT’s high-resolution, industrial-grade printer isn’t affordable or available to the Perkins School — or most schools, for that matter — which is hugely constraining if the design is supposed to satisfy the students’ daily needs.
Satyanarayan: It’s also very important to compensate participants fairly, especially with marginalized communities. In participatory design, we don’t treat folks we work with as target users. Rather, they are collaborators throughout the process, and with specific skills. For instance, people who are blind have far more experience reading braille. We consider that a highly specialized skill that should be compensated accordingly. A key tenet of participatory design is recognizing that people in the community have lived experience that is valuable and necessary for a design to be successful.
Q: In your paper, you say you hope to avoid pitfalls of “parachute research.” What is that and why is it important to address?  
Lundgard: “Parachute research” is where researchers — particularly from wealthy universities — drop into a community; take advantage of local infrastructure, expertise, and resources; write an academic paper; and then take off. That is, after publishing a research paper, they completely disengage from the community. That’s harmful to community members who engage in the collaboration in good faith and help to facilitate the research, sometimes without reciprocal benefits.
Lee: In accessible design, you often make a prototype based on some abstract knowledge of what a given community may want. Then, the people in that community evaluate the efficacy of the prototype, instead of being directly involved in the design process. But that can diverge from creating solutions that are beneficial for the communities the designers are purporting to help. In our paper, we didn’t just build something, test it, and report on it — we thought it would be more important to contribute guidelines for approaching similar participatory design problems.
Q: What does the future look like for you and for your work? 
Lee: I’m starting a collaboration with Massachusetts Association for the Blind and Visually Impaired. They have a large group of senior citizens who are experiencing blindness later in life, and have to learn to interact with technology in different ways. Understanding how people interact with technology ethnographically will be necessary for understanding accessibility — in technology, in the built environment, and in digital infrastructure. That’s a big part of my research moving forward. 
Lundgard: Really, our paper is not just about data visualization, but also about how to approach accessible design more generally. In that sense, our paper tees up how to do future work, with a concise set of guidelines that researchers — ourselves and others — can apply to different problems. For example, I’ve recently encountered researchers at a loss for how to describe their visualizations in ways that make them more accessible. When visualizations appear in, say, textbooks, scientific publications, or educational materials, they might appear as braille translations of the image, but more often they appear as textual descriptions. But what is the best way to describe a visualization? Does it make more sense to refer to its visual or statistical properties? Maybe we can collaboratively come up with different encodings that are more intelligible to someone who’s not used to interpreting information visually.
Satyanarayan: Along those lines, one thread is captioning online visualizations. There’s a lot of work to do in figuring out what’s important to caption to present some high-level insight of what the visualization is saying, as well as find a way to automatically generate those captions. That’s a deep technological solution. But we still have to make sure our sociotechnical considerations are adhered to.
Looking long-term, we’re interested in alternative ways of encoding data that are usable and accessible to people who are blind. Before braille, text was embossed on paper, but that’s not really how people who are blind process language. Louis Braille, who was blind himself, came up with something vastly different that became the standard way for blind people to read text. We first need to take a step back and understand the audience for and with whom we are designing, and work directly with them.
To do that, we have to address several things. How do people who are blind think about data? I was introduced to data through line graphs and bar charts. What is the equivalent for people who don’t process information visually? Once we answer those questions, we can start thinking about what the best way to encode data, because we’re not sure 3D-printing a line chart is the best solution.


",3Q: Collaborating with users to develop accessible designs,2020-03-13,['Rob Matheson'],Research/Computer science and technology/Assistive technology/Data/Technology and society/Program in STS/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/School of Engineering/3-D printing,"['data', 'research', 'accessible', 'develop', 'community', 'users', 'researchers', 'blind', '3q', 'work', 'designs', 'design', 'paper', 'collaborating', 'visualization']","Academic researchers and others have long struggled with making data visualizations accessible to people who are blind.
That is, after publishing a research paper, they completely disengage from the community.
Lee: In accessible design, you often make a prototype based on some abstract knowledge of what a given community may want.
Lundgard: Really, our paper is not just about data visualization, but also about how to approach accessible design more generally.
Looking long-term, we’re interested in alternative ways of encoding data that are usable and accessible to people who are blind.",Mit
157,https://news.mit.edu/2020/creating-next-generation-data-scientists-peru-0309,"


“Participating in the MIT MicroMasters in Statistics and Data Science, I have discovered new concepts and skills that will allow me to become a data scientist,” says Karen Velasquez. “I am excited to apply what I have learned to challenges that will help NGOs in Peru.”
When Velasquez graduated with a bachelor’s degree in statistical engineering from the Universidad Nacional de Ingeniería in Lima, Peru, she was among the top 10 percent of students in her class. Now, while working for a marketing and intelligence company in Peru, she’s expanding her education as one of the first 25 participants in the Aporta’s Advanced Program in Data Science and Global Skills, which supports a cohort of Peruvians through the MIT MicroMasters Program in Statistics and Data Science.
Training future data scientists
Both Aporta and the MIT Institute for Data, Systems, and Society (IDSS) recognize the urgent need to solve global challenges through rigorous and systemic analysis of large and complex datasets, using tools from statistics and computing. These approaches and techniques can bring new insights to societal challenges by detecting fake news, designing real-time demand response for the power grid, or maximizing the efficacy of vaccine intervention to prevent the spread of disease.
This critical need led Aporta and IDSS to join forces to advance education in powerful data science methods and tools to train the next generation of data scientists in Peru. Aporta is leveraging the IDSS MicroMasters for a program of their own: the Advanced Program in Data Science and Global Skills. In partnership with IDSS faculty and staff, Aporta — a subsidiary of Peruvian conglomerate Breca Group — is offering the IDSS MicroMasters Program in Statistics and Data Science to a carefully vetted group of learners, along with additional content to develop skills in cross-cultural communication, teamwork, and leadership.
The IDSS MicroMasters Program offers a rigorous MIT education, available from anywhere in the world. Through four online courses, learners in the MicroMasters program gain in-demand skills in data analysis and machine learning, plus get hands-on experience in applying these skills in challenges centered in economics and development.
To support the Aporta cohort’s progress through the challenging courses of the MicroMasters program, IDSS recruits teaching assistants (TAs) with areas of expertise specific to each course. Learners interact with each other in physical space while receiving live instruction and feedback from TAs through online office hours. TAs use these sessions to identify challenge areas and develop individualized course materials. This personalized and interactive method creates a vibrant classroom experience for the learners, similar to being in a residential program on MIT’s campus.
Custom TA-led sessions have “been beyond helpful to complement the online material,” said David Ascencios, a learner who is already working as a data scientist in Peru.
The cohort has cleared the halfway mark of their journey through the program, and already the impact is significant. “I am very grateful to Aporta and to MIT,” says Johan Veramendi, a systems engineering graduate working in finance. “The program is an excellent opportunity to advance and guide my career into the world of data science.”
Giving back
Aporta’s educational outreach program began with a gift from Ana Maria Brescia Cafferata, the daughter of Grupo Breca’s late founder. It is a philanthropic endeavor with the goal of empowering Peruvian professionals with learning opportunities to enhance their careers, while providing much-needed talent across different industries and government. Data science is a young and growing field in South America, with untapped potential, an expanding job market, and increasing opportunity for both the private and public sectors.
“This unique program has the vision to make Peru a hub in Latin America for analytics and artificial intelligence,” says Luis Herrera, who is balancing the program with his job as a software engineer and his role as a new father. “I share this vision and I think they are doing a great job. The MIT courses are very challenging and rewarding at the same time.”
The pilot class of 25 learners represent a variety of socio-economic backgrounds. Most have college degrees. Thanks to Brescia Cafferata’s philanthropy, Aporta made a commitment to support all of them with scholarships throughout the program. Going forward, the initiative intends to become self-sustainable, granting as many scholarships as possible.
“Her wish is to dedicate part of her parents’ legacy to the country she’s from, and to give back,” says Luz Fernandez Gandarias, director of the Institute for Advanced Analytics and Data Science within Aporta. “Her spirit is also behind the design of the program’s academic model, keeping people as the key point around which everything evolves, rather than technology. Ensuring the presence of an ethical conscience, recognizing the impact on people of technology — that humanistic view is something she’s always promoted.”
For IDSS Director Munther Dahleh, the collaboration of Aporta and IDSS presents a compelling model of how MIT and IDSS can share their elite faculty and courses with the rest of the world: “IDSS wants to provide a rigorous data science education to the world. We think these skills are critical in the private sector, but also to solving global societal challenges.”
This was the initial vision of Ana Maria Brescia Cafferata, who wants to give back to the country that gave her parents so much. Says Dahleh: “I am delighted to share the hopes and vision of Ana Maria. We have developed a unique program and partnership that aspires to educate students in an emerging field that is fundamentally changing the nature of work. In line with MIT’s mission of creating a better world, our goal is to create a more educated workforce capable of tackling the world’s challenges through enhanced data analysis and insights.”


",Creating Peru’s next generation of data scientists,2020-03-09,['Scott Murray'],IDSS/Latin America/MITx/Massive open online courses (MOOCs)/Data/Analytics/online learning/Classes and programs/EdX/International initiatives/Global/MIT Schwarzman College of Computing,"['data', 'perus', 'scientists', 'micromasters', 'generation', 'skills', 'mit', 'world', 'aporta', 'idss', 'learners', 'program', 'creating', 'science']","“Participating in the MIT MicroMasters in Statistics and Data Science, I have discovered new concepts and skills that will allow me to become a data scientist,” says Karen Velasquez.
This critical need led Aporta and IDSS to join forces to advance education in powerful data science methods and tools to train the next generation of data scientists in Peru.
Aporta is leveraging the IDSS MicroMasters for a program of their own: the Advanced Program in Data Science and Global Skills.
The IDSS MicroMasters Program offers a rigorous MIT education, available from anywhere in the world.
Data science is a young and growing field in South America, with untapped potential, an expanding job market, and increasing opportunity for both the private and public sectors.",Mit
158,https://news.mit.edu/2020/2020-macvicar-faculty-fellows-named-0309,"


This article has been updated to reflect the cancellation of the 2020 MacVicar Day symposium.
The Office of the Vice Chancellor and the Registrar’s Office have announced this year’s Margaret MacVicar Faculty Fellows: materials science and engineering Professor Polina Anikeeva, literature Professor Mary Fuller, chemical engineering Professor William Tisdale, and electrical engineering and computer science Professor Jacob White.
Role models both in and out of the classroom, the new fellows have tirelessly sought to improve themselves, their students, and the Institute writ large. They have reimagined curricula, crossed disciplines, and pushed the boundaries of what education can be. They join a matchless academy of scholars committed to exceptional instruction and innovation.
For nearly three decades, the MacVicar Faculty Fellows Program has been recognizing exemplary undergraduate teaching and advising around the Institute. The program was named after Margaret MacVicar, the first dean for undergraduate education and founder of the Undergraduate Research Opportunities Program (UROP). Nominations are made by departments and include letters of support from colleagues, students, and alumni. Fellows are appointed to 10-year terms in which they receive $10,000 per year of discretionary funds.
This year’s MacVicar Day symposium — which had been scheduled for this Friday, March 13 — has been canceled after new MIT policies on events were set in response to the 2019 novel coronavirus.
Polina Anikeeva
“I’m speechless,” Polina Anikeeva, associate professor of materials science and engineering and brain and cognitive sciences, says of becoming a MacVicar Fellow. “In my opinion, this is the greatest honor one could have at MIT.”
Anikeeva received her PhD from MIT in 2009 and became a professor in the Department of Materials Science and Engineering two years later. She attended St. Petersburg State Polytechnic University for her undergraduate education. Through her research — which combines materials science, electronics, and neurobiology — she works to better understand and treat brain disorders.
Anikeeva’s colleague Christopher Schuh says, “Her ability and willingness to work with students however and whenever they need help, her engaging classroom persona, and her creative solutions to real-time challenges all culminate in one of MIT’s most talented and beloved undergraduate professors.”
As an instructor, advisor, and marathon runner, Anikeeva has learned the importance of finding balance. Her colleague Lionel Kimerling reflects on this delicate equilibrium: “As a teacher, Professor Anikeeva is among the elite who instruct, inspire, and nurture at the same time. It is a difficult task to demand rigor with a gentle mentoring hand.”
Students call her classes “incredibly hard” but fun and exciting at the same time. She is “the consummate scientist, splitting her time evenly between honing her craft, sharing knowledge with students and colleagues, and mentoring aspiring researchers,” wrote one.
Her passion for her work and her devotion to her students are evident in the nomination letters. One student recounted their first conversation: “We spoke for 15 minutes, and after talking to her about her research and materials science, I had never been so viscerally excited about anything.” This same student described the guidance and support Anikeeva provided her throughout her time at MIT. After working with Anikeeva to apply what she learned in the classroom to a real-world problem, this student recalled, “I honestly felt like an engineer and a scientist for the first time ever. I have never felt so fulfilled and capable. And I realize that’s what I want for the rest of my life — to feel the highs and lows of discovery.”
Anikeeva champions her students in faculty and committee meetings as well. She is a “reliable advocate for student issues,” says Caroline Ross, associate department head and professor in DMSE. “Professor Anikeeva is always engaged with students, committed to student well-being, and passionate about education.”
“Undergraduate teaching has always been a crucial part of my MIT career and life,” Anikeeva reflects. “I derive my enthusiasm and energy from the incredibly talented MIT students — every year they surprise me with their ability to rise to ever-expanding intellectual challenges. Watching them grow as scientists, engineers, and — most importantly — people is like nothing else.”
Mary Fuller
Experimentation is synonymous with education at MIT and it is a crucial part of literature Professor Mary Fuller’s classes. As her colleague Arthur Bahr notes, “Mary’s habit of starting with a discrete practical challenge can yield insights into much broader questions.”
Fuller attended Dartmouth College as an undergraduate, then received both her MA and PhD in English and American literature from The Johns Hopkins University. She began teaching at MIT in 1989. From 2013 to 2019, Fuller was head of the Literature Section. Her successor in the role, Shankar Raman, says that her nominators “found [themselves] repeatedly surprised by the different ways Mary has pushed the limits of her teaching here, going beyond her own comfort zones to experiment with new texts and techniques.”
“Probably the most significant thing I’ve learned in 30 years of teaching here is how to ask more and better questions,” says Fuller. As part of a series of discussions on ethics and computing, she has explored the possibilities of artificial intelligence from a literary perspective. She is also developing a tool for the edX platform called PoetryViz, which would allow MIT students and students around the world to practice close reading through poetry annotation in an entirely new way.
“We all innovate in our teaching. Every year. But, some of us innovate more than others,” Krishna Rajagopal, dean for digital learning, observes. “In addition to being an outstanding innovator, Mary is one of those colleagues who weaves the fabric of undergraduate education across the Institute.”
Lessons learned in Fuller’s class also underline the importance of a well-rounded education. As one alumna reflected, “Mary’s teaching carried a compassion and ethic which enabled non-humanities students to appreciate literature as a diverse, valuable, and rewarding resource for personal and social reflection.”
Professor Fuller, another student remarked, has created “an environment where learning is not merely the digestion of rote knowledge, but instead the broad-based exploration of ideas and the works connected to them.”
“Her imagination is capacious, her knowledge is deep, and students trust her — so that they follow her eagerly into new and exploratory territory,” says Professor of Literature Stephen Tapscott.
Fuller praises her students’ willingness to take that journey with her, saying, “None of my classes are required, and none are technical, so I feel that students have already shown a kind of intellectual generosity by putting themselves in the room to do the work.”
For students, the hard work is worth it. Mary Fuller, one nominator declared, is exactly “the type of deeply impactful professor that I attended MIT hoping to learn from.”
William Tisdale
William Tisdale is the ARCO Career Development Professor of chemical engineering and, according to his colleagues, a “true star” in the department.
A member of the faculty since 2012, he received his undergraduate degree from the University of Delaware and his PhD from the University of Minnesota. After a year as a postdoc at MIT, Tisdale became an assistant professor. His research interests include nanotechnology and energy transport.
Tisdale’s colleague Kristala Prather calls him a “curriculum fixer.” During an internal review of Course 10 subjects, the department discovered that 10.213 (Chemical and Biological Engineering) was the least popular subject in the major and needed to be revised. After carefully evaluating the coursework, and despite having never taught 10.213 himself, Tisdale envisioned a novel way of teaching it. With his suggestions, the class went from being “despised” to loved, with subject evaluations improving by 70 percent from one spring to the next. “I knew Will could make a difference, but I had no idea he could make that big of a difference in just one year,” remarks Prather. One student nominator even went so far as to call 10.213, as taught by Tisdale, “one of my best experiences at MIT.”
Always patient, kind, and adaptable, Tisdale’s willingness to tackle difficult problems is reflected in his teaching. “While the class would occasionally start to mutiny when faced with a particularly confusing section, Prof. Tisdale would take our groans on with excitement,” wrote one student. “His attitude made us feel like we could all get through the class together.” Regardless of how they performed on a test, wrote another, Tisdale “clearly sent the message that we all always have so much more to learn, but that first and foremost he respected you as a person.”
“I don’t think I could teach the way I teach at many other universities,” Tisdale says. “MIT students show up on the first day of class with an innate desire to understand the world around them; all I have to do is pull back the curtain!”
“Professor Tisdale remains the best teacher, mentor, and role model that I have encountered,” one student remarked. “He has truly changed the course of my life.”
“I am extremely thankful to be at a university that values undergraduate education so highly,” Tisdale says. “Those of us who devote ourselves to undergraduate teaching and mentoring do so out of a strong sense of responsibility to the students as well as a genuine love of learning. There are few things more validating than being rewarded for doing something that already brings you joy.”
Jacob White
Jacob White is the Cecil H. Green Professor of Electrical Engineering and Computer Science (EECS) and chair of the Committee on Curricula. After completing his undergraduate degree at MIT, he received a master’s degree and doctorate from the University of California at Berkeley. He has been a member of the Course 6 faculty since 1987.
Colleagues and students alike observed White’s dedication not just to teaching, but to improving teaching throughout the Institute. As Luca Daniel and Asu Ozdaglar of the EECS department noted in their nomination letter, “Jacob completely understands that the most efficient way to make his passion and ideas for undergraduate education have a real lasting impact is to ‘teach it to the teachers!’”
One student wrote that White “has spent significant time and effort educating the lab assistants” of 6.302 (Feedback System Design). As one of these teaching assistants confirmed, White’s “enthusiastic spirit” inspired them to spend hours discussing how to best teach the subject. “Many people might think this is not how they want to spend their Thursday nights,” the student wrote. “I can speak for myself and the other TAs when I say that it was an incredibly fun and educational experience.”
His work to improve instruction has even expanded to other departments. A colleague describes White’s efforts to revamp 8.02 (Physics II) as “Herculean.” Working with a group of students and postdocs to develop experiments for this subject, “he seemed to be everywhere at once … while simultaneously teaching his own class.” Iterations took place over a year and a half, after which White trained the subject’s TAs as well. Hundreds of students are benefitting from these improved experiments.
White is, according to Daniel and Ozdaglar, “a colleague who sincerely, genuinely, and enormously cares about our undergraduate students and their education, not just in our EECS department, but also in our entire MIT home.”
When he’s not fine-tuning pedagogy or conducting teacher training, he is personally supporting his students. A visiting student described White’s attention: “He would regularly meet with us in groups of two to make sure we were learning. In a class of about 80 students in a huge lecture hall, it really felt like he cared for each of us.”
And his zeal has rubbed off: “He made me feel like being excited about the material was the most important thing,” one student wrote.
The significance of such a spark is not lost on White. ""As an MIT freshman in the late 1970s, I joined an undergraduate research program being pioneered by Professor Margaret MacVicar,"" he says. ""It was Professor MacVicar and UROP that put me on the academic's path of looking for interesting problems with instructive solutions. It is a path I have walked for decades, with extraordinary colleagues and incredible students. So, being selected as a MacVicar Fellow? No honor could mean more to me.""



",2020 MacVicar Faculty Fellows named,2020-03-09,['Alison Trachy'],"Office of the Vice Chancellor/MacVicar fellows/Undergraduate Research Opportunities Program (UROP)/Materials Science and Engineering/Literature/EdX/Electrical engineering and computer science (EECS)/School of Engineering/School of Humanities Arts and Social Sciences/Faculty/Awards, honors and fellowships/Education, teaching, academics/Mentoring/Undergraduate/Chemical engineering","['education', 'fellows', 'named', 'teaching', 'student', 'faculty', '2020', 'engineering', 'tisdale', 'professor', 'undergraduate', 'macvicar', 'mit', 'students']","The Office of the Vice Chancellor and the Registrar’s Office have announced this year’s Margaret MacVicar Faculty Fellows: materials science and engineering Professor Polina Anikeeva, literature Professor Mary Fuller, chemical engineering Professor William Tisdale, and electrical engineering and computer science Professor Jacob White.
For nearly three decades, the MacVicar Faculty Fellows Program has been recognizing exemplary undergraduate teaching and advising around the Institute.
The program was named after Margaret MacVicar, the first dean for undergraduate education and founder of the Undergraduate Research Opportunities Program (UROP).
She attended St. Petersburg State Polytechnic University for her undergraduate education.
""It was Professor MacVicar and UROP that put me on the academic's path of looking for interesting problems with instructive solutions.",Mit
159,https://news.mit.edu/2020/aleksander-madry-machine-learning-0308,"


The work of MIT computer scientist Aleksander Madry is fueled by one core mission: “doing machine learning the right way.”Madry’s research centers largely on making machine learning — a type of artificial intelligence — more accurate, efficient, and robust against errors. In his classroom and beyond, he also worries about questions of ethical computing, as we approach an age where artificial intelligence will have great impact on many sectors of society.“I want society to truly embrace machine learning,” says Madry, a recently tenured professor in the Department of Electrical Engineering and Computer Science. “To do that, we need to figure out how to train models that people can use safely, reliably, and in a way that they understand.”Interestingly, his work with machine learning dates back only a couple of years, to shortly after he joined MIT in 2015. In that time, his research group has published several critical papers demonstrating that certain models can be easily tricked to produce inaccurate results — and showing how to make them more robust.In the end, he aims to make each model’s decisions more interpretable by humans, so researchers can peer inside to see where things went awry. At the same time, he wants to enable nonexperts to deploy the improved models in the real world for, say, helping diagnose disease or control driverless cars.“It’s not just about trying to crack open the machine-learning black box. I want to open it up, see how it works, and pack it back up, so people can use it without needing to understand what’s going on inside,” he says.For the love of algorithmsMadry was born in Wroclaw, Poland, where he attended the University of Wroclaw as an undergraduate in the mid-2000s. While he harbored interest in computer science and physics, “I actually never thought I’d become a scientist,” he says.An avid video gamer, Madry initially enrolled in the computer science program with intentions of programming his own games. But in joining friends in a few classes in theoretical computer science and, in particular, theory of algorithms, he fell in love with the material. Algorithm theory aims to find efficient optimization procedures for solving computational problems, which requires tackling difficult mathematical questions. “I realized I enjoy thinking deeply about something and trying to figure it out,” says Madry, who wound up double-majoring in physics and computer science.When it came to delving deeper into algorithms in graduate school, he went to his first choice: MIT. Here, he worked under both Michel X. Goemans, who was a major figure in applied math and algorithm optimization, and Jonathan A. Kelner, who had just arrived to MIT as a junior faculty working in that field. For his PhD dissertation, Madry developed algorithms that solved a number of longstanding problems in graph algorithms, earning the 2011 George M. Sprowls Doctoral Dissertation Award for the best MIT doctoral thesis in computer science.After his PhD, Madry spent a year as a postdoc at Microsoft Research New England, before teaching for three years at the Swiss Federal Institute of Technology Lausanne — which Madry calls “the Swiss version of MIT.” But his alma mater kept calling him back: “MIT has the thrilling energy I was missing. It’s in my DNA.”Getting adversarialShortly after joining MIT, Madry found himself swept up in a novel science: machine learning. In particular, he focused on understanding the re-emerging paradigm of deep learning. That’s an artificial-intelligence application that uses multiple computing layers to extract high-level features from raw input — such as using pixel-level data to classify images. MIT’s campus was, at the time, buzzing with new innovations in the domain.But that begged the question: Was machine learning all hype or solid science? “It seemed to work, but no one actually understood how and why,” Madry says.Answering that question set his group on a long journey, running experiment after experiment on deep-learning models to understand the underlying principles. A major milestone in this journey was an influential paper they published in 2018, developing a methodology for making machine-learning models more resistant to “adversarial examples.” Adversarial examples are slight perturbations to input data that are imperceptible to humans — such as changing the color of one pixel in an image — but cause a model to make inaccurate predictions. They illuminate a major shortcoming of existing machine-learning tools.Continuing this line of work, Madry’s group showed that the existence of these mysterious adversarial examples may contribute to how machine-learning models make decisions. In particular, models designed to differentiate images of, say, cats and dogs, make decisions based on features that do not align with how humans make classifications. Simply changing these features can make the model consistently misclassify cats as dogs, without changing anything in the image that’s really meaningful to humans.Results indicated some models — which may be used to, say, identify abnormalities in medical images or help autonomous cars identify objects in the road — aren’t exactly up to snuff. “People often think these models are superhuman, but they didn’t actually solve the classification problem we intend them to solve,” Madry says. “And their complete vulnerability to adversarial examples was a manifestation of that fact. That was an eye-opening finding.”That’s why Madry seeks to make machine-learning models more interpretable to humans. New models he’s developed show how much certain pixels in images the system is trained on can influence the system’s predictions. Researchers can then tweak the models to focus on pixels clusters more closely correlated with identifiable features — such as detecting an animal’s snout, ears, and tail. In the end, that will help make the models more humanlike — or “superhumanlike” — in their decisions. To further this work, Madry and his colleagues recently founded the MIT Center for Deployable Machine Learning, a collaborative research effort within the MIT Quest for Intelligence that is working toward building machine-learning tools ready for real-world deployment. “We want machine learning not just as a toy, but as something you can use in, say, an autonomous car, or health care. Right now, we don’t understand enough to have sufficient confidence in it for those critical applications,” Madry says.Shaping education and policyMadry views artificial intelligence and decision making (“AI+D” is one of the three new academic units in the Department of Electrical Engineering and Computer Science) as “the interface of computing that’s going to have the biggest impact on society.”In that regard, he makes sure to expose his students to the human aspect of computing. In part, that means considering consequences of what they’re building. Often, he says, students will be overly ambitious in creating new technologies, but they haven’t thought through potential ramifications on individuals and society. “Building something cool isn’t a good enough reason to build something,” Madry says. “It’s about thinking about not if we can build something, but if we should build something.”Madry has also been engaging in conversations about laws and policies to help regulate machine learning. A point of these discussions, he says, is to better understand the costs and benefits of unleashing machine-learning technologies on society.“Sometimes we overestimate the power of machine learning, thinking it will be our salvation. Sometimes we underestimate the cost it may have on society,” Madry says. “To do machine learning right, there’s still a lot still left to figure out.”


",“Doing machine learning the right way”,2020-03-08,['Rob Matheson'],Computer science and technology/Algorithms/Artificial intelligence/Machine learning/Computer vision/Technology and society/Faculty/Profile/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/School of Engineering/MIT Schwarzman College of Computing/Quest for Intelligence,"['madry', 'computer', 'learning', 'models', 'understand', 'work', 'right', 'doing', 'way', 'machine', 'machinelearning', 'science', 'mit']","The work of MIT computer scientist Aleksander Madry is fueled by one core mission: “doing machine learning the right way.”Madry’s research centers largely on making machine learning — a type of artificial intelligence — more accurate, efficient, and robust against errors.
It’s in my DNA.”Getting adversarialShortly after joining MIT, Madry found himself swept up in a novel science: machine learning.
“We want machine learning not just as a toy, but as something you can use in, say, an autonomous car, or health care.
“Sometimes we overestimate the power of machine learning, thinking it will be our salvation.
“To do machine learning right, there’s still a lot still left to figure out.”",Mit
160,https://news.mit.edu/2020/showing-robots-learn-chores-0306,"


Training interactive robots may one day be an easy job for everyone, even those without programming expertise. Roboticists are developing automated robots that can learn new tasks solely by observing humans. At home, you might someday show a domestic robot how to do routine chores. In the workplace, you could train robots like new employees, showing them how to perform many duties.Making progress on that vision, MIT researchers have designed a system that lets these types of robots learn complicated tasks that would otherwise stymie them with too many confusing rules. One such task is setting a dinner table under certain conditions.  At its core, the researchers’ “Planning with Uncertain Specifications” (PUnS) system gives robots the humanlike planning ability to simultaneously weigh many ambiguous — and potentially contradictory — requirements to reach an end goal. In doing so, the system always chooses the most likely action to take, based on a “belief” about some probable specifications for the task it is supposed to perform.In their work, the researchers compiled a dataset with information about how eight objects — a mug, glass, spoon, fork, knife, dinner plate, small plate, and bowl — could be placed on a table in various configurations. A robotic arm first observed randomly selected human demonstrations of setting the table with the objects. Then, the researchers tasked the arm with automatically setting a table in a specific configuration, in real-world experiments and in simulation, based on what it had seen.To succeed, the robot had to weigh many possible placement orderings, even when items were purposely removed, stacked, or hidden. Normally, all of that would confuse robots too much. But the researchers’ robot made no mistakes over several real-world experiments, and only a handful of mistakes over tens of thousands of simulated test runs.  “The vision is to put programming in the hands of domain experts, who can program robots through intuitive ways, rather than describing orders to an engineer to add to their code,” says first author Ankit Shah, a graduate student in the Department of Aeronautics and Astronautics (AeroAstro) and the Interactive Robotics Group, who emphasizes that their work is just one step in fulfilling that vision. “That way, robots won’t have to perform preprogrammed tasks anymore. Factory workers can teach a robot to do multiple complex assembly tasks. Domestic robots can learn how to stack cabinets, load the dishwasher, or set the table from people at home.”Joining Shah on the paper are AeroAstro and Interactive Robotics Group graduate student Shen Li and Interactive Robotics Group leader Julie Shah, an associate professor in AeroAstro and the Computer Science and Artificial Intelligence Laboratory.







Play video






Bots hedging betsRobots are fine planners in tasks with clear “specifications,” which help describe the task the robot needs to fulfill, considering its actions, environment, and end goal. Learning to set a table by observing demonstrations, is full of uncertain specifications. Items must be placed in certain spots, depending on the menu and where guests are seated, and in certain orders, depending on an item’s immediate availability or social conventions. Present approaches to planning are not capable of dealing with such uncertain specifications.A popular approach to planning is “reinforcement learning,” a trial-and-error machine-learning technique that rewards and penalizes them for actions as they work to complete a task. But for tasks with uncertain specifications, it’s difficult to define clear rewards and penalties. In short, robots never fully learn right from wrong.The researchers’ system, called PUnS (for Planning with Uncertain Specifications), enables a robot to hold a “belief” over a range of possible specifications. The belief itself can then be used to dish out rewards and penalties. “The robot is essentially hedging its bets in terms of what’s intended in a task, and takes actions that satisfy its belief, instead of us giving it a clear specification,” Ankit Shah says.The system is built on “linear temporal logic” (LTL), an expressive language that enables robotic reasoning about current and future outcomes. The researchers defined templates in LTL that model various time-based conditions, such as what must happen now, must eventually happen, and must happen until something else occurs. The robot’s observations of 30 human demonstrations for setting the table yielded a probability distribution over 25 different LTL formulas. Each formula encoded a slightly different preference — or specification — for setting the table. That probability distribution becomes its belief.“Each formula encodes something different, but when the robot considers various combinations of all the templates, and tries to satisfy everything together, it ends up doing the right thing eventually,” Ankit Shah says.Following criteriaThe researchers also developed several criteria that guide the robot toward satisfying the entire belief over those candidate formulas. One, for instance, satisfies the most likely formula, which discards everything else apart from the template with the highest probability. Others satisfy the largest number of unique formulas, without considering their overall probability, or they satisfy several formulas that represent highest total probability. Another simply minimizes error, so the system ignores formulas with high probability of failure.Designers can choose any one of the four criteria to preset before training and testing. Each has its own tradeoff between flexibility and risk aversion. The choice of criteria depends entirely on the task. In safety critical situations, for instance, a designer may choose to limit possibility of failure. But where consequences of failure are not as severe, designers can choose to give robots greater flexibility to try different approaches.With the criteria in place, the researchers developed an algorithm to convert the robot’s belief — the probability distribution pointing to the desired formula — into an equivalent reinforcement learning problem. This model will ping the robot with a reward or penalty for an action it takes, based on the specification it’s decided to follow.In simulations asking the robot to set the table in different configurations, it only made six mistakes out of 20,000 tries. In real-world demonstrations, it showed behavior similar to how a human would perform the task. If an item wasn’t initially visible, for instance, the robot would finish setting the rest of the table without the item. Then, when the fork was revealed, it would set the fork in the proper place. “That’s where flexibility is very important,” Ankit Shah says. “Otherwise it would get stuck when it expects to place a fork and not finish the rest of table setup.”Next, the researchers hope to modify the system to help robots change their behavior based on verbal instructions, corrections, or a user’s assessment of the robot’s performance. “Say a person demonstrates to a robot how to set a table at only one spot. The person may say, ‘do the same thing for all other spots,’ or, ‘place the knife before the fork here instead,’” Ankit Shah says. “We want to develop methods for the system to naturally adapt to handle those verbal commands, without needing additional demonstrations.”  


",Showing robots how to do your chores,2020-03-06,['Rob Matheson'],Research/Computer science and technology/Algorithms/Artificial intelligence/Machine learning/Robots/Robotics/Assistive technology/Aeronautical and astronautical engineering/Computer Science and Artificial Intelligence Laboratory (CSAIL)/School of Engineering,"['chores', 'table', 'shah', 'showing', 'researchers', 'specifications', 'robots', 'probability', 'task', 'system', 'robot', 'tasks']","But the researchers’ robot made no mistakes over several real-world experiments, and only a handful of mistakes over tens of thousands of simulated test runs.
Training interactive robots may one day be an easy job for everyone, even those without programming expertise.
In the workplace, you could train robots like new employees, showing them how to perform many duties.
The robot’s observations of 30 human demonstrations for setting the table yielded a probability distribution over 25 different LTL formulas.
But where consequences of failure are not as severe, designers can choose to give robots greater flexibility to try different approaches.",Mit
161,https://news.mit.edu/2020/computer-model-brain-vision-0304,"


When we open our eyes, we immediately see our surroundings in great detail. How the brain is able to form these richly detailed representations of the world so quickly is one of the biggest unsolved puzzles in the study of vision.Scientists who study the brain have tried to replicate this phenomenon using computer models of vision, but so far, leading models only perform much simpler tasks such as picking out an object or a face against a cluttered background. Now, a team led by MIT cognitive scientists has produced a computer model that captures the human visual system’s ability to quickly generate a detailed scene description from an image, and offers some insight into how the brain achieves this.“What we were trying to do in this work is to explain how perception can be so much richer than just attaching semantic labels on parts of an image, and to explore the question of how do we see all of the physical world,” says Josh Tenenbaum, a professor of computational cognitive science and a member of MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Center for Brains, Minds, and Machines (CBMM).The new model posits that when the brain receives visual input, it quickly performs a series of computations that reverse the steps that a computer graphics program would use to generate a 2D representation of a face or other object. This type of model, known as efficient inverse graphics (EIG), also correlates well with electrical recordings from face-selective regions in the brains of nonhuman primates, suggesting that the primate visual system may be organized in much the same way as the computer model, the researchers say.Ilker Yildirim, a former MIT postdoc who is now an assistant professor of psychology at Yale University, is the lead author of the paper, which appears today in Science Advances. Tenenbaum and Winrich Freiwald, a professor of neurosciences and behavior at Rockefeller University, are the senior authors of the study. Mario Belledonne, a graduate student at Yale, is also an author.Inverse graphicsDecades of research on the brain’s visual system has studied, in great detail, how light input onto the retina is transformed into cohesive scenes. This understanding has helped artificial intelligence researchers develop computer models that can replicate aspects of this system, such as recognizing faces or other objects.“Vision is the functional aspect of the brain that we understand the best, in humans and other animals,” Tenenbaum says. “And computer vision is one of the most successful areas of AI at this point. We take for granted that machines can now look at pictures and recognize faces very well, and detect other kinds of objects.”However, even these sophisticated artificial intelligence systems don’t come close to what the human visual system can do, Yildirim says.“Our brains don’t just detect that there’s an object over there, or recognize and put a label on something,” he says. “We see all of the shapes, the geometry, the surfaces, the textures. We see a very rich world.”More than a century ago, the physician, physicist, and philosopher Hermann von Helmholtz theorized that the brain creates these rich representations by reversing the process of image formation. He hypothesized that the visual system includes an image generator that would be used, for example, to produce the faces that we see during dreams. Running this generator in reverse would allow the brain to work backward from the image and infer what kind of face or other object would produce that image, the researchers say.However, the question remained: How could the brain perform this process, known as inverse graphics, so quickly? Computer scientists have tried to create algorithms that could perform this feat, but the best previous systems require many cycles of iterative processing, taking much longer than the 100 to 200 milliseconds the brain requires to create a detailed visual representation of what you’re seeing. Neuroscientists believe perception in the brain can proceed so quickly because it is implemented in a mostly feedforward pass through several hierarchically organized layers of neural processing.The MIT-led team set out to build a special kind of deep neural network model to show how a neural hierarchy can quickly infer the underlying features of a scene — in this case, a specific face. In contrast to the standard deep neural networks used in computer vision, which are trained from labeled data indicating the class of an object in the image, the researchers’ network is trained from a model that reflects the brain’s internal representations of what scenes with faces can look like.Their model thus learns to reverse the steps performed by a computer graphics program for generating faces. These graphics programs begin with a three-dimensional representation of an individual face and then convert it into a two-dimensional image, as seen from a particular viewpoint. These images can be placed on an arbitrary background image. The researchers theorize that the brain’s visual system may do something similar when you dream or conjure a mental image of someone’s face.The researchers trained their deep neural network to perform these steps in reverse — that is, it begins with the 2D image and then adds features such as texture, curvature, and lighting, to create what the researchers call a “2.5D” representation. These 2.5D images specify the shape and color of the face from a particular viewpoint. Those are then converted into 3D representations, which don’t depend on the viewpoint.“The model gives a systems-level account of the processing of faces in the brain, allowing it to see an image and ultimately arrive at a 3D object, which includes representations of shape and texture, through this important intermediate stage of a 2.5D image,” Yildirim says.Model performanceThe researchers found that their model is consistent with data obtained by studying certain regions in the brains of macaque monkeys. In a study published in 2010, Freiwald and Doris Tsao of Caltech recorded the activity of neurons in those regions and analyzed how they responded to 25 different faces, seen from seven different viewpoints. That study revealed three stages of higher-level face processing, which the MIT team now hypothesizes correspond to three stages of their inverse graphics model: roughly, a 2.5D viewpoint-dependent stage; a stage that bridges from 2.5 to 3D; and a 3D, viewpoint-invariant stage of face representation.“What we show is that both the quantitative and qualitative response properties of those three levels of the brain seem to fit remarkably well with the top three levels of the network that we’ve built,” Tenenbaum says.The researchers also compared the model’s performance to that of humans in a task that involves recognizing faces from different viewpoints. This task becomes harder when researchers alter the faces by removing the face’s texture while preserving its shape, or distorting the shape while preserving relative texture. The new model’s performance was much more similar to that of humans than computer models used in state-of-the-art face-recognition software, additional evidence that this model may be closer to mimicking what happens in the human visual system.“This work is exciting because it introduces interpretable stages of intermediate representation into a feedforward neural network model of face recognition,” says Nikolaus Kriegeskorte, a professor of psychology and neuroscience at Columbia University, who was not involved in the research. “Their approach merges the classical idea that vision inverts a model of how the image was generated, with modern deep feedforward networks. It’s very interesting that this model better explains neural representations and behavioral responses.”The researchers now plan to continue testing the modeling approach on additional images, including objects that aren’t faces, to investigate whether inverse graphics might also explain how the brain perceives other kinds of scenes. In addition, they believe that adapting this approach to computer vision could lead to better-performing AI systems.“If we can show evidence that these models might correspond to how the brain works, this work could lead computer vision researchers to take more seriously and invest more engineering resources in this inverse graphics approach to perception,” Tenenbaum says. “The brain is still the gold standard for any kind of machine that sees the world richly and quickly.”The research was funded by the Center for Brains, Minds, and Machines at MIT, the National Science Foundation, the National Eye Institute, the Office of Naval Research, the New York Stem Cell Foundation, the Toyota Research Institute, and Mitsubishi Electric.


",A new model of vision,2020-03-04,['Anne Trafton'],Research/Computer vision/Brain and cognitive sciences/Center for Brains Minds and Machines/Computer Science and Artificial Intelligence Laboratory (CSAIL)/School of Science/School of Engineering/National Science Foundation (NSF)/Artificial intelligence/Machine learning/Neuroscience,"['face', 'graphics', 'faces', 'model', 'visual', 'computer', 'brains', 'researchers', 'image', 'vision', 'brain']","This type of model, known as efficient inverse graphics (EIG), also correlates well with electrical recordings from face-selective regions in the brains of nonhuman primates, suggesting that the primate visual system may be organized in much the same way as the computer model, the researchers say.
“Vision is the functional aspect of the brain that we understand the best, in humans and other animals,” Tenenbaum says.
Their model thus learns to reverse the steps performed by a computer graphics program for generating faces.
Model performanceThe researchers found that their model is consistent with data obtained by studying certain regions in the brains of macaque monkeys.
“Their approach merges the classical idea that vision inverts a model of how the image was generated, with modern deep feedforward networks.",Mit
162,https://news.mit.edu/2020/integrating-electronics-physical-prototypes-0304,"


MIT researchers have invented a way to integrate “breadboards” — flat platforms widely used for electronics prototyping — directly onto physical products. The aim is to provide a faster, easier way to test circuit functions and user interactions with products such as smart devices and flexible electronics.Breadboards are rectangular boards with arrays of pinholes drilled into the surface. Many of the holes have metal connections and contact points between them. Engineers can plug components of electronic systems — from basic circuits to full computer processors — into the pinholes where they want them to connect. Then, they can rapidly test, rearrange, and retest the components as needed.But breadboards have remained that same shape for decades. For that reason, it’s difficult to test how the electronics will look and feel on, say, wearables and various smart devices. Generally, people will first test circuits on traditional breadboards, then slap them onto a product prototype. If the circuit needs to be modified, it’s back to the breadboard for testing, and so on.In a paper being presented at CHI (Conference on Human Factors in Computing Systems), the researchers describe “CurveBoards,” 3D-printed objects with the structure and function of a breadboard integrated onto their surfaces. Custom software automatically designs the objects, complete with distributed pinholes that can be filled with conductive silicone to test electronics. The end products are accurate representations of the real thing, but with breadboard surfaces.CurveBoards “preserve an object’s look and feel,” the researchers write in their paper, while enabling designers to try out component configurations and test interactive scenarios during prototyping iterations. In their work, the researchers printed CurveBoards for smart bracelets and watches, Frisbees, helmets, headphones, a teapot, and a flexible, wearable e-reader.“On breadboards, you prototype the function of a circuit. But you don’t have context of its form — how the electronics will be used in a real-world prototype environment,” says first author Junyi Zhu, a graduate student in the Computer Science and Artificial Intelligence Laboratory (CSAIL). “Our idea is to fill this gap, and merge form and function testing in very early stage of prototyping an object. …  CurveBoards essentially add an additional axis to the existing [three-dimensional] XYZ axes of the object — the ‘function’ axis.”Joining Zhu on the paper are CSAIL graduate students Lotta-Gili Blumberg, Martin Nisser, and Ethan Levi Carlson; Department of Electrical Engineering and Computer Science (EECS) undergraduate students Jessica Ayeley Quaye and Xin Wen; former EECS undergraduate students Yunyi Zhu and Kevin Shum; and Stefanie Mueller, the X-Window Consortium Career Development Assistant Professor in EECS.







Play video






Custom software and hardwareA core component of the CurveBoard is custom design-editing software. Users import a 3D model of an object. Then, they select the command “generate pinholes,” and the software automatically maps all pinholes uniformly across the object. Users then choose automatic or manual layouts for connectivity channels. The automatic option lets users explore a different layout of connections across all pinholes with the click of a button. For manual layouts, interactive tools can be used to select groups of pinholes and indicate the type of connection between them. The final design is exported to a file for 3D printing.When a 3D object is uploaded, the software essentially forces its shape into a “quadmesh” — where the object is represented as a bunch of small squares, each with individual parameters. In doing so, it creates a fixed spacing between the squares. Pinholes — which are cones, with the wide end on the surface and tapering down — will be placed at each point where the corners of the squares touch. For channel layouts, some geometric techniques ensure the chosen channels will connect the desired electrical components without crossing over one another.In their work, the researchers 3D printed objects using a flexible, durable, nonconductive silicone. To provide connectivity channels, they created a custom conductive silicone that can be syringed into the pinholes and then flows through the channels after printing. The silicone is a mixture of a silicone materials designed to have minimal electricity resistance, allowing various types electronics to function.To validate the CurveBoards, the researchers printed a variety of smart products. Headphones, for instance, came equipped with menu controls for speakers and music-streaming capabilities. An interactive bracelet included a digital display, LED, and photoresistor for heart-rate monitoring, and a step-counting sensor. A teapot included a small camera to track the tea’s color, as well as colored lights on the handle to indicate hot and cold areas. They also printed a wearable e-book reader with a flexible display.Better, faster prototypingIn a user study, the team investigated the benefits of CurveBoards prototyping. They split six participants with varying prototyping experience into two sections: One used traditional breadboards and a 3D-printed object, and the other used only a CurveBoard of the object. Both sections designed the same prototype but switched back and forth between sections after completing designated tasks. In the end, five of six of the participants preferred prototyping with the CurveBoard. Feedback indicated the CurveBoards were overall faster and easier to work with.But CurveBoards are not designed to replace breadboards, the researchers say. Instead, they’d work particularly well as a so-called “midfidelity” step in the prototyping timeline, meaning between initial breadboard testing and the final product. “People love breadboards, and there are cases where they’re fine to use,” Zhu says. “This is for when you have an idea of the final object and want to see, say, how people interact with the product. It’s easier to have a CurveBoard instead of circuits stacked on top of a physical object.”Next, the researchers hope to design general templates of common objects, such as hats and bracelets. Right now, a new CurveBoard must built for each new object. Ready-made templates, however, would let designers quickly experiment with basic circuits and user interaction, before designing their specific CurveBoard.Additionally, the researchers want to move some early-stage prototyping steps entirely to the software side. The idea is that people can design and test circuits — and possibly user interaction — entirely on the 3D model generated by the software. After many iterations, they can 3D print a more finalized CurveBoard. “That way you’ll know exactly how it’ll work in the real world, enabling fast prototyping,” Zhu says. “That would be a more ‘high-fidelity’ step for prototyping.”


",Integrating electronics onto physical prototypes,2020-03-04,['Rob Matheson'],Research/Computer science and technology/3-D printing/Design/Manufacturing/electronics/Computer graphics/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/School of Engineering,"['electronics', 'prototyping', 'curveboard', 'integrating', 'researchers', 'object', 'curveboards', 'pinholes', 'software', 'breadboards', '3d', 'physical', 'prototypes', 'test']","Custom software automatically designs the objects, complete with distributed pinholes that can be filled with conductive silicone to test electronics.
For that reason, it’s difficult to test how the electronics will look and feel on, say, wearables and various smart devices.
MIT researchers have invented a way to integrate “breadboards” — flat platforms widely used for electronics prototyping — directly onto physical products.
In their work, the researchers 3D printed objects using a flexible, durable, nonconductive silicone.
Better, faster prototypingIn a user study, the team investigated the benefits of CurveBoards prototyping.",Mit
163,https://news.mit.edu/2020/warning-labels-fake-news-trustworthy-0303,"


After the 2016 U.S. presidential election, Facebook began putting warning tags on news stories fact-checkers judged to be false. But there’s a catch: Tagging some stories as false makes readers more willing to believe other stories and share them with friends, even if those additional, untagged stories also turn out to be false.That is the main finding of a new study co-authored by an MIT professor, based on multiple experiments with news consumers. The researchers call this unintended consequence — in which the selective labeling of false news makes other news stories seem more legitimate — the “implied-truth effect” in news consumption.“Putting a warning on some content is going to make you think, to some extent, that all of the other content without the warning might have been checked and verified,” says David Rand, the Erwin H. Schell Professor at the MIT Sloan School of Management and co-author of a newly published paper detailing the study.“There’s no way the fact-checkers can keep up with the stream of misinformation, so even if the warnings do really reduce belief in the tagged stories, you still have a problem, because of the implied truth effect,” Rand adds.Moreover, Rand observes, the implied truth effect “is actually perfectly rational” on the part of readers, since there is ambiguity about whether untagged stories were verified or just not yet checked. “That makes these warnings potentially problematic,” he says. “Because people will reasonably make this inference.”Even so, the findings also suggest a solution: Placing “Verified” tags on stories found to be true eliminates the problem.The paper, “The Implied Truth Effect,” has just appeared in online form in the journal Management Science. In addition to Rand, the authors are Gordon Pennycook, an assistant professor of psychology at the University of Regina; Adam Bear, a postdoc in the Cushman Lab at Harvard University; and Evan T. Collins, an undergraduate researcher on the project from Yale University.BREAKING: More labels are betterTo conduct the study, the researchers conducted a pair of online experiments with a total of 6,739 U.S. residents, recruited via Amazon’s Mechanical Turk platform. Participants were given a variety of true and false news headlines in a Facebook-style format. The false stories were chosen from the website Snopes.com and included headlines such as “BREAKING NEWS: Hillary Clinton Filed for Divorce in New York Courts” and “Republican Senator Unveils Plan To Send All Of America’s Teachers Through A Marine Bootcamp.”The participants viewed an equal mix of true stories and false stories, and were asked whether they would consider sharing each story on social media. Some participants were assigned to a control group in which no stories were labeled; others saw a set of stories where some of the false ones displayed a “FALSE” label; and some participants saw a set of stories with warning labels on some false stories and “TRUE” verification labels for some true stories.In the first place, stamping warnings on false stories does make people less likely to consider sharing them. For instance, with no labels being used at all, participants considered sharing 29.8 percent of false stories in the sample. That figure dropped to 16.1 percent of false stories that had a warning label attached.However, the researchers also saw the implied truth effect take effect. Readers were willing to share 36.2 percent of the remaining false stories that did not have warning labels, up from 29.8 percent.“We robustly observe this implied-truth effect, where if false content doesn’t have a warning, people believe it more and say they would be more likely to share it,” Rand notes.But when the warning labels on some false stories were complemented with verification labels on some of the true stories, participants were less likely to consider sharing false stories, across the board. In those circumstances, they shared only 13.7 percent of the headlines labeled as false, and just 26.9 percent of the nonlabeled false stories.“If, in addition to putting warnings on things fact-checkers find to be false, you also put verification panels on things fact-checkers find to be true, then that solves the problem, because there’s no longer any ambiguity,” Rand says. “If you see a story without a label, you know it simply hasn’t been checked.”Policy implicationsThe findings come with one additional twist that Rand emphasizes, namely, that participants in the survey did not seem to reject warnings on the basis of ideology. They were still likely to change their perceptions of stories with warning or verifications labels, even if discredited news items were “concordant” with their stated political views.“These results are not consistent with the idea that our reasoning powers are hijacked by our partisanship,” Rand says.Rand notes that, while continued research on the subject is important, the current study suggests a straightforward way that social media platforms can take action to further improve their systems of labeling online news content.“I think this has clear policy implications when platforms are thinking about attaching warnings,” he says. “They should be very careful to check not just the effect of the warnings on the content with the tag, but also check the effects on all the other content.”Support for the research was provided, in part, by the Ethics and Governance of Artificial Intelligence Initiative of the Miami Foundation, and the Social Sciences and Humanities Research Council of Canada.


",The catch to putting warning labels on fake news,2020-03-03,['Peter Dizikes'],Social media/Internet/Politics/Marketing/Technology and society/Sloan School of Management,"['true', 'labels', 'catch', 'fake', 'effect', 'warnings', 'sharing', 'putting', 'truth', 'rand', 'warning', 'false', 'participants']","After the 2016 U.S. presidential election, Facebook began putting warning tags on news stories fact-checkers judged to be false.
The paper, “The Implied Truth Effect,” has just appeared in online form in the journal Management Science.
However, the researchers also saw the implied truth effect take effect.
Readers were willing to share 36.2 percent of the remaining false stories that did not have warning labels, up from 29.8 percent.
But when the warning labels on some false stories were complemented with verification labels on some of the true stories, participants were less likely to consider sharing false stories, across the board.",Mit
164,https://news.mit.edu/2020/president-reif-testifies-congress-us-competitiveness-0227,"


No U.S. strategy to respond to competition from China will succeed unless it includes increased investment in research, a concerted effort to attract more students to key research fields, and a more creative approach to turning ideas into commercial products, MIT President L. Rafael Reif said in congressional testimony on Wednesday, Feb. 26.
Reif spoke at a hearing of the House Ways and Means Committee on “U.S.-China Trade and Competition.”“Whatever else the U.S. does to counter the challenges posed by China, we must increase our investment in research in key technology areas, and we must enhance our capacity to get the most out of that investment,” he told the panel. “U.S. strategy is unlikely to succeed if it is merely defensive; to stay ahead, the U.S. needs to do more to capitalize on our own strengths.”Reif’s Capitol Hill appearance came immediately after he delivered an opening talk at a National Academy of Sciences (NAS)_event commemorating the 75th anniversary of “Science, The Endless Frontier,” a 1945 report to U.S. President Harry S. Truman that is seen as the founding document of the post-World War II research system in the U.S. The report was written by the late Vannevar Bush, who had a long career at MIT, including service as the Institute’s vice president and dean of engineering.At both the NAS and on Capitol Hill, Reif called for a “visible, focused, and sustained” federal program that would increase funding for research and target the increase at key technologies, such as artificial intelligence, quantum computing, and advanced communications.“The U.S. lacks an effective, coordinated way to target research toward specific areas and funding has fallen far behind what’s needed to stay ahead of our competitors,” Reif told Congress. “One promising proposal is to create a new directorate at the National Science Foundation with that mission, and giving that new unit the authority to be run more like the Defense Advanced Research Projects Agency (DARPA).”Reif also said that attracting top talent is another essential element of a successful strategy. “At the university level, that requires two parallel tasks — attracting top U.S. students to key fields, and attracting and retaining the best researchers from around the world,” he said.Specifically, he called for new programs to offer federal support to undergraduates, graduate students, and postdocs who are willing to study in fields related to key technologies. He also said foreign students who receive a U.S. doctorate should immediately be given a green card to settle in the U.S., and he warned against anti-immigrant rhetoric.Finally, Reif said the U.S. needs to experiment with ways to speed the transition of ideas from lab to market. He called for new ways to de-risk technologies and to create more patient capital, and suggested that the Ways and Means Committee, which has jurisdiction over tax policy, should look at tax policies to create incentives for longer-term investment and to foster more university-industry cooperation.“The U.S. edge in science and technology has been a foundation for U.S. security, prosperity, and quality of life,” Reif said, in conclusion. “But that edge has to be regularly honed; it is not ours by right or by nature. We can best sharpen it with a strategy founded on confidence in ourselves, not fear of others.”Two weeks ago, Vice President for Research Maria Zuber delivered a similar message to Congress, in testimony before the House Permanent Select Committee on Intelligence on how to improve the intelligence services’ access to science and technology.Zuber said that to help the intelligence services, the U.S. needs to capitalize on its strengths, which she said include “world-class universities, an open research system, and the ability to attract and retain top talent from around the world.”Like Reif, Zuber highlighted a proposal to create a new technology directorate at the National Science Foundation, as well as the need to attract talent domestically and from abroad. She also cited MIT’s AI Accelerator — a cooperative project between MIT and the U.S. Air Force — as the kind of cooperative work that the intelligence services could foster.In her testimony, Zuber emphasized the need to maintain an open U.S. research system: “The U.S. faces new challenges and competitors,” she said, “but we are well-placed to succeed if we get the most from our unrivaled strengths.”


",President Reif testifies before Congress on U.S. competitiveness,2020-02-27,[],President L. Rafael Reif/Policy/Government/Innovation and Entrepreneurship (I&E)/Funding/Administration/Technology and society/National Science Foundation (NSF)/China/Quantum computing,"['congress', 'research', 'reif', 'key', 'investment', 'testifies', 'president', 'ways', 'intelligence', 'strategy', 'students', 'science', 'competitiveness']","The report was written by the late Vannevar Bush, who had a long career at MIT, including service as the Institute’s vice president and dean of engineering.
“The U.S. lacks an effective, coordinated way to target research toward specific areas and funding has fallen far behind what’s needed to stay ahead of our competitors,” Reif told Congress.
Finally, Reif said the U.S. needs to experiment with ways to speed the transition of ideas from lab to market.
“The U.S. edge in science and technology has been a foundation for U.S. security, prosperity, and quality of life,” Reif said, in conclusion.
“But that edge has to be regularly honed; it is not ours by right or by nature.",Mit
165,https://news.mit.edu/2020/to-self-drive-in-snow-look-under-road-0226,"


Car companies have been feverishly working to improve the technologies behind self-driving cars. But so far even the most high-tech vehicles still fail when it comes to safely navigating in rain and snow. 
This is because these weather conditions wreak havoc on the most common approaches for sensing, which usually involve either lidar sensors or cameras. In the snow, for example, cameras can no longer recognize lane markings and traffic signs, while the lasers of lidar sensors malfunction when there’s, say, stuff flying down from the sky.
MIT researchers have recently been wondering whether an entirely different approach might work. Specifically, what if we instead looked under the road? 







Play video






A team from MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) has developed a new system that uses an existing technology called ground-penetrating radar (GPR) to send electromagnetic pulses underground that measure the area’s specific combination of soil, rocks, and roots. Specifically, the CSAIL team used a particular form of GPR instrumentation developed at MIT Lincoln Laboratory called localizing ground-penetrating radar, or LGPR. The mapping process creates a unique fingerprint of sorts that the car can later use to localize itself when it returns to that particular plot of land.
“If you or I grabbed a shovel and dug it into the ground, all we’re going to see is a bunch of dirt,” says CSAIL PhD student Teddy Ort, lead author on a new paper about the project that will be published in the IEEE Robotics and Automation Letters journal later this month. “But LGPR can quantify the specific elements there and compare that to the map it’s already created, so that it knows exactly where it is, without needing cameras or lasers.”
In tests, the team found that in snowy conditions the navigation system’s average margin of error was on the order of only about an inch compared to clear weather. The researchers were surprised to find that it had a bit more trouble with rainy conditions, but was still only off by an average of 5.5 inches. (This is because rain leads to more water soaking into the ground, leading to a larger disparity between the original mapped LGPR reading and the current condition of the soil.)
The researchers said the system’s robustness was further validated by the fact that, over a period of six months of testing, they never had to unexpectedly step in to take the wheel. 
“Our work demonstrates that this approach is actually a practical way to help self-driving cars navigate poor weather without actually having to be able to ‘see’ in the traditional sense using laser scanners or cameras,” says MIT Professor Daniela Rus, director of CSAIL and senior author on the new paper, which will also be presented in May at the International Conference on Robotics and Automation in Paris.
While the team has only tested the system at low speeds on a closed country road, Ort said that existing work from Lincoln Laboratory suggests that the system could easily be extended to highways and other high-speed areas. 
This is the first time that developers of self-driving systems have employed ground-penetrating radar, which has previously been used in fields like construction planning, landmine detection, and even lunar exploration. The approach wouldn’t be able to work completely on its own, since it can’t detect things above ground. But its ability to localize in bad weather means that it would couple nicely with lidar and vision approaches.
“Before releasing autonomous vehicles on public streets, localization and navigation have to be totally reliable at all times,” says Roland Siegwart, a professor of autonomous systems at ETH Zurich who was not involved in the project. “The CSAIL team’s innovative and novel concept has the potential to push autonomous vehicles much closer to real-world deployment.” 
One major benefit of mapping out an area with LGPR is that underground maps tend to hold up better over time than maps created using vision or lidar, since features of an above-ground map are much more likely to change. LGPR maps also take up only about 80 percent of the space used by traditional 2D sensor maps that many companies use for their cars. 
While the system represents an important advance, Ort notes that it’s far from road-ready. Future work will need to focus on designing mapping techniques that allow LGPR datasets to be stitched together to be able to deal with multi-lane roads and intersections. In addition, the current hardware is bulky and 6 feet wide, so major design advances need to be made before it’s small and light enough to fit into commercial vehicles.
Ort and Rus co-wrote the paper with CSAIL postdoc Igor Gilitschenski. The project was supported, in part, by MIT Lincoln Laboratory.


","To self-drive in the snow, look under the road",2020-02-26,['Adam Conner-Simons'],Computer Science and Artificial Intelligence Laboratory (CSAIL)/Lincoln Laboratory/Robotics/Electrical engineering and computer science (EECS)/School of Engineering/Research/Distributed Robotics Laboratory/Artificial intelligence/Automobiles/Autonomous vehicles/Faculty/Computer vision/Machine learning/Transportation/MIT Schwarzman College of Computing,"['lidar', 'csail', 'systems', 'snow', 'lgpr', 'look', 'work', 'road', 'maps', 'system', 'selfdrive', 'weather', 'mit', 'team']","This is because these weather conditions wreak havoc on the most common approaches for sensing, which usually involve either lidar sensors or cameras.
But so far even the most high-tech vehicles still fail when it comes to safely navigating in rain and snow.
Specifically, the CSAIL team used a particular form of GPR instrumentation developed at MIT Lincoln Laboratory called localizing ground-penetrating radar, or LGPR.
But its ability to localize in bad weather means that it would couple nicely with lidar and vision approaches.
LGPR maps also take up only about 80 percent of the space used by traditional 2D sensor maps that many companies use for their cars.",Mit
166,https://news.mit.edu/2020/new-way-prepare-graduate-students-lead-tech-0226,"


Before coming to MIT, Benjamin Lienhard focused most of his energy exploring fragile quantum states, dwelling in the world of nanotechnology and filling in gaps in the research to help steer and stabilize new technologies. Now that he’s a fifth-year graduate student in electrical engineering and computer science, he’s still investigating tiny quantum bits, looking for novel ways to support enormous breakthroughs in quantum computing.
But for all his advanced technical knowledge and forward-thinking momentum, Lienhard found himself suddenly in a tenuous state in 2017. Asked to coordinate a conference, he realized developing leadership skills was an aspect of his work that he’d overlooked through all those years investigating quantum states at exceptionally small scales.
Not wanting to miss an opportunity, Lienhard accepted the conference role and other leadership roles like it, and each time he agreed to step in to lead, he arrived at the same uneasy conclusion. “I really noticed the only way to improve yourself and learn [leadership] is by actually experiencing it, executing it yourself and seeing how the people around you react to your leadership style,” Lienhard says. A background in theoretical leadership skills could’ve made that transition smoother, recognizing new situations on the job to adjust at a faster pace.
Since then, Lienhard has joined the Graduate Student Advisory Group (GradSAGE) in the School of Engineering, a group established by Anantha P. Chandrakasan, dean of the School of Engineering and the Vannevar Bush Professor of Electrical Engineering and Computer Science, to hear from students and bolster initiatives. Through GradSAGE, Lienhard is positioned to help other MIT students. On the GradSAGE Leadership Sub-Committee with engineering graduate students Vamsi Mangena, Laureen Meroueh, Lucio Milanese, Clinton Wang, and Elise Wilcox, he’s provided input that has helped pave the way for a new MIT offering this spring, designed to make those transitions from lab research into leadership roles less of a shock to the system for MIT graduate students.
Becoming a leader is nearly inevitable for engineering students, says Milanese, a fourth-year nuclear science and engineering graduate student. Even for those planning to remain in academia. “In most cases, MIT graduate students will be leading,” Milanese says. “If you become a professor, the first thing you do is set up your lab. You hire a couple graduate students, you hire a couple postdocs, and you are already, early in your 30s, essentially a manager of a small research enterprise.”
Meroueh, a fifth-year mechanical engineering graduate student and entrepreneur, puts it another way: “It’s not just our technical skills we need to make a change in the real world.” She became interested in thinking beyond the tech after co-founding a startup company called MetaStorage during her master’s program. She plans to launch a new startup after graduating, and advancing her leadership skills is part of that plan.
Recognizing how many engineering graduate students were lacking a leadership program that catered to their future goals, GradSAGE Leadership Sub-Committee approached the Bernard M. Gordon-MIT Engineering Leadership Program (GEL). This led to the creation of a new interim MIT Graduate Certificate in Technical Leadership, which will launch in a permanent form this fall. Completing the certificate requires that students complete a course called Leading Creative Teams and an additional 12 units of graduate leadership courses, plus attendance of four workshops. It’s designed to deliver both leadership theory and practical experience to engineering students by providing technical leadership-focused courses alongside hands-on workshops required to complete the certificate.
For engineering students, the GEL courses cover how to conduct multi-stakeholder negotiations, influence others, and provide leadership in the age of artificial intelligence — with coursework all contextualized within tech companies. The program also offers custom paths for graduate students in any program to create a leadership certificate that suits different career goals, with the only required course GEL’s Leading Creative Teams. It’s taught by David Niño, who has been piloting Leading Creative Teams for the past three years. For the GradSAGE students enrolled, taking Niño’s course served as inspiration for building the certificate, and forms its essential core. To complete the additional units, students from any program can choose from dozens of graduate courses from across MIT to build their own certificate, including subjects in building successful careers and organizations; advanced leadership communications; and science, technology, and public policy. “We envision it as being for everybody,” Milanese says of the certificate in technical leadership.
This spring, there are six workshops available, scheduled at different dates and times to accommodate a range of student schedules. Workshops will cover topics like how to deliver objectives in technical organizations, leadership paths in technical organizations, what to do during your first 90 days in a new professional role, and what happens when technical leaders fail to stand up to unrealistic or unethical pressures.
“If you want to improve your leadership skills, you need to exercise them in practice,” Lienhard says, adding that the workshops are not simply extensions of these courses, but immersive experiences of their own.
In addition to delivering educational value, another goal of the workshops is to build a community among graduate students interested in technical leadership. Meroueh says the workshops present an opportunity to meet students with different engineering backgrounds. “We wanted to create a sense of community,” she says. Their plan seems to be working (or perhaps it’s the free pizza). Earlier this month, Meroueh and Wilcox both attended the first workshop on technical leadership and finance, led by Olivier L. de Weck, professor of aeronautics and astronautics and engineering systems, and faculty co-director of GEL. The workshop drew twice as many attendees as the GradSAGE sub-committee had predicted.
Wilcox, a fifth-year graduate student in medical engineering and medical physics, says she left de Weck’s workshop with a fresh perspective on approaching the job market, taking away actionable advice like how to check a company’s financial health before agreeing to come onboard. She also learned how companies make decisions based on finances, a way of thinking she says will help her better pitch her ideas. Citing a need for female leadership in engineering, Meroueh adds that participating in leadership programs can help women navigate to the top in a male-dominated field.
To earn the certificate, students must complete four out of six workshops, attendance of which can be spread out over different semesters. The workshops take two hours to complete, with registration required and food and drinks provided to attendees.
Although half of engineering graduate students that GradSAGE sub-committee surveyed indicated an interest in a leadership certificate like GEL’s new initiative, two-thirds of respondents were concerned they wouldn’t have time to hone leadership skills during their graduate degree program. Lienhard says for doctoral programs that require minors, the leadership certificate’s courses can be simultaneously used to meet that requirement, which provides the further benefit of acquiring leaderships skills while working closely with an advisor.
This spring, an Interim Certificate in Technical Leadership will be available through the Graduate Program in Engineering Leadership. Any eligible courses completed can be retroactively applied once the certificate debuts next fall. For Lienhard, this bundling of tailored courses combined with practical workshops gives MIT graduate students a “less painful” and more productive adjustment period on the path to specific ambitions, so somebody who is gunning to be chief technology officer doesn’t waste time learning insights more appropriate for tomorrow’s next top CEO.
Milanese says the first thing the GradSAGE subcommittee did when they met was land on their own definition of leadership, which serves as a simple summation of the wide array of ambitions being pursued by aspiring tech leaders at MIT. According to Milanese, GradSAGE hopes the new certificate instills in graduate students interested in developing leadership skills “the ability to work with others to create great things.”


",A new way to prepare graduate students to lead in tech,2020-02-26,[],Electrical engineering and computer science (EECS)/Nuclear science and engineering/Mechanical engineering/Leadership/Aeronautical and astronautical engineering/Classes and programs/School of Engineering/GEL Program,"['graduate', 'technical', 'lienhard', 'leadership', 'engineering', 'tech', 'prepare', 'way', 'workshops', 'certificate', 'gradsage', 'lead', 'mit', 'students']","Becoming a leader is nearly inevitable for engineering students, says Milanese, a fourth-year nuclear science and engineering graduate student.
“In most cases, MIT graduate students will be leading,” Milanese says.
Recognizing how many engineering graduate students were lacking a leadership program that catered to their future goals, GradSAGE Leadership Sub-Committee approached the Bernard M. Gordon-MIT Engineering Leadership Program (GEL).
In addition to delivering educational value, another goal of the workshops is to build a community among graduate students interested in technical leadership.
This spring, an Interim Certificate in Technical Leadership will be available through the Graduate Program in Engineering Leadership.",Mit
167,https://news.mit.edu/2020/protecting-sensitive-metadata-from-surveillance-0226,"


MIT researchers have designed a scalable system that secures the metadata — such as who’s corresponding and when — of millions of users in communications networks, to help protect the information against possible state-level surveillance.Data encryption schemes that protect the content of online communications are prevalent today. Apps like WhatsApp, for instance, use “end-to-end encryption” (E2EE), a scheme that ensures third-party eavesdroppers can’t read messages sent by end users.But most of those schemes overlook metadata, which contains information about who’s talking, when the messages are sent, the size of message, and other information. Many times, that’s all a government or other hacker needs to know to track an individual. This can be especially dangerous for, say, a government whistleblower or people living in oppressive regimes talking with journalists.Systems that fully protect user metadata with cryptographic privacy are complex, and they suffer scalability and speed issues that have so far limited their practicality. Some methods can operate quickly but provide much weaker security. In a paper being presented at the USENIX Symposium on Networked Systems Design and Implementation, the MIT researchers describe “XRD” (for Crossroads), a metadata-protection scheme that can handle cryptographic communications from millions of users in minutes, whereas traditional methods with the same level of security would take hours to send everyone’s messages.“There is a huge lack in protection for metadata, which is sometimes very sensitive. The fact that I’m sending someone a message at all is not protected by encryption,” says first author Albert Kwon PhD ’19, a recent graduate from the Computer Science and Artificial Intelligence Laboratory (CSAIL). “Encryption can protect content well. But how can we fully protect users from metadata leaks that a state-level adversary can leverage?”Joining Kwon on the paper are David Lu, an undergraduate in the Department of Electrical Engineering and Computer Science; and Srinivas Devadas, the Edwin Sibley Webster Professor of Electrical Engineering and Computer Science in CSAIL.New spin on mix netsStarting in 2013, disclosures of classified information by Edward Snowden revealed widespread global surveillance by the U.S. government. Although the mass collection of metadata by the National Security Agency was subsequently discontinued, in 2014 former director of the NSA and the Central Intelligence Agency Michael Hayden explained that the government can often rely solely on metadata to find the information it’s seeking. As it happens, this is right around the time Kwon started his PhD studies.“That was like a punch to the cryptography and security communities,” Kwon says. “That meant encryption wasn’t really doing anything to stop spying in that regard.”
Kwon spent most of his PhD program focusing on metadata privacy. With XRD, Kwon says he “put a new spin” on a traditional E2EE metadata-protecting scheme, called “mix nets,” which was invented decades ago but suffers from scalability issues.Mix nets use chains of servers, known as mixes, and public-private key encryption. The first server receives encrypted messages from many users and decrypts a single layer of encryption from each message. Then, it shuffles the messages in random order and transmits them to the next server, which does the same thing, and so on down the chain. The last server decrypts the final encryption layer and sends the message to the target receiver.Servers only know the identities of the immediate source (the previous server) and immediate destination (the next server). Basically, the shuffling and limited identity information breaks the link between source and destination users, making it very difficult for eavesdroppers to get that information. As long as one server in the chain is “honest”— meaning it follows protocol — metadata is almost always safe.However, “active attacks” can occur, in which a malicious server in a mix net tampers with the messages to reveal user sources and destinations. In short, the malicious server can drop messages or modify sending times to create communications patterns that reveal direct links between users.Some methods add cryptographic proofs between servers to ensure there’s been no tampering. These rely on public key cryptography, which is secure, but it’s also slow and limits scaling. For XRD, the researchers invented a far more efficient version of the cryptographic proofs, called “aggregate hybrid shuffle,” that guarantees servers are receiving and shuffling message correctly, to detect any malicious server activity.Each server has a secret private key and two shared public keys. Each server must know all the keys to decrypt and shuffle messages. Users encrypt messages in layers, using each server’s secret private key in its respective layer. When a server receives messages, it decrypts and shuffles them using one of the public keys combined with its own private key. Then, it uses the second public key to generate a proof confirming that it had, indeed, shuffled every message without dropping or manipulating any. All other servers in the chain use their secret private keys and the other servers’ public keys in a way that verifies this proof. If, at any point in the chain, a server doesn’t produce the proof or provides an incorrect proof, it’s immediately identified as malicious.This relies on a clever combination of the popular public key scheme with one called “authenticated encryption,” which uses only private keys but is very quick at generating and verifying the proofs. In this way, XRD achieves tight security from public key encryption while running quickly and efficiently.   To further boost efficiency, they split the servers into multiple chains and divide their use among users. (This is another traditional technique they improved upon.) Using some statistical techniques, they estimate how many servers in each chain could be malicious, based on IP addresses and other information. From that, they calculate how many servers need to be in each chain to guarantee there’s at least one honest server.  Then, they divide the users into groups that send duplicate messages to multiple, random chains, which further protects their privacy while speeding things up.Getting to real-timeIn computer simulations of activity from 2 million users sending messages on a network of 100 servers, XRD was able to get everyone’s messages through in about four minutes. Traditional systems using the same server and user numbers, and providing the same cryptographic security, took one to two hours.“This seems slow in terms of absolute speed in today’s communication world,” Kwon says. “But it’s important to keep in mind that the fastest systems right now [for metadata protection] take hours, whereas ours takes minutes.”Next, the researchers hope to make the network more robust to few users and in instances where servers go offline in the midst of operations, and to speed things up. “Four minutes is acceptable for sensitive messages and emails where two parties’ lives are in danger, but it’s not as natural as today’s internet,” Kwon says. “We want to get to the point where we’re sending metadata-protected messages in near real-time.”


",Protecting sensitive metadata so it can’t be used for surveillance,2020-02-26,['Rob Matheson'],Research/Computer science and technology/Algorithms/Cyber security/Data/Technology and society/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/School of Engineering,"['cant', 'protecting', 'key', 'server', 'public', 'servers', 'users', 'messages', 'surveillance', 'sensitive', 'metadata', 'kwon', 'information', 'encryption', 'used']","“There is a huge lack in protection for metadata, which is sometimes very sensitive.
These rely on public key cryptography, which is secure, but it’s also slow and limits scaling.
Then, it uses the second public key to generate a proof confirming that it had, indeed, shuffled every message without dropping or manipulating any.
All other servers in the chain use their secret private keys and the other servers’ public keys in a way that verifies this proof.
In this way, XRD achieves tight security from public key encryption while running quickly and efficiently.",Mit
168,https://news.mit.edu/2020/bringing-deep-learning-to-life-0224,"


Gaby Ecanow loves listening to music, but never considered writing her own until taking 6.S191 (Introduction to Deep Learning). By her second class, the second-year MIT student had composed an original Irish folk song with the help of a recurrent neural network, and was considering how to adapt the model to create her own Louis the Child-inspired dance beats.
“It was cool,” she says. “It didn’t sound at all like a machine had made it.” 
This year, 6.S191 kicked off as usual, with students spilling into the aisles of Stata Center’s Kirsch Auditorium during Independent Activities Period (IAP). But the opening lecture featured a twist: a recorded welcome from former President Barack Obama. The video was quickly revealed to be an AI-generated fabrication, one of many twists that Alexander Amini ’17 and Ava Soleimany ’16 introduce throughout their for-credit course to make the equations and code come alive. 







Play video






As hundreds of their peers look on, Amini and Soleimany take turns at the podium. If they appear at ease, it’s because they know the material cold; they designed the curriculum themselves, and have taught it for the past three years. The course covers the technical foundations of deep learning and its societal implications through lectures and software labs focused on real-world applications. On the final day, students compete for prizes by pitching their own ideas for research projects. In the weeks leading up to class, Amini and Soleimany spend hours updating the labs, refreshing their lectures, and honing their presentations.
A branch of machine learning, deep learning harnesses massive data and algorithms modeled loosely on how the brain processes information to make predictions. The class has been credited with helping to spread machine-learning tools into research labs across MIT. That’s by design, says Amini, a graduate student in MIT’s Department of Electrical Engineering and Computer Science (EECS), and Soleimany, a graduate student at MIT and Harvard University.
Both are using machine learning in their own research — Amini in engineering robots, and Soleimany in developing diagnostic tools for cancer — and they wanted to make sure the curriculum would prepare students to do the same. In addition to the lab on developing a music-generating AI, they offer labs on building a face-recognition model with convolutional neural networks and a bot that uses reinforcement learning to play the vintage Atari video game, Pong. After students master the basics, those taking the class for credit go on to create applications of their own. 
This year, 23 teams presented projects. Among the prize winners was Carmen Martin, a graduate student in the Harvard-MIT Program in Health Sciences and Technology (HST), who proposed using a type of neural net called a graph convolutional network to predict the spread of coronavirus. She combined several data streams: airline ticketing data to measure population fluxes, real-time confirmation of new infections, and a ranking of how well countries are equipped to prevent and respond to a pandemic. 
“The goal is to train the model to predict cases to guide national governments and the World Health Organization in their recommendations to limit new cases and save lives,” she says.
A second prize winner, EECS graduate student Samuel Sledzieski, proposed building a model to predict protein interactions using only their amino acid sequences. Predicting protein behavior is key to designing drug targets, among other clinical applications, and Sledzieski wondered if deep learning could speed up the search for viable protein pairs. 
“There’s still work to be done, but I’m excited by how far I was able to get in three days,” he says. “Having easy-to-follow examples in TensorFlow and Keras helped me understand how to actually build and train these models myself.” He plans to continue the work in his current lab rotation with Bonnie Berger, the Simons Professor of Mathematics in EECS and the Computer Science and Artificial Intelligence Laboratory (CSAIL). 
Each year, students also hear about emerging deep-learning applications from companies sponsoring the course. David Cox, co-director of the MIT-IBM Watson AI Lab, covered neuro-symbolic AI, a hybrid approach that combines symbolic programs with deep learning’s expert pattern-matching ability. Alex Wiltschko, a senior researcher at Google Brain, spoke about using a network analysis tool to predict the scent of small molecules. Chuan Li, chief scientific officer at Lambda Labs, discussed neural rendering, a tool for reconstructing and generating graphics scenes. Animesh Garg, a senior researcher at NVIDIA, covered strategies for developing robots that perceive and act more human-like.
With 350 students taking the live course each year, and more than a million people who have watched the lectures online, Amini and Soleimany have become prominent ambassadors for deep learning. Yet, it was tennis that first brought them together. 
Amini competed nationally as a high school student in Ireland and built an award-winning AI model to help amateur and pro tennis players improve their strokes; Soleimany was a two-time captain of the MIT women’s tennis team. They met on the court as undergraduates and discovered they shared a passion for machine learning. 
After finishing their undergraduate degrees, they decided to challenge themselves and fill what they saw as an increasing need at MIT for a foundational course in deep learning. 6.S191 was launched in 2017 by two grad students, Nick Locascio and Harini Suresh, and Amini and Soleimany had a vision for transforming the course into something more. They created a series of software labs, introduced new cutting-edge topics like robust and ethical AI, and added content to appeal to a broad range of students, from computer scientists to aerospace engineers and MBAs.
“Alexander and I are constantly brainstorming, and those discussions are key to how 6.S191 and some of our own collaborative research projects have developed,” says Soleimany. 
They cover one of those research collaborations in class. During the computer vision lab, students learn about algorithmic bias and how to test for and address racial and gender bias in face-recognition tools. The lab is based on an algorithm that Amini and Soleimany developed with their respective advisors, Daniela Rus, director of CSAIL, and Sangeeta Bhatia, the John J. and Dorothy Wilson Professor of HST and EECS. This year they also covered hot topics in robotics, including recent work of Amini’s on driverless cars. 
But they don’t plan to stop there. “We’re committed to making 6.S191 the best that it can be, each year we teach it,” says Amini “and that means moving the course forward as deep learning continues to evolve.” 



",Bringing deep learning to life,2020-02-24,['Kim Martineau'],Quest for Intelligence/Independent Activities Period/Electrical engineering and computer science (EECS)/Harvard-MIT Health Sciences and Technology/Computer Science and Artificial Intelligence Laboratory (CSAIL)/MIT-IBM Watson AI Lab/School of Engineering/Artificial intelligence/Algorithms/Computer science and technology/Machine learning/Classes and programs/Faculty,"['bringing', 'research', 'amini', 'student', 'model', 'life', 'learning', 'course', 'labs', 'deep', 'soleimany', 'students']","Gaby Ecanow loves listening to music, but never considered writing her own until taking 6.S191 (Introduction to Deep Learning).
The course covers the technical foundations of deep learning and its societal implications through lectures and software labs focused on real-world applications.
A branch of machine learning, deep learning harnesses massive data and algorithms modeled loosely on how the brain processes information to make predictions.
Predicting protein behavior is key to designing drug targets, among other clinical applications, and Sledzieski wondered if deep learning could speed up the search for viable protein pairs.
After finishing their undergraduate degrees, they decided to challenge themselves and fill what they saw as an increasing need at MIT for a foundational course in deep learning.",Mit
169,https://news.mit.edu/2020/patternex-machine-learning-cybersecurity-0221,"


Being a cybersecurity analyst at a large company today is a bit like looking for a needle in a haystack — if that haystack were hurtling toward you at fiber optic speed.Every day, employees and customers generate loads of data that establish a normal set of behaviors. An attacker will also generate data while using any number of techniques to infiltrate the system; the goal is to find that “needle” and stop it before it does any damage.The data-heavy nature of that task lends itself well to the number-crunching prowess of machine learning, and an influx of AI-powered systems have indeed flooded the cybersecurity market over the years. But such systems can come with their own problems, namely a never-ending stream of false positives that can make them more of a time suck than a time saver for security analysts.MIT startup PatternEx starts with the assumption that algorithms can’t protect a system on their own. The company has developed a closed loop approach whereby machine-learning models flag possible attacks and human experts provide feedback. The feedback is then incorporated into the models, improving their ability to flag only the activity analysts care about in the future.“Most machine learning systems in cybersecurity have been doing anomaly detection,” says Kalyan Veeramachaneni, a co-founder of PatternEx and a principal research scientist at MIT. “The problem with that, first, is you need a baseline [of normal activity]. Also, the model is usually unsupervised, so it ends up showing a lot of alerts, and people end up shutting it down. The big difference is that PatternEx allows the analyst to inform the system and then it uses that feedback to filter out false positives.”The result is an increase in analyst productivity. When compared to a generic anomaly detection software program, PatternEx’s Virtual Analyst Platform successfully identified 10 times more threats through the same number of daily alerts, and its advantage persisted even when the generic system gave analysts five times more alerts per day.First deployed in 2016, today the company’s system is being used by security analysts at large companies in a variety of industries along with firms that offer cybersecurity as a service.Merging human and machine approaches to cybersecurityVeeramachaneni came to MIT in 2009 as a postdoc and now directs a research group in the Laboratory for Information and Decision Systems. His work at MIT primarily deals with big data science and machine learning, but he didn’t think deeply about applying those tools to cybersecurity until a brainstorming session with PatternEx co-founders Costas Bassias, Uday Veeramachaneni, and Vamsi Korrapati in 2013.Ignacio Arnaldo, who worked with Veeramachaneni as a postdoc at MIT between 2013 and 2015, joined the company shortly after. Veeramachaneni and Arnaldo knew from their time building tools for machine-learning researchers at MIT that a successful solution would need to seamlessly integrate machine learning with human expertise.“A lot of the problems people have with machine learning arise because the machine has to work side by side with the analyst,” Veeramachaneni says, noting that detected attacks still must be presented to humans in an understandable way for further investigation. “It can’t do everything by itself. Most systems, even for something as simple as giving out a loan, is augmentation, not machine learning just taking decisions away from humans.”The company’s first partnership was with a large online retailer, which allowed the founders to train their models to identify potentially malicious behavior using real-world data. One by one, they trained their algorithms to flag different types of attacks using sources like Wi-Fi access logs, authentication logs, and other user behavior in the network.The early models worked best in retail, but Veeramachaneni knew how much businesses in other industries were struggling to apply machine learning in their operations from his many conversations with company executives at MIT (a subject PatternEx recently published a paper on).“MIT has done an incredible job since I got here 10 years ago bringing industry through the doors,” Veeramachaneni says. He estimates that in the past six years as a member of MIT’s Industrial Liaison Program he’s had 200 meetings with members of the private sector to talk about the problems they’re facing. He has also used those conversations to make sure his lab’s research is addressing relevant problems.In addition to enterprise customers, the company began offering its platform to security service providers and teams that specialize in hunting for undetected cyberattacks in networks.Today analysts can build machine learning models through PatternEx’s platform without writing a line of code, lowering the bar for people to use machine learning as part of a larger trend in the industry toward what Veeramachaneni calls the democratization of AI.“There’s not enough time in cybersecurity; it can’t take hours or even days to understand why an attack is happening,” Veeramachaneni says. “That’s why getting the analyst the ability to build and tweak machine learning models  is the most critical aspect of our system.”Giving security analysts an armyPatternEx’s Virtual Analyst Platform is designed to make security analysts feel like they have an army of assistants combing through data logs and presenting them with the most suspicious behavior on their network.The platform uses machine learning models to go through more than 50 streams of data and identify suspicious behavior. It then presents that information to the analyst for feedback, along with charts and other data visualizations that help the analyst decide how to proceed. After the analyst determines whether or not the behavior is an attack, that feedback is incorporated back into the models, which are updated across PatternEx’s entire customer base.“Before machine learning, someone would catch an attack, probably a little late, they might name it, and then they’ll announce it, and all the other companies will call and find out about it and go in and check their data,” Veeramachaneni says. “For us, if there’s an attack, we take that data, and because we have multiple customers, we have to transfer that in real time to other customer’s data to see if it’s happening with them too. We do that very efficiently on a daily basis.”The moment the system is up and running with new customers, it is able to identify 40 different types of cyberattacks using 170 different prepackaged machine learning models. Arnaldo notes that as the company works to grow those figures, customers are also adding to PatternEx’s model base by building solutions on the platform that address specific threats they’re facing.Even if customers aren’t building their own models on the platform, they can deploy PatternEx’s system out of the box, without any machine learning expertise, and watch it get smarter automatically.By providing that flexibility, PatternEx is bringing the latest tools in artificial intelligence to the people who understand their industries most intimately. It all goes back to the company’s founding principle of empowering humans with artificial intelligence instead of replacing them.“The target users of the system are not skilled data scientists or machine learning experts — profiles that are hard for cybersecurity teams to hire — but rather domain experts already on their payroll that have the deepest understanding of their data and uses cases,” Arnaldo says.


",A human-machine collaboration to defend against cyberattacks,2020-02-21,['Zach Winn'],Innovation and Entrepreneurship (I&E)/Startups/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Machine learning/Artificial intelligence/Computer science and technology/Data/Cyber security/MIT Schwarzman College of Computing/Laboratory for Information and Decision Systems (LIDS),"['collaboration', 'data', 'veeramachaneni', 'cybersecurity', 'customers', 'learning', 'models', 'defend', 'cyberattacks', 'system', 'platform', 'machine', 'humanmachine', 'analyst']","Being a cybersecurity analyst at a large company today is a bit like looking for a needle in a haystack — if that haystack were hurtling toward you at fiber optic speed.
“Most machine learning systems in cybersecurity have been doing anomaly detection,” says Kalyan Veeramachaneni, a co-founder of PatternEx and a principal research scientist at MIT.
Veeramachaneni and Arnaldo knew from their time building tools for machine-learning researchers at MIT that a successful solution would need to seamlessly integrate machine learning with human expertise.
The platform uses machine learning models to go through more than 50 streams of data and identify suspicious behavior.
We do that very efficiently on a daily basis.”The moment the system is up and running with new customers, it is able to identify 40 different types of cyberattacks using 170 different prepackaged machine learning models.",Mit
170,https://news.mit.edu/2020/starr-videgaray-artificial-intelligence-policy-0220,"


The rapid development of artificial intelligence technologies around the globe has led to increasing calls for robust AI policy: laws that let innovation flourish while protecting people from privacy violations, exploitive surveillance, biased algorithms, and more.But the drafting and passing of such laws has been anything but easy.“This is a very complex problem,” Luis Videgaray PhD ’98, director of MIT’s AI Policy for the World Project, said in a lecture on Wednesday afternoon. “This is not something that will be solved in a single report. This has got to be a collective conversation, and it will take a while. It will be years in the making.”Throughout his talk, Videgaray outlined an ambitious vision of AI policy around the globe, one that is sensitive to economic and political dynamics, and grounded in material fairness and democratic deliberation.   “Trust is probably the most important problem we have,” Videgaray said.Videgaray’s talk, “From Principles to Implementation: The Challenge of AI Policy Around the World,” was part of the Starr Forum series of public discussions about topics of global concern. The Starr Forum is hosted by MIT’s Center for International Studies. Videgaray gave his remarks to a standing-room crowd of over 150 in MIT’s Building E25.Videgaray, who is also a senior lecturer at the MIT Sloan School of Management, previously served as the finance minister of Mexico from 2012 to 2016, and foreign minister of Mexico from 2017 to 2018. Videgaray has also worked extensively in investment banking.Information lag and media hypeIn his talk, Videgaray began by outlining several “themes” related to AI that he thinks policymakers should keep in mind. These include government uses of AI; the effects of AI on the economy, including the possibility it could help giant tech firms consolidate market power; social responsibility issues, such as privacy, fairness, and bias; and the implications of AI for democracy, at a time when bots can influence political discussion. Videgaray also noted a “geopolitics” of AI regulation — from China’s comprehensive efforts to control technology to the looser methods used in the U.S.Videgaray observed that it is difficult for AI regulators to stay current with technology.“There’s an information lag,” Videgaray said. “Things that concern computer scientists today might become the concerns of policymakers a few years in the future.”Moreover, he noted, media hype can distort perceptions of AI and its applications. Here Videgaray contrasted the recent report of MIT’s Task Force on the Future of Work, which finds uncertainty about how many jobs will be replaced with technology, with a recent television documentary presenting a picture of automated vehicles replacing all truck drivers.“Clearly the evidence is nowhere near [indicating] that all jobs in truck driving, in long-distance driving, are going to be lost,” he said. “That is not the case.”With these general issues in mind, what should policymakers do about AI now? Videgaray offered several concrete suggestions. For starters: Policymakers should no longer just outline general philosophical principles, something that has been done many times, with a general convergence of ideas occurring.“Working on principles has very, very small marginal returns,” Videgaray said. “We can go to the next phase … principles are a necessary but not sufficient condition for AI policy. Because policy is about making hard choices in uncertain conditions.”Indeed, he emphasized, more progress can be made by having many AI policy decisions be particular to specific industries. When it comes to, say, medical diagnostics, policymakers want technology “to be very accurate, but you also want it to be explainable, you want it to be fair, without bias, you want the information to be secure … there are many objectives that can conflict with each other. So, this is all about the tradeoffs.” In many cases, he said, algorithm-based AI tools could go through a rigorous testing process, as required in some other industries: “Pre-market testing makes sense,” Videgaray said. “We do that for drugs, clinical trials, we do that for cars, why shouldn’t we do pre-market testing for algorithms?”But while Videgaray sees value in industry-specific regulations, he is not as keen on having a patchwork of varying state-level AI laws being used to regulate technology in the U.S.“Is this a problem for Facebook, for Google? I don’t think so,” Videgaray said. “They have enough resources to navigate through this complexity. But what about startups? What about students from MIT or Cornell or Stanford that are trying to start something, and would have to go through, at the extreme, 55 [pieces of] legislation?”A collaborative conversationAt the event, Videgaray was introduced by Kenneth Oye, a professor of political science at MIT who studies technological regulation, and who asked Videgaray questions after the lecture. Among other things, Oye suggested U.S. states could serve as a useful laboratory for regulatory innovation.“In an area characterized by significant uncertainty, complexity, and controversy, there can be benefits to experimentation, having different models being pursued in different areas to see which works best or worse,” Oye suggested.Videgaray did not necessarily disagree, but emphasized the value of an eventual convergence in regulation. The U.S. banking industry, he noted, also followed this trajectory, until “eventually the regulation we have for finance [became] federal,” rather than determined by states.Prior to his remarks, Videgaray acknowledged some audience members, including his PhD thesis adviser at MIT, James Poterba, the Mitsui Professor of Economics, whom Videgaray called “one of the best teachers, not only in economics but about a lot of things in life.” Mexico’s Consul General in Boston, Alberto Fierro, also attended the event.Ultimately, Videgaray emphasized to the audience, the future of AI policy will be collaborative.“You cannot just go to a computer lab and say, ‘Okay, get me some AI policy,’” he stressed. “This has got to be a collective conversation.”


",A road map for artificial intelligence policy,2020-02-20,['Peter Dizikes'],Artificial intelligence/Law/Ethics/Computer science and technology/Political science/Economics/Special events and guest speakers/Global/Center for International Studies/School of Humanities/Arts/Sloan School of Management,"['mits', 'technology', 'principles', 'videgaray', 'general', 'regulation', 'road', 'policy', 'ai', 'policymakers', 'artificial', 'map', 'intelligence', 'mit']","The rapid development of artificial intelligence technologies around the globe has led to increasing calls for robust AI policy: laws that let innovation flourish while protecting people from privacy violations, exploitive surveillance, biased algorithms, and more.
“This is a very complex problem,” Luis Videgaray PhD ’98, director of MIT’s AI Policy for the World Project, said in a lecture on Wednesday afternoon.
“We can go to the next phase … principles are a necessary but not sufficient condition for AI policy.
Because policy is about making hard choices in uncertain conditions.”Indeed, he emphasized, more progress can be made by having many AI policy decisions be particular to specific industries.
“You cannot just go to a computer lab and say, ‘Okay, get me some AI policy,’” he stressed.",Mit
171,https://news.mit.edu/2020/artificial-intelligence-identifies-new-antibiotic-0220,"


Using a machine-learning algorithm, MIT researchers have identified a powerful new antibiotic compound. In laboratory tests, the drug killed many of the world’s most problematic disease-causing bacteria, including some strains that are resistant to all known antibiotics. It also cleared infections in two different mouse models.The computer model, which can screen more than a hundred million chemical compounds in a matter of days, is designed to pick out potential antibiotics that kill bacteria using different mechanisms than those of existing drugs.“We wanted to develop a platform that would allow us to harness the power of artificial intelligence to usher in a new age of antibiotic drug discovery,” says James Collins, the Termeer Professor of Medical Engineering and Science in MIT’s Institute for Medical Engineering and Science (IMES) and Department of Biological Engineering. “Our approach revealed this amazing molecule which is arguably one of the more powerful antibiotics that has been discovered.”In their new study, the researchers also identified several other promising antibiotic candidates, which they plan to test further. They believe the model could also be used to design new drugs, based on what it has learned about chemical structures that enable drugs to kill bacteria.“The machine learning model can explore, in silico, large chemical spaces that can be prohibitively expensive for traditional experimental approaches,” says Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science in MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL).Barzilay and Collins, who are faculty co-leads for MIT’s Abdul Latif Jameel Clinic for Machine Learning in Health (J-Clinic), are the senior authors of the study, which appears today in Cell. The first author of the paper is Jonathan Stokes, a postdoc at MIT and the Broad Institute of MIT and Harvard.A new pipelineOver the past few decades, very few new antibiotics have been developed, and most of those newly approved antibiotics are slightly different variants of existing drugs. Current methods for screening new antibiotics are often prohibitively costly, require a significant time investment, and are usually limited to a narrow spectrum of chemical diversity.“We’re facing a growing crisis around antibiotic resistance, and this situation is being generated by both an increasing number of pathogens becoming resistant to existing antibiotics, and an anemic pipeline in the biotech and pharmaceutical industries for new antibiotics,” Collins says.To try to find completely novel compounds, he teamed up with Barzilay, Professor Tommi Jaakkola, and their students Kevin Yang, Kyle Swanson, and Wengong Jin, who have previously developed machine-learning computer models that can be trained to analyze the molecular structures of compounds and correlate them with particular traits, such as the ability to kill bacteria.
The idea of using predictive computer models for “in silico” screening is not new, but until now, these models were not sufficiently accurate to transform drug discovery. Previously, molecules were represented as vectors reflecting the presence or absence of certain chemical groups. However, the new neural networks can learn these representations automatically, mapping molecules into continuous vectors which are subsequently used to predict their properties.
In this case, the researchers designed their model to look for chemical features that make molecules effective at killing E. coli. To do so, they trained the model on about 2,500 molecules, including about 1,700 FDA-approved drugs and a set of 800 natural products with diverse structures and a wide range of bioactivities.Once the model was trained, the researchers tested it on the Broad Institute’s Drug Repurposing Hub, a library of about 6,000 compounds. The model picked out one molecule that was predicted to have strong antibacterial activity and had a chemical structure different from any existing antibiotics. Using a different machine-learning model, the researchers also showed that this molecule would likely have low toxicity to human cells.This molecule, which the researchers decided to call halicin, after the fictional artificial intelligence system from “2001: A Space Odyssey,” has been previously investigated as possible diabetes drug. The researchers tested it against dozens of bacterial strains isolated from patients and grown in lab dishes, and found that it was able to kill many that are resistant to treatment, including Clostridium difficile, Acinetobacter baumannii, and Mycobacterium tuberculosis. The drug worked against every species that they tested, with the exception of Pseudomonas aeruginosa, a difficult-to-treat lung pathogen.To test halicin’s effectiveness in living animals, the researchers used it to treat mice infected with A. baumannii, a bacterium that has infected many U.S. soldiers stationed in Iraq and Afghanistan. The strain of A. baumannii that they used is resistant to all known antibiotics, but application of a halicin-containing ointment completely cleared the infections within 24 hours.Preliminary studies suggest that halicin kills bacteria by disrupting their ability to maintain an electrochemical gradient across their cell membranes. This gradient is necessary, among other functions, to produce ATP (molecules that cells use to store energy), so if the gradient breaks down, the cells die. This type of killing mechanism could be difficult for bacteria to develop resistance to, the researchers say.“When you’re dealing with a molecule that likely associates with membrane components, a cell can’t necessarily acquire a single mutation or a couple of mutations to change the chemistry of the outer membrane. Mutations like that tend to be far more complex to acquire evolutionarily,” Stokes says.In this study, the researchers found that E. coli did not develop any resistance to halicin during a 30-day treatment period. In contrast, the bacteria started to develop resistance to the antibiotic ciprofloxacin within one to three days, and after 30 days, the bacteria were about 200 times more resistant to ciprofloxacin than they were at the beginning of the experiment.The researchers plan to pursue further studies of halicin, working with a pharmaceutical company or nonprofit organization, in hopes of developing it for use in humans.Optimized moleculesAfter identifying halicin, the researchers also used their model to screen more than 100 million molecules selected from the ZINC15 database, an online collection of about 1.5 billion chemical compounds. This screen, which took only three days, identified 23 candidates that were structurally dissimilar from existing antibiotics and predicted to be nontoxic to human cells.In laboratory tests against five species of bacteria, the researchers found that eight of the molecules showed antibacterial activity, and two were particularly powerful. The researchers now plan to test these molecules further, and also to screen more of the ZINC15 database.The researchers also plan to use their model to design new antibiotics and to optimize existing molecules. For example, they could train the model to add features that would make a particular antibiotic target only certain bacteria, preventing it from killing beneficial bacteria in a patient’s digestive tract.“This groundbreaking work signifies a paradigm shift in antibiotic discovery and indeed in drug discovery more generally,” says Roy Kishony, a professor of biology and computer science at Technion (the Israel Institute of Technology), who was not involved in the study. “Beyond in silica screens, this approach will allow using deep learning at all stages of antibiotic development, from discovery to improved efficacy and toxicity through drug modifications and medicinal chemistry.”The research was funded by the Abdul Latif Jameel Clinic for Machine Learning in Health, the Defense Threat Reduction Agency, the Broad Institute, the DARPA Make-It Program, the Canadian Institutes of Health Research, the Canadian Foundation for Innovation, the Canada Research Chairs Program, the Banting Fellowships Program, the Human Frontier Science Program, the Pershing Square Foundation, the Swiss National Science Foundation, a National Institutes of Health Early Investigator Award, the National Science Foundation Graduate Research Fellowship Program, and a gift from Anita and Josh Bekenstein.


",Artificial intelligence yields new antibiotic,2020-02-20,['Anne Trafton'],Research/Biological engineering/Electrical Engineering & Computer Science (eecs)/Institute for Medical Engineering and Science (IMES)/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Broad Institute/School of Engineering/Bacteria/Microbes/Medicine/Health/Machine learning/Artificial intelligence/Algorithms/J-Clinic/National Institutes of Health (NIH)/National Science Foundation (NSF),"['drug', 'antibiotics', 'existing', 'yields', 'model', 'researchers', 'chemical', 'artificial', 'intelligence', 'antibiotic', 'bacteria', 'molecules', 'science']","Using a machine-learning algorithm, MIT researchers have identified a powerful new antibiotic compound.
The model picked out one molecule that was predicted to have strong antibacterial activity and had a chemical structure different from any existing antibiotics.
This molecule, which the researchers decided to call halicin, after the fictional artificial intelligence system from “2001: A Space Odyssey,” has been previously investigated as possible diabetes drug.
This screen, which took only three days, identified 23 candidates that were structurally dissimilar from existing antibiotics and predicted to be nontoxic to human cells.
The researchers also plan to use their model to design new antibiotics and to optimize existing molecules.",Mit
172,https://news.mit.edu/2020/benjamin-chang-might-technology-tip-scales-0219,"


The United States and China seem locked in an ever-tightening embrace, superpowers entangled in a web of economic and military concerns. ""Every issue critical to world order — whether climate change, terrorism, or trade — is clearly and closely intertwined with U.S.-China relations,"" says Benjamin Chang, a fourth-year PhD candidate in political science concentrating in international relations and security studies. ""Competition between these nations will shape all outcomes anyone cares about in the next 50 years or more.""
Little surprise, then, that Chang is homing in on this relationship for his thesis, which broadly examines the impact of artificial intelligence on military power. As China and the United States circle each other as rivals and uneasy partners on the global stage, Chang hopes to learn what the integration of artificial intelligence in different domains might mean for the balance of power.
""There is a set of questions related to how technology will be used in the world in general, where the U.S. and China are the two actors with the most influence,"" says Chang. ""I want to know, for instance, how AI will affect strategic stability between them.""
The nuclear balance
In the domain of military power, one question Chang has been pursuing is whether the use of AI in nuclear strategy offers a battlefield advantage. ""For the U.S., the main issue involves locating China's elusive mobile missile launchers,"" Chang says. ""The U.S. has satellite and other remote sensors that provide too much intelligence for human analysts, but AI, with its image classifiers based on deep learning, could sort through all this data to locate Chinese assets in a timely fashion.""
While Chang's data draws on publicly available information about each side's military capabilities, these sources can't provide specific numbers for China's nuclear arsenal. ""We don't know if China has 250 or 300 nukes, so I design programs to run combat simulations with high and low numbers of weapons to try and isolate the effects of AI on combat outcomes."" Chang credits J. Chappell Lawson, Vipin Narang, and Eric Heginbotham — his advisors in international relations and security studies — for helping shape his research methodology.
If the United States develops the capacity to locate these mobile nuclear assets quickly, ""that could change the battlefield outcome and hold China's arsenal at risk,"" says Chang. ""And if China feels it isn't able to protect its nuclear arsenal, it might have an incentive to use it or lose it.""
In subsequent research, Chang will examine the impacts of AI on cybersecurity and on autonomous weaponry such as drones.
A start in policy debate
Pondering international and security issues began early for Chang. ""I developed a big interest in these subjects through policy debate, which motivated me to read hundreds of pages and gave me a breadth and depth of knowledge on disparate topics,"" he says. ""Debate exposed me to the study of military affairs and got me interested in America's role in the world generally.""
Chang's engagement with policy deepened at Princeton University, where he earned his BA summa cum laude from the Woodrow Wilson School of Public and International Affairs. While he knew he wanted to focus on foreign policy of some kind, his special focus on China came fortuitously: He was assigned to a junior seminar where students developed a working paper on ""Building the Rule of Law in China."" He took a series of Mandarin language courses, and produced a thesis comparing 19th century American nationalist behavior with modern-day Chinese nationalism.
By graduation, Chang knew he wanted to aim for a career in national security and policy by way of a graduate school education. But he sought real-world seasoning first: a two-year stint as an analyst at Long Term Strategy Group, a Washington defense research firm. At LTSG, Chang facilitated wargames simulating Asia-Pacific conflicts, and wrote monographs on Chinese foreign policy, nuclear signaling, and island warfare doctrine.
Bridging a divide
Today, he is applying this expertise. ""I'm trying to use my computer science understanding to bridge the gap between people working at a highly technical level of AI, and folks in security studies who are perhaps less familiar with the technology,"" he says. Propelled by a National Science Foundation Graduate Research Fellowship and a research fellowship with the Center for Security and Emerging Technology, Chang continues with his simulations and is beginning to write up some of his analysis. He thinks some of his findings might prove surprising.
""There is an assumption — based on China's vast collection of personal data and surveillance of citizens — that AI is the means by which China will leapfrog the U.S. in military power,"" says Chang. ""But I think this is wrong."" In fact, the United States ""has much more military-relevant data than China does, because it collects on so many platforms — in the deep ocean, and from satellites — that are a holdover from fighting the Soviet Union.""
Among Chang's research challenges: the fact that AI is not a mature technology and hasn't been fully implemented in modern militaries. ""There's not yet much literature or data to draw on when assessing its impact,"" he notes. Also, he would like to nail down a good definition of AI for his field. ""With current definitions of AI, thinking about its influence is a bit like investigating the effect of explosives on international affairs: you could be talking about nuclear weapons or dynamite and gunpowder,"" he says. ""In my dissertation I'm attempting a scoping of AI so that it's more amenable to good political science analysis.""
Getting these ideas down on paper will be Chang's job for at least the next year. The writing occasionally feels like a struggle. ""Some days I'll sit there and it won't come out, and other days, after a long walk along the Charles, I can write all day, and it feels good.""


",Benjamin Chang: Might technology tip the global scales?,2020-02-19,['Leda Zimmerman'],"Political science/School of Humanities Arts and Social Sciences/Artificial intelligence/Security studies and military/China/Policy/Students/Graduate, postdoctoral/Computer science and technology","['technology', 'data', 'tip', 'security', 'global', 'research', 'military', 'chang', 'scales', 'china', 'ai', 'policy', 'benjamin', 'nuclear', 'international']","The nuclear balanceIn the domain of military power, one question Chang has been pursuing is whether the use of AI in nuclear strategy offers a battlefield advantage.
In subsequent research, Chang will examine the impacts of AI on cybersecurity and on autonomous weaponry such as drones.
A start in policy debatePondering international and security issues began early for Chang.
By graduation, Chang knew he wanted to aim for a career in national security and policy by way of a graduate school education.
At LTSG, Chang facilitated wargames simulating Asia-Pacific conflicts, and wrote monographs on Chinese foreign policy, nuclear signaling, and island warfare doctrine.",Mit
173,https://news.mit.edu/2020/bringing-artificial-intelligence-classroom-research-lab-and-beyond-0213,"


Artificial intelligence is reshaping how we live, learn, and work, and this past fall, MIT undergraduates got to explore and build on some of the tools coming out of research labs at MIT. Through the Undergraduate Research Opportunities Program (UROP), students worked with researchers at the MIT Quest for Intelligence and elsewhere on projects to improve AI literacy and K-12 education, understand face recognition and how the brain forms new memories, and speed up tedious tasks like cataloging new library material. Six projects are featured below.
Programming Jibo to forge an emotional bond with kids
Nicole Thumma met her first robot when she was 5, at a museum. “It was incredible that I could have a conversation, even a simple conversation, with this machine,” she says. “It made me think robots are the most complicated manmade thing, which made me want to learn more about them.”
Now a senior at MIT, Thumma spent last fall writing dialogue for the social robot Jibo, the brainchild of MIT Media Lab Associate Professor Cynthia Breazeal. In a UROP project co-advised by Breazeal and researcher Hae Won Park, Thumma scripted mood-appropriate dialogue to help Jibo bond with students while playing learning exercises together.
Because emotions are complicated, Thumma riffed on a set of basic feelings in her dialogue — happy/sad, energized/tired, curious/bored. If Jibo was feeling sad, but energetic and curious, she might program it to say, “I'm feeling blue today, but something that always cheers me up is talking with my friends, so I'm glad I'm playing with you.​” A tired, sad, and bored Jibo might say, with a tilt of its head, “I don't feel very good. It's like my wires are all mixed up today. I think this activity will help me feel better.” 
In these brief interactions, Jibo models its vulnerable side and teaches kids how to express their emotions. At the end of an interaction, kids can give Jibo a virtual token to pick up its mood or energy level. “They can see what impact they have on others,” says Thumma. In all, she wrote 80 lines of dialogue, an experience that led to her to stay on at MIT for an MEng in robotics. The Jibos she helped build are now in kindergarten classrooms in Georgia, offering emotional and intellectual support as they read stories and play word games with their human companions.
Understanding why familiar faces stand out
With a quick glance, the faces of friends and acquaintances jump out from those of strangers. How does the brain do it? Nancy Kanwisher’s lab in the Department of Brain and Cognitive Sciences (BCS) is building computational models to understand the face-recognition process. Two key findings: the brain starts to register the gender and age of a face before recognizing its identity, and that face perception is more robust for familiar faces.
This fall, second-year student Joanne Yuan worked with postdoc Katharina Dobs to understand why this is so. In earlier experiments, subjects were shown multiple photographs of familiar faces of American celebrities and unfamiliar faces of German celebrities while their brain activity was measured with magnetoencephalography. Dobs found that subjects processed age and gender before the celebrities’ identity regardless of whether the face was familiar. But they were much better at unpacking the gender and identity of faces they knew, like Scarlett Johansson, for example. Dobs suggests that the improved gender and identity recognition for familiar faces is due to a feed-forward mechanism rather than top-down retrieval of information from memory. 
Yuan has explored both hypotheses with a type of model, convolutional neural networks (CNNs), now widely used in face-recognition tools. She trained a CNN on the face images and studied its layers to understand its processing steps. She found that the model, like Dobs’ human subjects, appeared to process gender and age before identity, suggesting that both CNNs and the brain are primed for face recognition in similar ways. In another experiment, Yuan trained two CNNs on familiar and unfamiliar faces and found that the CNNs, again like humans, were better at identifying the familiar faces.
Yuan says she enjoyed exploring two fields — machine learning and neuroscience — while gaining an appreciation for the simple act of recognizing faces. “It’s pretty complicated and there’s so much more to learn,” she says.
Exploring memory formation
Protruding from the branching dendrites of brain cells are microscopic nubs that grow and change shape as memories form. Improved imaging techniques have allowed researchers to move closer to these nubs, or spines, deep in the brain to learn more about their role in creating and consolidating memories.
Susumu Tonegawa, the Picower Professor of Biology and Neuroscience, has pioneered a technique for labeling clusters of brain cells, called “engram cells,” that are linked to specific memories in mice. Through conditioning, researchers train a mouse, for example, to recognize an environment. By tracking the evolution of dendritic spines in cells linked to a single memory trace, before and after the learning episode, researchers can estimate where memories may be physically stored. 
But it takes time. Hand-labeling spines in a stack of 100 images can take hours — more, if the researcher needs to consult images from previous days to verify that a spine-like nub really is one, says Timothy O’Connor, a software engineer in BCS helping with the project. With 400 images taken in a typical session, annotating the images can take longer than collecting them, he adds.
O’Connor contacted the Quest Bridge to see if the process could be automated. Last fall, undergraduates Julian Viera and Peter Hart began work with Bridge AI engineer Katherine Gallagher to train a neural network to automatically pick out the spines. Because spines vary widely in shape and size, teaching the computer what to look for is one big challenge facing the team as the work continues. If successful, the tool could be useful to a hundred other labs across the country.
“It’s exciting to work on a project that could have a huge amount of impact,” says Viera. “It’s also cool to be learning something new in computer science and neuroscience.”
Speeding up the archival process
Each year, Distinctive Collections at the MIT Libraries receives a large volume of personal letters, lecture notes, and other materials from donors inside and outside of MIT that tell MIT’s story and document the history of science and technology. Each of these unique items must be organized and described, with a typical box of material taking up to 20 hours to process and make available to users.
To make the work go faster, Andrei Dumitrescu and Efua Akonor, undergraduates at MIT and Wellesley College respectively, are working with Quest Bridge’s Katherine Gallagher to develop an automated system for processing archival material donated to MIT. Their goal: to develop a machine-learning pipeline that can categorize and extract information from scanned images of the records. To accomplish this task, they turned to the U.S. Library of Congress (LOC), which has digitized much of its extensive holdings. 
This past fall, the students pulled images of about 70,000 documents, including correspondence, speeches, lecture notes, photographs, and books housed at the LOC, and trained a classifier to distinguish a letter from, say, a speech. They are now using optical character recognition and a text-analysis tool to extract key details like the date, author, and recipient of a letter, or the date and topic of a lecture. They will soon incorporate object recognition to describe the content of a photograph, and are looking forward to testing their system on the MIT Libraries’ own digitized data.
One highlight of the project was learning to use Google Cloud. “This is the real world, where there are no directions,” says Dumitrescu. “It was fun to figure things out for ourselves.” 
Inspiring the next generation of robot engineers
From smartphones to smart speakers, a growing number of devices live in the background of our daily lives, hoovering up data. What we lose in privacy we gain in time-saving personalized recommendations and services. It’s one of AI’s defining tradeoffs that kids should understand, says third-year student Pablo Alejo-Aguirre. “AI brings us beautiful and elegant solutions, but it also has its limitations and biases,” he says.
Last year, Alejo-Aguirre worked on an AI literacy project co-advised by Cynthia Breazeal and graduate student Randi Williams. In collaboration with the nonprofit i2 Learning, Breazeal’s lab has developed an AI curriculum around a robot named Gizmo that teaches kids how to train their own robot with an Arduino micro-controller and a user interface based on Scratch-X, a drag-and-drop programming language for children. 
To make Gizmo accessible for third-graders, Alejo-Aguirre developed specialized programming blocks that give the robot simple commands like, “turn left for one second,” or “move forward for one second.” He added Bluetooth to control Gizmo remotely and simplified its assembly, replacing screws with acrylic plates that slide and click into place. He also gave kids the choice of rabbit and frog-themed Gizmo faces. “The new design is a lot sleeker and cleaner, and the edges are more kid-friendly,” he says. 
After building and testing several prototypes, Alejo-Aguirre and Williams demoed their creation last summer at a robotics camp. This past fall, Alejo-Aguirre manufactured 100 robots that are now in two schools in Boston and a third in western Massachusetts. “I’m proud of the technical breakthroughs I made through designing, programming, and building the robot, but I’m equally proud of the knowledge that will be shared through this curriculum,” he says.
Predicting stock prices with machine learning
In search of a practical machine-learning application to learn more about the field, sophomores Dolapo Adedokun and Daniel Adebi hit on stock picking. “We all know buy, sell, or hold,” says Adedokun. “We wanted to find an easy challenge that anyone could relate to, and develop a guide for how to use machine learning in that context.”
The two friends approached the Quest Bridge with their own idea for a UROP project after they were turned away by several labs because of their limited programming experience, says Adedokun. Bridge engineer Katherine Gallagher, however, was willing to take on novices. “We’re building machine-learning tools for non-AI specialists,” she says. “I was curious to see how Daniel and Dolapo would approach the problem and reason through the questions they encountered.”
Adebi wanted to learn more about reinforcement learning, the trial-and-error AI technique that has allowed computers to surpass humans at chess, Go, and a growing list of video games. So, he and Adedokun worked with Gallagher to structure an experiment to see how reinforcement learning would fare against another AI technique, supervised learning, in predicting stock prices.
In reinforcement learning, an agent is turned loose in an unstructured environment with one objective: to maximize a specific outcome (in this case, profits) without being told explicitly how to do so. Supervised learning, by contrast, uses labeled data to accomplish a goal, much like a problem set with the correct answers included.
Adedokun and Adebi trained both models on seven years of stock-price data, from 2010-17, for Amazon, Microsoft, and Google. They then compared profits generated by the reinforcement learning model and a trading algorithm based on the supervised model’s price predictions for the following 18 months; they found that their reinforcement learning model produced higher returns.
They developed a Jupyter notebook to share what they learned and explain how they built and tested their models. “It was a valuable exercise for all of us,” says Gallagher. “Daniel and Dolapo got hands-on experience with machine-learning fundamentals, and I got insight into the types of obstacles users with their background might face when trying to use the tools we’re building at the Bridge.”


","Bringing artificial intelligence into the classroom, research lab, and beyond",2020-02-13,['Kim Martineau'],Quest for Intelligence/Brain and cognitive sciences/Media Lab/Libraries/School of Engineering/School of Science/Algorithms/Computer science and technology/Machine learning/Undergraduate Research Opportunities Program (UROP)/Students/Undergraduate/Electrical engineering and computer science (EECS),"['face', 'bringing', 'research', 'faces', 'learning', 'classroom', 'familiar', 'jibo', 'images', 'robot', 'ai', 'lab', 'brain', 'artificial', 'intelligence', 'mit']","Understanding why familiar faces stand outWith a quick glance, the faces of friends and acquaintances jump out from those of strangers.
In earlier experiments, subjects were shown multiple photographs of familiar faces of American celebrities and unfamiliar faces of German celebrities while their brain activity was measured with magnetoencephalography.
Dobs suggests that the improved gender and identity recognition for familiar faces is due to a feed-forward mechanism rather than top-down retrieval of information from memory.
She trained a CNN on the face images and studied its layers to understand its processing steps.
In another experiment, Yuan trained two CNNs on familiar and unfamiliar faces and found that the CNNs, again like humans, were better at identifying the familiar faces.",Mit
174,https://news.mit.edu/2020/voting-voatz-app-hack-issues-0213,"


In recent years, there has been a growing interest in using internet and mobile technology to increase access to the voting process. At the same time, computer security experts caution that paper ballots are the only secure means of voting.
Now, MIT researchers are raising another concern: They say they have uncovered security vulnerabilities in a mobile voting application that was used during the 2018 midterm elections in West Virginia. Their security analysis of the application, called Voatz, pinpoints a number of weaknesses, including the opportunity for hackers to alter, stop, or expose how an individual user has voted. Additionally, the researchers found that Voatz’s use of a third-party vendor for voter identification and verification poses potential privacy issues for users.
The findings are described in a new technical paper by Michael Specter, a graduate student in MIT’s Department of Electrical Engineering and Computer Science (EECS) and a member of MIT’s Internet Policy Research Initiative, and James Koppel, also a graduate student in EECS. The research was conducted under the guidance of Daniel Weitzner, a principal research scientist at MIT’s Computer Science and Artificial Intelligence Lab (CSAIL) and founding director of the Internet Policy Research Initiative.
After uncovering these security vulnerabilities, the researchers disclosed their findings to the Department of Homeland Security’s Cybersecurity and Infrastructure Agency (CISA). The researchers, along with the Boston University/MIT Technology Law Clinic, worked in close coordination with election security officials within CISA to ensure that impacted elections officials and the vendor were aware of the findings before the research was made public. This included preparing written summaries of the findings with proof-of-concept code, and direct discussions with affected elections officials on calls arranged by CISA.
In addition to its use in the 2018 West Virginia elections, the app was deployed in elections in Denver, Oregon, and Utah, as well as at the 2016 Massachusetts Democratic Convention and the 2016 Utah Republican Convention. Voatz was not used during the 2020 Iowa caucuses.
The findings underscore the need for transparency in the design of voting systems, according to the researchers.
“We all have an interest in increasing access to the ballot, but in order to maintain trust in our elections system, we must assure that voting systems meet the high technical and operation security standards before they are put in the field,” says Weitzner. “We cannot experiment on our democracy.”     
“The consensus of security experts is that running a secure election over the internet is not possible today,” adds Koppel. “The reasoning is that weaknesses anywhere in a large chain can give an adversary undue influence over an election, and today’s software is shaky enough that the existence of unknown exploitable flaws is too great a risk to take.”
Breaking down the results
The researchers were initially inspired to perform a security analysis of Voatz based on Specter’s research with Ronald Rivest, Institute Professor at MIT; Neha Narula, director of the MIT Digital Currency Initiative; and Sunoo Park SM ’15, PhD ’18 , exploring the feasibility of using blockchain systems in elections. According to the researchers, Voatz claims to use a permissioned blockchain to ensure security, but has not released any source code or public documentation for how their system operates.
Specter, who co-teaches an MIT Independent Activities Period course founded by Koppel that is focused on reverse engineering software, broached the idea of reverse engineering Voatz’s application, in an effort to better understand how its system worked. To ensure that they did not interfere with any ongoing elections or expose user records, Specter and Koppel reverse-engineered the application and then created a model of Voatz’s server.
They found that an adversary with remote access to the device can alter or discover a user’s vote, and that the server, if hacked, could easily change those votes. “It does not appear that the app’s protocol attempts to verify [genuine votes] with the back-end blockchain,” Specter explains.
“Perhaps most alarmingly, we found that a passive network adversary, like your internet service provider, or someone nearby you if you’re on unencrypted Wi-Fi, could detect which way you voted in some configurations of the election. Worse, more aggressive attackers could potentially detect which way you’re going to vote and then stop the connection based on that alone.”
In addition to detecting vulnerabilities with Voatz’s voting process, Specter and Koppel found that the app poses privacy issues for users. As the app uses an external vendor for voter ID verification, a third party could potentially access a voter’s photo, driver’s license data, or other forms of identification, if that vendor’s platform isn’t also secure.      
“Though Voatz’s privacy policy does talk about sending some information to third parties, as far as we can tell the fact that any third party is getting the voter’s driver’s license and selfie isn’t explicitly mentioned,” Specter notes.
Calls for increased openness
Specter and Koppel say that their findings point to the need for openness when it comes to election administration, in order to ensure the integrity of the election process. Currently, they note, the election process in states that use paper ballots is designed to be transparent, and citizens and political party representatives are given opportunities to observe the voting process.
In contrast, Koppel notes, “Voatz’s app and infrastructure were completely closed-source; we were only able to get access to the app itself.     
“I think this type of analysis is extremely important. Right now, there’s a drive to make voting more accessible, by using internet and mobile-based voting systems. The problem here is that sometimes those systems aren’t made by people who have expertise in keeping voting systems secure, and they’re deployed before they can get proper review,” says Matthew Green, an associate professor at the Johns Hopkins Information Security Institute. In the case of Voatz, he adds, “It looks like there were many good intentions here, but the result lacks key features that would protect a voter and protect the integrity of elections.”
Going forward, the researchers caution that software developers should prove their systems are as secure as paper ballots.
“The biggest issue is transparency,” says Specter. “When you have part of the election that is opaque, that is not viewable, that is not public, that has some sort of proprietary component, that part of the system is inherently suspect and needs to be put under a lot of scrutiny.”



",MIT researchers identify security vulnerabilities in voting app,2020-02-13,[],Cyber security/Voting and elections/Computer science and technology/Apps/Technology and society/Internet Policy Research Initiative/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/School of Engineering,"['election', 'voting', 'systems', 'security', 'koppel', 'research', 'researchers', 'vulnerabilities', 'elections', 'voatzs', 'specter', 'identify', 'app', 'mit']","Now, MIT researchers are raising another concern: They say they have uncovered security vulnerabilities in a mobile voting application that was used during the 2018 midterm elections in West Virginia.
After uncovering these security vulnerabilities, the researchers disclosed their findings to the Department of Homeland Security’s Cybersecurity and Infrastructure Agency (CISA).
The findings underscore the need for transparency in the design of voting systems, according to the researchers.
In contrast, Koppel notes, “Voatz’s app and infrastructure were completely closed-source; we were only able to get access to the app itself.
Right now, there’s a drive to make voting more accessible, by using internet and mobile-based voting systems.",Mit
175,https://news.mit.edu/2020/automated-rewrite-wikipedia-articles-0212,"


A system created by MIT researchers could be used to automatically update factual inconsistencies in Wikipedia articles, reducing time and effort spent by human editors who now do the task manually.Wikipedia comprises millions of articles that are in constant need of edits to reflect new information. That can involve article expansions, major rewrites, or more routine modifications such as updating numbers, dates, names, and locations. Currently, humans across the globe volunteer their time to make these edits.  In a paper being presented at the AAAI Conference on Artificial Intelligence, the researchers describe a text-generating system that pinpoints and replaces specific information in relevant Wikipedia sentences, while keeping the language similar to how humans write and edit.The idea is that humans would type into an interface an unstructured sentence with updated information, without needing to worry about style or grammar. The system would then search Wikipedia, locate the appropriate page and outdated sentence, and rewrite it in a humanlike fashion. In the future, the researchers say, there’s potential to build a fully automated system that identifies and uses the latest information from around the web to produce rewritten sentences in corresponding Wikipedia articles that reflect updated information.“There are so many updates constantly needed to Wikipedia articles. It would be beneficial to automatically modify exact portions of the articles, with little to no human intervention,” says Darsh Shah, a PhD student in the Computer Science and Artificial Intelligence Laboratory (CSAIL) and one of the lead authors. “Instead of hundreds of people working on modifying each Wikipedia article, then you’ll only need a few, because the model is helping or doing it automatically. That offers dramatic improvements in efficiency.”Many other bots exist that make automatic Wikipedia edits. Typically, those work on mitigating vandalism or dropping some narrowly defined information into predefined templates, Shah says. The researchers’ model, he says, solves a harder artificial intelligence problem: Given a new piece of unstructured information, the model automatically modifies the sentence in a humanlike fashion. “The other [bot] tasks are more rule-based, while this is a task requiring reasoning over contradictory parts in two sentences and generating a coherent piece of text,” he says.The system can be used for other text-generating applications as well, says co-lead author and CSAIL graduate student Tal Schuster. In their paper, the researchers also used it to automatically synthesize sentences in a popular fact-checking dataset that helped reduce bias, without manually collecting additional data. “This way, the performance improves for automatic fact-verification models that train on the dataset for, say, fake news detection,” Schuster says.Shah and Schuster worked on the paper with their academic advisor Regina Barzilay, the Delta Electronics Professor of Electrical Engineering and Computer Science and a professor in CSAIL.Neutrality masking and fusingBehind the system is a fair bit of text-generating ingenuity in identifying contradictory information between, and then fusing together, two separate sentences. It takes as input an “outdated” sentence from a Wikipedia article, plus a separate “claim” sentence that contains the updated and conflicting information. The system must automatically delete and keep specific words in the outdated sentence, based on information in the claim, to update facts but maintain style and grammar. That’s an easy task for humans, but a novel one in machine learning.For example, say there’s a required update to this sentence (in bold): “Fund A considers 28 of their 42 minority stakeholdings in operationally active companies to be of particular significance to the group.” The claim sentence with updated information may read: “Fund A considers 23 of 43 minority stakeholdings significant.” The system would locate the relevant Wikipedia text for “Fund A,” based on the claim. It then automatically strips out the outdated numbers (28 and 42) and replaces them with the new numbers (23 and 43), while keeping the sentence exactly the same and grammatically correct. (In their work, the researchers ran the system on a dataset of specific Wikipedia sentences, not on all Wikipedia pages.)The system was trained on a popular dataset that contains pairs of sentences, in which one sentence is a claim and the other is a relevant Wikipedia sentence. Each pair is labeled in one of three ways: “agree,” meaning the sentences contain matching factual information; “disagree,” meaning they contain contradictory information; or “neutral,” where there’s not enough information for either label. The system must make all disagreeing pairs agree, by modifying the outdated sentence to match the claim. That requires using two separate models to produce the desired output.The first model is a fact-checking classifier — pretrained to label each sentence pair as “agree,” “disagree,” or “neutral” — that focuses on disagreeing pairs. Running in conjunction with the classifier is a custom “neutrality masker” module that identifies which words in the outdated sentence contradict the claim. The module removes the minimal number of words required to “maximize neutrality” — meaning the pair can be labeled as neutral. That’s the starting point: While the sentences don’t agree, they no longer contain obviously contradictory information. The module creates a binary “mask” over the outdated sentence, where a 0 gets placed over words that most likely require deleting, while a 1 goes on top of keepers.After masking, a novel two-encoder-decoder framework is used to generate the final output sentence. This model learns compressed representations of the claim and the outdated sentence. Working in conjunction, the two encoder-decoders fuse the dissimilar words from the claim, by sliding them into the spots left vacant by the deleted words (the ones covered with 0s) in the outdated sentence.In one test, the model scored higher than all traditional methods, using a technique called “SARI” that measures how well machines delete, add, and keep words compared to the way humans modify sentences. They used a dataset with manually edited Wikipedia sentences, which the model hadn’t seen before. Compared to several traditional text-generating methods, the new model was more accurate in making factual updates and its output more closely resembled human writing. In another test, crowdsourced humans scored the model (on a scale of 1 to 5) based on how well its output sentences contained factual updates and matched human grammar. The model achieved average scores of 4 in factual updates and 3.85 in matching grammar.Removing biasThe study also showed that the system can be used to augment datasets to eliminate bias when training detectors of “fake news,” a form of propaganda containing disinformation created to mislead readers in order to generate website views or steer public opinion. Some of these detectors train on datasets of agree-disagree sentence pairs to “learn” to verify a claim by matching it to given evidence.In these pairs, the claim will either match certain information with a supporting “evidence” sentence from Wikipedia (agree) or it will be modified by humans to include information contradictory to the evidence sentence (disagree). The models are trained to flag claims with refuting evidence as “false,” which can be used to help identify fake news.Unfortunately, such datasets currently come with unintended biases, Shah says: “During training, models use some language of the human written claims as “give-away” phrases to mark them as false, without relying much on the corresponding evidence sentence. This reduces the model’s accuracy when evaluating real-world examples, as it does not perform fact-checking.”The researchers used the same deletion and fusion techniques from their Wikipedia project to balance the disagree-agree pairs in the dataset and help mitigate the bias. For some “disagree” pairs, they used the modified sentence’s false information to regenerate a fake “evidence” supporting sentence. Some of the give-away phrases then exist in both the “agree” and “disagree” sentences, which forces models to analyze more features. Using their augmented dataset, the researchers reduced the error rate of a popular fake-news detector by 13 percent.“If you have a bias in your dataset, and you’re fooling your model into just looking at one sentence in a disagree pair to make predictions, your model will not survive the real world,” Shah says. “We make models look at both sentences in all agree-disagree pairs.”


",Automated system can rewrite outdated sentences in Wikipedia articles,2020-02-12,['Rob Matheson'],Research/Computer science and technology/Algorithms/Machine learning/Data/Internet/Crowdsourcing/Social media/Technology and society/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Electrical Engineering & Computer Science (eecs)/School of Engineering,"['sentences', 'automated', 'model', 'researchers', 'used', 'articles', 'rewrite', 'outdated', 'information', 'system', 'sentence', 'wikipedia', 'claim']","The system would then search Wikipedia, locate the appropriate page and outdated sentence, and rewrite it in a humanlike fashion.
It takes as input an “outdated” sentence from a Wikipedia article, plus a separate “claim” sentence that contains the updated and conflicting information.
(In their work, the researchers ran the system on a dataset of specific Wikipedia sentences, not on all Wikipedia pages.)
The system must make all disagreeing pairs agree, by modifying the outdated sentence to match the claim.
They used a dataset with manually edited Wikipedia sentences, which the model hadn’t seen before.",Mit
176,https://news.mit.edu/2020/tagup-equipment-maintenance-0212,"


Most people only think about the systems that power their cities when something goes wrong. Unfortunately, many people in the San Francisco Bay Area had a lot to think about recently when their utility company began scheduled power outages in an attempt to prevent wildfires. The decision came after devastating fires last year were found to be the result of faulty equipment, including transformers.Transformers are the links between power plants, power transmission lines, and distribution networks. If something goes wrong with a transformer, entire power plants can go dark. To fix the problem, operators work around the clock to assess various components of the plant, consider disparate data sources, and decide what needs to be repaired or replaced.Power equipment maintenance and failure is such a far-reaching problem it’s difficult to attach a dollar sign to. Beyond the lost revenue of the plant, there are businesses that can’t operate, people stuck in elevators and subways, and schools that can’t open.Now the startup Tagup is working to modernize the maintenance of transformers and other industrial equipment. The company’s platform lets operators view all of their data streams in one place and use machine learning to estimate if and when components will fail.Founded by CEO Jon Garrity ’11 and CTO Will Vega-Brown ’11, SM ’13 — who recently completed his PhD program in MIT’s Department of Mechanical Engineering and will be graduating this month — Tagup is currently being used by energy companies to monitor approximately 60,000 pieces of equipment around North America and Europe. That includes transformers, offshore wind turbines, and reverse osmosis systems for water filtration, among other things.“Our mission is to use AI to make the machines that power the world safer, more reliable, and more efficient,” Garrity says.A light bulb goes onVega-Brown and Garrity crossed paths in a number of ways at MIT over the years. As undergraduates, they took a few of the same courses, with Vega-Brown double majoring in mechanical engineering and physics and Garrity double majoring in economics and physics. They were also fraternity brothers as well as teammates on the football team.Garrity was first exposed to entrepreneurship as an undergraduate in MIT’s Energy Ventures class and in the Martin Trust Center for Entrepreneurship. Later, when Garrity returned to campus while attending Harvard Business School and Vega-Brown was pursuing his doctorate, they were again classmates in MIT’s New Enterprises course.Still, the founders didn’t think about starting a company until 2015, after Garrity had worked at GE Energy and Vega-Brown was well into his PhD work at MIT’s Computer Science and Artificial Intelligence Laboratory.At GE, Garrity discovered an intriguing business model through which critical assets like jet engines were leased by customers — in this case airlines — rather than purchased, and manufacturers held responsibility for remotely monitoring and maintaining them. The arrangement allowed GE and others to leverage their engineering expertise while the customers focused on their own industries.""When I worked at GE, I always wondered: Why isn’t this service available for any equipment type? The answer is economics.” Garrity says. “It is expensive to set up a remote monitoring center, to instrument the equipment in the field, to staff the 50 or more engineering subject matter experts, and to provide the support required to end customers. The cost of equipment failure, both in terms of business interruption and equipment breakdown, must be enormous to justify the high average fixed cost.""“We realized two things,” Garrity continues. “With the increasing availability of sensors and cloud infrastructure, we can dramatically reduce the cost [of monitoring critical assets] from the infrastructure and communications side. And, with new machine-learning methods, we can increase the productivity of engineers who review equipment data manually.”That realization led to Tagup, though it would take time to prove the founders’ technology. “The problem with using AI for industrial applications is the lack of high-quality data,” Vega-Brown explains. “Many of our customers have giant datasets, but the information density in industrial data is often quite low. That means we need to be very careful in how we hunt for signal and validate our models, so that we can reliably make accurate forecasts and predictions.”The founders leveraged their MIT ties to get the company off the ground. They received guidance from MIT’s Venture Mentoring Service, and Tagup was in the first cohort of startups accepted into the MIT Industrial Liaison Program’s (ILP) STEX 25 accelerator, which connects high potential startups with members of industry. Tagup has since secured several customers through ILP, and those early partnerships helped the company train and validate some of its machine-learning models.Making power more reliableTagup’s platform combines all of a customer’s equipment data into one sortable master list that displays the likelihood of each asset causing a disruption. Users can click on specific assets to see charts of historic data and trends that feed into Tagup’s models.The company doesn’t deploy any sensors of its own. Instead, it combines customers’ real-time sensor measurements with other data sources like maintenance records and machine parameters to improve its proprietary machine-learning models.The founders also began with a focused approach to building their system. Transformers were one of the first types of equipment they worked with, and they’ve expanded to other groups of assets gradually.Tagup’s first deployment was in August of 2016 with a power plant that faces the Charles River close to MIT’s campus. Just a few months after it was installed, Garrity was at a meeting overseas when he got a call from the plant manager about a transformer that had just gone offline unexpectedly. From his phone, Garrity was able to inspect real-time data from the transformer and give the manager the information he needed to restart the system. Garrity says it saved the plant about 26 hours of downtime and $150,000 in revenue.“These are really catastrophic events in terms of business outcomes,” Garrity says, noting transformer failures are estimated to cost $23 billion annually.Since then they’ve secured partnerships with several large utility companies, including National Grid and Consolidated Edison Company of New York.Down the line, Garrity and Vega-Brown are excited about using machine learning to control the operation of equipment. For example, a machine could manage itself in the same way an autonomous car can sense an obstacle and steer around it.Those capabilities have major implications for the systems that ensure the lights go on when we flip switches at night.“Where it gets really exciting is moving toward optimization,” Garrity says. Vega-Brown agrees, adding, “Enormous amounts of power and water are wasted because there aren't enough experts to tune the controllers on every industrial machine in the world. If we can use AI to capture some of the expert knowledge in an algorithm, we can cut inefficiency and improve safety at scale.”


",Maintaining the equipment that powers our world,2020-02-12,['Zach Winn'],Innovation and Entrepreneurship (I&E)/Startups/Alumni/ae/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Mechanical engineering/School of Engineering/Machine learning/Energy/Artificial intelligence,"['mits', 'customers', 'data', 'equipment', 'vegabrown', 'powers', 'world', 'plant', 'power', 'maintaining', 'tagup', 'company', 'garrity']","Power equipment maintenance and failure is such a far-reaching problem it’s difficult to attach a dollar sign to.
The cost of equipment failure, both in terms of business interruption and equipment breakdown, must be enormous to justify the high average fixed cost.""
“The problem with using AI for industrial applications is the lack of high-quality data,” Vega-Brown explains.
Making power more reliableTagup’s platform combines all of a customer’s equipment data into one sortable master list that displays the likelihood of each asset causing a disruption.
Down the line, Garrity and Vega-Brown are excited about using machine learning to control the operation of equipment.",Mit
177,https://news.mit.edu/2020/out-of-the-media-lab-into-the-world-0211,"


In collaboration with the E14 Fund, on Jan. 28-29 the MIT Media Lab hosted the inaugural Media Lab Venture Summit — the first-ever celebration of the myriad spinoff companies created by the extended community of Media Lab alumni, research staff, and faculty members.
Slated to become an annual event, the summit convened on the sixth floor of Building E14, with introductory talks by Deb Roy, professor of media arts and sciences and Media Lab executive director of operations and communications, and E14 managing partners Habib Haddad and Calvin Chin. Later events included a panel discussion between entrepreneurs from the Media Lab, presentations by nearly 40 spinoff companies, a networking lunch hosted by the MIT Industrial Liaison Program, some 25 demos, and three breakout sessions focused on radical reinvention of traditional industries, digitizing product value chains, and radical sustainability for future products. On the second day of the summit, participants were invited to tour Formlabs, Tulip, and Ginkgo Bioworks — local startups with MIT roots.  Started in 2014, the E14 Fund began as a small experiment, and has since grown into a robust network of support for Media Lab spinoffs, students, alumni, and other members of the extended lab community. As Roy noted in his introduction, entrepreneurship is a natural outgrowth of the lab’s approach to research. “I’ve long described the Media Lab, and one of the core aspects of the spirit of the lab, as entrepreneurial — enterprising, and characterized by the taking of research risks in the hope of intellectual and practical advances. It’s part of the ethos of the lab, and it’s amazing to see this rich collection of startups that the E14 family has recognized and been fostering over the last several years.”The program for the first day of the summit reflected the broad diversity of those startups, which range from early-stage companies founded by recent graduates and based on their Media Lab research to companies created by alumni who left the lab some time ago. Moderated by Joe Chung SM ’89, the opening panel provided an informative, emotionally honest, and sometimes surprising discussion of the different paths labbers have taken to starting their own companies. Chung himself left the PhD program to co-found Art Technology Group with Jeet Singh ’86, while Nan-Wei Gong PhD ’13, founder of Figur8, co-founded her first company (3dim Tech Inc., which was acquired in 2014) after winning the 2013 MIT $100K Competition with friends. Former Media Lab postdoc Rana El Kaliouby co-founded Affectiva with Professor Rosalind Picard when it became clear that their research project had outgrown the lab. LittleBits founder Ayah Bdeir SM ’06, meanwhile, shifted her focus from creative electronics for everyone to creative electronics for children after the 2009 Maker Faire in San Mateo, California, where so many kids swarmed around her booth and refused to leave that she had to pretend she was closing so that their parents could take them home. A recurring theme was that many of the panelists didn’t leave the lab with the intention of starting a company — rather, they started companies because it seemed like the best way to accomplish a specific mission. “We build starting from the passion,” says Gong, “and then we figure out a business model and ask how we scale up.” John Underkoffler ’88, SM ’91, PhD ’99, whose dissertation work inspired the production design of the film “Minority Report” — on which he served as a consultant — and whose company, Oblong, continues to build upon that work, was more blunt: “I had no idea what I was doing.” He credited fellow Media Lab alumnus David Kung ’93, SM ’95, now vice president for product strategy at Oblong, with making him understand that the calls he was getting from Fortune 500 companies meant there was interest in his technology. “It was sidelong and sideways and unanticipated.” The panelists also spoke candidly about both their successes and their greatest challenges. Harmonix co-founder Eran Egozy ’93, MEng ’95 described the unprecedented surge of interest in “Guitar Hero,” which saw its sales figures double month after month, bypassing the usual post-holiday slump. Others talked about stresses and failures, from running out of funding to making painful business decisions to the difficulty of balancing personal relationships with the needs of an early-stage startup. “I always used to think when people said things like, ‘If I’d known how hard it was, I never would have tried,’ were being dramatic, but it’s literally true,” Underkoffler says. El Kaliouby offered advice on how to weather those storms: “Go back to core values. They’re not important when things are rosy, but they’re especially important in these tough times, when you have to make tough decisions.” After the panel discussion, presenters from 36 ventures delivered lightning-round overviews, inviting attendees to learn more about their organizations during the demo and networking sessions in the afternoon. These presentations further showcased the diversity of the enterprises, from artificial intelligence applications designed to improve crop yields and reduce overuse of fertilizer and pesticides in commercial farming, to high-efficiency heating and cooling systems, to autonomous mobility solutions at scales from individuals to mass transit. The demos also included room for the whimsical — like the beautiful, networked touch lamps developed by John Harrison’s SM ’05 Filimin. Ann Perrin, a liaison from Media Lab member company Deloitte who has long advocated for an event like this, says, “The inaugural Venture Summit exemplified the power of the lab by bringing together faculty, innovative spinoffs rooted in research pioneered at the lab, and corporate members scouting for emerging tech and exploring partnerships. A great success.” Haddad agrees: “The summit is a great opportunity to celebrate the impact of the lab beyond the lab. It’s great to see all those startups continue building on top of the work they did at the lab, creating ventures at scale to tackle large and tough problems.” Ryan McCarthy, director of member relations at the Media Lab, adds, “It was amazing to hear from older spinoffs, and see how much they've accomplished, alongside these new companies that have so much potential.” The event also highlighted something that Roy, Chin, and Haddad all emphasized in their opening remarks — that promising research from the lab may take years to come to fruition. “Some of the ideas that the Media Lab works on,” Roy says, “have gestation periods that will actually span not just years but decades, and eventually come into material practice.”


",Out of the lab and into the world,2020-02-11,['Chia Evers'],Media Lab/Startups/Alumni/ae/Staff/Faculty/Innovation and Entrepreneurship (I&E)/Special events and guest speakers/Business and management/School of Architecture and Planning,"['roy', 'research', 'media', 'world', 'summit', 'e14', 'lab', 'companies', 'sm', 'startups', 'mit']","In collaboration with the E14 Fund, on Jan. 28-29 the MIT Media Lab hosted the inaugural Media Lab Venture Summit — the first-ever celebration of the myriad spinoff companies created by the extended community of Media Lab alumni, research staff, and faculty members.
Started in 2014, the E14 Fund began as a small experiment, and has since grown into a robust network of support for Media Lab spinoffs, students, alumni, and other members of the extended lab community.
“I’ve long described the Media Lab, and one of the core aspects of the spirit of the lab, as entrepreneurial — enterprising, and characterized by the taking of research risks in the hope of intellectual and practical advances.
Former Media Lab postdoc Rana El Kaliouby co-founded Affectiva with Professor Rosalind Picard when it became clear that their research project had outgrown the lab.
A great success.” Haddad agrees: “The summit is a great opportunity to celebrate the impact of the lab beyond the lab.",Mit
178,https://news.mit.edu/2020/brainstorming-energy-saving-hacks-satori-mit-supercomputer-0211,"


Mohammad Haft-Javaherian planned to spend an hour at the Green AI Hackathon — just long enough to get acquainted with MIT’s new supercomputer, Satori. Three days later, he walked away with $1,000 for his winning strategy to shrink the carbon footprint of artificial intelligence models trained to detect heart disease. 
“I never thought about the kilowatt-hours I was using,” he says. “But this hackathon gave me a chance to look at my carbon footprint and find ways to trade a small amount of model accuracy for big energy savings.” 
Haft-Javaherian was among six teams to earn prizes at a hackathon co-sponsored by the MIT Research Computing Project and MIT-IBM Watson AI Lab Jan. 28-30. The event was meant to familiarize students with Satori, the computing cluster IBM donated to MIT last year, and to inspire new techniques for building energy-efficient AI models that put less planet-warming carbon dioxide into the air. 
The event was also a celebration of Satori’s green-computing credentials. With an architecture designed to minimize the transfer of data, among other energy-saving features, Satori recently earned fourth place on the Green500 list of supercomputers. Its location gives it additional credibility: It sits on a remediated brownfield site in Holyoke, Massachusetts, now the Massachusetts Green High Performance Computing Center, which runs largely on low-carbon hydro, wind and nuclear power.
A postdoc at MIT and Harvard Medical School, Haft-Javaherian came to the hackathon to learn more about Satori. He stayed for the challenge of trying to cut the energy intensity of his own work, focused on developing AI methods to screen the coronary arteries for disease. A new imaging method, optical coherence tomography, has given cardiologists a new tool for visualizing defects in the artery walls that can slow the flow of oxygenated blood to the heart. But even the experts can miss subtle patterns that computers excel at detecting.
At the hackathon, Haft-Javaherian ran a test on his model and saw that he could cut its energy use eight-fold by reducing the time Satori’s graphics processors sat idle. He also experimented with adjusting the model’s number of layers and features, trading varying degrees of accuracy for lower energy use. 
A second team, Alex Andonian and Camilo Fosco, also won $1,000 by showing they could train a classification model nearly 10 times faster by optimizing their code and losing a small bit of accuracy. Graduate students in the Department of Electrical Engineering and Computer Science (EECS), Andonian and Fosco are currently training a classifier to tell legitimate videos from AI-manipulated fakes, to compete in Facebook’s Deepfake Detection Challenge. Facebook launched the contest last fall to crowdsource ideas for stopping the spread of misinformation on its platform ahead of the 2020 presidential election.
If a technical solution to deepfakes is found, it will need to run on millions of machines at once, says Andonian. That makes energy efficiency key. “Every optimization we can find to train and run more efficient models will make a huge difference,” he says.
To speed up the training process, they tried streamlining their code and lowering the resolution of their 100,000-video training set by eliminating some frames. They didn’t expect a solution in three days, but Satori’s size worked in their favor. “We were able to run 10 to 20 experiments at a time, which let us iterate on potential ideas and get results quickly,” says Andonian. 
As AI continues to improve at tasks like reading medical scans and interpreting video, models have grown bigger and more calculation-intensive, and thus, energy intensive. By one estimate, training a large language-processing model produces nearly as much carbon dioxide as the cradle-to-grave emissions from five American cars. The footprint of the typical model is modest by comparison, but as AI applications proliferate its environmental impact is growing. 
One way to green AI, and tame the exponential growth in demand for training AI, is to build smaller models. That’s the approach that a third hackathon competitor, EECS graduate student Jonathan Frankle, took. Frankle is looking for signals early in the training process that point to subnetworks within the larger, fully-trained network that can do the same job. The idea builds on his award-winning Lottery Ticket Hypothesis paper from last year that found a neural network could perform with 90 percent fewer connections if the right subnetwork was found early in training.
The hackathon competitors were judged by John Cohn, chief scientist at the MIT-IBM Watson AI Lab, Christopher Hill, director of MIT’s Research Computing Project, and Lauren Milechin, a research software engineer at MIT. 
The judges recognized four other teams: Department of Earth, Atmospheric and Planetary Sciences (EAPS) graduate students Ali Ramadhan, Suyash Bire, and James Schloss, for adapting the programming language Julia for Satori; MIT Lincoln Laboratory postdoc Andrew Kirby, for adapting code he wrote as a graduate student to Satori using a library designed for easy programming of computing architectures; and Department of Brain and Cognitive Sciences graduate students Jenelle Feather and Kelsey Allen, for applying a technique that drastically simplifies models by cutting their number of parameters.
IBM developers were on hand to answer questions and gather feedback.  “We pushed the system — in a good way,” says Cohn. “In the end, we improved the machine, the documentation, and the tools around it.” 
Going forward, Satori will be joined in Holyoke by TX-Gaia, Lincoln Laboratory’s new supercomputer. Together, they will provide feedback on the energy use of their workloads. “We want to raise awareness and encourage users to find innovative ways to green-up all of their computing,” says Hill. 


","Brainstorming energy-saving hacks on Satori, MIT’s new supercomputer",2020-02-11,['Kim Martineau'],"Quest for Intelligence/MIT-IBM Watson AI Lab/Electrical engineering and computer science (EECS)/EAPS/Lincoln Laboratory/Brain and cognitive sciences/School of Engineering/School of Science/Algorithms/Artificial intelligence/Computer science and technology/Data/Machine learning/Software/Climate change/Awards, honors and fellowships/Hackathon/Special events and guest speakers","['mits', 'energysaving', 'satori', 'brainstorming', 'graduate', 'hackathon', 'model', 'energy', 'supercomputer', 'models', 'computing', 'hacks', 'ai', 'training', 'students']","Mohammad Haft-Javaherian planned to spend an hour at the Green AI Hackathon — just long enough to get acquainted with MIT’s new supercomputer, Satori.
With an architecture designed to minimize the transfer of data, among other energy-saving features, Satori recently earned fourth place on the Green500 list of supercomputers.
A postdoc at MIT and Harvard Medical School, Haft-Javaherian came to the hackathon to learn more about Satori.
One way to green AI, and tame the exponential growth in demand for training AI, is to build smaller models.
“In the end, we improved the machine, the documentation, and the tools around it.”Going forward, Satori will be joined in Holyoke by TX-Gaia, Lincoln Laboratory’s new supercomputer.",Mit
179,https://news.mit.edu/2020/drawing-daily-doodles-chalk-of-the-day-brightens-mit-0210,"


The Ray and Maria Stata Center is an architectural staple of MIT’s campus. Inside the angled walls and modern exterior lives the Computer Science and Artificial Intelligence Laboratory (CSAIL), the Laboratory for Information and Decision Systems (LIDS), and the Department of Linguistics and Philosophy. It's also a central hub for conferences, lunch meetings, and regular events like Choose to Reuse. Building 32 also houses the canvas used by student group Chalk of the Day to share daily works of art.
Chalk of the Day was started in 2015 by Benjamin Chan ’17 as a way to give back to the MIT community through inspirational messages and doodles. Today, Chalk of the Day remains a tight-knit group of friends who craft new pieces of art that are visible to passers-by for a day, memorialized on the group's Instagram account — and then erased every night.







Play video






Priscilla Wong, a chalker who finished her coursework in computer science and engineering last fall and will be graduating in May, says she began chalking as a way to find an escape from the typical routine at MIT. Each semester, students schedule and claim a day based on their availability. Wong and her chalking partner Jessica Xu, a junior in mechanical engineering, have chalked before, and last semester, they made sure they shared free Tuesday mornings to continue their tradition of making art together.
Using a pointillist technique, Wong taps the chalk repeatedly against the board to create a snow effect as she discusses the ways in which chalk is an unusual medium. “Some of the most difficult things about chalking are also what makes it the most interesting,” she says. “If you chalk over a really big area it ends up snowing down on everything below. Sometimes it’s an effect you want to achieve.” Her chalk partner, Jessica Xu, adds “for the most part we come up with techniques on our own.” Wong echoes how there’s a learning curve and the way they learn is simply by chalking.
The works of art span from inspirational quotes to more political works, like a drawing in response to the Australian wildfires: a mama and child koala sit in a tree with the text “save us” above, the letters connecting like a crossword puzzle to configure AUS for Australia. The art is often incredibly detailed: One homage to the film “Up” displayed the characters lifted by a house tied to balloons with the message “adventure is out there,” while another featured a hummingbird eating nectar from a blossoming flower.
Sarah Wu, a senior mathematics major, has been chalking since her first year at MIT, when she was looking for more artsy things to do around campus. She compares chalking with solving a math problem: Both require a level of creativity, but approaching a blank canvas is a totally different process and engages a different part of her mind. Chalking is a way to relax and de-stress for Wu: “Normally, I’m always thinking about the next assignment or the next test, but this is an opportunity where once a week I can actually remove myself from that and try to focus only on the art I’m making and the things I’m contributing to the community.” She and her chalking partner, Charleen Wang, a senior in electrical engineering and computer science, worked on a lettering piece that reads “Catch your breath, take your time,” filled with snowflakes mimicking the weather conditions outside.
Wang shares a similar sentiment to Wu about the importance of Chalk of the Day in her routine. “I think sometimes I forget to engage in a more creative side of me. I learned a lot about how to put in other priorities that I might be forgetting into my schedule. It’s not all about grades,” she says. She likes how temporary chalk is as a medium. “I feel more free to try different things because it’s not something so permanent like pen or painting.” Every day is an opportunity for chalk artists to try something new, create a new work of art, and feel empowered to think outside of the box.
Chalking helps students de-stress, but more than that, their artwork spreads positivity and inspiration to the entire MIT community. Passersby “send it to their boyfriend or girlfriend or friend or mother. I like that it has an impact that is beyond Stata or MIT,” Wong reflects. Chalk of the Day members hope that sharing the daily chalk-works encourages others to be more creative in their everyday lives.


",Drawing daily doodles: Chalk of the Day brightens MIT,2020-02-10,['Julia Newman'],Student life/Community/Arts/Computer Science and Artificial Intelligence Laboratory (CSAIL)/Students/Alumni/ae/Electrical Engineering & Computer Science (eecs)/School of Engineering/School of Humanities Arts and Social Sciences/Laboratory for Information and Decision Systems (LIDS)/Clubs and activities,"['daily', 'chalk', 'doodles', 'things', 'brightens', 'mit', 'day', 'chalking', 'wong', 'way', 'wu', 'art', 'works', 'drawing']","Chalk of the Day was started in 2015 by Benjamin Chan ’17 as a way to give back to the MIT community through inspirational messages and doodles.
Building 32 also houses the canvas used by student group Chalk of the Day to share daily works of art.
Wang shares a similar sentiment to Wu about the importance of Chalk of the Day in her routine.
I like that it has an impact that is beyond Stata or MIT,” Wong reflects.
Chalk of the Day members hope that sharing the daily chalk-works encourages others to be more creative in their everyday lives.",Mit
