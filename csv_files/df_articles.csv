,link,title,date,text,keywords,summary
0,https://www.marktechpost.com/2021/02/13/microsoft-azure-ai-is-bringing-iconic-characters-to-life-with-the-help-of-custom-neural-voice-and-5g-network/,Microsoft Azure AI is Bringing Iconic Characters to Life with the Help of Custom Neural Voice and 5G Network,2021-02-13 00:00:00,"Microsoft Azure AI technology has used 5G, augmented reality, artificial intelligence, and a Custom Neural Voice to bring the iconic character Bugs Bunny to life. This character can be seen in the AT&T Experience Store in Dallas. The major breakthrough for the creation of such a character was achieved with the use of deep learning to facilitate pronunciation. The use of deep learning also perfected the duration and tone of the character. One of the significant reasons to demonstrate this technology at the store was to make people aware of the 5G cellular network’s capabilities. When a customer enters the store, the character engages in a real-time conversation, greeting the customer by name and then asking for help to find numerous golden carrots that have been hidden in the store. The customer then navigates the character via chat throughout the store.

The Impeccable Technology

For several years speech has been very robotic, but the custom neural voice has powered speech to a level where it sounds exceedingly natural. The 5G network makes Bugs Bunny appear in HD in mere seconds and flawlessly move all across the store. The consumers also feel a better connection through this form of speech. The technology that makes this entire process smooth is the neural text to speech capability, a part of the Azure Cognitive Service. Compared to the 4G network, 5G offers better computing power and much a higher speed.

https://www.youtube.com/watch?v=MkeI7Aaf7hk

The Iconic Character and Transparency

The main motive behind creating and bringing such an iconic character to life was to unify the physical environment with the virtual one harmoniously. The first character selected for this purpose was Bugs Bunny, but in all likelihood, it will not be the last. The journey to building this character involved pre-recording more than 2000 phrases and lines by a voiceover actor and the Warner Bros. and Microsoft team working tirelessly to ensure that the character accurately reflects Bugs Bunny’s personality and traits.

The result of the effort was that a character was created that looked as real as possible. The technology’s general availability also signifies that it is available to the Azure cloud regions and not the public in order to prevent misuse. A set of guidelines and ethics have also been put into place by Microsoft to use this technology.

Creating the Perfect Custom Voice

After recording the phrases and lines, the second step was to create a font of sounds to create a natural-sounding voice that can say anything in any situation. The font of sounds is similar to the font of a computer, where different letters all combine to make up words and sentences that make sense. Everything was connected beautifully by using deep learning to create a voice that sounded as good as that of an original person.

Deep learning is a phenomenon that is a part of the machine learning process wherein the machines are taught to learn and analyze a set of given data to mimic human-like activities. As the word suggests, deep learning goes in-depth about the particular phenomenon to create more layers within the neural network. In creating the custom voice, two neural networks play with different layers to perform various functions. When two neural networks work in harmony with each other, a more natural sound is created.

Creativity at its Best

The Custom Neural Voice is a unique technology in itself. It can be manifested in the field of education as well to optimize the benefits. Microsoft partnered with a non-profit organization in Beijing, China to generate AI audio content using Custom Neural Voice to help individuals facing vision problems. Not only this, but Microsoft also collaborated with Duolingo (a company offering various language learning courses) to better the quality of services and personalize the process of language learning. The Microsoft team has reiterated nine different characters to give a diverse outlook to the program. Hundreds of characters were tried and tested by the researchers to reflect upon the various cultural influences that would help the users connect better with the app’s characters. Every character has a distinctive and unique personality. The custom neural voice acted as a catalyst for all the users in their learning process. The characters will be speaking English, Spanish, French, German, and Japanese for now. Experiments are underway to create more characters and increase the range of languages offered by Duolingo.

Responsibly Moving Forward

This technology aims to leave a positive and lasting impact upon the people. But while doing so, Microsoft is responsible for ensuring that no harm is caused anywhere in the world. Time and again, Microsoft conducts specific tests and assesses the potential risks posed by the technology. Efforts are then aligned towards mitigating those risks and creating protocols of use. Azure Cognitive Services wants to empower its customers but also tread carefully. Microsoft says that it is committed to responsible AI, and whenever working on new technology, its foremost concern is to follow all the guidelines.

Source: https://blogs.microsoft.com/ai-for-business/custom-neural-voice-ga/

Suggested","['create', 'iconic', 'bringing', 'network', 'voice', 'custom', 'store', 'life', 'learning', 'neural', 'help', 'characters', 'technology', 'microsoft', 'character']","Microsoft Azure AI technology has used 5G, augmented reality, artificial intelligence, and a Custom Neural Voice to bring the iconic character Bugs Bunny to life.
The Impeccable TechnologyFor several years speech has been very robotic, but the custom neural voice has powered speech to a level where it sounds exceedingly natural.
Creativity at its BestThe Custom Neural Voice is a unique technology in itself.
Microsoft partnered with a non-profit organization in Beijing, China to generate AI audio content using Custom Neural Voice to help individuals facing vision problems.
The custom neural voice acted as a catalyst for all the users in their learning process."
1,https://venturebeat.com/2021/02/13/ai-progress-depends-on-us-using-less-data-not-more/,"AI progress depends on us using less data, not more",2021-02-13 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

In the data science community, we’re witnessing the beginnings of an infodemic — where more data becomes a liability rather than an asset. We’re continuously moving towards ever more data-hungry and more computationally expensive state-of-the-art AI models. And that is going to result in some detrimental and perhaps counter-intuitive side-effects (I’ll get to those shortly).

To avoid serious downsides, the data science community has to start working with some self-imposed constraints: specifically, more limited data and compute resources.

A minimal-data practice will enable several AI-driven industries — including cyber security, which is my own area of focus — to become more efficient, accessible, independent, and disruptive.

When data becomes a curse rather than a blessing

Before we go any further, let me explain the problem with our reliance of increasingly data-hungry AI algorithms. In simplistic terms, AI-powered models are “learning” without being explicitly programed to do so, through a trial and error process that relies on an amassed slate of samples. The more data points you have – even if many of them seem indistinguishable to the naked eye, the more accurate and robust AI-powered models you should get, in theory.

In search of higher accuracy and low false-positive rates, industries like cyber security — which was once optimistic about its ability to leverage the unprecedented amount of data that followed from enterprise digital transformation — are now encountering a whole new set of challenges:

1. AI has a compute addiction. The growing fear is that new advancements in experimental AI research, which frequently require formidable datasets supported by an appropriate compute infrastructure, might be stemmed due to compute and memory constraints, not to mention the financial and environmental costs of higher compute needs.

While we may reach several more AI milestones with this data-heavy approach, over time, we’ll see progress slow. The data science community’s tendency to aim for data-“insatiable” and compute-draining state-of-the-art models in certain domains (e.g. the NLP domain and its dominant large-scale language models) should serve as a warning sign. OpenAI analyses suggest that the data science community is more efficient at achieving goals that have already been obtained but demonstrate that it requires more compute, by a few orders of magnitude, to reach new dramatic AI achievements. MIT researchers estimated that “three years of algorithmic improvement is equivalent to a 10 times increase in computing power.” Furthermore, creating an adequate AI model that will withstand concept-drifts over time and overcome “underspecification” usually requires multiple rounds of training and tuning, which means even more compute resources.

If pushing the AI envelope means consuming even more specialized resources at greater costs, then, yes, the leading tech giants will keep paying the price to stay in the lead, but most academic institutions would find it difficult to take part in this “high risk – high reward” competition. These institutions will most likely either embrace resource-efficient technologies or persue adjacent fields of research. The significant compute barrier might have an unwarranted cooling effect on academic researchers themselves, who might choose to self-restrain or completely refrain from persuing revolutionary AI-powered advancements.

2. Big data can mean more spurious noise. Even if you assume you have properly defined and designed an AI model’s objective and architecture and that you have gleaned, curated, and adequately prepared enough relevant data, you have no assurance the model will yield beneficial and actionable results. During the training process, as additional data points are consumed, the model might still identify misleading spurious correlations between different variables. These variables might be associated in what seems to be a statistically significant manner, but are not causally related and so don’t serve as useful indicators for prediction purposes.

I see this in the cyber security field: The industry feels compelled to take as many features as possible into account, in the hope of generating better detection and discovery mechanisms, security baselines, and authentication processes, but spurious correlations can overshadow the hidden correlations that actually matter.

3. We’re still only making linear progress. The fact that large-scale data-hungry models perform very well under specific circumstances, by mimicking human-generated content or surpassing some human detection and recognition capabilities, might be misleading. It might obstruct data practitioners from realizing that some of the current efforts in applicative AI research are only extending existing AI-based capabilities in a linear progression rather than producing real leapfrog advancements — in the way organizations secure their systems and networks, for example.

Unsupervised deep learning models fed on large datasets have yielded remarkable results over the years — especially through transfer learning and generative adversarial networks (GANs). But even in light of progress in neuro-symbolic AI research, AI-powered models are still far from demonstrating human-like intuition, imagination, top-down reasoning, or artificial general intelligence (AGI) that could be applied broadly and effectively on fundamentally different problems — such as varying, unscripted, and evolving security tasks while facing dynamic and sophisticated adversaries.

4. Privacy concerns are expanding. Last but not least, collecting, storing, and using extensive volumes of data (including user-generated data) — which is especially valid for cyber security applications — raises a plethora of privacy, legal, and regulatory concerns and considerations. Arguments that cyber security-related data points don’t carry or constitute personally identifiable information (PII) are being refuted these days, as the strong binding between personal identities and digital attributes are extending the legal definition PII to include, for example, even an IP address.

How I learned to stop worrying and enjoy data scarcity



In order to overcome these challenges, specifically in my area, cyber security, we have to, first and foremost, align expectations.

The unexpected emergence of Covid-19 has underscored the difficulty of AI models to effectively adapt to unseen, and perhaps unforeseeable, circumstances and edge-cases (such as a global transition to remote work), especially in cyberspace where many datasets are naturally anomalous or characterized by high variance. The pandemic only underscored the importance of clearly and precisely articulating a model’s objective and adequately preparing its training data. These tasks are usually as important and labor-intensive as accumulating additional samples or even choosing and honing the model’s architecture.

These days, the cyber security industry is required to go through yet another recalibration phase as it comes to terms with its inability to cope with the “data overdose,” or infodemic, that has been plaguing the cyber realm. The following approaches can serve as guiding principles to accelerate this recalibration process, and they’re valid for other areas of AI, too, not just cyber security:

Algorithmic efficacy as top priority. Taking stock of the plateauing Moore’s law, companies and AI researchers are working to ramp-up algorithmic efficacy by testing innovative methods and technologies, some of which are still in a nascent stage of deployment. These approaches, which are currently applicable only to specific tasks, range from the application of Switch Transformers, to the refinement of Few Shots, One-Shot, and Less-Than-One-Shot Learning methods.

Human augmentation-first approach. By limiting AI models to only augment the security professional’s workflows and allowing human and artificial intelligence to work in tandem, these models could be applied to very narrow, well-defined security applications, which by their nature require less training data. These AI guardrails could be manifested in terms of human intervention or by incorporating rule-based algorithms that hard-code human judgment. It is no coincidence that a growing number of security vendors favor offering AI-driven solutions that only augment the human-in-the-loop, instead of replacing human judgment all together.

Regulators could also look favorably on this approach, since they look for human accountability, oversight, and fail-safe mechanisms, especially when it comes to automated, complex, and “black box” processes. Some vendors are trying to find middle ground by introducing active learning or reinforcement learning methodologies, which leverage human input and expertise to enrich the underlining models themselves. In parallel, researchers are working on enhancing and refining human-machine interaction by teaching AI models when to defer a decision to human experts.

Leveraging hardware improvements. It’s not yet clear whether dedicated, highly optimized chip architectures and processors alongside new programming technologies and frameworks, or even completely different computerized systems, would be able to accommodate the ever-growing AI computation demand. Tailor-made for AI applications, some of these new technological foundations that closely bind and align specialized hardware and software, are more capable than ever of performing unimaginable volumes of parallel computations, matrix multiplications, and graph processing.

Additionally, purpose-built cloud instances for AI computation, federated learning schemes, and frontier technologies (neuromorphic chips, quantum computing, etc.) might also play a key role this effort. In any case, these advancements alone are not likely to curb the need for algorithmic optimization that might “outpace gains from hardware efficiency.” Still, they could prove to be critical, as the ongoing semiconductor battle for AI dominance has yet to produce a clear winner.

The merits of data discipline

Up to now, conventional wisdom in data science has usually dictated that when it comes to data, the more you have, the better. But we’re now beginning to see that the downsides of data-hungry AI models might, over time, outweigh their undisputed advantages.

Enterprises, cyber security vendors, and other data practitioners have multiple incentives to be more disciplined in the way they collect, store, and consume data. As I’ve illustrated here, one incentive that should be top of mind is the ability to elevate the accuracy and sensitivity of AI models while alleviating privacy concerns. Organizations that embrace this approach, which relies on data dearth rather than data abundance, and exercise self-restraint, may be better equipped to drive more actionable and cost-effective AI-driven innovation over the long haul.

Eyal Balicer is Senior Vice President for Global Cyber Partnership and Product Innovation at Citi.","['models', 'using', 'data', 'depends', 'cyber', 'learning', 'ai', 'progress', 'science', 'compute', 'security', 'human', 'training']","In the data science community, we’re witnessing the beginnings of an infodemic — where more data becomes a liability rather than an asset.
To avoid serious downsides, the data science community has to start working with some self-imposed constraints: specifically, more limited data and compute resources.
The merits of data disciplineUp to now, conventional wisdom in data science has usually dictated that when it comes to data, the more you have, the better.
But we’re now beginning to see that the downsides of data-hungry AI models might, over time, outweigh their undisputed advantages.
Enterprises, cyber security vendors, and other data practitioners have multiple incentives to be more disciplined in the way they collect, store, and consume data."
2,https://towardsdatascience.com/classification-in-security-operations-dc6f43adcae8,Classification in Security Operations,2021-02-13 19:32:04.075000+00:00,"Photo by qimono on Pixabay

Everyone in the cyber security industry is rushing to get their hands on the requisite artificial intelligence and machine learning required to get ahead of their attackers. While many cybersecurity companies certainly only employ AI/ML in bouts of buzzword salesmanship, most cyber security professionals do feel that AI/ML has its place in the security world.

The foundational problem in ML or any data analytics for that matter is classification. Security operations centers are constantly solving a set of classification issues. Given some set of input data, security analysts initially must determine if activity is malicious or non-malicious. This first issue is therefore binary classification. In the image below, I break this problem down a bit further. The red dots mark where classification occurs, and the yellow boxes mark the results of classification.

Incident Detection (Figure by author)

Classification Decision 1: Is this data useful for detection?

Security analysts love to have their logs and IDS alerts, but all data needs to be continuously scrutinized for relevance in incident detection. Security engineers not only have to consider removing useless sources of incident detection but also adding new, innovative data sources. Generally a human does not operate this part of the loop — SOCs get what data they can from networks and logs and hope it will be sufficient for them to do their jobs. AI/ML may therefore naturally fit here. Host computing power presents the major blocker; for if the host must intelligently determine which data may help detection in real time, it will use compute overhead to run such an algorithm. Whatever data that the system does choose to send will pass to the next classification decision.

Classification Decision 2: Is this an incident?

After deciding what data to pass to our intrusion detection system or systems, the intrusion detection system must decide whether or not the data characterizes malicious activity. This is a binary classification problem, so why three cases in the image above? A machine learning engineer will develop an algorithm to determine whether system data characterizes an incident based on input training data. This algorithm will return the probability of incident occurrence. The cases therefore represent thresholds in this returned probability. Analysts will have to determine the proper threshold to avoid alert fatigue and process the proper number of alerts per day. The case 1 threshold will be very high. Note that since incidents tend to be the less common occurrence, this may not be a threshold close to one — the threshold for case 1 could in fact be very close to zero, just greater than the next threshold. The threshold for case 2 will still be high, but it will generate several alerts. This will enable to analyst to read through the data which generated the alert and make her decision on how to classify the event. The loop shown with the SIEM feeding back the decision point represents this iterative process.

Incident Investigation and Response

The NIST Cybersecurity Framework defines a cybersecurity incident as “a cybersecurity event that has been determined to have an impact on the organization prompting the need for response and recovery.” I never liked this definition and here we need something more concrete. I prefer a simpler definition: a cybersecurity incident is an event that an analyst decided to investigate further. This leads naturally to the first item from the NIST Cybersecurity Framework in Response Analysis: “Notifications from detection systems are investigated.” Incident investigation and response are also classification problems. The figure below displays three binary classification problems that analysts must solve to close an incident after generation.

Incident Investigation and Response (Figure by author)

Classification Decision 3: Sufficiency of Context Data

Classification again returns to quality of the provided data. Decision point 2 classified the incident based on data from decision point one, but this data was focused on detection and not investigation. In other words, we just needed to answer the question: is this anomalous behavior that we should look into? Now we need to answer: what exactly happened, and how can we fix it? The analyst will likely need to query additional data sources in order to answer these questions. AI/ML can provide significant value here, for it can quickly determine what data may be useful to the analyst for investigation, based on the context data, and automatically add this data to the incident case object (data hydration) for further analyst review.

Classification Decision 4: True Incident

In the process of gathering additional context data, the analyst will frequently determine that this incident was a false positive. In this case, she will simply close the incident without finding. This error will always occur because context data is focused and massive. Moving this decision point back to decision point 2, thus requiring all possibly requisite context data to hit the IDS would overload networks and make incident detection a computationally impossible task. The two-step incident determination uses triaging in order to cut down on computation and networking requirements. The AI/ML algorithms in this step would therefore need to adapt to the inputs which the analyst adds to the context data. An algorithm could run continuously in the background or upon changes to the incident case file to alter incident likelihood values which were generated in classification decision 2.

Classification Decision 5: Mitigation Effectiveness

One day our AI overlords may automatically generate mitigations based on context data. For now, however, analysts must use their best judgement to develop mitigations upon investigation completion. Analysts use inductive reasoning and the scientific method rather than deductive reasoning. In other words, they have to use their best judgement to develop mitigations and validate them through test rather than a series of if-then statements about the incident to lead to the correct set of mitigation actions. Upon actual execution of the mitigation, the analyst will close the incident if the mitigation is successful. Analysts will develop automated tests which validate their implemented mitigations as they develop the mitigation. Since analysts already build these tests and they naturally emerge from mitigation development, AI/ML does not present significant value in this decision point.

Targeting Machine Learning and Analytics to the Problem

Those unfamiliar with security operations may think that a single ML algorithm could assist and eventually replace security analysts. While such a super-machine may exist one day, each of the five decision points above represents a unique problem with varying data sources and desired classification results. In fact, the images above and narrowing of security operations to five decisions are oversimplifications of the challenges of security operations. The value in this simplification lies in identifying where we can start using machine learning to make security operations more effective. Machine learning engineers should seek to implement classifications in some of these five decision points in order to assist, ease, and focus security operators.","['analyst', 'data', 'threshold', 'classification', 'incident', 'analysts', 'decision', 'detection', 'security', 'context', 'operations']","Security operations centers are constantly solving a set of classification issues.
Given some set of input data, security analysts initially must determine if activity is malicious or non-malicious.
Classification Decision 5: Mitigation EffectivenessOne day our AI overlords may automatically generate mitigations based on context data.
Targeting Machine Learning and Analytics to the ProblemThose unfamiliar with security operations may think that a single ML algorithm could assist and eventually replace security analysts.
In fact, the images above and narrowing of security operations to five decisions are oversimplifications of the challenges of security operations."
3,https://www.forbes.com/sites/hannahmayer/2021/02/14/from-mission-control-to-mission-connect-how-nasa-is-repositioning-itself-with-digital-transformation/,,,,,
4,https://www.finanzen.at/nachrichten/aktien/wellai-scientists-to-present-research-based-on-the-covid-19-research-tool-at-the-global-ifcc-conference-1030079873,WellAI Scientists to Present Research Based on the COVID-19 Research Tool at the Global IFCC Conference,,"SHERIDAN, Wyo., Feb. 13, 2021 /PRNewswire/ -- WellAI data scientists Sergei Polevikov and Daniel Satchkov are to present their COVID-19 research based on an advanced machine learning technology at the IFCC (International Federation of Clinical Chemistry and Laboratory Medicine) annual conference on February 15, 2021. The title of their talk is ""Comprehending Hundreds of Thousands of COVID-19 Studies using Mathematical Models of Language and AI"".

The WellAI COVID-19 tool is available at https://wellai.health/covid. This research behind this tool was published in a peer-reviewed article ""Artificial Intelligence-Powered Search Tools and Resources in the Fight Against COVID-19"" in the eJIFCC (electronic Journal of the IFCC) in June 2020. The article is available at https://pubmed.ncbi.nlm.nih.gov/32549878/.

Daniel Satchkov, the lead data scientist at WellAI, emphasized: ""The analytical tool we've developed processes an enormous amount of data, based on over 30 million medical studies. It is impossible for any human to know this much information. Our algorithms understand and sort medical concepts in the order of their relevance to other medical concepts, in this case – to COVID-19. With all the synonyms and correlated concepts, there is currently a total of 4,224,512 medical concepts in the WellAI database, 60,892 of which are specifically related to COVID. We have been able to resolve the so-called dimensionality problem in machine learning by assigning medical concepts to clusters (also known as factors). In fact, we have saved these clusters in a GitHub repository and will offer complimentary access to the repository to the IFCC conference participants.""

Sergei Polevikov, CEO of WellAI, explained: ""We are proud to make our contribution to the fight against COVID. The machine learning tool we have developed at WellAI is unique, as it can read and comprehend medical articles at human expert level or better, and certainly much faster. This tool could help researchers from many fields of medicine understand how COVID and other coronaviruses develop and how they affect human body, and what the possible side effects and consequences are. This knowledge will help us be better prepared for the next pandemic, should it ever happen. In addition, we have been working on a number of exciting AI applications in genomics research, as part of the international IFCC Working Group on Artificial Intelligence and Genomic Diagnostics (WG-AIGD). Daniel Satchkov and I have recently had an honor to join this prestigious group.""

For more information about WellAI, visit https://wellai.health or call 307-278-1819.

View original content:http://www.prnewswire.com/news-releases/wellai-scientists-to-present-research-based-on-the-covid-19-research-tool-at-the-global-ifcc-conference-301227976.html

SOURCE WellAI, LLC","['research', 'conference', 'satchkov', 'present', 'learning', 'based', 'covid19', 'global', 'scientists', 'ifcc', 'concepts', 'machine', 'wellai', 'tool', 'medical']","SHERIDAN, Wyo., Feb. 13, 2021 /PRNewswire/ -- WellAI data scientists Sergei Polevikov and Daniel Satchkov are to present their COVID-19 research based on an advanced machine learning technology at the IFCC (International Federation of Clinical Chemistry and Laboratory Medicine) annual conference on February 15, 2021.
The WellAI COVID-19 tool is available at https://wellai.health/covid.
Our algorithms understand and sort medical concepts in the order of their relevance to other medical concepts, in this case – to COVID-19.
We have been able to resolve the so-called dimensionality problem in machine learning by assigning medical concepts to clusters (also known as factors).
The machine learning tool we have developed at WellAI is unique, as it can read and comprehend medical articles at human expert level or better, and certainly much faster."
5,https://www.wsj.com/articles/ai-emerges-as-crucial-tool-for-groups-seeking-justice-for-syria-war-crimes-11613228401,,,,,
6,https://www.varesenews.it/2021/02/laboratory-automation-serving-scientific-research/1306918/,Laboratory automation serving scientific research,2021-02-13 18:55:42+01:00,,"['automation', 'research', 'scientific', 'serving', 'laboratory']",
7,https://aithority.com/technology/analytics/realpage-presents-new-data-reinforcing-the-value-of-ai-screening/,RealPage Presents New Data Reinforcing the Value of AI Screening,2021-02-13 10:44:23+00:00,"RealPage, Inc., a leading global provider of software and data analytics to the real estate industry, announced the full-year results of AI Screening, its AI-based resident screening solution. The results were published in a white paper available here. A key finding: AI Screening delivers an average savings of $39 per unit per year, proving that it pushes NOI higher by markedly reducing non-payment and lease default risks. The documented savings exceed early estimates of $31 per unit by 25%.

“The software is paying for itself many times over in bottom-line benefits.”

One national REIT reported savings of $1.1 million across its 52,000 units. Early lease terminations were reduced by 24% YOY, while skips and evictions were decreased by 38%. Smaller property managers reported similar savings in proportion with their portfolio sizes. Across 50,000 same-store leases representing 487 unique assets and 130,000 units, RealPage AI Screening also drove a 10.2% reduction in the average balance owed per move-in.

Recommended AI News: Cornerstone to Bring Learning into the Flow of Work, Powered by Microsoft Viva

RealPage AI Screening taps into a database of more than 30 million lease outcomes to more accurately predict “willingness to pay,” a more sophisticated screening method than the conventional credit and background check. The study did not quantify increased income due to higher occupancy resulting from the software’s ability to identify good renters who might be rejected by conventional screening software.

“These are just stunning numbers,” said Rich Hughes, SVP Data Science at RealPage. “We’re not only talking about reducing risk of default and its costs, which is what we measured. Many users are also going to hit higher occupancy rates by being able to say ‘yes’ to people they’d otherwise have turned away. They are also able to reduce the high costs of turns due to defaults, boost their renewal rates, lower their marketing spend and perhaps be able to nudge rents up. So, the payoff goes well beyond the numbers documented in the study.”

Recommended AI News: Accounts Payable Departments Lag in AI Automation, New Ephesoft Survey Finds

RealPage introduced AI Screening to beta clients in April of 2019, and to the general public a month later, giving RealPage over a year to compare against the legacy screening model without AI capabilities.

“We believe these impressive results are going to turn a lot of heads and get people to move away from their conventional screening model and transition to our AI solution,” concluded Hughes. “The software is paying for itself many times over in bottom-line benefits.”

Recommended AI News: Cornerstone to Bring Learning into the Flow of Work, Powered by Microsoft Viva","['savings', 'presents', 'able', 'data', 'value', 'ai', 'higher', 'conventional', 'realpage', 'reinforcing', 'software', 'results', 'lease', 'screening']","RealPage, Inc., a leading global provider of software and data analytics to the real estate industry, announced the full-year results of AI Screening, its AI-based resident screening solution.
A key finding: AI Screening delivers an average savings of $39 per unit per year, proving that it pushes NOI higher by markedly reducing non-payment and lease default risks.
Across 50,000 same-store leases representing 487 unique assets and 130,000 units, RealPage AI Screening also drove a 10.2% reduction in the average balance owed per move-in.
The study did not quantify increased income due to higher occupancy resulting from the software’s ability to identify good renters who might be rejected by conventional screening software.
“We believe these impressive results are going to turn a lot of heads and get people to move away from their conventional screening model and transition to our AI solution,” concluded Hughes."
8,https://microcapdaily.com/major-move-on-plyz-plyzer-technologies-plyzer-intelligence-ai-machine-learning-product-matching/130317/,Major Move on PLYZ (Plyzer Technologies) Plyzer Intelligence; (AI) Machine Learning & Product Matching,2021-02-14 06:03:51+00:00,"PLYZ (Plyzer Technologies Inc) is gearing up and heating up in recent months skyrocketing out of the triple zeroes and emerging well into pennyland with recent highs of $0.006 per share. The stock has been under heavy accumulation recently and volume has picked up substantially with PLYZ regularly trading several 100 million shares per day and topped $1.5 million in dollar volume on Friday alone. Microcapdaily first covered PLYZ on December 8 when the stock was triple zeroes; since than PLYZ has risen dramatically as a new era of penny stock speculators fueled by robinhood and its 100 million new trading accounts take on the bulletin boards. These are different times than just a few short years ago; now penny stocks such as TSNP can achieve a $6 billion plus market valuation and trade $375 million in dollar volume in a day on the bulletin boards. And TSNP has no stronger fundamentals than PLYZ has.

There is a lot to get excited about on PLYZ; PLYZ built Plyzer Intelligence; a tool built on artificial intelligence through internally developed algorithms, machine learning, geo-localization and product matching. It allows companies to apply business intelligence to their decisions through the simplification of big data. Brands and retailers can learn more about how their products, as well as their competitors’ products, are performing online. PLYZ was trading over $0.20 per share this time last year with no material changes to the Company. According to their filings the Company’s Spain subsidiary, Plyzer Spain s.l., now has 35 full time employees including 9 freelancers. Investors are anxiously awaiting PLYZ earnings announcement for the period ending March 30, 2020 set to be released on Tuesday, February 16.

PLYZ (Plyzer Technologies Inc) provides custom, real-time, cloud-based business intelligence solutions for brands to analyze critical online price and market data. Plyzer’s highly customizable dashboard enables country, regional and local sales, production and logistics operations to adapt to prevailing market conditions quickly. The Company’s technology is also being used to provide real-time price comparison reporting to the consumer market. These solutions are both driven by Plyzer’s proprietary artificial intelligence and machine learning technologies. Plyzer Technologies has offices in Barcelona, Spain and Toronto, Canada

The Company has four subsidiaries, Plyzer Corporation, Plyzer Technologies (Canada) Inc. Plyzer BlockChain Technologies Inc., and Plyzer Technologies Spain s.l. According to PLYZ last 10k the Company’s Spain subsidiary Plyzer Spain s.l., now has 35 full time employees including 9 freelancers working full time for Plyzer Spain. Further hiring plans will depend on the continued implementation of the current business strategy and if the required funds are raised. It is likely that for now, the Company will resort to hiring consultants as and when needed rather than employ people on a full-time basis to keep its operational costs at a minimum. All the development was outsourced to Lupama and other independent consultants until incorporation of Plyzer Spain s.l. However, currently, the Company has most of the development and commercialization work carried out in-house.

The Company has a consulting agreement with an independent Spanish Corporation, Lupama Producciones, S.L. Lupama’s key owner, Luis Pallares became the CEO of Plyzer Technologies (Canada) Inc. and Plyzer Corporation. Lupama is creating an artificial intelligence driven engine for price comparison for the Company, known as “Plyzer.” Lupama created Plyzer.com; a comparison engine for prices of over-the-counter medical products currently available in Spain and Canada. Users can search for the stores selling the products of their choosing focusing on the lowest prices but also by proximity. The prices of the products vary and are compared across more than 400 stores.

PLYZ built Plyzer Intelligence; a tool built on artificial intelligence through internally developed algorithms, machine learning, geo-localization and product matching. It allows companies to apply business intelligence to their decisions through the simplification of big data. Brands and retailers can learn more about how their products, as well as their competitors’ products, are performing online.

Microcapdaily first reported on PLYZ on December 8 when the stock was in the triple zeroes stating at the time: “Plyzer Technologies Inc (OTCMKTS: PLYZ) is making a powerful move up the charts in recent weeks trading billions of shares in the double zeroes and attracting legions of new shareholders who continue to accumulate and drive up the price. PLYZ was trading over $0.20 per share this time last year with no material changes to the Company. According to their filings the Company’s Spain subsidiary, Plyzer Spain s.l., now has 35 full time employees including 9 freelancers. Plyzer Spain subsidiary has made several announcements this year including signing Vitae Health Innovation, Global Life Sciences Company and Pharmaceutical Chiesi Spain as a new SAAS Customers for its business analytics platform, Plyzer intelligence. On November 13 PLYZ filed an 8k stating: “Management hopes to bring its filings current by early 2021.”

On December 16 PLYZ provided an update on deal with YYZBCN financing group and settlement with the original founder and other corporate matters. Management states: “With respect to the filing of our year-end March 31/2020, the company has been able to gather most of the information needed to complete the filing. With respect to the quarters ending June 30/2020 and September 30/2020, the company has been gathering the necessary information to carry out the filing. Due to restrictions and difficulties surrounding Covid 19, the company has not been in a position to complete the filings on time. In addition, the CEO of the company had Covid 19 and was unable to work for almost a month. When the Q’s and Ks are completed, they will be filed with the SEC and attached to the company’s website.

A settlement agreement was reached between the former CEO of Plyzer Spain, Luis Pallares, who was the original founder of the company and Plyzer Technologies/Plyzer Spain. The settlement averted litigation between the parties and ensured continuity of business within the operating company, Plyzer Spain. A cash payment of fifty thousand euros (50 k/Euros) plus twenty-four million (24 M) restricted shares of Plyzer Technologies will be made to Mr. Pallares. Mr. Pallares had previously returned twenty-four million shares to the treasury, so, the new shares being issued only represent what he originally had as a founding shareholder. Mr. Pallares no longer has any involvement in the company but has agreed to work with Plyzer Spain to resolve any issues that YYZBCN Inc. may encounter. The company wishes Mr. Pallares the best of success in his new ventures.

Investor sentiment on PLYZ is high:

$PLYZ thanks DJrob @jaggeddeath and RelativEdge Trading @relativ_edge for the updates on Plyzer Technologies! Follow both traders for great content and updates! https://t.co/aYsnje76yC — Hi-Velocity Trading (@HV_Trader) February 14, 2021

To Find out the inside Scoop on PLYZ Subscribe to Microcapdaily.com Right Now by entering your Email in the box below

Further to the previous announcement concerning YYZBCN Inc.’s (YYZ) financing of Plyzer Spain, YYZ has now advanced approximately EUR 960 K in senior equity-linked notes. YYZ has now elected to convert part of their loans into common shares of Plyzer Spain representing 51 % of the company. As such, YYZ now controls the company and the ultimate direction of Plyzer Spain. As previously announced, YYZ has the right to finance up to US $2 M in senior equity-linked notes, that should the loans be converted, would end up in the ownership of 82 % of the equity of Plyzer Spain. Plyzer has not participated in the financing of the recent notes, as per the agreement between the parties, as the company does not currently have any excess capital on hand. Plyzer Spain is the operating company based in Barcelona, Spain that has developed and operates the ongoing business of Plyzer Intelligence, a SAS based data analytics platform. Although the company has paying customers and has cut costs where possible, the company is still operating at a deficit. It hopes to expand its offering into various verticals as well as expand globally (outside of Spain). With respect to the apps that had been developed for the purpose of price comparison, including the cannabis version, they are currently not being further developed nor likely to be continued. Plyzer Spain owns the rights to the apps and any intellectual property attached.

With respect to the status of the outstanding convertible debt held by various funds in Plyzer, the face amount of approximately US $1.3 M has changed little since the last release. The company is in contact with the debt holders and hopes that at some time an organized plan to deal with the debt can be figured out. All of the funds are based in the United States.

At this time, Plyzer has not engaged any firm for the purpose of public or investor relations. The Twitter feed that the company previously had is not active. Any published “newsletters’ that appear on various sites through the internet has not been organized or paid for by Plyzer in any way. At this time, Plyzer knows of no pending deals with any other companies for the purpose of financing or joint ventures other than the deal announced with YYZ. As the company does not have any available capital, there is no plan to execute any form of a stock buyback. YYZBCN Inc. is a Toronto, Ontario, Canada based private company.

For More on PLYZ Subscribe Right Now!

PLYZ is gearing up and heating up in recent months skyrocketing out of the triple zeroes and emerging well into pennyland with recent highs of $0.006 per share. The stock has been under heavy accumulation recently and volume has picked up substantially with PLYZ regularly trading several 100 million shares per day and topped $1.5 million in dollar volume on Friday alone. Microcapdaily first covered PLYZ on December 8 when the stock was triple zeroes; since than PLYZ has risen dramatically as a new era of penny stock speculators fueled by robinhood and its 100 million new trading accounts take on the bulletin boards. These are different times than just a few short years ago; now penny stocks such as TSNP can achieve a $6 billion plus market valuation and trade $375 million in dollar volume in a day on the bulletin boards. And TSNP has no stronger fundamentals than PLYZ has. There is a lot to get excited about on PLYZ; PLYZ built Plyzer Intelligence; a tool built on artificial intelligence through internally developed algorithms, machine learning, geo-localization and product matching. It allows companies to apply business intelligence to their decisions through the simplification of big data. Brands and retailers can learn more about how their products, as well as their competitors’ products, are performing online. PLYZ was trading over $0.20 per share this time last year with no material changes to the Company. According to their filings the Company’s Spain subsidiary, Plyzer Spain s.l., now has 35 full time employees including 9 freelancers. Investors are anxiously awaiting PLYZ earnings announcement for the period ending March 30, 2020 set to be released on Tuesday, February 16. We will be updating on PLYZ on a daily basis so make sure you are subscribed to microcapdaily.com so you know what is going on with PLYZ.

Disclosure: we hold no position in PLYZ either long or short and we have not been compensated for this article.","['plyzer', 'matching', 'learning', 'product', 'ai', 'spain', 'trading', 'products', 'stock', 'machine', 'major', 'technologies', 'million', 'intelligence', 'plyz', 'company']","There is a lot to get excited about on PLYZ; PLYZ built Plyzer Intelligence; a tool built on artificial intelligence through internally developed algorithms, machine learning, geo-localization and product matching.
Plyzer Technologies has offices in Barcelona, Spain and Toronto, CanadaThe Company has four subsidiaries, Plyzer Corporation, Plyzer Technologies (Canada) Inc. Plyzer BlockChain Technologies Inc., and Plyzer Technologies Spain s.l.
According to PLYZ last 10k the Company’s Spain subsidiary Plyzer Spain s.l., now has 35 full time employees including 9 freelancers working full time for Plyzer Spain.
PLYZ built Plyzer Intelligence; a tool built on artificial intelligence through internally developed algorithms, machine learning, geo-localization and product matching.
There is a lot to get excited about on PLYZ; PLYZ built Plyzer Intelligence; a tool built on artificial intelligence through internally developed algorithms, machine learning, geo-localization and product matching."
9,https://www.dailystar.co.uk/news/latest-news/real-life-minority-report-ai-23496623,Real-life Minority Report as AI set to 'read emotions' to stop crime,2021-02-13 22:03:48+00:00,"The video will auto-play soon 8 Cancel

The Daily Star's FREE newsletter is spectacular! Sign up today for the best stories straight to your inbox Sign up today! Thank you for subscribing We have more newsletters Show me See our privacy notice Invalid Email

AI can now read human emotions and use 5G to foil terror attacks and crimes before they happen.

Scientists have unveiled an early version of the real-life Minority Report technology in a new study.

The 2002 Steven Spielberg sci-fi starred Tom Cruise and saw officers able to arrest murderers before they commit their crimes.

Experts previously told the Daily Star the next generation of mobile internet will allow pretty much everything to connect to the web.

It is expected to bring self-driving cars, revolutionise healthcare and city infrastructure and even let your coffee machine know you're waking up.

Now South Korean researchers have reportedly created an AI system that can in theory tap into 5G networks to ""detect human emotions"" across entire cities.

(Image: Internet Unknown)

They said their AI-based, 5G-integrated virtual emotion recognition system called 5G-I-VEmoSYS can recognise joy, pleasure, a neutral state, sadness, and anger.

It can then create a ""virtual emotional map"" that can in theory be sent to police and alert smartphones in the area.

Project leader Professor Hyunbum Kim from Incheon National University said: ""Emotions are a critical characteristic of human beings and separates humans from machines, defining daily human activity.

(Image: Getty Images)

Piers for PM? Have your say! The Daily Star has projected an image of Piers Morgan onto the Houses of Parliament – and we’re asking you are we ready for #PMFORPM? Last week the Daily Star jokingly asked the question on nobody’s lips: Would PM be a better PM than our current PM? And it seems we may have inadvertently unleashed a monster. Shy and retiring Piers – no stranger to holding our politicians to account on Good Morning Britain - leapt on the idea. His odds were slashed from 500/1 to just 20/1 in a single day. So, are you Team Piers or Team Bozo? Take part in our survey!

""However, some emotions can also disrupt the normal functioning of a society and put people's lives in danger, such as those of an unstable driver.

""Emotion detection technology thus has great potential for recognising any disruptive emotion and in tandem with 5G and beyond-5G communication, warning others of potential dangers.""

The report from Eurekalert adds: ""Furthermore, when a serious emotion, such as anger or fear, is detected in a public area, the information is rapidly conveyed to the nearest police department or relevant entities who can then take steps to prevent any potential crime or terrorism threats.""

(Image: Unknown)

But the study, published in IEEE Network, is already warning of the ""serious security issues"" of hackers tampering with the AI.

It can send false alarms and is said to be vulnerable to illegal signal tampering, abuse of anonymity and ""hacking-related cyber-security threats"".

The system doesn't reveal the face or private parts of anyone it scans in public and subjects can supposedly be anonymised in private areas like homes.

Prof Kim added: ""This is only an initial study. In the future, we need to achieve rigorous information integrity and accordingly devise robust AI-based algorithms that can detect compromised or malfunctioning devices and offer protection against potential system hacks.

""Only then will it enable people to have safer and more convenient lives in the advanced smart cities of the future.""","['potential', 'crime', 'star', 'warning', 'piers', 'set', 'ai', 'daily', 'emotion', 'system', 'read', 'minority', 'emotions', 'human', 'reallife', 'stop', '5g', 'report']","Thank you for subscribing We have more newsletters Show me See our privacy notice Invalid EmailAI can now read human emotions and use 5G to foil terror attacks and crimes before they happen.
Experts previously told the Daily Star the next generation of mobile internet will allow pretty much everything to connect to the web.
Now South Korean researchers have reportedly created an AI system that can in theory tap into 5G networks to ""detect human emotions"" across entire cities.
Project leader Professor Hyunbum Kim from Incheon National University said: ""Emotions are a critical characteristic of human beings and separates humans from machines, defining daily human activity.
Last week the Daily Star jokingly asked the question on nobody’s lips: Would PM be a better PM than our current PM?"
10,https://analyticsindiamag.com/how-machine-learning-streamlines-risk-management/,How Machine Learning Streamlines Risk Management,2021-02-13 07:30:00+00:00,"It is essential for us to establish the rigorous governance processes and policies that can quickly identify when the model begins to fail, said Abhaya K Srivastava, SVP at Northern Trust Corporation, at Machine Learning Developers Summit 2021. In the first talk of Day 1, Srivastava delved into how different sectors including financial, healthcare, retail etc use emerging technologies like AI and machine learning. He talked about the use of machine learning in risk management, such as the techniques used in risk analytics across risk categories (credit/market/operational and PPNR), regulatory view on machine learning models as well as challenges for banks using ML.

The second part of the session covered related governance and policies in machine learning models. Srivastava stated, “The terms of AI are not new but businesses and organisations have started using these technologies in a different way. We have noticed the influence of machine learning in business applications, ML is playing an important role in risk management and there has been a constant focus on how risks are being detected, reported, managed, etc. Specifically, if you look at the model building, validation, audit and governance, the focus is a lot more.”

He added, “There is a lot of pressure from the regulators and senior management to predict with accuracy, but by following the regulations and policies”

The speaker mentioned that a large number of areas in risk management have significantly benefited from machine learning techniques.

Srivastava then deliberated on the techniques of machine learning. He said machine learning algorithms like neural networks, support vector machines and random forests are widely used in simple analytical tasks. He also explained how to create a machine learning model that provides maximum accuracy and is robust in nature.

The speaker concluded the talk by discussing the importance of regulatory policies and guidelines for machine learning models. Srivastava said, “We need model governance because machine learning models learn and enable better accuracy as well as the ability to predict, but there involves an increase in the risk when someone uses a different source of data.”

Subscribe to our Newsletter

You can write for us and be one of the 500+ experts who have contributed stories at AIM. Share your nominations here.

Get the latest updates and relevant offers by sharing your email.","['models', 'management', 'model', 'learning', 'different', 'policies', 'governance', 'machine', 'streamlines', 'srivastava', 'risk']","He talked about the use of machine learning in risk management, such as the techniques used in risk analytics across risk categories (credit/market/operational and PPNR), regulatory view on machine learning models as well as challenges for banks using ML.
The second part of the session covered related governance and policies in machine learning models.
He said machine learning algorithms like neural networks, support vector machines and random forests are widely used in simple analytical tasks.
He also explained how to create a machine learning model that provides maximum accuracy and is robust in nature.
The speaker concluded the talk by discussing the importance of regulatory policies and guidelines for machine learning models."
11,http://www.tribtown.com/2021/02/13/ap-hkn-nhl-tech-upgrade/,,,,,
12,https://www.analyticsinsight.net/unfortunately-commercial-ai-is-failing-heres-why/,,,,,
13,https://indiaeducationdiary.in/artificial-intelligence-for-dementia-prevention/,Artificial intelligence for dementia prevention,2021-02-13 11:08:55+00:00,"AI-Mind will create intelligent digital tools for screening of brain connectivity and dementia risk estimation in people affected by mild cognitive impairment. During the project’s 5-year lifecycle, two new artificial intelligence-based digital tools will be developed. The AI-Mind Connector will identify dysfunctional brain networks, and the AI-Mind Predictor will assess individual dementia risk using data from the Connector, advanced cognitive tests, and genetic biomarkers. These two tools aim at creating personalized patient reports for further intervention recommendations.

Aalto university and HUS have well-established experience with both magnetoencephalography (MEG) brain imaging, and using Artificial Intelligence to analyse imaging data. ‘We have incorporated computational models and machine learning methods in our neuroimaging research, to help us understand neural mechanisms and representations of cognition, as well as individual variation in neural function and behaviour.’ explains Riitta Salmelin, professor in the department of Neuroscience and Biomedical Engineering (NBE) at Aalto University. ‘Together with professors Samuel Kaski and Hanna Renvall, we have succeeded in identifying simple spectral features of MEG signals that differentiate between individuals, and appear stable within an individual. We anticipate that this tool may serve as an easy-to-use, robust probe of brain networks and their breakdown in neurological diseases, thus linking directly with the goals of AI Mind.’

Dementia affects tens of million worldwide

Currently, there are over 50 million people living with dementia across the globe. By 2030 we can expect that number to reach 82 million. Besides time-consuming patient investigations with low discriminative power for dementia risk, current treatment options focus on late symptom management. This has numerous implications in terms of familial, medical, and care costs.

‘Our research has revealed individually highly distinctive features, “brain fingerprints”, in healthy subjects, and we believe such approaches to have also wide clinical potential.’ says Hanna Renvall, who holds a joint professorship at both HUS and Aalto University NBE, ‘I have also worked for years as a clinical neurologist, and thus I recognize the huge challenges related to the increasing burden of memory disorders both at the individual and societal level.’ AI Mind provides an intriguing possibility to bring together both the research and clinical neurological work that I have been involved with.’

AI-Mind enables earlier preventative therapies

For people with mild cognitive impairment (MCI), the dementia risk is almost 30% higher than unaffected individuals. With the current clinical approaches, many patients developing into dementia receive their diagnosis only rather late in the course of disease. The risk of dementia could, however, be reduced by adopting healthy lifestyle habits and managing treatable conditions such as diabetes and high blood pressure. Thanks to the AI-Mind tools, the time needed to estimate the risk of developing clinical dementia could be potentially reduced down to only one week. This would give doctors and patients opportunities for preventive interventions, therapies, and rehabilitation measures early in the course of the disease.

‘Machine learning can provide us with great insights into complex data.’ says Samuel Kaski, professor at Aalto University Department of Computer Science and director of the Finnish Center for Artificial Intelligence (FCAI), ‘Our slogan at FCAI is that we create ‘Real AI for Real People in the Real World’ and our work with AI Mind is an example of this. We’re bringing together engineers and scientists with expertise and experience across a number of fields to develop viable solutions for the next generation of medicine.’

The doctor’s new best friend: AI

What is now complex, labour-intensive, costly, and poorly predictive screening in mild cognitive impairment (MCI) shall be replaced by automated diagnostic screening tools. These are driven by artificial intelligence to address the urgent need for early accurate prediction of disease risk.

‘AI-Mind is an excellent chance to bring together highly relevant questions with top-level clinical, neuroscience, and computational expertise at the European level, ’ Professor Renvall continues, ‘I believe that the data collected and analysed within this consortium will inform us about the most informative ways to approach other clinical neurological questions, and pave the way for more future studies and new lines of research.’

More information

The Norwegian coordinated AI-Mind project has received substantial funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No 964220. AI-Mind is a five-year Research and Innovation Action (RIA) that officially starts in March 2021, with a budget of EUR14 million.

Fifteen project partners, from eight European countries, including academic institutions, medical centres, SMEs and patient organizations, make up The AI-Mind consortium: Tallinn University from Estonia, Aalto University and Helsinki University Hospital from Finland, Oslo University Hospital, BrainSymph AS, DNV-GL, and Oslo Metropolitan University from Norway, Scientific Institute for Research, Hospitalization and Healthcare, San Raffaele Pisana, Neuroconnect Srl, Università Cattolica del Sacro Cuore from Italy, Radboud University Medical Center from the Netherlands, Alzheimer Europe from Luxembourg, Complutense University of Madrid and Lurtis Rules from Spain, and accelopment Schweiz AG from Switzerland.

AI-Mind is a partner project of DigitalLife Norway. Supporting organizations: CLAIRE and NORA.","['artificial', 'research', 'university', 'data', 'brain', 'clinical', 'aimind', 'prevention', 'tools', 'individual', 'intelligence', 'dementia', 'risk']","AI-Mind will create intelligent digital tools for screening of brain connectivity and dementia risk estimation in people affected by mild cognitive impairment.
The AI-Mind Connector will identify dysfunctional brain networks, and the AI-Mind Predictor will assess individual dementia risk using data from the Connector, advanced cognitive tests, and genetic biomarkers.
Aalto university and HUS have well-established experience with both magnetoencephalography (MEG) brain imaging, and using Artificial Intelligence to analyse imaging data.
Besides time-consuming patient investigations with low discriminative power for dementia risk, current treatment options focus on late symptom management.
Thanks to the AI-Mind tools, the time needed to estimate the risk of developing clinical dementia could be potentially reduced down to only one week."
14,https://towardsdatascience.com/introducing-transformers-interpret-explainable-ai-for-transformers-890a403a9470,Introducing Transformers Interpret — Explainable AI for Transformers,2021-02-13 15:07:11.003000+00:00,"Introducing Transformers Interpret — Explainable AI for Transformers

Photo by Emily Morter on Unsplash

TL:DR: Transformers Interpret brings explainable AI to the transformers package with just 2 lines of code. It allows you to get word attributions and visualizations for those attributions simply. Right now the package supports all transformer models with a sequence classification head. Check out the project here: https://github.com/cdpierse/transformers-interpret

Introduction

Model explainability and state-of-the-art research seem to constantly be engaged in a tug of war. Complex models are difficult to interpret and simple models are often much easier to understand.

Explainable AI is one of the most vital emerging subfields in machine learning today. For both ethical, practical, and - in the corporate world — legal reasons we must be able to explain the inner workings of so called “black box” models, it’s vital to the long term success of AI’s adoption into our lives.

Research in explainable AI is ongoing and there are a number of fantastic projects and groups out there doing work. The library on which transformers interpret is built is called Captum which is a package designed for model interpretability in pytorch. What I love about Captum is that it effectively combines all the leading research in explainable AI into a single package, and of course it is optimized for pytorch which is a big plus. While transformers interpret is focused solely on NLP, Captum is multi modal and works with text, computer vision, and tabular data. The team at Captum have also done a fantastic job giving write ups of all the algorithms they are using throughout the package, it’s certainly worth checking out if you are interested in understanding the internals of transformers interpret.

Getting Started With Transformers Interpret

Much like the design philosophy behind Hugging Faces Transformers package transformers interpret was designed with ease of use at the forefront. It is opinionated in its selection of attribution methods and how it summarizes attributions. All of this allows end users to get word attributions and visualizations for their model’s output in just 2 lines of code. Right now the package has support for all models with sequence classification heads — so (should) work for all classification models. Support for question answering models and NER models is planned.

Let’s jump into a hands on example. To install the package:

pip install transformers-interpret

With the package installed we are going to start by instantiating a transformers model and tokenizer. I’ve chosen to go with distilbert-base-uncased-finetuned-sst-2-english it’s a distilbert model finetuned on a sentiment analysis task, I chose this model mostly because it is popular and lightweight compared to some of the other behemoths out there.

from transformers import AutoModelForSequenceClassification, AutoTokenizer model_name = ""distilbert-base-uncased-finetuned-sst-2-english"" model = AutoModelForSequenceClassification.from_pretrained(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name)

With both the model and tokenizer in hand we can now go about creating an explainer, and it’s pretty simple:

from transformers_interpret import SequenceClassificationExplainer

cls_explainer = SequenceClassificationExplainer(""I love you, I like you"", model, tokenizer)

That’s all it takes. To get the attributions for the sentence “I love you, I like you” we simply call:

attributions = cls_explainer()

If we want to know which class was predicted:

>>> cls_explainer.predicted_class_name 'POSITIVE'

To see the raw numeric attributions:

>>> attributions.word_attributions [('BOS_TOKEN', 0.0), ('I', 0.46820529249283205), ('love', 0.46061853275727177), ('you', 0.566412765400519), (',', -0.017154456486408547), ('I', -0.053763869433472), ('like', 0.10987746237531228), ('you', 0.48221682341218103), ('EOS_TOKEN', 0.0)]

This is interesting we can see that the model is placing a significant amount of attention on “I love you” while “I like you” is not as important. Given how self-attention works this seems to fit with the intuition that the word “love” is increasing the importance of the words surrounding it’s context.

The numeric attributions can be difficult to read especially for much longer sentences. The package also has an inbuilt visualize method built on top of Captums’ visualization to give easy to interpret visual explanations of word attributions.

If you are working in a jupyter notebook calling the method will display the visualization in-line, if you are running from a script simply pass a html filename and you can view the output file in your browser.

cls_explainer.visualize(""distilbert_example.html"")

This will produce an output that looks like this:","['models', 'attributions', 'word', 'transformers', 'model', 'explainable', 'ai', 'package', 'interpret', 'love', 'introducing']","Introducing Transformers Interpret — Explainable AI for TransformersPhoto by Emily Morter on UnsplashTL:DR: Transformers Interpret brings explainable AI to the transformers package with just 2 lines of code.
Explainable AI is one of the most vital emerging subfields in machine learning today.
Research in explainable AI is ongoing and there are a number of fantastic projects and groups out there doing work.
While transformers interpret is focused solely on NLP, Captum is multi modal and works with text, computer vision, and tabular data.
Getting Started With Transformers InterpretMuch like the design philosophy behind Hugging Faces Transformers package transformers interpret was designed with ease of use at the forefront."
15,https://micky.com.au/ai-can-now-learn-to-manipulate-human-behaviour/,AI can now learn to manipulate human behaviour,2021-02-13 12:18:44+00:00,"Artificial intelligence (AI) is learning more about how to work with (and on) humans. A recent study has shown how AI can learn to identify vulnerabilities in human habits and behaviours and use them to influence human decision-making.

It may seem cliched to say AI is transforming every aspect of the way we live and work, but it’s true. Various forms of AI are at work in fields as diverse as vaccine development, environmental management and office administration. And while AI does not possess human-like intelligence and emotions, its capabilities are powerful and rapidly developing.

There’s no need to worry about a machine takeover just yet, but this recent discovery highlights the power of AI and underscores the need for proper governance to prevent misuse.

How AI can learn to influence human behavior

A team of researchers at CSIRO’s Data61, the data and digital arm of Australia’s national science agency, devised a systematic method of finding and exploiting vulnerabilities in the ways people make choices, using a kind of AI system called a recurrent neural network and deep reinforcement-learning. To test their model they carried out three experiments in which human participants played games against a computer.

The first experiment involved participants clicking on red or blue colored boxes to win a fake currency, with the AI learning the participant’s choice patterns and guiding them towards a specific choice. The AI was successful about 70% of the time.

In the second experiment, participants were required to watch a screen and press a button when they are shown a particular symbol (such as an orange triangle) and not press it when they are shown another (say a blue circle). Here, the AI set out to arrange the sequence of symbols so the participants made more mistakes, and achieved an increase of almost 25%.

The third experiment consisted of several rounds in which a participant would pretend to be an investor giving money to a trustee (the AI). The AI would then return an amount of money to the participant, who would then decide how much to invest in the next round. This game was played in two different modes: in one the AI was out to maximize how much money it ended up with, and in the other the AI aimed for a fair distribution of money between itself and the human investor. The AI was highly successful in each mode.

In each experiment, the machine learned from participants’ responses and identified and targeted vulnerabilities in people’s decision-making. The end result was the machine learned to steer participants towards particular actions.

What the research means for the future of AI

These findings are still quite abstract and involved limited and unrealistic situations. More research is needed to determine how this approach can be put into action and used to benefit society.

But the research does advance our understanding not only of what AI can do but also of how people make choices. It shows machines can learn to steer human choice-making through their interactions with us.

The research has an enormous range of possible applications, from enhancing behavioral sciences and public policy to improve social welfare, to understanding and influencing how people adopt healthy eating habits or renewable energy. AI and machine learning could be used to recognize people’s vulnerabilities in certain situations and help them to steer away from poor choices.

The method can also be used to defend against influence attacks. Machines could be taught to alert us when we are being influenced online, for example, and help us shape a behavior to disguise our vulnerability (for example, by not clicking on some pages, or clicking on others to lay a false trail).

What’s next?

Like any technology, AI can be used for good or bad, and proper governance is crucial to ensure it is implemented in a responsible way. Last year CSIRO developed an AI Ethics Framework for the Australian government as an early step in this journey.

AI and machine learning are typically very hungry for data, which means it is crucial to ensure we have effective systems in place for data governance and access. Implementing adequate consent processes and privacy protection when gathering data is essential.

Organizations using and developing AI need to ensure they know what these technologies can and cannot do, and be aware of potential risks as well as benefits.

Images used courtesy of Pexels/Pixabay

This article is republished from The Conversation under a Creative Commons license. Read the original article.","['behaviour', 'participants', 'research', 'vulnerabilities', 'used', 'experiment', 'learning', 'ai', 'manipulate', 'machine', 'human', 'money', 'learn']","Artificial intelligence (AI) is learning more about how to work with (and on) humans.
A recent study has shown how AI can learn to identify vulnerabilities in human habits and behaviours and use them to influence human decision-making.
To test their model they carried out three experiments in which human participants played games against a computer.
AI and machine learning could be used to recognize people’s vulnerabilities in certain situations and help them to steer away from poor choices.
AI and machine learning are typically very hungry for data, which means it is crucial to ensure we have effective systems in place for data governance and access."
16,https://techcrunch.com/2021/02/13/9-investors-discuss-challenges-opportunities-and-the-impact-of-cloud-vendors-in-enterprise-data-lakes/,TechCrunch is now a part of Verizon Media,2021-02-13 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
17,https://analyticsindiamag.com/a-birds-eye-view-on-use-of-ai-ml-in-airlines-industry/,A Bird’s Eye-View On Use Of AI & ML In Airlines Industry,2021-02-13 10:30:00+00:00,"The advent of many low-cost airlines across the world in the early 2000s fuelled the business transformation of the overall industry, said Rahul Chogle, Head of Data and Analytics at SpiceJet, during his talk at the third edition of MLDS 2021.

The keynote speaker started the session by providing a quick overview of the airline industry and how SpiceJet started using emerging technologies like AI and machine learning almost three years ago. The session gave an insider’s perspective on airline business and the role of AI and machine learning in facilitating a smooth travel experience.

The airline industry has embraced data science and machine learning-based decision making with a full heart. He said, “At SpiceJet, our journey started with a focus on maximising revenue. A collaborative effort with key stakeholders, and with the guidance of executive management, we focused on multiple facets of commercial initiatives to improve decision making.” He added, “Currently, we are also looking into potential cost initiatives that could lead to significant savings for the internal value chain.”

Chogle also shed light on the “under the hood” factors shaping the industry, such as compliance, ATF costs, etc. He then gave a detailed explanation on the functional map of the airline that includes two categories- revenue and operations along with other support functions, such as financial, legal, admin, training, etc. While revenue is all about growing in the network, selling seats at the highest possible price,etc, operations include everything related to the airports.

The speaker mentioned some of the core airline functions to maximise revenues, minimise costs and provide a differentiated customer experience. The core functions include:

Revenue Management: Generate, sustain, match demand for optimal revenue with relevant pricing

Generate, sustain, match demand for optimal revenue with relevant pricing Marketing & Loyalty: Branding, product planning and improved customer understanding, etc.

Branding, product planning and improved customer understanding, etc. Flight Ops: Aircraft performance monitoring, technical route planning, minimising the impact of disruptions, etc.

Aircraft performance monitoring, technical route planning, minimising the impact of disruptions, etc. Engineering: Optimal inventory and minimal costs towards maintenance

Talking about the industry and analysis, Chogle detailed the role of AI and machine learning in airlines industry. According to him, AI and machine learning can help by:

Enabling scenario planning: Machine learning can help bring richer perspectives to scenario planning by deciphering the trends in competitive capacity/ fare strategy, etc.

Machine learning can help bring richer perspectives to scenario planning by deciphering the trends in competitive capacity/ fare strategy, etc. Quicken the pace of analytics: Delay, predictions, forecasting congestions at airports can provide a critical lead time for the airline to minimise disruptions.

Delay, predictions, forecasting congestions at airports can provide a critical lead time for the airline to minimise disruptions. Deal with the complexity of datasets as well as complex business challenges: AI and ML are useful for forecasting demand, passenger no-show or cancellation and access revenue impact of the course of action.

AI and ML are useful for forecasting demand, passenger no-show or cancellation and access revenue impact of the course of action. Encourage being “Future Ready”: Support functions such as HR, financial, legal, have been increasingly looking into AI-led tools for data-driven decision making.

Before wrapping up, Chogle stressed on the need to invest more in training and delivering solutions that create a measurable impact on business outcomes.

Subscribe to our Newsletter

You can write for us and be one of the 500+ experts who have contributed stories at AIM. Share your nominations here.

Get the latest updates and relevant offers by sharing your email.","['ml', 'birds', 'eyeview', 'industry', 'ai', 'learning', 'planning', 'functions', 'machine', 'revenue', 'airlines', 'forecasting', 'airline', 'impact']","The airline industry has embraced data science and machine learning-based decision making with a full heart.
The speaker mentioned some of the core airline functions to maximise revenues, minimise costs and provide a differentiated customer experience.
Engineering: Optimal inventory and minimal costs towards maintenanceTalking about the industry and analysis, Chogle detailed the role of AI and machine learning in airlines industry.
According to him, AI and machine learning can help by:Enabling scenario planning: Machine learning can help bring richer perspectives to scenario planning by deciphering the trends in competitive capacity/ fare strategy, etc.
AI and ML are useful for forecasting demand, passenger no-show or cancellation and access revenue impact of the course of action."
18,https://www.forbes.com/sites/tomtaulli/2021/02/13/aiops-how-to-get-started/,,,,,
19,https://towardsdatascience.com/forecasting-of-periodic-events-with-ml-5081db493c46,Forecasting of periodic events with ML,2021-02-13 14:33:37.503000+00:00,"Forecasting of periodic events with ML

Photo by Roman Bozhko on Unsplash

Periodic events forecasting is quite useful if you are, for example, the data aggregator. Data aggregators or data providers are organizations that collect statistical, financial or any other data from different sources, transform it and then offer it for further analysis and exploration (data as a service).

Data as a Service (DaaS)

It is really important for such organizations to monitor release dates in order to gather data as soon as it is released in the world and plan capacity to handle the incoming volumes of data.

Sometimes authorities that publish data have a schedule of future releases, sometimes not. In some cases, they announce schedule only for the next one or two months and, hence, you may want to make the publication schedule by yourself and predict release dates.

For the majority of statistical releases, you may find a pattern like the day of the week or month. For example, statistics can be released

every last working day of the month,

every third Tuesday of the month,

every last second working day of the month, etc.

Having this in mind and previous history of release dates, we want to predict potential date or range of dates when the next data release might happen.

Case Study

As a case study let’s take the U.S. Conference Board (CB) Consumer Confidence Indicator. It is a leading indicator which measures the level of consumer confidence in economic activity. By using it, we can predict consumer spending, which plays a major role in overall economic activity.

The official data provider does not provide the schedule for this series, but many data aggregators like Investing.com have been collecting the data for a while and series’ release history is available there.

Goal: we need to predict what is the date of the next release(s).

Data preparation

We start with importing all packages for data manipulation, building machine learning models, and other data transformations.

# Data manipulation

import pandas as pd # Manipulation with dates

from datetime import date

from dateutil.relativedelta import relativedelta # Machine learning

import xgboost as xgb

from sklearn import metrics

from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.neighbors import KNeighborsClassifier

from sklearn.ensemble import RandomForestClassifier

The next step is to get the list of release history dates. You may have a database with all data and history of release dates that you can use. To make this simple and focus on release dates prediction I will add history to DataFrame manually.

data = pd.DataFrame({'Date': ['2021-01-26','2020-12-22',

'2020-11-24','2020-10-27','2020-09-29',

'2020-08-25','2020-07-28','2020-06-30',

'2020-05-26','2020-04-28','2020-03-31',

'2020-02-25','2020-01-28','2019-12-31',

'2019-11-26','2019-10-29','2019-09-24',

'2019-08-27', '2019-07-30','2019-06-25',

'2019-05-28']})

We also should add a column with 0 and 1 values to specify if release happened on this date. For now, we only have dates of releases, so we create a column filled with 1 values.

data['Date'] = pd.to_datetime(data['Date'])

data['Release'] = 1

After that, you need to create all rows for dates between releases in DataFrame and fill release column with zeros for them.

r = pd.date_range(start=data['Date'].min(), end=data['Date'].max())

data = data.set_index('Date').reindex(r).fillna(0.0)

.rename_axis('Date').reset_index()

Now dataset is ready for further manipulations.

Feature engineering

Prediction of next release dates heavily relies on feature engineering because actually, we do not have any features besides release date itself. Therefore, we will create the following features:

month

a calendar day of the month

working day number

day of the week

week of month number

monthly weekday occurrence (second Wednesday of the month)

data['Month'] = data['Date'].dt.month

data['Day'] = data['Date'].dt.day

data['Workday_N'] = np.busday_count(

data['Date'].values.astype('datetime64[M]'),

data['Date'].values.astype('datetime64[D]'))

data['Week_day'] = data['Date'].dt.weekday

data['Week_of_month'] = (data['Date'].dt.day

- data['Date'].dt.weekday - 2) // 7 + 2

data['Weekday_order'] = (data['Date'].dt.day + 6) // 7

data = data.set_index('Date')

Training Machine learning model

By default, we need to split our dataset into two parts: train and test. Don’t forget to set shuffle argument to False, because our goal is to create a forecast based on past events.

x_train, x_test, y_train, y_test = train_test_split(data.drop(['Release'], axis=1), data['Release'],

test_size=0.3, random_state=1, shuffle=False)

In general, shuffle helps to get rid of overfitting by choosing different training observations. But it is not our case, every time we should have all history of publication events.

In order to choose the best prediction model, we will test the following models:

XGBoost

K-nearest Neighbors (KNN)

RandomForest

XGBoost

We will use XGBoost with tree base learners and grid search method to choose the best parameters. It searches over all possible combinations of parameters and chooses the best based on cross-validation evaluation.

A drawback of this approach is a long computation time.

Alternatively, the random search can be used. It iterates over the given range given the number of times, choosing values randomly. After a certain number of iterations, it chooses the best model.

However, when you have a large number of parameters, random search tests a relatively low number of combinations. It makes finding a really optimal combination almost impossible.

To use grid search you need to specify the list of possible values for each parameter.

DM_train = xgb.DMatrix(data=x_train, label=y_train)

grid_param = {""learning_rate"": [0.01, 0.1],

""n_estimators"": [100, 150, 200],

""alpha"": [0.1, 0.5, 1],

""max_depth"": [2, 3, 4]}

model = xgb.XGBRegressor()

grid_mse = GridSearchCV(estimator=model, param_grid=grid_param,

scoring=""neg_mean_squared_error"",

cv=4, verbose=1)

grid_mse.fit(x_train, y_train)

print(""Best parameters found: "", grid_mse.best_params_)

print(""Lowest RMSE found: "", np.sqrt(np.abs(grid_mse.best_score_)))

As you see the best parameters for our XGBoost model are: alpha = 0.5, n_estimators = 200, max_depth = 4, learning_rate = 0.1 .

Let’s train the model with obtained parameters.

xgb_model = xgb. XGBClassifier (objective ='reg:squarederror',

colsample_bytree = 1,

learning_rate = 0.1,

max_depth = 4,

alpha = 0.5,

n_estimators = 200)

xgb_model.fit(x_train, y_train)

xgb_prediction = xgb_model.predict(x_test)

K-nearest Neighbors (KNN)

K-nearest neighbors model is meant to be used when you are trying to find similarities between observations. This is exactly our case because we are trying to find patterns in the past release dates.

KNN algorithm has less parameters to tune, so it is more simple for those who have not used it before.

knn = KNeighborsClassifier(n_neighbors = 3, algorithm = 'auto',

weights = 'distance')

knn.fit(x_train, y_train)

knn_prediction = knn.predict(x_test)

Random Forest

Random forest basic model parameters tuning usually doesn’t take a lot of time. You simply iterate over the possible number of estimators and the maximum depth of trees and choose optimal ones using elbow method.

random_forest = RandomForestClassifier(n_estimators=50,

max_depth=10, random_state=1)

random_forest.fit(x_train, y_train)

rf_prediction = random_forest.predict(x_test)

Comparing the results

We will use confusion matrix to evaluate performance of trained models. It helps us compare models side by side and understand whether our parameters should be tuned any further.

xgb_matrix = metrics.confusion_matrix(xgb_prediction, y_test)

print(f""""""

Confusion matrix for XGBoost model:

TN:{xgb_matrix[0][0]} FN:{xgb_matrix[0][1]}

FP:{xgb_matrix[1][0]} TP:{xgb_matrix[1][1]}"""""") knn_matrix = metrics.confusion_matrix(knn_prediction, y_test)

print(f""""""

Confusion matrix for KNN model:

TN:{knn_matrix[0][0]} FN:{knn_matrix[0][1]}

FP:{knn_matrix[1][0]} TP:{knn_matrix[1][1]}"""""") rf_matrix = metrics.confusion_matrix(rf_prediction, y_test)

print(f""""""

Confusion matrix for Random Forest model:

TN:{rf_matrix[0][0]} FN:{rf_matrix[0][1]}

FP:{rf_matrix[1][0]} TP:{rf_matrix[1][1]}"""""")

As you see, both XGBoost and RandomForest show good performance. They both were able to catch the pattern and predict dates correctly in most cases. However, both models made a mistake with December 2020 release, because it breaks release pattern.

KNN is less accurate than the previous two. It failed to predict three dates correctly and missed 5 releases. At this point, we do not proceed with KNN. In general, it works better if data is normalized, so you can try to tune it if you want.

Concerning the remaining two, for the initial goal, XGBoost model is considered to be overcomplicated in terms of hyperparameters tuning, so RandomForest should be our choice.

Now we need to create DataFrame with future dates for prediction and use trained RandomForest model to predict future releases for one year ahead.

x_predict = pd.DataFrame(pd.date_range(date.today(), (date.today() +

relativedelta(years=1)),freq='d'), columns=['Date'])

x_predict['Day'] = x_predict['Date'].dt.day

x_predict['Workday_N'] = np.busday_count(

x_predict['Date'].values.astype('datetime64[M]'),

x_predict['Date'].values.astype('datetime64[D]'))

x_predict['Week_day'] = x_predict['Date'].dt.weekday

x_predict['Week_of_month'] = (x_predict['Date'].dt.day -

x_predict['Date'].dt.weekday - 2)//7+2

x_predict['Weekday_order'] = (x_predict['Date'].dt.day + 6) // 7

x_predict['Month'] = x_predict['Date'].dt.month

x_predict = x_predict.set_index('Date') prediction = xgb_model.predict(x_predict)

That’s it — we created forecast of release dates for U.S. CB Consumer Confidence series for one year ahead.

Conclusion

If you want to predict future dates for periodic events, you should think about meaningful features to create. They should include all information about patterns you can find in history. As you can see we did not spend a lot of time on model’s tuning — even simple models can give good results if you use the right features.","['release', 'models', 'ml', 'data', 'dates', 'events', 'parameters', 'model', 'predict', 'releases', 'xgboost', 'history', 'periodic', 'forecasting']","Forecasting of periodic events with MLPhoto by Roman Bozhko on UnsplashPeriodic events forecasting is quite useful if you are, for example, the data aggregator.
Feature engineeringPrediction of next release dates heavily relies on feature engineering because actually, we do not have any features besides release date itself.
This is exactly our case because we are trying to find patterns in the past release dates.
knn = KNeighborsClassifier(n_neighbors = 3, algorithm = 'auto',weights = 'distance')knn.fit(x_train, y_train)knn_prediction = knn.predict(x_test)Random ForestRandom forest basic model parameters tuning usually doesn’t take a lot of time.
ConclusionIf you want to predict future dates for periodic events, you should think about meaningful features to create."
20,https://towardsdatascience.com/the-ai-ml-fda-plan-efb384c9bf31,The AI/ML FDA Plan,2021-02-12 22:26:30.424000+00:00,"In case you are not familiar with it, the Food and Drugs Administration (FDA) is a federal agency of the Department of Health and Human Services. that is responsible for the control and supervision of food safety, drugs, vaccines, medical devices, and the likes. In recent times the FDA has been thinking about how to regulate medical devices that are driven or based on AI as a result of several recent efforts, they published their “Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) Action Plan” in January 2021. To understand this action plan, you need to go and read a number of related documents. I did that for you, and my hope is that this post summarizes the most important aspects of the FDA’s plan around Artificial Intelligence and Machine Learning. Make sure to read the last section for a surprise “ending” to it all.

I will start by saying that if you, like myself, come from a technical background, you might be coming into this thinking that the FDA is likely an old bureaucratic institution filled with old-school lawyers that know nothing about novel AI/ML approaches. I can neither confirm or deny that, but what I will say is that this particular action plan has very interesting aspects that I am sure you will agree with if you are working in applied AI in general, and AI for healthcare in particular. As a teaser, here is what the FDA’s vision is, in their own words:

“The ability for AI/ML software to learn from real-world feedback (training) and improve its performance (adaptation) makes these technologies uniquely situated among SaMD and a rapidly expanding area of research and development. Our vision is that with appropriately tailored regulatory oversight, AI/ML-based SaMD will deliver safe and effective software functionality that improves the quality of care that patients receive. “

You probably have noted by now that this action plan is focusing in the so-called SaMD (Software as a medical device). SaMD is a piece of software that on its own performs a medical function. This is in contrast to SiMD (Software in a medical device) where the software needs to be embedded in a piece of hardware to perform its medical function.

The Action Plan that was recently presented is a continuation of the so-called “Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD)” published in April 2019. The proposed framework included a number of questions for feedback from stakeholders that the current action plan takes into account and responds to by proposing a follow up plan in 5 different directions. So, in order to understand the current Action Plan, we need to first understand the proposed Framework.

The “Proposed Framework”?

Traditionally, the FDA reviews medical devices through an appropriate premarket pathway, such as premarket clearance (510(k)), De Novo classification, or premarket approval. Generally speaking, these premarket approvals need to happen in 3 situations:

A change that introduces a new risk or modifies an existing risk that could result in significant harm

A change to risk controls to prevent significant harm

A change that significantly affects clinical functionality or performance specifications of the device

However, this cannot be used in an AI/ML case where the “device” is expected to continuously improve as it gets more data and models are improved. The proposed framework addresses this particular concern as well as other requirements of AI-based medical devices.

In order to get there, it first defines a risk categorization framework based on: (1) the significance of information provided by the SaMD to the healthcare decision, and (2) the state of healthcare situation or condition, which identifies the intended user, disease or condition, and the population for the SaMD. See table below:

The framework also defines 3 types of not mutually exclusive AI/ML-based SaMD modifications:

Performance (e.g. retraining, change in AI architecture…) Inputs used by the algorithm (e.g. adding new input data types and sources) Intended use (e.g. from a confidence score that is ‘an aid in diagnosis’ (drive clinical management) to a ‘definitive diagnosis’ (diagnose) or inclusion of pediatric population where the SaMD was initially intended for adults ages 18 years or older)

Finally, and very importantly, it describes a Total Product Lifecycle (TPLC) Regulatory Approach for AI/ML-Based SaMD illustrated below. The TPLC has 4 different regulatory components (i.e. touchpoints with the FDA): (1) A set of Good Machine Learning Practices, (2) a Premarket Assurance, (3) a Change Protocol Review Process, and (4) ways to define Real-World Performance. Let’s look briefly into these 4 components.

Quality Systems and Good Machine Learning Practices (GMLP)

All SaMD, devices that rely on AI/ML are expected to demonstrate analytical and clinical validation, as described in the SaMD: Clinical Evaluation guidance (see figure below). Example considerations include: Relevance of available data to the clinical problem and current clinical practice; data acquired in a consistent, clinically relevant and generalizable manner; appropriate separation between training, tuning, and test datasets; and appropriate level of transparency (clarity) of the output and the algorithm aimed at users.

2. Initial Premarket Assurance of Safety and Effectiveness

SaMD will require a “Predetermined change plan” that should include:

SaMD Pre-Specifications (SPS) : manufacturer’s anticipated modifications to “performance”, “inputs,” or “intended use”

: manufacturer’s anticipated modifications to “performance”, “inputs,” or “intended use” Algorithm Change Protocol (ACP): Specific methods in place to achieve and control the risks of the anticipated modifications in the SPS. See table below for more details on what is expected in the ACP.

3. Approach for modifications after initial review with established SPS & ACP

Besides the predetermined change plan, the SaMD also needs to specify an approach for modifications. The requirements depend on the concrete situation and whether the SPS and ACP above had been approved. See workflow below.

4. Transparency & real-world performance monitoring of AI/ML-based SaMD

Finally, the regulatory framework requires AI-based medical devices to address transparency and real-world performance. There are not many details of how that should be implemented in the framework, but mostly some examples on how to ensure transparency and monitoring. E.g.1. Transparency may include updates to FDA, device companies and collaborators of the manufacturer, and the public, such as clinicians, patients, and general users. E.g. 2. Real-world performance monitoring may also be achieved in a variety of suggested mechanisms that are currently employed or under pilot at FDA, such as adding to file or an annual report

Each of the sections above comes with a number of questions to the “stakeholders”. Besides, the document includes a number of real-world examples and scenarios including:

Intensive Care Unit (ICU) SaMD

Skin Lesion Mobile Medical App (MMA)

X-Ray Feeding Tube Misplacement SaMD

Feedback

After its publication, and for the following few months, the framework above received 133 public comments. Those came from large corporations such as GE Healthcare or Anthem, but also individuals. There are even anonymous comments. All of them are available in the public site.

Pilot: AI-guided Cardiac Ultrasound

As a way to test the framework in the real-world, the FDA applied it to a pilot for regulating a device developed by a startup called Caption Health. This device uses AI to guide ultrasounds, and to enable untrained registered nurses to perform ultrasounds at the same level of quality as trained specialists

Digital Health Center of Excellence

Also in the context of these AI/ML initiatives, the FDA started the Digital Health Center of Excellence late 2020. The center includes many external collaborators from universities and other non-profits (not for profit companies as far as I can see). Its 3 listed objectives are:

Connect and build partnerships

Share knowledge

Innovate regulatory approaches

The Action Plan

Ok, so we are now that we understand the original framework, and the process, we are ready to go back to the January ’21 action plan. The plan includes a summary of the feedback received (“What they heard”), and a plan on what the next actions will be (“What they’ll do”), all of it grouped in 5 different areas:

Tailored Regulatory Framework for AI/ML-based SaMD

What they heard: Many suggestions including re:Predetermined Change Control Plan

Many suggestions including re:Predetermined Change Control Plan What they’ll do: Update the proposed framework, including issuance of Draft Guidance on the Predetermined Change Control Plan for public comment

2. Good Machine Learning Practices (GMLP)

What they heard: Strong general support for the GMLP + a call for FDA to encourage harmonization through consensus standards efforts.

Strong general support for the GMLP + a call for FDA to encourage harmonization through consensus standards efforts. What they’ll do: Encourage harmonization of Good Machine Learning Practice development through institutions such as IEEE and AAMI

3. Patient-Centered Approach Incorporating Transparency to Users

What they heard: Call for further discussion on how AI/ML-based technologies interact with people, including their transparency to users and to patients

Call for further discussion on how AI/ML-based technologies interact with people, including their transparency to users and to patients What they’ll do: Following up on recent Patient Engagement Advisory Committee meeting, next step: hold public workshop on how to support transparency to users and enhance trust in AI/ML-based devices

4. Regulatory Science Methods Related to Algorithm Bias & Robustness

What they heard: : Stakeholders described the need for improved methods to evaluate and address algorithmic bias and to promote algorithm robustness.

: Stakeholders described the need for improved methods to evaluate and address algorithmic bias and to promote algorithm robustness. What they’ll do: Support regulatory science efforts to develop methodology for the evaluation and improvement of algorithms, including for the identification and elimination of bias, and promotion of algorithm robustness (e.g. Centers for Excellence in Regulatory Science and Innovation (CERSIs) at UCSF, Stanford, and Johns Hopkins University)

5. Real-World Performance (RWP)

What they heard: Stakeholders described the need for clarity on Real-World Performance (RWP) monitoring for AI/ML software

Stakeholders described the need for clarity on Real-World Performance (RWP) monitoring for AI/ML software What they’ll do: Work with stakeholders who are piloting the RWP process for AI/ML-based SaMD

To summarize, the action plan proposes to:

Update the proposed regulatory framework in the AI/ML-based SaMD discussion paper, including issuance of a Draft Guidance on the Predetermined Change Control Plan.

Harmonize development of GMLP through additional FDA participation in collaborative communities & standards development efforts.

Continue to host discussions on the role of transparency to users, including holding a public workshop on medical device labeling to support transparency

Support regulatory efforts on methodology for evaluation & improvement of algorithms, including for the identification & elimination of bias, & robustness and resilience of algorithms.

Advance real-world performance pilots in coordination with stakeholders and other FDA programs, to provide additional clarity.

A last-minute hurdle?

If you think everything you read until now makes sense, you will be surprised to read what happened next. In the middle of the chaos of Trump’s administration last days, the HHS published a request to exempt a large number AI devices from FDA regulation. This piece of news was pretty shocking for the community following these efforts and it is still unclear what it means, particularly since the new Biden administration will need to decide whether to approve or not the requested exemption (read more here, paywalled, or here, free)

In any case, these next few months promise to be really important and decisive for the whole space of AI in healthcare regulation. I look forward for the outcome of it all to provide a good avenue for aggressive innovation plus patient safety and medical quality.","['realworld', 'change', 'aimlbased', 'performance', 'regulatory', 'framework', 'aiml', 'samd', 'plan', 'medical', 'fda']","“You probably have noted by now that this action plan is focusing in the so-called SaMD (Software as a medical device).
Traditionally, the FDA reviews medical devices through an appropriate premarket pathway, such as premarket clearance (510(k)), De Novo classification, or premarket approval.
Approach for modifications after initial review with established SPS & ACPBesides the predetermined change plan, the SaMD also needs to specify an approach for modifications.
Transparency & real-world performance monitoring of AI/ML-based SaMDFinally, the regulatory framework requires AI-based medical devices to address transparency and real-world performance.
Advance real-world performance pilots in coordination with stakeholders and other FDA programs, to provide additional clarity."
21,https://www.marktechpost.com/2021/02/12/baidu-team-introduces-ernie-m-a-multilingual-model-that-learns-96-languages-from-monolingual-corpora/,Baidu Team Introduces ERNIE-M: A Multilingual Model that Learns 96 Languages from Monolingual Corpora,2021-02-12 00:00:00,"The Baidu team is excited to present ERNIE-M as a new multilingual model capable of understanding 96 languages. ERNIE-M is also a new training method capable of improving the model’s cross-lingual transferability on data-sparse languages. ERNIE-M delivers recent state-of-the-art results in five cross-lingual downstream tasks and tops XTREME.

Most natural language processing innovations are spawned in languages like English and Chinese (high resource language). More than 6,500 languages worldwide have scarce data resources, presenting a massive challenge for machines to understand those languages and limit AI’s democratization. The above leaves behind thousands of low-resource languages.

Although training a model on each language might be possible, but multilingual model research has seen significant advancements over the past few years. Cross-lingual models learn a shared language-agnostic representation across multiple languages and enable transfer learning from a high-resource language to a low-resource language.

The existing method, which has been proven to be effective, trains a model on different monolingual datasets to learn semantic representation and capture semantic alignment across other languages on parallel corpora. However, the sizes of parallel corpora are somewhat limited, which restricts the model’s performance.

The team proposed a novel cross-lingual pre-training method in a paper that can learn semantic alignment across multiple languages on monolingual corpora.

The Key of ERNIE-M: Cross-lingual Alignment and Back-translation

The training of ERNIE-M consists of two stages:

The first stage includes aligning the cross-lingual semantic representation by CAMLM on a small parallel corpus. In CAMLM, the model learns the multilingual semantic word using restoration of the MASK tokens in the input sentences. The second stage is related to Back-translation Masked Language Modeling (BTMLM) to align cross-lingual semantics with the monolingual corpus. The team uses BTMLM to train the model to generate pseudo-parallel sentences from the monolingual sentences. The generated pairs are then used as the model’s input to further align the cross-lingual semantics, thus enhancing multilingual representation.

Experimental Results

The team utilized five cross-lingual evaluation benchmarks to test the efficacy of ERNIE-M:

lXNLI for cross-lingual natural language inference,

lMLQA for cross-lingual question answering,

lCoNLL for cross-lingual named entity recognition,

lPAWS-X for cross-lingual paraphrase identification,

lTatoeba for cross-lingual retrieval.

The team evaluated ERNIE-M in two formats:

a cross-lingual setting to fine-tune the model with an English training set and consider it on a foreign language test. a Multilingual fine-tuning setting to refine the model on all other languages’ concatenation and evaluate it on each language test set.

https://arxiv.org/pdf/2012.15674.pdf

Cross-lingual Sentence Retrieval: The goal is to extract parallel sentences from bilingual corpora. ERNIE-M allows retrieving results in multiple languages, such as French, English, and German, using only Chinese. This technology can bridge the gap between information expressed in various languages, thus helping people search for more valuable information. An accuracy rate of 87.9% was achieved on a subset of the Tatoeba dataset that contained 36 languages.

Cross-lingual Natural Language Inference: It’s a task to determine whether the relationship between two input sentences is a contradiction, entailment, or neural. ERNIE-M achieved 82.0% specifically on the English training set and 84.2% on all training sets.

Cross-lingual Question Answering: Question answering is a classic NLP task used to test a machine’s ability to provide an automated answer in a natural language. ERNIE-M was fine-tunned by training with English data. The model achieved an accuracy of 55.3%.

Named entity recognition task: NER seeks to locate and classify the named entities in text. ERNIE-M is evaluated on CoNLL-2002 and CoNLL-2003 datasets involving Dutch, English, Spanish, and German. ERNIE-M was tunned on English data and considered it in Spanish, Dutch, and German. The average F1 score achieved was 81.6%.

Cross-lingual paraphrase identification: The team evaluated ERNIE-M on PAWS-X. It is a paraphrase identification dataset with seven languages. ERNIE-M has achieved an accuracy of 89.5%, specifically on the English training set and 91.8% on all the training sets.

ERNIE-M has wide-ranging applications and implications. The codes and pre-trained models will be made publicly available soon.

Paper: https://arxiv.org/abs/2012.15674

Source: http://research.baidu.com/Blog/index-view?id=151

Suggested","['english', 'erniem', 'semantic', 'team', 'baidu', 'monolingual', 'model', 'sentences', 'language', 'multilingual', 'corpora', 'crosslingual', 'introduces', 'training', 'languages', 'learns']","The Baidu team is excited to present ERNIE-M as a new multilingual model capable of understanding 96 languages.
Although training a model on each language might be possible, but multilingual model research has seen significant advancements over the past few years.
The team proposed a novel cross-lingual pre-training method in a paper that can learn semantic alignment across multiple languages on monolingual corpora.
The Key of ERNIE-M: Cross-lingual Alignment and Back-translationThe training of ERNIE-M consists of two stages:The first stage includes aligning the cross-lingual semantic representation by CAMLM on a small parallel corpus.
ERNIE-M has achieved an accuracy of 89.5%, specifically on the English training set and 91.8% on all the training sets."
22,https://towardsdatascience.com/deepmind-releases-a-new-state-of-the-art-image-classification-model-nfnets-75c0b3f37312,Deepmind releases a new State-Of-The-Art Image Classification model — NFNets,2021-02-14 10:06:48.880000+00:00,"Deepmind releases a new State-Of-The-Art Image Classification model — NFNets

Photo by Boitumelo Phetla on Unsplash

Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7× faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%.

Source: arxiv

One of the most annoying things about training a model is the time it takes to train it and the amount of memory needed to fit in the data and the models. Since image classification is one of the most common machine learning tasks, Deepmind released a new model that matches the state-of-art (SOTA) performance with significantly less size, higher training speed, and fewer optimization techniques for simplicity.

In their work, they examine the current SOTA models such as EfficientNets and ResNets. In their analysis they pindown some of the optimization techniques that utilize a lot of memory without producing a significant value for performance. They prove that these networks can achieve the same performance without those optimization techniques.

Although the proposed model might be the most interesting bit, I still find the analysis of previous work to be very interesting. Simply because this is where most of the learning happens, we start understanding what could have been done better and why the newly proposed method/technique is an improvement over the old one.

Pre-requisite: Batch Normalisation

The paper starts off with an analysis of batch normalisation. Why? because although it has shown great results and has been used heavily in tons of SOTA models, it has several disadvantages outlined by the paper [1], such as:

Very expensive computational costs Introduces a lot of extra hyper-parameters that need further fine-tuning Causes a lot of implementation errors in distributed training Performs poorly on small batch sizes, which are used often in training larger models

But first, before removing batch normalization, we have to understand what benefits it brought to the models. Because we want to find a smarter way to still have those benefits, but with fewer cons. Those benefits are [1]:

It downscales residual branches in deep ResNets. ResNets are one of the most widely used image classification networks. They usually extend to thousands of layers, and batch normalization reduces the scale of “hidden activations” that often cause gradients to behave in a funny way (gradient exploding problem) Eliminates mean-shift for popular activation functions such as ReLU and GeLU. In large networks, the output of those activation functions typically shifts towards very large values on average. This causes the network to predict the same label for all samples in certain situations (such as initialization) decreasing its performance. Batch normalization solves this mean-shift problem.

There are some other benefits, but I think you got the gist that its all mainly about regularisation and smoothing the training process.

NFNets — Normaliser Free Networks:

Although there have been previous attempts to remove batch normalization (BN) in various papers, the results didn’t match the SOTA performance or training latency and seemed to fail on large batch sizes, and this is the main selling point of this paper. They succeed in removing (BN) without affecting performance, and with improving the training latency by a large margin.

To do that, they propose a gradient clipping technique called Adaptive Gradient Clipping (AGC) [1]. Essentially, gradient clipping is used to stabilize model training [1] by not allowing the gradient to go beyond a certain threshold. This allows using larger training rates and thus faster convergence without the exploding gradient problem.

However, the main issue is setting the threshold hyper-parameter, which is quite a difficult and manual task. The main benefit of AGC is to remove this hyperparameter. To do this we have to examine the gradient norms and the parameter norms.

Although I am quite interested in the mathematics behind every machine learning model, I understand that a lot of ML enthusiasts don’t enjoy reading a bunch of long differential equations, that’s why I will explain AGC from a theoretical/intuitive perspective rather than a mathematically rigorous one.

A norm is simply a measure of the magnitude of a vector. AGC is built on the premise that:

the unit-wise ratio of the norm of the gradients to the norm of the weights of a layer provides a simple measure of how much a single gradient descent step will change the original weights.

Source: arxiv

But why is that premise valid? Let’s back up a little. A very high gradient will make our learning unstable, and if that's the case then the ratio of the gradient of the weight matrix to the weight matrix will be very high.

That weight ratio is equivalent to:

learning rate x the ratio between the gradient and the weight matrix (which is our premise).

So essentially, the ratio proposed by that premise is a valid indicator as to whether we should clip the gradient or not. There is also another minor tweak. They have found that through multiple experiments, it's much better to use a unit-wise ratio of gradient norms instead of a layer-wise ratio (because each layer can have more than one gradient).

In addition to AGC, they also used dropout to substitute the regularisation effect that Batch normalization was offering.

They also used an optimization technique called Sharpness-Aware Minimization (SAM) [1].

Motivated by the connection between the geometry of the loss landscape and generalization — including a generalization bound that we prove here — we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several.

Source: SAM arxiv paper

The idea of loss sharpness seems quite interesting and I might be exploring it in another article for the sake of brevity here. One final point to note here though is that they make a small modification to SAM [1] to reduce its computational cost by 20–40%! and they only employ it on their 2 largest model variants. It’s always great to see additions being made to such techniques instead of just using them out of the box. I think this shows that they have analyzed it greatly before using it (and thus were able to optimize it a bit).

Final thoughts and take away

Who would have thought that replacing a minor optimization technique such as batch normalization would result in a 9x improvement in training latency. I think this sends a message of being a bit more skeptical about popular optimization techniques that are used everywhere. In all fairness, I have been a victim of this crime before, I used to just put every popular optimization technique into my machine learning projects without fully examining its pros and cons. I guess this is one of the main benefits of reading ML papers, the analysis of previous SOTAs!

References:

[1] High-Performance Large-Scale Image Recognition Without Normalization. Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan. 2021

If you are interested in reading more about other novel papers, check out my articles here:","['models', 'nfnets', 'used', 'model', 'batch', 'optimization', 'releases', 'image', 'classification', 'ratio', 'performance', 'gradient', 'normalization', 'deepmind', 'stateoftheart', 'training']","Deepmind releases a new State-Of-The-Art Image Classification model — NFNetsPhoto by Boitumelo Phetla on UnsplashOur smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7× faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%.
Although the proposed model might be the most interesting bit, I still find the analysis of previous work to be very interesting.
Batch normalization solves this mean-shift problem.
Essentially, gradient clipping is used to stabilize model training [1] by not allowing the gradient to go beyond a certain threshold.
In addition to AGC, they also used dropout to substitute the regularisation effect that Batch normalization was offering."
23,https://venturebeat.com/2021/02/13/ai-progress-depends-on-us-using-less-data-not-more/,"AI progress depends on us using less data, not more",2021-02-13 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

In the data science community, we’re witnessing the beginnings of an infodemic — where more data becomes a liability rather than an asset. We’re continuously moving towards ever more data-hungry and more computationally expensive state-of-the-art AI models. And that is going to result in some detrimental and perhaps counter-intuitive side-effects (I’ll get to those shortly).

To avoid serious downsides, the data science community has to start working with some self-imposed constraints: specifically, more limited data and compute resources.

A minimal-data practice will enable several AI-driven industries — including cyber security, which is my own area of focus — to become more efficient, accessible, independent, and disruptive.

When data becomes a curse rather than a blessing

Before we go any further, let me explain the problem with our reliance of increasingly data-hungry AI algorithms. In simplistic terms, AI-powered models are “learning” without being explicitly programed to do so, through a trial and error process that relies on an amassed slate of samples. The more data points you have – even if many of them seem indistinguishable to the naked eye, the more accurate and robust AI-powered models you should get, in theory.

In search of higher accuracy and low false-positive rates, industries like cyber security — which was once optimistic about its ability to leverage the unprecedented amount of data that followed from enterprise digital transformation — are now encountering a whole new set of challenges:

1. AI has a compute addiction. The growing fear is that new advancements in experimental AI research, which frequently require formidable datasets supported by an appropriate compute infrastructure, might be stemmed due to compute and memory constraints, not to mention the financial and environmental costs of higher compute needs.

While we may reach several more AI milestones with this data-heavy approach, over time, we’ll see progress slow. The data science community’s tendency to aim for data-“insatiable” and compute-draining state-of-the-art models in certain domains (e.g. the NLP domain and its dominant large-scale language models) should serve as a warning sign. OpenAI analyses suggest that the data science community is more efficient at achieving goals that have already been obtained but demonstrate that it requires more compute, by a few orders of magnitude, to reach new dramatic AI achievements. MIT researchers estimated that “three years of algorithmic improvement is equivalent to a 10 times increase in computing power.” Furthermore, creating an adequate AI model that will withstand concept-drifts over time and overcome “underspecification” usually requires multiple rounds of training and tuning, which means even more compute resources.

If pushing the AI envelope means consuming even more specialized resources at greater costs, then, yes, the leading tech giants will keep paying the price to stay in the lead, but most academic institutions would find it difficult to take part in this “high risk – high reward” competition. These institutions will most likely either embrace resource-efficient technologies or persue adjacent fields of research. The significant compute barrier might have an unwarranted cooling effect on academic researchers themselves, who might choose to self-restrain or completely refrain from persuing revolutionary AI-powered advancements.

2. Big data can mean more spurious noise. Even if you assume you have properly defined and designed an AI model’s objective and architecture and that you have gleaned, curated, and adequately prepared enough relevant data, you have no assurance the model will yield beneficial and actionable results. During the training process, as additional data points are consumed, the model might still identify misleading spurious correlations between different variables. These variables might be associated in what seems to be a statistically significant manner, but are not causally related and so don’t serve as useful indicators for prediction purposes.

I see this in the cyber security field: The industry feels compelled to take as many features as possible into account, in the hope of generating better detection and discovery mechanisms, security baselines, and authentication processes, but spurious correlations can overshadow the hidden correlations that actually matter.

3. We’re still only making linear progress. The fact that large-scale data-hungry models perform very well under specific circumstances, by mimicking human-generated content or surpassing some human detection and recognition capabilities, might be misleading. It might obstruct data practitioners from realizing that some of the current efforts in applicative AI research are only extending existing AI-based capabilities in a linear progression rather than producing real leapfrog advancements — in the way organizations secure their systems and networks, for example.

Unsupervised deep learning models fed on large datasets have yielded remarkable results over the years — especially through transfer learning and generative adversarial networks (GANs). But even in light of progress in neuro-symbolic AI research, AI-powered models are still far from demonstrating human-like intuition, imagination, top-down reasoning, or artificial general intelligence (AGI) that could be applied broadly and effectively on fundamentally different problems — such as varying, unscripted, and evolving security tasks while facing dynamic and sophisticated adversaries.

4. Privacy concerns are expanding. Last but not least, collecting, storing, and using extensive volumes of data (including user-generated data) — which is especially valid for cyber security applications — raises a plethora of privacy, legal, and regulatory concerns and considerations. Arguments that cyber security-related data points don’t carry or constitute personally identifiable information (PII) are being refuted these days, as the strong binding between personal identities and digital attributes are extending the legal definition PII to include, for example, even an IP address.

How I learned to stop worrying and enjoy data scarcity



In order to overcome these challenges, specifically in my area, cyber security, we have to, first and foremost, align expectations.

The unexpected emergence of Covid-19 has underscored the difficulty of AI models to effectively adapt to unseen, and perhaps unforeseeable, circumstances and edge-cases (such as a global transition to remote work), especially in cyberspace where many datasets are naturally anomalous or characterized by high variance. The pandemic only underscored the importance of clearly and precisely articulating a model’s objective and adequately preparing its training data. These tasks are usually as important and labor-intensive as accumulating additional samples or even choosing and honing the model’s architecture.

These days, the cyber security industry is required to go through yet another recalibration phase as it comes to terms with its inability to cope with the “data overdose,” or infodemic, that has been plaguing the cyber realm. The following approaches can serve as guiding principles to accelerate this recalibration process, and they’re valid for other areas of AI, too, not just cyber security:

Algorithmic efficacy as top priority. Taking stock of the plateauing Moore’s law, companies and AI researchers are working to ramp-up algorithmic efficacy by testing innovative methods and technologies, some of which are still in a nascent stage of deployment. These approaches, which are currently applicable only to specific tasks, range from the application of Switch Transformers, to the refinement of Few Shots, One-Shot, and Less-Than-One-Shot Learning methods.

Human augmentation-first approach. By limiting AI models to only augment the security professional’s workflows and allowing human and artificial intelligence to work in tandem, these models could be applied to very narrow, well-defined security applications, which by their nature require less training data. These AI guardrails could be manifested in terms of human intervention or by incorporating rule-based algorithms that hard-code human judgment. It is no coincidence that a growing number of security vendors favor offering AI-driven solutions that only augment the human-in-the-loop, instead of replacing human judgment all together.

Regulators could also look favorably on this approach, since they look for human accountability, oversight, and fail-safe mechanisms, especially when it comes to automated, complex, and “black box” processes. Some vendors are trying to find middle ground by introducing active learning or reinforcement learning methodologies, which leverage human input and expertise to enrich the underlining models themselves. In parallel, researchers are working on enhancing and refining human-machine interaction by teaching AI models when to defer a decision to human experts.

Leveraging hardware improvements. It’s not yet clear whether dedicated, highly optimized chip architectures and processors alongside new programming technologies and frameworks, or even completely different computerized systems, would be able to accommodate the ever-growing AI computation demand. Tailor-made for AI applications, some of these new technological foundations that closely bind and align specialized hardware and software, are more capable than ever of performing unimaginable volumes of parallel computations, matrix multiplications, and graph processing.

Additionally, purpose-built cloud instances for AI computation, federated learning schemes, and frontier technologies (neuromorphic chips, quantum computing, etc.) might also play a key role this effort. In any case, these advancements alone are not likely to curb the need for algorithmic optimization that might “outpace gains from hardware efficiency.” Still, they could prove to be critical, as the ongoing semiconductor battle for AI dominance has yet to produce a clear winner.

The merits of data discipline

Up to now, conventional wisdom in data science has usually dictated that when it comes to data, the more you have, the better. But we’re now beginning to see that the downsides of data-hungry AI models might, over time, outweigh their undisputed advantages.

Enterprises, cyber security vendors, and other data practitioners have multiple incentives to be more disciplined in the way they collect, store, and consume data. As I’ve illustrated here, one incentive that should be top of mind is the ability to elevate the accuracy and sensitivity of AI models while alleviating privacy concerns. Organizations that embrace this approach, which relies on data dearth rather than data abundance, and exercise self-restraint, may be better equipped to drive more actionable and cost-effective AI-driven innovation over the long haul.

Eyal Balicer is Senior Vice President for Global Cyber Partnership and Product Innovation at Citi.","['models', 'using', 'data', 'depends', 'cyber', 'learning', 'ai', 'progress', 'science', 'compute', 'security', 'human', 'training']","In the data science community, we’re witnessing the beginnings of an infodemic — where more data becomes a liability rather than an asset.
To avoid serious downsides, the data science community has to start working with some self-imposed constraints: specifically, more limited data and compute resources.
The merits of data disciplineUp to now, conventional wisdom in data science has usually dictated that when it comes to data, the more you have, the better.
But we’re now beginning to see that the downsides of data-hungry AI models might, over time, outweigh their undisputed advantages.
Enterprises, cyber security vendors, and other data practitioners have multiple incentives to be more disciplined in the way they collect, store, and consume data."
24,https://venturebeat.com/2021/02/13/michael-hurlston-how-synaptics-pivoted-from-mobile-pc-sensors-to-the-internet-of-things/,Michael Hurlston: How Synaptics pivoted from mobile/PC sensors to the internet of things,2021-02-13 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Synaptics pioneered sensors for touchscreens for PCs and mobile devices. But the San Jose-based hardware company has shifted to where the processing is happening — at the edge of the network.

Under CEO Michael Hurlston, the 35-year-old company has pivoted away from its early markets and focused on artificial intelligence at the edge to bring greater efficiency to internet of things (IoT) devices. With AI at the edge, the company can process the sensor data that it collects and only send alerts when they’re relevant to the network.

Hurlston said that processing paradigm will offload crowded home and business networks and ensure privacy for customer data that doesn’t have to be stored in big datacenters. In the company’s most recent quarter ended December 31, the internet of things now accounts for 43% of the company’s overall $358 million in quarterly revenue, while the PC is 26% and mobile is 31%. Synaptics has 1,400 employees.

Synaptics’ customers now span consumer, enterprise, service provider, automotive, and industrial markets. IoT markets for chips are expected to grow 10% to 15% a year, and the company recently picked up better wireless chip products from Broadcom. Synaptics also launched its new Katana low-power AI processors for the edge. I spoke with Hurlston, who has been in the top job for 18 months, about this transformation.

Here’s an edited transcript of our interview.

Image Credit: Synaptics

Michael Hurlston: You understand the business probably better than most. We’ve been thought of as mobile, mobile, mobile, and then maybe a PC subhead. We’ve tried to move the company into IoT, and then mobile where we can attack opportunistically. We’re trying to make IoT our big thrust. That’s what came out this quarter. IoT was our largest business. People started believing that we could make that happen. That’s the main thing.

VentureBeat: What do you think about the future in terms of haptics and the sense of touch? I’m a science fiction fan, and I just got done with the latest Ready Player Two book. They had a VR system in there that could reproduce all of the senses for you.

Hurlston: It sounds both interesting and dangerous.

VentureBeat: Starting where we are, though, do you see anything interesting along those lines that’s coming along?

Hurlston: With the AR/VR glasses, that’s been an interesting intersection of our technology. We have these display drivers that create the ultra-HD images you can see. There’s touch that goes with it, typically, and a lot of the systems have a video processor that feeds the images into the glass. All of those things, we supply them. The AR/VR market has been good for us. It’s obviously still pretty small, but I’m much more optimistic that it’s going to take off. It plays nicely to the bag of technologies we have in the company.

Haptics is advancing. We don’t have haptics today. We do all the touch controllers on glass surfaces. Where we are trying to invest is touch on non-glass surfaces. We can see things coming — headsets are a good example, where you’re trying to touch a piece of plastic and generate sensation through there. In automobiles, on steering wheels, on things like that. We’re trying to move our touch sensors from a typical glass application to other areas where glass isn’t present, and trying to generate accuracy and precision through plastics or other materials.

VentureBeat: It’s interesting that you’re moving into IoT, and IoT devices are getting to the point where you can put AI into them. That feels like quite an advance in computing.

Hurlston: What’s going on for us, and this is something probably in your sweet spot to think about — a lot of companies now do these audio wake words, where you’re waking up a Google Home or Alexa using voice, and some simple commands are processed on the edge. The wake up doesn’t have to go to the cloud. What we’re trying to advance is a visual wake word, where we can have AI in a low-power sensor that can detect an incident, whether it’s people coming in a room or chickens moving in a coop.

We have agricultural applications for the idea, where you’re counting or sensing livestock. Counting people might apply to, do I need to turn an air conditioner on or off? Do I need to turn a display on or off? Do I need to reduce the number of people? Maybe now, in the COVID environment, you have too many people in a room. You have this low-power battery sensor that can be stuck anywhere, but rather than using voice, have a camera attached to it, a simple camera, and do some intelligence at the edge where we can identify a person or something else. Maybe the wind blowing and creating an event in front of the camera. We have a bit of inferencing and training that can happen on the device to enable those applications.

VentureBeat: It feels like we need some choices among those sensors, too. There’s a lot of places where you don’t want to put cameras, but you want that 3D detection of people or objects. You don’t want to put face recognition in a bathroom.

Hurlston: Right. That’s why these little low-power sensors can do that. They can detect motion where you don’t want to have full recognition. It can just detect that something in here is moving, so let’s turn on the lights. Particularly for industrial applications where you want to save power. It all makes sense and flows. We can have pretty high precision, where you do face recognition because there’s an AI network on the chip, but you can also just do simple motion and on/off. It just depends on how precise you need your sensor to be.

Image Credit: Synaptics

VentureBeat: Do we credit Moore’s Law for some of this advance, being able to put more computing power into small devices? I suppose we can also credit neural networks actually working now.

Hurlston: It’s more the latter. We got reasonably good at neural networks on a high-power chip, and we were able to train the classic things. You talked about facial recognition or seeing in the dark, where we can pull out an image and train, train, train with very low light. Light turns out to be measured in luxes, which is candlelight, and we can pull out an image now with 1/16 of a lux. That’s almost total darkness. You can’t see it with your eyes, but you can pull out and enhance an image in low light.

We did that first. We developed the neural networks on high-power chips, and then migrated it to lower-power, and obviously shrunk it in the process. We were able to condense the inferencing and some of the training sequences on that low-power chip. Now we think we can deliver — it’s not going to be the same use case, but we can deliver at least an AI algorithm on a battery-powered IC.

VentureBeat: It feels like that’s important for the further existence of the planet, with things like too much cloud computing. AI at the edge is a more ecologically sound solution.

Hurlston: We’re seeing two key applications. One is obvious, and that’s power consumption. All this traffic that’s cluttering up the datacenters is consuming gigawatts, as Doc Brown would say, of power. The other one is privacy. If the decisions are made on the edge, there’s less chance that your data gets hacked and things like that. Those are the two things that people understand very simply. The third bullet is latency, making decisions much faster at the edge than having to go back to the cloud, do the calculation, and come back. But the two most important are power and privacy.

VentureBeat: Did you already have a lot of people who can do this in the company or did you have to hire a new kind of engineer to make AI and machine learning happen?

Hurlston: It’s a confluence of three things. We initially had this for video. If you look back at when we adopted it on higher-power chips that are more generally understood for machine learning, there we had to bring in our own talent. Our second step was to take an audio solution. The original idea was the wake word, following the market trend to do compute at the edge for voice. We had taken these AI and machine learning engineers, shrunk the neural network, put it into an audio chip, but we found we were behind. A lot of people can do all that wake word training. The third leg of the stool was we recently announced a partnership with a company called ETA Compute. It’s a small startup in southern California. They had a lot of machine learning and AI experts. The big language is TensorFlow, and they have the compiler that can take the TensorFlow engine and compile it into our audio chip.

The confluence of those things created this low-power AI at the edge solution that we think is different. It has quite a bit of market traction. But it’s a totally different approach to apply what I call “visual wake word” to this whole space.

VentureBeat: It seems like a good example of how AI is changing companies and industries. You wouldn’t necessarily expect it in sensing, but it makes sense that you’d have to invest in this.

Hurlston: You’ve covered technology for long enough, and you’ve been through all the cycles. Right now, the AI cycle is there. Everybody has to talk about it as part of the technology portfolio. We’re no different. We got lucky to a certain extent because we’d invested in it for a pretty clear problem, but we were able to apply it to this new situation. We have some runway.

Image Credit: Synaptics

VentureBeat: When it comes to making these things better, either better at giving you the right information or better at the sensing, it feels like where we are with the current devices, we still need a lot of improvement. Do you see that improvement coming?

Hurlston: It comes from training data. You know better than most that it’s all about being able to provide these neural networks with the right training data. The hardest problem you have is generating datasets on which to train. Before I came here, I was at a software AI company. I spent a lot of time — we participated in a very interesting competition. The University of North Carolina had all the software AI companies together, and we were shown different dogs, from a chihuahua to a German shepherd to a pit bull. Who could best identify a dog and call it a dog from a series of pictures? They tried to throw giraffes in and things like that.

In the competition, we didn’t win, but the winner was able to get dogs to about 99% accuracy. It was amazing how well they were able to get their dataset and training to be able to identify dogs. They took the picture and they flipped it upside down, though, and nobody could get it. Once it was upside down, nobody could identify it as a dog as well as people had done when it was right side up. This thing is all about being able to train, to train on the corner cases.

This low light thing we’ve done on our video processor, we take snapshots over and over again in super low light conditions to be able to train the engine to recognize a new situation. That’s what this is all about. You know the existing situation. It’s being able to apply the existing to the new. That’s a lot harder than it sounds.

VentureBeat: If we get to the actual business, what’s doing well right now, and what do you think is going to be the source of major products in the future?

Hurlston: We’re sort of IoT of IoT. Within IoT, what our business we call IoT — we have lots of different technologies. We touched on our audio technology. That’s done very well. You have headsets that are going into a lot of work-from-home situations, with the over-ear design and active noise canceling. That business has done super well for us. We have Wi-Fi assets. We did a deal last year where we bought Broadcom’s Wi-Fi technology that they were applying to markets other than mobile phones. That business has done super well. We have docking station solutions, video processors applied to docking stations, or video conferencing systems. That’s done well for us.

In IoT, we have lots of different moving pieces, all of which are hitting at the moment, which is understandable. Work from home is good for our business. Wi-Fi in general — everything needs to be connected, and that’s driven our business. It’s been a lot of different moving parts, all of them moving simultaneously in a positive direction right now.

VentureBeat: How much emphasis do you see on IoT versus the traditional smartphone space or tablets?

Image Credit: Synaptics

Hurlston: Smartphones is an area where we’ve done well historically as a company. Our business there was display drivers, and then the touch circuit that drives the panel. We’ll continue to play there. We’re going to approach that business, I would say, opportunistically, when we see a good opportunity to apply our technology to mobile.

But touch and display drivers — you touched on this with one of your first questions. That’s becoming more IoT-ish. Our technology that had done well in mobile, we’ll obviously continue to play in mobile where we can, but that market is competitive. A lot of players in it. Margins are tight. But what’s interesting is the market is much more open in AR/VR glasses, in games, in automobiles. We can take that same touch and display driver technology, reapply it to different end markets, and then you have something that looks more IoT-ish and commands better prices, better gross margins, things like that.

VentureBeat: As far as the role of a fabless semiconductor chip designer versus making larger systems or sub-systems, has anything changed on that front for you?

Hurlston: We’re almost entirely chips, and obviously I think that gets us further upstream of technology, given the fact that we have to drive our chips. That goes into sub-systems that ultimately go into end products. Given the lead times, we see these technical trends before others do, like this concept of the visual wake word. That’s something we’re getting out in front of.

We do sub-systems here and there. We’re unique in that context. Our historic business is the touch controllers for PCs and fingerprint sensors. Some of the PCs have fingerprint sensing for biometrics. In some cases, we’ll make that whole sub-assembly — not just the IC that does the discrimination of where your finger is, but the entire pad itself and the paint and so on. Same with the fingerprint sensor. But that’s an increasingly small part of our business. Even our historic PC business, we’re getting more into chip sales than we are into sub-assembly sales.

VentureBeat: How many people are at the company now?

Hurlston: We have about 1,400 people, most of whom are engineers, as you’d expect.

VentureBeat: On the gaming side, do you see much changing as far as the kind of detection or sensing that’s going on?

Hurlston: AR/VR is going to be a much bigger thing. For the displays, that seems to be changing a lot as well, particularly in handheld games. You have the move from some of the pioneers to go to OLED. OLED has characteristics relative to latency and other things that are not particularly ideal. You can see it move — a lot of the gaming guys are talking about mini-LED or micro-OLED, which has much faster properties than the traditional OLED. We see display changes on the horizon. We’re trying to gear our technology up for that if and when those come up.

VentureBeat: What sort of applications are you looking forward to that don’t exist today?

Hurlston: We talked about embedded touch. We talked about the push for augmented reality, although of course that’s already here. We talked about these low-power visual sensors. That’s an area in which we’re pushing. We continue to evolve our video display technology into higher resolution, both panels and displays. Obviously being able to take lower bitstreams and upconvert those — that’s where we apply a lot of our AI in the video sector, upconversion from a lower pixel count to a higher pixel count. Those are the big vectors.

With these low-power sensors, again, it comes back to getting at — in my view the big application is just solving energy. It’s not necessarily a consumer problem. But it’s not just the energy required on chip to go back and forth to the datacenter. It’s now having a lot more control of light and power and air conditioning to turn that on and off. We’re trying to take the technology, in a micro sense — it’s more environmental, and that’s obvious when you have AI at the edge. But we’re then applying it to a more macro problem, which is the useless energy consumption that happens all the time. We’re trying to drive that message and apply the technology to that problem to the extent that we can.

Image Credit: Synaptics

VentureBeat: It feels like without some of these things, IoT was either incomplete or impractical. If you didn’t have energy efficiency or AI, you were brute-forcing these things into the world. You’d either need a lot more sensors or you were causing more pollution, whether on the network or in terms of the number of devices. When you add AI and energy efficiency, it feels more sensible to deploy all these things.

Hurlston: That’s absolutely true. Maybe taking it one step further back, having wireless connectivity has been a huge enabler for these kinds of gadgets. I never imagined that I’d have a doorbell that had electronic gadgets in it. I never imagined that you’d have a bike that has electronic gadgets in it. IoT started with low-power wireless connectivity that enabled things like scales or smoke detectors or bicycles to connect to other things. That was one.

Then, to your point, the next step in the evolution has been adding AI and other sensors to a connected device to make it more useful. I’ve been surprised by how many things we’re getting into on the wireless that have this connectivity. It’s crazy stuff that you wouldn’t imagine. That was the first enabler, the low-power wireless, whether it’s Bluetooth or wireless LAN or in some instances GPS. That capability is key. We have a Bluetooth and GPS chip inside a golf ball. It’s pretty obvious what the use case is. But think about that. OK, I can find my ball when it’s at the bottom of the lake. It started with the wireless connectivity.

VentureBeat: I wrote a story about one of the companies that are doing neural networks inside hearing aids. I never thought it would be useful in that context, but apparently they’re using it to suppress noise. It recognizes the sounds you don’t want to hear and suppresses them so you only hear people talking to you.

Hurlston: Right, you have to pick out the right frequencies. Going back to your point, the second leg of the stool is certainly AI now. Whether it’s voice or visuals as we’ve been discussing, you need AI as the second leg. You’d be surprised at where you can put these simple neural networks that make a difference.

VentureBeat: The new multimedia processor you just announced, can you talk about that?

Hurlston: That’s really slotted for these set-top box applications. It was the starting point — when we talked about the AI journey, we have bigger video processors where we can do training on the chip around object detection. The big use case in this particular area is around enhancing the video, being able to upscale from a low bitrate to a higher bitrate if your feed is relatively modest, like on these Roku streamers. You can get a really low bandwidth if you’re challenged as far as your internet connection. We can upscale the video using these processors, which is what the neural network is for.

The real catalyst for us is to get into a rather bland market, which is the service provider set-top box market, where we think we have some unique advantages. We can make a good business out of that. Another cool application we just announced is a voice biometrics partnership with a company that does voice prints. Instead of just recognizing a word, you recognize the speaker. That’s running on that same processor.","['able', 'michael', 'things', 'lot', 'ai', 'iot', 'business', 'hurlston', 'mobilepc', 'touch', 'internet', 'pivoted', 'sensors', 'technology', 'thats', 'synaptics', 'edge']","Under CEO Michael Hurlston, the 35-year-old company has pivoted away from its early markets and focused on artificial intelligence at the edge to bring greater efficiency to internet of things (IoT) devices.
VentureBeat: It’s interesting that you’re moving into IoT, and IoT devices are getting to the point where you can put AI into them.
We got reasonably good at neural networks on a high-power chip, and we were able to train the classic things.
Image Credit: SynapticsVentureBeat: It feels like without some of these things, IoT was either incomplete or impractical.
IoT started with low-power wireless connectivity that enabled things like scales or smoke detectors or bicycles to connect to other things."
25,https://www.sciencedaily.com/releases/2021/02/210211113917.htm,"Artificial emotional intelligence: a safer, smarter future with 5G and emotion recognition: Researchers introduce a 5G-enabled, AI-based emotion detection system and discuss its operation, application",2021-02-21 00:00:00,"With the advent of 5G communication technology and its integration with AI, we are looking at the dawn of a new era in which people, machines, objects, and devices are connected like never before. This smart era will be characterized by smart facilities and services such as self-driving cars, smart UAVs, and intelligent healthcare. This will be the aftermath of a technological revolution.

But the flip side of such technological revolution is that AI itself can be used to attack or threaten the security of 5G-enabled systems which, in turn, can greatly compromise their reliability. It is, therefore, imperative to investigate such potential security threats and explore countermeasures before a smart world is realized.

In a recent study published in IEEE Network, a team of researchers led by Prof. Hyunbum Kim from Incheon National University, Korea, address such issues in relation to an AI-based, 5G-integrated virtual emotion recognition system called 5G-I-VEmoSYS, which detects human emotions using wireless signals and body movement. ""Emotions are a critical characteristic of human beings and separates humans from machines, defining daily human activity. However, some emotions can also disrupt the normal functioning of a society and put people's lives in danger, such as those of an unstable driver. Emotion detection technology thus has great potential for recognizing any disruptive emotion and in tandem with 5G and beyond-5G communication, warning others of potential dangers,"" explains Prof. Kim. ""For instance, in the case of the unstable driver, the AI enabled driver system of the car can inform the nearest network towers, from where nearby pedestrians can be informed via their personal smart devices.""

The virtual emotion system developed by Prof. Kim's team, 5G-I-VEmoSYS, can recognize at least five kinds of emotion (joy, pleasure, a neutral state, sadness, and anger) and is composed of three subsystems dealing with the detection, flow, and mapping of human emotions. The system concerned with detection is called Artificial Intelligence-Virtual Emotion Barrier, or AI-VEmoBAR, which relies on the reflection of wireless signals from a human subject to detect emotions. This emotion information is then handled by the system concerned with flow, called Artificial Intelligence-Virtual Emotion Flow, or AI-VEmoFLOW, which enables the flow of specific emotion information at a specific time to a specific area. Finally, the Artificial Intelligence-Virtual Emotion Map, or AI-VEmoMAP, utilizes a large amount of this virtual emotion data to create a virtual emotion map that can be utilized for threat detection and crime prevention.

A notable advantage of 5G-I-VEmoSYS is that it allows emotion detection without revealing the face or other private parts of the subjects, thereby protecting the privacy of citizens in public areas. Moreover, in private areas, it gives the user the choice to remain anonymous while providing information to the system. Furthermore, when a serious emotion, such as anger or fear, is detected in a public area, the information is rapidly conveyed to the nearest police department or relevant entities who can then take steps to prevent any potential crime or terrorism threats.

However, the system suffers from serious security issues such as the possibility of illegal signal tampering, abuse of anonymity, and hacking-related cyber-security threats. Further, the danger of sending false alarms to authorities remains.

While these concerns do put the system's reliability at stake, Prof. Kim's team are confident that they can be countered with further research. ""This is only an initial study. In the future, we need to achieve rigorous information integrity and accordingly devise robust AI-based algorithms that can detect compromised or malfunctioning devices and offer protection against potential system hacks,"" explains Prof. Kim, ""Only then will it enable people to have safer and more convenient lives in the advanced smart cities of the future.""","['potential', 'future', 'prof', 'information', 'virtual', 'operation', 'researchers', 'emotion', 'system', 'recognition', 'detection', 'introduce', 'smarter', 'human', 'emotions', 'smart', 'safer', 'intelligence']","This smart era will be characterized by smart facilities and services such as self-driving cars, smart UAVs, and intelligent healthcare.
Emotion detection technology thus has great potential for recognizing any disruptive emotion and in tandem with 5G and beyond-5G communication, warning others of potential dangers,"" explains Prof. Kim.
This emotion information is then handled by the system concerned with flow, called Artificial Intelligence-Virtual Emotion Flow, or AI-VEmoFLOW, which enables the flow of specific emotion information at a specific time to a specific area.
Finally, the Artificial Intelligence-Virtual Emotion Map, or AI-VEmoMAP, utilizes a large amount of this virtual emotion data to create a virtual emotion map that can be utilized for threat detection and crime prevention.
A notable advantage of 5G-I-VEmoSYS is that it allows emotion detection without revealing the face or other private parts of the subjects, thereby protecting the privacy of citizens in public areas."
26,https://venturebeat.com/2021/02/13/thought-detection-ai-has-infiltrated-our-last-bastion-of-privacy/,Thought-detection: AI has infiltrated our last bastion of privacy,2021-02-13 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Our thoughts are private – or at least they were. New breakthroughs in neuroscience and artificial intelligence are changing that assumption, while at the same time inviting new questions around ethics, privacy, and the horizons of brain/computer interaction.

Research published last week from Queen Mary University in London describes an application of a deep neural network that can determine a person’s emotional state by analyzing wireless signals that are used like radar. In this research, participants in the study watched a video while radio signals were sent towards them and measured when they bounced back. Analysis of body movements revealed “hidden” information about an individual’s heart and breathing rates. From these findings, the algorithm can determine one of four basic emotion types: anger, sadness, joy, and pleasure. The researchers proposed this work could help with the management of health and wellbeing and be used to perform tasks like detecting depressive states.

Ahsan Noor Khan, a PhD student and first author of the study, said: “We’re now looking to investigate how we could use low-cost existing systems, such as Wi-Fi routers, to detect emotions of a large number of people gathered, for instance in an office or work environment.” Among other things, this could be useful for HR departments to assess how new policies introduced in a meeting are being received, regardless of what the recipients might say. Outside of an office, police could use this technology to look for emotional changes in a crowd that might lead to violence.

The research team plans to examine public acceptance and ethical concerns around the use of this technology. Such concerns would not be surprising and conjure up a very Orwellian idea of the ‘thought police’ from 1984. In this novel, the thought police watchers are expert at reading people’s faces to ferret out beliefs unsanctioned by the state, though they never mastered learning exactly what a person was thinking.

This is not the only thought technology example on the horizon with dystopian potential. In “Crocodile,” an episode of Netflix’s series Black Mirror, the show portrayed a memory-reading technique used to investigate accidents for insurance purposes. The “corroborator” device used a square node placed on a victim’s temple, then displayed their memories of an event on screen. The investigator says the memories: “may not be totally accurate, and they’re often emotional. But by collecting a range of recollections from yourself and any witnesses, we can help build a corroborative picture.”

If this seems farfetched, consider that researchers at Kyoto University in Japan developed a method to “see” inside people’s minds using an fMRI scanner, which detects changes in blood flow in the brain. Using a neural network, they correlated these with images shown to the individuals, and projected the results onto a screen. Though far from polished, this was essentially a reconstruction of what they were thinking about. One prediction estimates this technology could be in use by the 2040s.

Brain computer interfaces (BCI) are making steady progress on several fronts. In 2016, research at Arizona State University showed a student wearing what looks like a swim cap that contained nearly 130 sensors connected to a computer to detect the student’s brain waves.

The student is controlling the flight of three drones with his mind. The device lets him move the drones simply by thinking directional commands: up, down, left, right.

Advance a few years to 2019 and the headgear is far more streamlined. Now there are brain-drone races.

Besides the flight examples, BCIs are being developed for medical applications. MIT researchers have developed a computer interface that can transcribe words that the user verbalizes internally but does not actually speak aloud. A wearable device with electrodes pick-up neuromuscular signals in the jaw and face that are triggered by internal verbalizations, also referred to as subvocalizations. The signals are fed to a neural network that has been trained to correlate these signals with particular words. The idea behind this development is to meld humans and machines “such that computing, the internet, and AI would weave into human personality as a ‘second self.’” Those who cannot speak could use the technology to communicate as the subvocalizations could connect to a synthesizer that would speak the words.

Chip implants could be coming soon

The ultimate BCI could be that proposed by Neuralink, owned by Elon Musk. Unlike the previous examples, Neuralink promises direct implants into the brain. The near-term goal of Neuralink and others is to build a BCI that can cure a wide variety of diseases. Longer-term, Musk has a grander vision: He believes this interface will be necessary for humans to keep pace with increasingly powerful AI. Just last week, Musk announced that human trials of the implants could begin later this year. He claims the company already has a monkey with “a wireless implant in [his] skull with tiny wires who can play video games with his mind.”

The advancements being made in BCI are beginning to match what science fiction authors have dreamed up in works of fiction. In The Resisters, a new novel by Gish Jen, a “RegiChip” is implanted at birth into all of those deemed “Surplus,” meaning there will not be work for them in the aftermath of mass automation. Instead, they will be issued a universal basic income and have no responsibilities but to consume, to keep the automated economy operating at an efficient level. Among other things, the RegiChip is used to track everyone, their physical location but also their activities, to complete a surveillance society. Of course, the RegiChip, like all digital technologies, has the potential to be hacked.

Cognitive scientists have said that the mind is the software of the brain. Increasingly, physical software has the capacity to meld with and augment the human mind. If AI-enabled BCI achievements already seem unbelievable, it stands to reason that BCI breakthroughs in the not-too-distant future could be truly momentous. Will the technology be harnessed for positive use cases to cure diseases or for mind control? As with most technology, there will likely be both good and bad. Software is poised to eat the mind. For now, our unexpressed thoughts remain private, but that may no longer be true in the near future.

Gary Grossman is the Senior VP of Technology Practice at Edelman and Global Lead of the Edelman AI Center of Excellence.","['thoughtdetection', 'university', 'used', 'privacy', 'brain', 'signals', 'ai', 'infiltrated', 'bci', 'thought', 'work', 'technology', 'mind', 'student', 'bastion']","Outside of an office, police could use this technology to look for emotional changes in a crowd that might lead to violence.
Such concerns would not be surprising and conjure up a very Orwellian idea of the ‘thought police’ from 1984.
This is not the only thought technology example on the horizon with dystopian potential.
Longer-term, Musk has a grander vision: He believes this interface will be necessary for humans to keep pace with increasingly powerful AI.
Gary Grossman is the Senior VP of Technology Practice at Edelman and Global Lead of the Edelman AI Center of Excellence."
27,https://techbullion.com/how-ai-technology-makes-manufacturing-smarter/,How AI Technology Makes Manufacturing Smarter,2021-02-12 19:46:17+00:00,"Advances in the Industrial Internet of Things (IIoT), artificial intelligence (AI), machine learning, and advanced analytics are turbocharging the move to Industry 4.0. To stay competitive, companies are using AI-powered IoT tools to alert manufacturers to failures in equipment, set maintenance reminders, improve quality control measures and automate processes.

Machine learning for Processes Optimization

Machine learning algorithms can be applied to historical data about consumed energy in the past, to detect patterns and trends. Then, future energy consumption can be predicted.

For instance, if your production facility is turning out high-quality products, but you are still incurring high production costs, data may reveal that your facility is using too much energy. Data scientists can choose to run autoregressive models that reveal cyclic patterns of energy use. Deep neural networks can define and detect patterns, and forecast energy use quickly so that you are able to manage your output as needed.

Improving data quality ensures that the information that is received by your AI-powered IoT sensors is accurate, customized, and relevant to your concerns and tools.

Machine Learning for Predictive Maintenance

Manufacturing centers with large workloads or long production hours can take advantage of machine learning for predictive maintenance. In many instances, machine learning models can be used to limit shut-down completely, or at least mitigate any loss of manufacturing time.

As mentioned earlier, data quality is imperative when designing machine learning models. But, choosing the best model for the outcomes you desire is an important step as well. Several machine learning models are used to address frequent issues within the manufacturing sector.

An anomaly detection method compares normal system behavior against failure events. When a piece of equipment veers from the norm, the model can flag a device. Supervisors and maintenance staff can then investigate the issue and correct it on-the-spot, if necessary.

Regression models can predict the Remaining Useful Life (RUL) of a piece of equipment. They compare usage history and static data to determine how long the piece of equipment can continue output before failing. This allows manufacturers to know how long and how hard they can push equipment to meet an order, before scheduling down-time for maintenance.

Classification models can predict a failure within a selected period of time. The failure can range from a significant shortfall, to a routine issue. A maintenance team can evaluate when, and how to schedule maintenance.

Computer Vision for Quality Control

While AI can improve the manufacturing process, it is also capable of assisting with product quality control. Using computer imaging techniques and clean image data, an AI-powered algorithm can efficiently check for quality.

AI visual inspection is a cost-effective way for manufacturers to monitor product quality in real-time. Using computer vision for quality control is especially helpful for manufacturers in highly regulated industries. For instance, Audi and other auto manufacturers have used AI to assist in quality control checks.

A camera-based computer vision system scans parts using high-resolution images and GPU technology. Thanks to real-time video processing, AI quality control checks can alert supervisors to sudden drops in quality mid-production.

Each part’s scan is compared against historic images of perfect parts. This is accomplished by employing deep learning neural network integration. This technology offers greater accuracy than other computer screening options, because it is built using instance segmentation algorithms. Because the system is constantly collecting images of surface defects, the quality control system is continuously improving. Parts that do not match the historical criteria are flagged by the AI program and removed from the sorting area.

Edge AI: The Future of Manufacturing

IoT use cases show that manufacturers can employ edge AI to enhance the industrial Internet of Things processes.

Edge AI technology does not rely on the cloud, or bandwidth to communicate information. Placing this technology in a manufacturing setting can reduce cross-communication problems when too many devices are connected to the same network and mitigate bandwidth limitations. Eventually, this makes it possible to speed up the production cycle and increases throughput.

We are witnessing numerous cases where emerging technologies are creating a smarter ecosystem. Using edge AI and data analytics, a Japanese industrial electronics company, Daihen Corporation, eliminated 5,000 hours of manual data entry per year for electric transformers production.

Manufacturing systems today perform at no more than 90 percent efficiency, but they can’t get the last 10 percent because machines break. The core idea of edge AI is to never let the production system stop. As more manufacturers invest in smart systems, this investment will make a tremendous difference in the world’s economic efficiency.","['models', 'using', 'data', 'learning', 'manufacturers', 'ai', 'makes', 'system', 'production', 'smarter', 'manufacturing', 'technology', 'quality', 'control']","In many instances, machine learning models can be used to limit shut-down completely, or at least mitigate any loss of manufacturing time.
Several machine learning models are used to address frequent issues within the manufacturing sector.
Computer Vision for Quality ControlWhile AI can improve the manufacturing process, it is also capable of assisting with product quality control.
Thanks to real-time video processing, AI quality control checks can alert supervisors to sudden drops in quality mid-production.
Because the system is constantly collecting images of surface defects, the quality control system is continuously improving."
28,https://newscenter.lbl.gov/2021/02/12/applying-quantum-computing-to-a-particle-process/,Applying Quantum Computing to a Particle Process,2021-02-12 00:00:00,"Reddit Share 115 Shares

A team of researchers at Lawrence Berkeley National Laboratory (Berkeley Lab) used a quantum computer to successfully simulate an aspect of particle collisions that is typically neglected in high-energy physics experiments, such as those that occur at CERN’s Large Hadron Collider.

The quantum algorithm they developed accounts for the complexity of parton showers, which are complicated bursts of particles produced in the collisions that involve particle production and decay processes.

Classical algorithms typically used to model parton showers, such as the popular Markov Chain Monte Carlo algorithms, overlook several quantum-based effects, the researchers note in a study published online Feb. 10 in the journal Physical Review Letters that details their quantum algorithm.

“We’ve essentially shown that you can put a parton shower on a quantum computer with efficient resources,” said Christian Bauer, who is Theory Group leader and serves as principal investigator for quantum computing efforts in Berkeley Lab’s Physics Division, “and we’ve shown there are certain quantum effects that are difficult to describe on a classical computer that you could describe on a quantum computer.” Bauer led the recent study.

Their approach meshes quantum and classical computing: It uses the quantum solution only for the part of the particle collisions that cannot be addressed with classical computing, and uses classical computing to address all of the other aspects of the particle collisions.

Researchers constructed a so-called “toy model,” a simplified theory that can be run on an actual quantum computer while still containing enough complexity that prevents it from being simulated using classical methods.

“What a quantum algorithm does is compute all possible outcomes at the same time, then picks one,” Bauer said. “As the data gets more and more precise, our theoretical predictions need to get more and more precise. And at some point these quantum effects become big enough that they actually matter,” and need to be accounted for.

In constructing their quantum algorithm, researchers factored in the different particle processes and outcomes that can occur in a parton shower, accounting for particle state, particle emission history, whether emissions occurred, and the number of particles produced in the shower, including separate counts for bosons and for two types of fermions.

The quantum computer “computed these histories at the same time, and summed up all of the possible histories at each intermediate stage,” Bauer noted.

The research team used the IBM Q Johannesburg chip, a quantum computer with 20 qubits. Each qubit, or quantum bit, is capable of representing a zero, one, and a state of so-called superposition in which it represents both a zero and a one simultaneously. This superposition is what makes qubits uniquely powerful compared to standard computing bits, which can represent a zero or one.

Researchers constructed a four-step quantum computer circuit using five qubits, and the algorithm requires 48 operations. Researchers noted that noise in the quantum computer is likely to blame for differences in results with the quantum simulator.

While the team’s pioneering efforts to apply quantum computing to a simplified portion of particle collider data are promising, Bauer said that he doesn’t expect quantum computers to have a large impact on the high-energy physics field for several years – at least until the hardware improves.

Quantum computers will need more qubits and much lower noise to have a real breakthrough, Bauer said. “A lot depends on how quickly the machines get better.” But he noted that there is a huge and growing effort to make that happen, and it’s important to start thinking about these quantum algorithms now to be ready for the coming advances in hardware.

As hardware improves it will be possible to account for more types of bosons and fermions in the quantum algorithm, which will improve its accuracy.

Such algorithms should eventually have broad impact in the high-energy physics field, he said, and could also find application in heavy-ion-collider experiments.

Also participating in the study were Benjamin Nachman and Davide Provasoli of the Berkeley Lab Physics Division, and Wibe de Jong of the Berkeley Lab Computational Research Division.

This work was supported by the U.S. Department of Energy Office of Science. It used resources at the Oak Ridge Leadership Computing Facility, which is a DOE Office of Science user facility.

# # #

Founded in 1931 on the belief that the biggest scientific challenges are best addressed by teams, Lawrence Berkeley National Laboratory and its scientists have been recognized with 14 Nobel Prizes. Today, Berkeley Lab researchers develop sustainable energy and environmental solutions, create useful new materials, advance the frontiers of computing, and probe the mysteries of life, matter, and the universe. Scientists from around the world rely on the Lab’s facilities for their own discovery science. Berkeley Lab is a multiprogram national laboratory, managed by the University of California for the U.S. Department of Energy’s Office of Science.

DOE’s Office of Science is the single largest supporter of basic research in the physical sciences in the United States, and is working to address some of the most pressing challenges of our time. For more information, please visit energy.gov/science.","['lab', 'process', 'researchers', 'berkeley', 'classical', 'computing', 'particle', 'physics', 'computer', 'applying', 'bauer', 'quantum']","Their approach meshes quantum and classical computing: It uses the quantum solution only for the part of the particle collisions that cannot be addressed with classical computing, and uses classical computing to address all of the other aspects of the particle collisions.
The research team used the IBM Q Johannesburg chip, a quantum computer with 20 qubits.
Researchers noted that noise in the quantum computer is likely to blame for differences in results with the quantum simulator.
Also participating in the study were Benjamin Nachman and Davide Provasoli of the Berkeley Lab Physics Division, and Wibe de Jong of the Berkeley Lab Computational Research Division.
Berkeley Lab is a multiprogram national laboratory, managed by the University of California for the U.S. Department of Energy’s Office of Science."
29,https://siliconangle.com/2021/02/12/future-compute-will-big-small-smart-way-edge/,,,,,
30,https://towardsdatascience.com/a-data-science-perspective-to-automated-valuation-models-435a19326dbc,A Data Science perspective to Automated Valuation Models,2021-02-12 20:57:15.093000+00:00,"Real-estate, AVM

A Data Science perspective to Automated Valuation Models

Photo by Avi Waxman on Unsplash

In this article, we are going to discuss and analyse the European standards for Statistical Valuation Methods for Residential Properties, published by the European AVM Alliance in August 2019. Our goal is to present a summary of this guide as we understand it from a data science perspective.

In an era of Alexa, Siri, blockchain and Artificial Intelligence (AI), property valuations require manual examination of documents and some times onsite inspection by experts to conclude on the price of an asset. This is an error-prone process as usually no two experts agree on the same price.

Automated Valuation Models (AVM) aim in automating this process removing bias and subjectivity from these decisions as much as possible. This is not a new concept but as AI has seen exponential growth due to its unprecedented accuracy and performance in the last few years, AVMs gain more popularity as a consequence. Your typical tutorial on machine learning will probably provide you with a real-estate related dataset to play with. Even so, only a small fraction of real-estate businesses actually take advantage of these systems and even worse some do not even use databases to store property-related data.

The European Standards is a guide on the development of property valuation models and specifically what to consider when building such systems. Importantly however it aims at increasing the understanding, transparency and clarity of the existing Statistical Valuation Methods and the selection of the most appropriate method and evaluation approach.

As mentioned, the aim of valuation models is to estimate the value of a given property on the basis of the values of other properties, similar to how an expert would do. The guide examines four main different approaches, namely, House Price index, single parameter valuation, hedonic models and comparable-based models.

Prior to delving into the details of the modelling approaches the guide defines a few main concepts such that to make sure all readers have a common understanding of the terms used throughout. For example, a key question is what do we mean by Market Value? For a lending institution, it is the estimated amount for which the property should exchange on the date of valuation between a willing buyer and a willing seller in an arm’s-length transaction after proper marketing wherein the parties had each acted knowledgeably, prudently and without being under compulsion. For accounting is the price which a buyer would be prepared to pay for it, having due regard to its condition and location and on the assumption that it could continue to be used.

A statistical valuation method may have different uses and thus may be different in their nature. Specifically, it may require different degrees of detail, accuracy and information on the result. For instance, the accuracy of the valuation of a property on a popular UK property search website is not as crucial as the valuation of the property for a mortgage application. For some intended uses the required amount of detail may or may not be available to feed a Statistical Valuation Method.

Another concept is Mass appraisal. This is the practice of portfolio valuations, i.e., valuing multiple properties as of a given date. With all things being equal, a lesser degree of granularity sometimes associated with data in property portfolios results in lower accuracy than a greater degree of granularity. However, the amount of time required to value these properties outweighs the risk, on some occasions. Thus it makes them the perfect candidate for automation.

The guide states that models must be calibrated before use. In data science, we typically use cross-validation and hyperparameter tuning to adjust any free model parameters to better fit our data; balancing this way the risk of overfitting (i.e., the risk of the model learning only the input — extremely well — and not being able to generalise on unseen data). In the guide, this is referred to as a requirement of mathematical skill and expertise. Additionally, equally important is to have relevant and sufficient market data.

As an AVM developer, you must ensure data quality. As we say, bad data in, bad output. Also, data and perhaps the model should be updated regularly in order to reflect new transactions and capture new trends or important characteristics. The AVM has to be objective and conform to any national laws applicable and the developer must keep an open door policy for auditing the model by 3rd parties.

Another aspect covered in the guide is model performance and model evaluation. It suggests that the model must be tested and validated. Since no model is perfect its error must be quantified and understood in order to grasp its limitations. The testing should replicate real-world situations such that the results to be representative of such scenarios. The de facto data science approach to validating models is cross-validation. Cross-validation is the technique of training a machine learning model on a subset of the data and test on another subset never seen by your model before. However, you can repeat the process by taking different subsets of your data for training and testing, while never training a model on data that it will be tested later on. In other words, the aim is to thoroughly test the model in a series of out-of-sample datasets. This is a so-called test set.

The guide also stresses a point about what the model aims to predict. In data science, we refer to that as a target variable. This is NOT the asking price but rather the actual value of the property which is reflected in the sales price. Note that this is not always true as someone might have to sell if they need to liquidate urgently but it is the best way to assess its value.

No matter what we aim to predict we always need a robust way to measure the success of the model. The metrics for success vary, but Coverage is an important concept in the guide that captures the portion of predictions that are within an acceptable range. In other words, this is the ratio of cases producing a valid result divided by the total number of cases. Also, the guide refers to bias and dispersion which is the classical Bias versus Variance trade-off in data science and machine learning. This is about whether the model always overestimates or underestimates the price, i.e., it is biased, while at the same time is consistent with its prediction in terms of how far each prediction is from the actual price. In case the predictions are always far off by a different margin, this means the model has high variance.

Finally, the guide talks about three main techniques for a valuation model, none of which utilises machine learning per se but rather implies traditional statistical models. The four approaches are the House Price Index (HPI), Single Parameter Valuation, Hedonic model and Comparable-based Automated Valuation Models.

In reality, HPIs are just indices that capture an aggregated statistic about properties in some areas and it is not the right tool to value a specific property. Sometimes HPIs are just expert opinions while other times are aggregations of properties in a time period. Another HPI method is the Basket of Goods Approach. In a Basket of Goods Approach, the HPI is calculated from individual values of the same properties over time. The quality of each valuation still depends on the quality of valuation of the properties in the basket and it still depends on what properties are in the basket. Repeat Sales Indices is also an HPI that reflects the recorded Sale Prices of the same properties at two or more points in time. However, what happens if there are no many sales in a specific region or the interval between sales is long? In any case, HPI does not give a prediction for a property but rather potential changes from a reference point.

Single parameter valuations are models that aim to estimate the value of a property based on a single parameter, along with its geography. A generalisation of this is the hedonic models.

Hedonic models are multivariate regression models that have property characteristics as input and the parameters are learned from the data. This is towards a data science approach but it excludes models that do not have the form of a linear or non-linear equation.

Comparable-based AVMs are tools that select the most similar properties in relation to some reference property. This seems more like a K-nearest neighbour approach, where each property is a data point in an N-dimensional space and the aim is to identify ‘K’ other properties that match the most to the referenced one. However, defining what is a comparable can be challenging. Is it a combination of location and size or a linear combination of all the property characteristics? So what property characteristics do we look for when looking for comparables and what happens if no exact comparable exists?

Photo by Possessed Photography on Unsplash

In data science, we employ different algorithms and approaches to utilise the historic data we have and we do not necessarily need to choose one of these four approaches.

In strategy when you are given four options you choose the fifth one.

That is not to say that existing approaches are not useful. Indeed, they are very useful and perhaps complement each other. In addition, however, there are machine learning techniques like Random Forests and Gradient Boosting that is shown to perform well in most regression and classification problems. There are stochastic non-parametric processes that capture non-linear relationships between input and target variables and there are deep learning approaches used in the latest Artificial Intelligence applications. All of these can capture relationships than no single linear or non-linear regression model can capture, they can provide predictions for individual properties and address the issue of no direct comparable by generalising on the whole dataset the model was taught.","['models', 'properties', 'data', 'price', 'property', 'model', 'automated', 'learning', 'science', 'valuation', 'perspective', 'guide']","Our goal is to present a summary of this guide as we understand it from a data science perspective.
Automated Valuation Models (AVM) aim in automating this process removing bias and subjectivity from these decisions as much as possible.
The European Standards is a guide on the development of property valuation models and specifically what to consider when building such systems.
The guide examines four main different approaches, namely, House Price index, single parameter valuation, hedonic models and comparable-based models.
The four approaches are the House Price Index (HPI), Single Parameter Valuation, Hedonic model and Comparable-based Automated Valuation Models."
31,https://towardsdatascience.com/cognitive-risk-management-7c7bcfe84219,Cognitive Risk Management,2021-02-13 17:29:55.980000+00:00,"Spotlight

Cognitive Risk Management

by Alexander Petrov, John Thomas, Ricardo Balduino, Maxime Allard, and Aakanksha Joshi

Image by authors

Introduction

Machine Learning (ML) applications have become ubiquitous. News about AI for self-driving cars, online customer support, virtual personal assistants, and so on come daily. And yet, it may not be obvious how to connect existing business practices with all these amazing innovations. A frequently overlooked area is the application of natural language processing (NLP) and deep learning to help process huge volumes of business documentation quickly and effectively to find the proverbial needle in the haystack.

One of the domains that allows organic application of ML is risk management for financial institutions and insurance companies. There are many questions that organizations face regarding how to apply ML to improve risk management. Here are just a few of them:

· How to identify impactful use cases that can benefit from using artificial intelligence?

· How to bridge the gap between intuitive expectations of subject matter experts and capabilities of technology?

· How to integrate ML into an existing enterprise information system?

· How to control the behavior of ML models in a production environment?

This article aims to share the experiences of the IBM Data Science and AI Elite (DSE) and IBM Expert Labs teams, based on multiple client engagements in the risk controls area. IBM DSE has built various accelerators which can help organizations jump start their adoption of ML. Here, we will go through use cases in the risk management space, introduce a cognitive risk controls accelerator, and discuss how machine learning can transform enterprise business practices in this space.

Risk Management Sketch

In 2020, multiple financial institutions were hit with fines exceeding hundreds of millions of dollars per individual organization. The reason for the fines was an inadequate risk controls state.

This triggered a call for financial companies to ensure high quality of the large numbers of risk controls they have to work with. This includes explicitly identifying risks, implementing risk controls to prevent risk development, and finally establishing testing procedures.

For non-specialists, risk control is a bit perplexing. What is this about? A simple definition is that risk controls are put in place to monitor risks for a company’s business operations. E.g., a security risk may be that an intruder guesses a password and as a result gets access to someone’s account. A possible risk control may be designed as establishing a policy that requires long and non-trivial passwords enforced through the organization’s systems. As a consequence of the Sarbanes-Oxley Act (SOX), public companies require means to efficiently manage such risks and, as part of this effort to build risk controls and assess the quality of these controls.

An important element for risk managers is whether the controls are well defined. The assessment for this may be done through answering questions like who monitors the risk, what should be done for risk identification or prevention, how often the control procedure should be done in the organization’s life cycle, etc. All these questions should be answered. Now we need to realize that the number of such controls in an enterprise is from thousands to hundreds of thousands and it is very difficult to make an assessment of the controls corpus manually. This is where contemporary AI technology is able to help.

Of course, this type of challenge is just an illustration and it would be impractical to attempt covering the vast area of risk management within a single article, so we focus on a few specific challenges that practitioners face in their day-to-day practice and that were already implemented using the Cognitive Risk Controls accelerator.

There are not many public risk controls databases available, so the solution in the accelerator is based on NIST Special Publication 800–53 for security controls that is available at https://nvd.nist.gov/800-53. This security controls data base is small, but it allows us to demonstrate the approaches that can be scaled to large volumes and different domains of risk controls.

Using Text Analytics and Deep Learning for Risk Controls

One of the key use case categories is to rationalize the existing risk controls: the challenge is that there may be a lot of historic aspects to how the existing risk controls were developed. E.g., some risk controls may be built by copying other existing controls with minimal modifications. Another example is that some risk controls may be formed by integrating multiple risk controls into one. Common consequences of this approach are duplicated controls and the presence of controls that are not relevant to the business any longer. One of the most difficult challenges is to assess the general state of quality for the existing risk controls. Hence, the first target from a business perspective is to build quality assessment: automatically assessing the quality of control descriptions saves huge time on routine reading of descriptions by focusing only on those which are really important to review and improve. A good question is how AI comes into the picture here. NLP-based ML models have become very effective at common language-related tasks and, in particular, at challenges such as answering questions. One type of model that can be referenced here is based on the Transformer architecture (for more details, please see an article about Transformer architecture at https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04).

In the risk management sketch, the ability to answer questions about a risk control description was key to assessing the quality of the control’s descriptions. From a birds-eye view, the number of the unanswered questions is a good indicator of the quality of a control description. The best news is that with the capabilities of contemporary AI models such as Transformers and with additional practical rules, this technique of asking the right questions becomes an effective mechanism to control a large volume of control descriptions by a small team with the assistance of AI.

Controls Quality Assessment (image by authors)

Frequently, finding duplicates in documents is considered a straightforward task and Levenshtein Distance can help find items expressed with similar wording. However, this becomes a much more challenging task if we would like to find semantically similar descriptions. This is another area where contemporary AI can be helpful — the embeddings built using large neural networks (e.g. autoencoders, language models, etc.) can capture semantic similarity. From a practical outcomes perspective, our experience has been that the identification of duplicates and overlaps may lead to reducing the volume of controls up to 30 per cent.

Analysis of Overlaps (image by authors)

Additionally, it became a common practice to analyze the internal structure of information through ML techniques such as clustering. This allows the business practitioners to better understand the content of controls on a larger scale and to see whether the existing taxonomy for risks and controls is well aligned with the content, or what may be missing in both.

Clustering Example (image by authors)

The previous use cases were focused on the analysis of existing controls. Another use case focuses on helping risk managers create new risk controls. Recommending controls for a given risk using semantic similarity can significantly reduce manual effort and provide a flexible template for building controls. Machine learning can help here with analysis of the risk description and figuring out the right set of controls to address each risk.

In large organizations, it is typical that teams work on solutions and best practices which may be used by other teams. Adopting best practices across the organization requires extensive training. Machine learning can be very useful in such situations. An example may be classification of controls as preventive or detective. In this use case, we use supervised machine learning to extend the classification of controls to the whole set of controls by using the existing labeled set from a particular team, i.e. knowledge transfer is done using machine learning as opposed to time consuming training of personnel.

Cognitive technology in the IBM DSE risk controls accelerator allows us to structure the risk controls, to recommend controls for the risk formulated in natural language, to identify overlaps within the controls, and to analyze the quality of controls.

The accelerator delivers a cognitive controls analytics application that integrates the developed models and applies them to unstructured risk controls content.

Cognitive Risk Controls Implementation Using IBM Cloud Pak for Data

Logically, the Cognitive Risk Controls Accelerator contains several components:

The first one is a so-called cognitive assistant — it is an application that applies ML models to facilitate content processing, e.g., by identifying the risk control priority, category, and assessing the quality of the control description. As part of productization, a cognitive assistant becomes a part of the enterprise informational system.

Cognitive Assistant (Image by authors)

The second component is content analysis: when the data is enriched via Machine Learning models, Watson Discovery content mining can be used to find insights in the enriched content

Content Analysis with Watson Discovery (image by authors)

Yet another component is a set of Jupyter notebooks that support Data Science models

Jupyter Notebook in Watson Studio (image by authors)

Let’s look under the hood of the accelerator-based implementation using IBM Cloud Pak for Data.

Before we do this let’s briefly review the IBM platform and approach. IBM has a prescriptive approach to the journey to AI called the AI ladder. In his “AI Ladder: Demystifying AI Challenges” Rob Thomas (SVP, IBM Cloud & Cognitive Software) substantiated that to turn your data into insights your organization should follow the phases listed below:

• Collect — ability for easy data access, including virtualizing the data

• Organize — the means to cataloguing your data, building data dictionaries, and ensuring rules and policies on accessing data

• Analyze — this includes delivering the ML models, using data science for identifying the insights using cognitive tools and AI techniques. This naturally requires building, deploying, and managing your machine learning models

• Infuse — from a lot of perspectives, a key phase. This refers to the ability to operationalize AI models in a way that allows the business to trust the outcomes, that is, to use your machine learning models in enterprise systems in a production mode while being able to ensure ongoing performance of these models and their explainability.

Cloud Pak for Data is IBM’s multi-cloud Data & AI platform delivering an information architecture and providing all the outlined capabilities. The following diagram captures the details of developing an implementation in the context of the AI Ladder.

Phases (image by authors)

It captures the phases of implementing a cognitive risk controls project based on the DSE accelerator:

The first two phases in implementing a risk controls project are acquiring and cataloguing the data set — as an example, in the accelerator we are using the NIST controls data set. Controls here are expressed as free text descriptions.

The next phase is the enrichment of the acquired unstructured data which is done in Watson Studio: clustering is used as a way to understand the internal structure of content. The risk control narrative may be quite long and multiple topics may be discussed, so some mechanism may be required to track changing topics as the description progresses. In our practice for clustering, we used both K-means on top of embeddings and Latent Dirichlet Allocation (LDA). It does require careful coordination of data scientists and subject matter experts as the mathematics may not align ideally with the expectations of SMEs. A wider range of enrichments is also possible here — a good example is classifying the quality of descriptions.

Topic Modeling (image by authors)

When the enrichment is finished, we need to understand the resulting data set. This leads us to the Exploration phase. In practice, the challenge is volume; content review is one of the most time-consuming processes as it requires careful reading of a large volume of text. How can we explore huge volumes of unstructured information? Watson Discovery content mining is the tool that makes this possible and greatly reduces the effort.

After the content is reviewed by SMEs, it forms the basis for building supervised machine learning models. The IBM platform provides the means to deploy the models, monitor drift, and get explainability for the decisions made by complex models. All of this is covered by the operationalization of machine learning and supported by IBM Cloud Pak For Data.

Conclusion

This article introduced one of the growing areas of application of machine learning in contemporary business — cognitive risk controls. Please do not hesitate to reach out to the IBM Data Science and AI Elite Team if you are interested in knowing more about cognitive risk controls and AI technology. Also, please contact IBM if you see that your use cases are similar to the ones presented, or if your business and technical challenges may be addressed by the mentioned approaches or tools.

Acknowledgements

Authors (IBM DSE and Expert Labs) express gratitude to their colleagues for the continued collaboration and development of the business and technological approaches for cognitive controls: Stephen Mills(Managing Director, IBM Promontory), Miles Ravitz (Sr. Principal, IBM Promontory), Rodney Rideout (Delivery Executive, IBM Global Business Services), Vinay Rao Dandin (Data Scientist), Aishwarya Srinivasan (Data Scientist, IBM DSE), and Rakshith Dasenahalli Lingaraju (Data Scientist, IBM DSE).","['models', 'data', 'management', 'ibm', 'learning', 'ai', 'business', 'cognitive', 'controls', 'control', 'risk']","Here, we will go through use cases in the risk management space, introduce a cognitive risk controls accelerator, and discuss how machine learning can transform enterprise business practices in this space.
This includes explicitly identifying risks, implementing risk controls to prevent risk development, and finally establishing testing procedures.
Another example is that some risk controls may be formed by integrating multiple risk controls into one.
Cognitive technology in the IBM DSE risk controls accelerator allows us to structure the risk controls, to recommend controls for the risk formulated in natural language, to identify overlaps within the controls, and to analyze the quality of controls.
Cognitive Risk Controls Implementation Using IBM Cloud Pak for DataLogically, the Cognitive Risk Controls Accelerator contains several components:The first one is a so-called cognitive assistant — it is an application that applies ML models to facilitate content processing, e.g., by identifying the risk control priority, category, and assessing the quality of the control description."
32,https://www.navsea.navy.mil/Media/News/SavedNewsModule/Article/2502702/nswc-dahlgren-division-automates-target-detection-and-tracking-for-armys-next-g/,NSWC Dahlgren Division Automates Target Detection and Tracking for Army’s Next Generation Combat Vehicle,,"Soldiers of 1st Squadron, 10th Cavalry Regiment, 2nd Brigade Combat Team, 4th Infantry Division drive their M3A3 Bradley Fighting Vehicles to reach a phase line where they will move into a defensive posture during platoon scout training. The Army anticipates that the Intelligent Tracker – a new technological capability developed by a team of Naval Surface Warfare Center Dahlgren Division (NSWCDD) scientists and engineers – will enhance automation for direct fire weapon systems while increasing the speed of operator engagements against targets. The automation applies to a wide range of applications to include the 30mm main gun on a Bradley Fighting Vehicle. (U.S. Army photo by Staff Sgt. Andrew Porch/Released)

DAHLGREN, Va. – Several potential targets walk in an urban setting as Intelligent Tracker automatically generates their optical target tracks. The Intelligent Tracker – a new product developed by Naval Surface Warfare Center Dahlgren Division (NSWCDD) – will increase the Army’s Next Generation Combat Vehicle’s intelligent fire control capability to control its medium and large caliber weapon systems. Optical target tracks are generated automatically for use in direct fire weapon systems. The tracks are used in conjunction with a fire control system to lock onto a specific threat, easing the burden on the operator. Intelligent Tracker is capable of automatically generating target tracks for a wide range of targets and adding additional automation to the weapon control processes. (U.S. Navy image/Released) (Photo by U.S. Navy)

DAHLGREN, Va. – When the Army, Navy and Marine Corps need technological expertise related to autonomous weaponized manned and unmanned systems on their platforms, they routinely collaborate with a cadre of experts at Naval Surface Warfare Center Dahlgren Division (NSWCDD).

The joint services – aware of the Intelligent Tracker team’s expertise and capabilities – have been contacting the NSWCDD Unmanned and Autonomous Systems Branch for the past decade with requests to make a highly automated detection and tracking capability a reality in their fire control systems.

In response, the NSWCDD team worked with sponsors to develop the Automated Remote Engagement System (ARES), which is fundamental to detecting, tracking and automating the kill chain for manned and unmanned weapon systems in platforms from Navy warships to Army and Marine Corps light armored vehicles.

“Our goal is to enhance the level of automation for direct fire weapon systems while increasing the speed of operator engagements against targets,” said Ben Goldman, NSWCDD engineer who is the project lead at the branch. The automation applies to a wide range of applications from “a remote weapon station shooting a .50-cal on the unmanned boat tested on the Potomac River Test Range or the 30mm main gun on a Bradley Fighting Vehicle or a Stryker Combat Vehicle.”

The ARES team installed an automated weapon system on an unmanned boat, known as the common unmanned surface vehicle, and demonstrated it for distinguished visitors on Dahlgren’s Potomac River Test Range in 2018. The event demonstrated prototype automation of significant portions of the kill chain that is tactically effective with minimal dependence on a datalink while operating beyond the warfighter’s line of sight.

In February 2020 – prior to COVID-19 pandemic travel and work restrictions – Goldman and his team of three engineers began to collaborate with Army engineers at the Weapons and Software Engineering Center at DEVCOM Armaments Center in New Jersey. Their collaboration focused on a novel effort to bring the ARES detection and tracking capability to the Next Generation Intelligent Fire Control (NGIFC) project.

“We have not been there since,” said Goldman. “Our team has been able to utilize the tools and the workflow that we worked really hard to put in place prior to the pandemic. We were able to execute the product this fiscal year with basically no schedule slip due to COVID.”

The new Dahlgren-developed product, called the Intelligent Tracker, will increase the Army’s Next Generation Combat Vehicle’s NGIFC capability to control its medium and large caliber weapon systems.

The Intelligent Tracker innovation – made possible with state-of-the-art algorithms developed over 10 years of cumulative research at NSWCDD – adds a rapid and precise automated target detection and tracking capability to the kill chain for manned and unmanned weapon systems.

What’s more, the Intelligent Tracker features a deep-learning based neural network prepared for image-based object detection on thermal and visible image inputs. This detector was programmed using open source datasets and datasets curated by the Dahlgren team.

“We’re processing the video from the gun turret cameras,” said Goldman. “We’re running a series of image processing algorithms upon those videos on embedded hardware so we’re able to do it in real time and apply state of the art processing algorithms on that video while providing operator aides and increased automation to shorten the kill chain cycle.”

The team is currently optimizing the performance for application on an Nvidia Jetson chip, taking advantage of Nvidia tools, to prune the neural network and reducing the computational precision to allow for faster calculation. The initial performance measures bode well for the use of these algorithms in detection, aided target recognition and tracking at the frame rate of the incoming video for use as an operator aide in direct fire gunnery.

The team’s future work on the Next Generation Intelligent Fire Control Project will focus on improving the performance of the algorithms and datasets while continuing to benchmark their effectiveness.

“We have really good collaboration tools in place for our research, development, test and evaluation moving forward,” said Goldman. “Our tools include a DevSecOps (Development Security Operations) pipeline in place to build, share and collaborate on our code.”

Although the Intelligent Tracker’s Navy civilian engineers are NSWCDD employees, “We work very closely with DEVCOM Armaments Center in New Jersey,” said Goldman. “Our previous work for the Navy and Marine Corps was sponsored by ONR (Office of Naval Research) and various places in OSD (Office of the Secretary of Defense). It is definitely a joint effort that led to the knowledge base we needed to execute the Intelligent Tracker project. To bring this technology to the warfighter in an ethical and responsible manner is what we’re after, and to give our troops the edge in any future conflicts is also what we’re after.”

Meanwhile, the Navy is interested in the technological edge that the project’s capabilities can bring to bear on its weapon systems.

“We were funded this year to do a study on what it would take to bundle up what we’ve done for the Army and transfer it into the fire control for the next generation MK 38 system, adding that capability to the Navy side,” said Goldman.

The Navy – specifically, the Surface Ship Weapons Directorate at the Naval Sea System Command’s Program Executive Office for Integrated Warfare Systems –– is in the process of upgrading the MK 38 to a 30mm gun system. The MK 38 is a medium caliber machine gun system installed for ship self-defense to counter small threats in multiple domains. “The work we do is tremendously stimulating on an academic and technical level,” said Goldman, who credits the scientists and engineers at the NSWCDD Unmanned and Autonomous Systems Branch for their research, development, test and evaluation that made the intelligent automation technology possible.

They are among hundreds of NSWCDD technical experts who are leading the development of unmanned and autonomous systems while providing solutions to meet warfighter needs. The solutions often require rapid integration of unmanned and autonomous systems and technologies in Army, Navy and Marine Corps systems, mobile systems, weapons systems, engagement systems and combat systems.

“We’re using the best of the best hardware, software and tools while working with people directly and indirectly throughout the world who are brilliant – people who have pushed the technology forward in the past 10 years,” said Goldman. “This is not just a theoretical exercise – we’re working toward live fire demonstrations and proof of concept prototype capabilities, so we get to see the tangible results of what we do. The work is incredibly interesting, especially knowing that we’re riding the wave of state-of-the-art technology and bringing it to bear on military problems.”","['nswcdd', 'weapon', 'target', 'detection', 'systems', 'nswc', 'goldman', 'dahlgren', 'tracker', 'intelligent', 'vehicle', 'combat', 'unmanned', 'division', 'generation', 'tracking', 'automation', 'navy', 'automates', 'control']","Andrew Porch/Released)DAHLGREN, Va. – Several potential targets walk in an urban setting as Intelligent Tracker automatically generates their optical target tracks.
Optical target tracks are generated automatically for use in direct fire weapon systems.
Intelligent Tracker is capable of automatically generating target tracks for a wide range of targets and adding additional automation to the weapon control processes.
What’s more, the Intelligent Tracker features a deep-learning based neural network prepared for image-based object detection on thermal and visible image inputs.
It is definitely a joint effort that led to the knowledge base we needed to execute the Intelligent Tracker project."
33,https://syncedreview.com/2021/02/12/deepmind-ucls-alchemy-is-a-best-of-both-worlds-3d-video-game-for-meta-rl/,DeepMind & UCL’s Alchemy Is a ‘Best-of-Both-Worlds’ 3D Video Game for Meta-RL DeepMind & UCL's Alchemy Is a 'Best-of-Both-Worlds' 3D Video Game for Meta-RLSynced,2021-02-12 00:00:00,"A research team from DeepMind and University College London have introduced Alchemy, an open-source benchmark for meta-RL research.

In recent years, reinforcement learning (RL) has garnered much attention in the field of machine learning. The approach does not require labelled data and has yielded remarkable successes on a wide variety of specific tasks. RL unfortunately continues to struggle with issues such as sample efficiency, generalization, and transfer learning. To address these drawbacks, researchers have been exploring meta-reinforcement learning (meta-RL), in which learning strategies can quickly adapt to novel tasks by using experience gained on a large set of tasks that have a shared structure.

Although a number of interesting and innovative meta-RL techniques have been proposed, there exists no ideal task benchmark for testing new algorithms. Progress in the field can only be sustained if existing work can be reproduced and accurately compared to assess the performance of new methods, and Alchemy is designed to address this.

Meta learning is inspired by humans’ ability to generate and tackle new tasks by drawing on experiences gleaned from other, related learning tasks. It provides a new learning paradigm wherein agents can gain experience over multiple learning episodes – often covering a distribution of related tasks – and use this experience to improve future learning performance. This leads to a variety of benefits as learning strategies improve on both lifetime and evolutionary timescales.

Meta-RL environments thus present the learner not with a single task, but instead with a task distribution, where all the involved tasks share high-level features. There are two ideal features for benchmark meta-RL task distribution: accessible and interesting. Accessible ensures a complete knowledge of the full task distribution, while interesting means the displayed properties are comparable to the structural richness of challenging real-world tasks.

Unfortunately, previous works on meta-RL benchmarks have failed to achieve both, and are either accessible without being interesting (such as bandit tasks), or interesting without being accessible (such as Atari games). Alchemy aims to check both boxes as a “best-of-both-worlds” benchmark for meta-RL research.

Alchemy is a 3D video game played in a series of trials that fit together into episodes. At the beginning of each trial, the agent is presented with a set of stones, containers filled with coloured liquids (potions), and a central cauldron. Gameplay involves using the potions to treat and boost the value of the stones, which are then added to the cauldron to register the maximum possible point value within a fixed time limit.

Visual observation for Alchemy

The value of the stones is tied to their perceptual features (size, color and shape), and the task thus constitutes learning a “chemistry” that governs how different potions affect different stones across trials in an episode. At the start of each new episode however, the stones and the potions’ transformative effects are changed. While this resampling creates many possible chemistries, there are also a number of principles that span all episodes. For example, potions come in fixed pairs (e.g. red/green) that always produce opposite effects. A good meta-learner will identify and exploit such regularities.

Four example chemistries

Alchemy involves a compositional set of latent causal relationships, and requires strategic experimentation and action sequencing. In addition, the game levels are created based on an explicit generative process, resulting in an accessible structure that is also interesting.

Benchmark and baseline-agent evaluation episode scores (mean ± standard error over 1000 episodes)

The researchers evaluated the Alchemy environment on two powerful deep RL agents (IMPALA and V-MPO). Although these agents have achieved impressive performances in single-task RL environments, they displayed very poor meta-learning performance in Alchemy even after extensive training. The team says the results reflect a failure of structure learning and latent-state inference involved in meta-learning, validating Alchemy as a useful benchmark task for meta-RL research.



The paper Alchemy: A Structured Task Distribution for Meta-Reinforcement Learning is on arXiv. The code and other resources have been open-sourced and are available on the project GitHub.

Author: Hecate He | Editor: Michael Sarazen

Share this: Twitter

Facebook

","['metarl', 'task', 'distribution', 'benchmark', 'learning', 'stones', 'game', 'tasks', 'bestofbothworlds', 'video', 'deepmind', 'alchemy', 'ucls', '3d', 'metarlsynced', 'interesting', 'potions']","A research team from DeepMind and University College London have introduced Alchemy, an open-source benchmark for meta-RL research.
Although a number of interesting and innovative meta-RL techniques have been proposed, there exists no ideal task benchmark for testing new algorithms.
Meta learning is inspired by humans’ ability to generate and tackle new tasks by drawing on experiences gleaned from other, related learning tasks.
There are two ideal features for benchmark meta-RL task distribution: accessible and interesting.
The paper Alchemy: A Structured Task Distribution for Meta-Reinforcement Learning is on arXiv."
34,https://uk.finance.yahoo.com/news/worldwide-automotive-artificial-intelligence-industry-110300294.html,"Worldwide Automotive Artificial Intelligence Industry to 2026 - Players Include Alphabet, Intel and IBM Among Others",,"Globe Newswire

Dublin, Feb. 15, 2021 (GLOBE NEWSWIRE) -- The ""Global Network Attached Storage Market By Solutions, By Deployment Type, By Design, By Storage Solution, By Industry Vertical, By Region, Industry Analysis and Forecast, 2020 - 2026"" report has been added to ResearchAndMarkets.com's offering. The Global Network Attached Storage (NAS) Market size is expected to reach $58.3 billion by 2026, rising at a market growth of 16.7% CAGR during the forecast period. A device that is used for retrieval and storage of data from a centralized disk capacity is known as Network Attached Storage (NAS). This device is easy to connect to the LAN and can be utilized by heterogeneous users during the time of storing & accessing their data securely and reliably. In order to control these devices, a browser-based utility is used and thus doesn't contain a display or keyboard in its architecture. NAS devices are quite adaptable in nature and can be scaled up when extra storage is needed. They are quicker, user-friendly, having superior capacity at a quite low cost. This technology has been adopted by small and medium enterprises, particularly in developing countries.The rising installation of NAS systems in enterprise environments is encouraging vendors toward developing customized NAS solutions for the companies focusing on NAS as a fully-fledged data management solution. The incorporation of on-premise NAS with cloud storage is anticipated to become popular in the coming years, promoting complete control over the data in the NAS and in storing and archiving data in the cloud. Many vendors are engaged in incorporating the existing NAS system with renowned cloud storage services such as Amazon S3 particularly, for storage provisioning.The growing number of government initiatives towards digitalization and rising R&D activities for enhancements of NAS devices that consume lower bandwidth, are anticipated to offer growth opportunities for the global network attached storage market. The installation of NAS devices is rising due to increasing demand from small and large businesses for an efficient and effective solution for managing, storing, and accessing contents across the networks.By Deployment TypeBased on Deployment Type, the market is segmented into On-premises and Cloud & Hybrid. The organizations that are installing on-premises solutions have significant investments in ICT infrastructure. On the other hand, the organizations are moving towards cloud-based solutions from on-premise to manage the operations of the business in a cost-effective manner. Hence, the on-premise segment is anticipated to showcase a diminishing demand in the next few years.By DesignBased on Design, the market is segmented into 1Bay to 8 Bays, 8 Bays to 12 Bays, 12 Bays to 20 Bays and Others. In 2019, NAS systems with 1 bay to 8 bays accounted for the highest revenue share, and the same trend will be witnessed in the next couple of years. This is credited to the increasing adoption of 1-bay to 8-bay NAS solutions by small and medium businesses, education, homes, and research centers, media and entertainment companies, and business and consulting service providers.By SolutionsBased on Solutions, the market is segmented into Hardware and Software. The Hardware market dominated the Global Network Attached Storage (NAS) Market by Solutions 2019. Additionally, The Software market is estimated to grow at a CAGR of 18.9% during (2020 - 2026).By Storage SolutionBased on Storage Solution, the market is segmented into Scale-up NAS and Scale-out NAS. The scale-out segment is expected to maintain a considerable development over the upcoming years. It enables improved performance, non-disruptive operations, and offers enhanced efficiency and agility. These factors will boost the installation of these solutions across the industries.By Industry VerticalBased on Industry Vertical, the market is segmented into BFSI, Energy & Utilities, Healthcare, Retail & eCommerce, Government & Defense, IT & Telecommunication, Manufacturing and Others. The retail & e-commerce sector is anticipated to grow at the significant CAGR during the forecast period. E-commerce has facilitated customers to carry out mobile banking activities which have produced a large customer database. Besides, due to technological developments, retailers are flourishing on information investing in and predictive analytics to obtain relevant insights.By RegionBased on Regions, the market is segmented into North America, Europe, Asia Pacific, and Latin America, Middle East & Africa. North America is anticipated to procure a significant market share. The developed economies like the U.S. are anticipated to push market development in North America. The Larin America, Middle East & Africa region is anticipated to showcase gradual development during the forecast period.The major strategies followed by the market participants are Product Launches and Partnerships. Based on the Analysis presented in the Cardinal matrix; Microsoft Corporation is the major forerunner in the Network Attached Storage (NAS) Market. Companies such as Hitachi, Ltd., Western Digital Corporation, NetApp, Inc., and Dell Technologies, Inc., Hewlett Packard Enterprise Company, Huawei Technologies Co., Ltd., Netgear, Inc., are some of the key innovators in the market.The market research report covers the analysis of key stake holders of the market. Key companies profiled in the report include Dell Technologies, Inc., Hitachi, Ltd., NetApp, Inc., Hewlett Packard Enterprise Company, IBM Corporation, Netgear, Inc., Huawei Technologies Co., Ltd. (Huawei Investment & Holding Co., Ltd.), Cisco Systems, Inc., Microsoft Corporation and Western Digital Corporation.Unique Offerings from the Publisher Exhaustive coverageHighest number of market tables and figuresSubscription based model availableGuaranteed best priceAssured post sales research support with 10% customization free Key Topics Covered: Chapter 1. Market Scope & Methodology1.1 Market Definition1.2 Objectives1.3 Market Scope1.4 Segmentation1.4.1 Global Network Attached Storage (NAS) Market, by Solutions1.4.2 Global Network Attached Storage (NAS) Market, by Deployment Type1.4.3 Global Network Attached Storage (NAS) Market, by Design1.4.4 Global Network Attached Storage (NAS) Market, by Storage Solution1.4.5 Global Network Attached Storage (NAS) Market, by Industry Vertical1.4.6 Global Network Attached Storage (NAS) Market, by Geography1.5 Methodology for the researchChapter 2. Market Overview2.1 Introduction2.1.1 Overview2.1.2 Market Composition and Scenario2.2 Key Factors Impacting the Market2.2.1 Market Drivers2.2.2 Market RestraintsChapter 3. Competition Analysis - Global3.1 Cardinal Matrix3.2 Recent Industry Wide Strategic Developments3.2.1 Partnerships, Collaborations and Agreements3.2.2 Product Launches and Product Expansions3.2.3 Acquisition and Mergers3.3 Top Winning Strategies3.3.1 Key Leading Strategies: Percentage Distribution (2016-2020)3.3.2 Key Strategic Move: (Partnerships, Collaborations, and Agreements : 2016, Apr - 2020, Dec) Leading PlayersChapter 4. Global Network Attached Storage (NAS) Market by Storage Solution4.1 Global Network Attached Storage (NAS) Scale-up NAS Market by Region4.2 Global Network Attached Storage (NAS) Scale-out NAS Market by RegionChapter 5. Global Network Attached Storage (NAS) Market by Industry Vertical5.1 Global BFSI Network Attached Storage (NAS) Market by Region5.2 Global Energy & Utilities Network Attached Storage (NAS) Market by Region5.3 Global Healthcare Network Attached Storage (NAS) Market by Region5.4 Global Retail & eCommerce Network Attached Storage (NAS) Market by Region5.5 Global Government & Defense Network Attached Storage (NAS) Market by Region5.6 Global IT & Telecommunication Network Attached Storage (NAS) Market by Region5.7 Global Manufacturing Network Attached Storage (NAS) Market by Region5.8 Global Other Industry Vertical Network Attached Storage (NAS) Market by RegionChapter 6. Global Network Attached Storage (NAS) Market by Solutions6.1 Global Network Attached Storage (NAS) Hardware Market by Region6.2 Global Network Attached Storage (NAS) Software Market by RegionChapter 7. Global Network Attached Storage (NAS) Market by Deployment Type7.1 Global On-premises Network Attached Storage (NAS) Market by Region7.2 Global Cloud & Hybrid Network Attached Storage (NAS) Market by RegionChapter 8. Global Network Attached Storage (NAS) Market by Design8.1 Global 1Bay to 8 Bays Network Attached Storage (NAS) Market by Region8.2 Global 8 Bays to 12 Bays Network Attached Storage (NAS) Market by Region8.3 Global 12 Bays to 20 Bays Network Attached Storage (NAS) Market by Region8.4 Global Others Network Attached Storage (NAS) Market by RegionChapter 9. Global Network Attached Storage (NAS) Market by Region9.1 North America Network Attached Storage (NAS) Market9.2 Europe Network Attached Storage (NAS) Market9.3 Asia Pacific Network Attached Storage (NAS) Market9.4 LAMEA Network Attached Storage (NAS) MarketChapter 10. Company Profiles10.1 Dell Technologies, Inc.10.1.1 Company Overview10.1.2 Financial Analysis10.1.3 Segmental and Regional Analysis10.1.4 Research & Development Expense10.1.5 Recent strategies and developments:10.1.5.1 Partnerships, Collaborations, and Agreements:10.1.5.2 Acquisition and Mergers:10.1.5.3 Product Launches and Product Expansions:10.1.6 SWOT Analysis:10.2 Hitachi, Ltd.10.2.1 Company Overview10.2.2 Financial Analysis10.2.3 Segmental and Regional Analysis10.2.4 Research & Development Expense10.2.5 Recent strategies and developments:10.2.5.1 Partnerships, Collaborations, and Agreements:10.2.5.2 Product Launches and Product Expansions:10.2.6 SWOT Analysis10.3 NetApp, Inc.10.3.1 Company Overview10.3.2 Financial Analysis10.3.3 Regional Analysis10.3.4 Research & Development Expense10.3.5 Recent strategies and developments:10.3.5.1 Partnerships, Collaborations, and Agreements:10.3.5.2 Acquisition and Mergers:10.3.5.3 Product Launches and Product Expansions:10.3.6 SWOT Analysis10.4 Hewlett Packard Enterprise Company10.4.1 Company Overview10.4.2 Financial Analysis10.4.3 Segmental Analysis10.4.4 Research & Development Expense10.4.5 Recent strategies and developments:10.4.5.1 Partnerships, Collaborations, and Agreements:10.4.5.2 Product Launches and Product Expansions:10.4.6 SWOT Analysis10.5 IBM Corporation10.5.1 Company Overview10.5.2 Financial Analysis10.5.3 Regional & Segmental Analysis10.5.4 Research & Development Expenses10.5.5 SWOT Analysis10.6 Netgear, Inc.10.6.1 Company Overview10.6.2 Financial Analysis10.6.3 Segmental and Regional Analysis10.6.4 Research & Development Expenses10.6.5 Recent strategies and developments:10.6.5.1 Partnerships, Collaborations, and Agreements:10.6.5.2 Product Launches and Product Expansions:10.7 Huawei Technologies Co., Ltd. (Huawei Investment & Holding Co., Ltd.)10.7.1 Company Overview10.7.2 Financial Analysis10.7.3 Segmental and Regional Analysis10.7.4 Research & Development Expense10.7.5 Recent strategies and developments:10.7.5.1 Partnerships, Collaborations, and Agreements:10.8 Cisco Systems, Inc.10.8.1 Company Overview10.8.2 Financial Analysis10.8.3 Segmental and Regional Analysis10.8.4 Research & Development Expense10.8.5 Recent strategies and developments:10.8.5.1 Partnerships, Collaborations, and Agreements:10.8.6 SWOT Analysis10.9 Microsoft Corporation10.9.1 Company Overview10.9.2 Financial Analysis10.9.3 Segmental and Regional Analysis10.9.4 Research & Development Expenses10.9.5 Recent strategies and developments:10.9.5.1 Partnerships, Collaborations, and Agreements:10.9.5.2 Acquisition and Mergers:10.9.6 SWOT Analysis10.10. Western Digital Corporation10.10.1 Company Overview10.10.2 Financial Analysis10.10.3 Regional Analysis10.10.4 Research & Development Expenses10.10.5 Recent strategies and developments:10.10.5.1 Product Launches and Product Expansions:10.10.5.2 Acquisition and Mergers:For more information about this report visit https://www.researchandmarkets.com/r/3u8jph CONTACT: CONTACT: ResearchAndMarkets.com Laura Wood, Senior Press Manager press@researchandmarkets.com For E.S.T Office Hours Call 1-917-300-0470 For U.S./CAN Toll Free Call 1-800-526-8630 For GMT Office Hours Call +353-1-416-8900","['storage', 'development', 'attached', 'company', 'nas', 'ibm', 'intel', 'industry', 'global', 'automotive', 'players', 'artificial', 'research', 'network', 'intelligence', 'product', 'worldwide', 'include', 'alphabet', 'market']","Market Scope & Methodology1.1 Market Definition1.2 Objectives1.3 Market Scope1.4 Segmentation1.4.1 Global Network Attached Storage (NAS) Market, by Solutions1.4.2 Global Network Attached Storage (NAS) Market, by Deployment Type1.4.3 Global Network Attached Storage (NAS) Market, by Design1.4.4 Global Network Attached Storage (NAS) Market, by Storage Solution1.4.5 Global Network Attached Storage (NAS) Market, by Industry Vertical1.4.6 Global Network Attached Storage (NAS) Market, by Geography1.5 Methodology for the researchChapter 2.
Global Network Attached Storage (NAS) Market by Storage Solution4.1 Global Network Attached Storage (NAS) Scale-up NAS Market by Region4.2 Global Network Attached Storage (NAS) Scale-out NAS Market by RegionChapter 5.
Global Network Attached Storage (NAS) Market by Industry Vertical5.1 Global BFSI Network Attached Storage (NAS) Market by Region5.2 Global Energy & Utilities Network Attached Storage (NAS) Market by Region5.3 Global Healthcare Network Attached Storage (NAS) Market by Region5.4 Global Retail & eCommerce Network Attached Storage (NAS) Market by Region5.5 Global Government & Defense Network Attached Storage (NAS) Market by Region5.6 Global IT & Telecommunication Network Attached Storage (NAS) Market by Region5.7 Global Manufacturing Network Attached Storage (NAS) Market by Region5.8 Global Other Industry Vertical Network Attached Storage (NAS) Market by RegionChapter 6.
Global Network Attached Storage (NAS) Market by Design8.1 Global 1Bay to 8 Bays Network Attached Storage (NAS) Market by Region8.2 Global 8 Bays to 12 Bays Network Attached Storage (NAS) Market by Region8.3 Global 12 Bays to 20 Bays Network Attached Storage (NAS) Market by Region8.4 Global Others Network Attached Storage (NAS) Market by RegionChapter 9.
Global Network Attached Storage (NAS) Market by Region9.1 North America Network Attached Storage (NAS) Market9.2 Europe Network Attached Storage (NAS) Market9.3 Asia Pacific Network Attached Storage (NAS) Market9.4 LAMEA Network Attached Storage (NAS) MarketChapter 10."
35,https://europeangaming.eu/portal/latest-news/2021/02/12/86619/agechecked-and-rdentify-partner-to-help-identify-vulnerable-players/,AgeChecked and Rdentify partner to help identify vulnerable players,2021-02-12 00:00:00,"Reading Time: 3 minutes

AgeChecked have strengthened their commitment to Responsible Gambling by forming a comprehensive partnership with Rdentify, the specialists in identifying vulnerable customers through machine learning.

Rdentify use AI and cutting-edge technology to monitor live chats, giving each customer a score based on their risk of gambling-related harm. Through natural language processing (NLP), they can monitor every line from a company’s live chat and use machine learning models to highlight early signs of self-exclusion and problem gambling.

Their risk-based scoring system works in real time and easily integrates with your company’s CRM using a simple traffic light system to denote the level of risk. Instances of concern will be highlighted to customer support staff, who can then deal with the situation in an effective manner in line with the operator’s safer gambling policies. This can be done either through the CRM or Rdentify’s own API.

AgeChecked are the market leaders in age verification software and offer monitoring solutions for the iGaming industry across every stage of the customer journey. Their partnership with Rdentify will particularly help operators fulfill their licensing conditions in ensuring the ongoing monitoring of all players with continued KYC and due diligence, as well as in the prevention of gambling-related harm.

The signs of problem gambling are often subtle and spread across multiple areas of the business. Players also often chat to different support agents over time, meaning vulnerable players and their common cues are often missed. The sheer number of live chats on a day-to-day basis also make it difficult for humans to monitor. The Rdentify solution can integrate and analyse all of this data, both historical and current. By integrating AI into your pre-existing systems, the chance of spotting the signs of problem gambling earlier are therefore significantly increased.

An operator’s risk data can also be visualised so in-depth trend analysis can be studied, while Rdentify never process or store any personal data. All such data is anonymised and only the player ID will be transferred through the engine. The system has been developed by consulting some of the iGaming industry’s best data science, compliance and operations experts, meaning it is tailor-made for gambling operators.

Like AgeChecked’s systems, Rdentify’s is regularly updated to accommodate new problem identification factors and legislation changes, meaning the solution is flexible as well as scalable.

With the issue of responsible gambling becoming increasingly important, particularly in the British media ahead of the review of the 2005 Gambling Act, the scrutiny on the gambling industry and their responsibilities has never been higher. There are approximately 430,000 problem gamblers in the UK and two million identified as being at risk, with the industry suffering huge reputational damage in recent years due to these figures.

Rdentify allows you to identify these problem gamblers with increased accuracy and prevent them from gambling on your site, thus increasing compliance with Safer Gambling legislation and reducing fines and reputational damage. Rdentify and AgeChecked also enable operators to mitigate legal threats to operators while putting player protection front and centre of your operation.

Speaking about this importance, Daniel Brookes, the CEO of Rdentify, said: “Online gambling operators rely solely on customer support agents to recognise the signs of problem gambling during their live chat interactions. Globally, the iGaming industry is highly regulated and fast-growing, with a strong focus on player protection. Since 2017 in the UK, remote operators have been fined more than £63.5m by the UK Gambling Commission for social responsibility failings.

“Rdentify provides a scalable natural language processing (NLP) and machine learning-based engine which flags end-user vulnerability risk during live chat interactions. Rdentify seamlessly integrates with the operators’ existing operational systems and outputs clear, actionable scores and risk categorisation in real-time. This solution de-risks the operator, protects end-users and allows for a superior customer experience.”

On announcing the partnership with AgeChecked, Brookes said: “I am delighted to be announcing our partnership with AgeChecked. Not only does it give us a fantastic opportunity to work with a forward-thinking, well-known tech company, but their customer-centered approach matches with our own.

“Their focus on customer protection, which is delivered in an easy to digest and business-friendly manner, goes hand-in-hand with our own ethos, which we believe will help to deliver a strong and successful partnership for many years to come.”

Alastair Graham, the Founder and CEO of AgeChecked, said: “The issue of Responsible Gambling has never been more important, so we are delighted to partner with Rdentify in offering a holistic solution for operators.

“Combining AgeChecked’s cutting-edge age verification software with Rdentify’s live chat monitoring, our solution can ensure full compliance across every stage of the customer journey. We are committed to reducing gambling-related harm and helping operators obtain and maintain their regulatory compliance. We see innovative technological solutions such as those employed by Rdentify as being key in the fight against problem gambling.”

To find out more about the AgeChecked Player Vulnerability Monitoring solution please go to https://www.agechecked.com/player-vulnerability-monitoring/","['players', 'data', 'customer', 'vulnerable', 'rdentify', 'partner', 'problem', 'help', 'gambling', 'operators', 'risk', 'live', 'partnership', 'solution', 'identify', 'agechecked']","The signs of problem gambling are often subtle and spread across multiple areas of the business.
Players also often chat to different support agents over time, meaning vulnerable players and their common cues are often missed.
The Rdentify solution can integrate and analyse all of this data, both historical and current.
By integrating AI into your pre-existing systems, the chance of spotting the signs of problem gambling earlier are therefore significantly increased.
An operator’s risk data can also be visualised so in-depth trend analysis can be studied, while Rdentify never process or store any personal data."
36,https://www.wfmz.com/news/pr_newswire/pr_newswire_technology/quantiphi-named-as-an-idc-innovator-in-artificial-intelligence-services/article_c228db75-a2a3-553e-93fb-fc8273fcec96.html,Quantiphi Named as an IDC Innovator in Artificial Intelligence Services,,"The views expressed by public comments are not those of this company or its affiliated companies. Please note by clicking on ""Post"" you acknowledge that you have read the TERMS OF USE and the comment you are posting is in compliance with such terms. Your comments may be used on air. Be polite. Inappropriate posts or posts containing offsite links, images, GIFs, inappropriate language, or memes may be removed by the moderator. Job listings and similar posts are likely automated SPAM messages from Facebook and are not placed by WFMZ-TV.","['artificial', 'terms', 'wfmztv', 'views', 'used', 'removed', 'posts', 'inappropriate', 'idc', 'services', 'similar', 'innovator', 'spam', 'named', 'comments', 'quantiphi', 'intelligence']","The views expressed by public comments are not those of this company or its affiliated companies.
Please note by clicking on ""Post"" you acknowledge that you have read the TERMS OF USE and the comment you are posting is in compliance with such terms.
Your comments may be used on air.
Inappropriate posts or posts containing offsite links, images, GIFs, inappropriate language, or memes may be removed by the moderator.
Job listings and similar posts are likely automated SPAM messages from Facebook and are not placed by WFMZ-TV."
37,https://www.tvtechnology.com/news/nhl-selects-aws-for-cloud-infrastructure,NHL Selects AWS for Cloud Infrastructure,2021-02-12 19:04:41+00:00,"SEATTLE —Amazon Web Services is hitting the ice with the National Hockey League, as the two sides have come to an agreement that will see AWS become the official cloud, AI and machine learning infrastructure provider for the NHL.

AWS’ services are designed to help the NHL automate video processing and content delivery in the cloud, including leveraging its Puck and Player Tracking (PPT) system that is shared with media partners, teams and fans.

The NHL is also expected to build an enterprise video platform on AWS to aggregate video, data and related applications into one central repository to improve search and retrieval of archival footage, give broadcasters instant access to NHL content for syndication and licensing and facilitate the creation and delivery of new in-game analysis, predictions and video highlights for mobile, online and broadcast experiences. This new system will encode, process, store and transmit game footage from a series of new camera angles to provide continuous video feeds that capture plays and events outside the view of traditional broadcast cameras.

With Amazon Kinesis, which collects and analyzes video and data streams in real time, and machine learning services like Amazon SageMaker, the NHL will be able to audit its feeds to broadcast partners in real time. This will help create a smart monitoring system that detects and automatically fixes potential feed issues to ensure a seamless viewing experience across viewing platforms.

“AWS’s state-of-the-art technology and services will provide us with capabilities to deliver analytics and insights that highlight the speed and skill of our game to drive deeper fan engagement,” said NHL Commissioner Gary Bettman. “AWS is unmatched in the portfolio of cloud services that it delivers, including computer vision and machine learning, and we intend to leverage them across the board to provide advanced analysis to our teams, officials, and media partners faster than ever before. We’re thrilled to have AWS join the NHL’s family of blue-chip technology partners as we continue our focus on innovation and building the most advanced technology solutions in sports.”

The NHL will introduce its new AWS services and technology throughout the upcoming NHL season.","['nhl', 'learning', 'services', 'selects', 'partners', 'system', 'cloud', 'video', 'machine', 'technology', 'provide', 'infrastructure', 'aws']","SEATTLE —Amazon Web Services is hitting the ice with the National Hockey League, as the two sides have come to an agreement that will see AWS become the official cloud, AI and machine learning infrastructure provider for the NHL.
AWS’ services are designed to help the NHL automate video processing and content delivery in the cloud, including leveraging its Puck and Player Tracking (PPT) system that is shared with media partners, teams and fans.
With Amazon Kinesis, which collects and analyzes video and data streams in real time, and machine learning services like Amazon SageMaker, the NHL will be able to audit its feeds to broadcast partners in real time.
“AWS’s state-of-the-art technology and services will provide us with capabilities to deliver analytics and insights that highlight the speed and skill of our game to drive deeper fan engagement,” said NHL Commissioner Gary Bettman.
We’re thrilled to have AWS join the NHL’s family of blue-chip technology partners as we continue our focus on innovation and building the most advanced technology solutions in sports.”The NHL will introduce its new AWS services and technology throughout the upcoming NHL season."
38,https://towardsdatascience.com/synthetic-data-applications-in-data-privacy-and-machine-learning-1078bb5dc1a7,Synthetic Data: Applications in Data Privacy and Machine Learning,2021-02-13 04:50:06.532000+00:00,"Synthetic Data: Applications in Data Privacy and Machine Learning

“Data is the new oil in the digital era”¹. Software engineers and data scientists often need access to large volumes of real data to develop, experiment, and innovate. Collecting such data unfortunately also introduces security liabilities and privacy concerns which affect individuals, organizations, and society at large. Data containing Personally Identifiable Information (PII) and Personal Health Information (PHI) are particularly vulnerable to disclosure, and need to be protected.

Regulations such as the General Data Protection Regulation (GDPR)² serve to provide a level of legal protection for user data, but consequently introduce new technical challenges by restricting data usage, collection, and storage methods. In light of this, synthetic data could serve as a viable solution to protect user data privacy, stay compliant with regulations, and still maintain the pace and ability for development and innovation.

In this article, we will dive into the details of the significant benefits synthetic data offers:

To support machine learning model development, allowing for faster iteration of experimentation and model building. To facilitate collaboration between data owners and external data scientists/engineers, in a more secure, and privacy compliant way.

What is Synthetic Data?

As the name suggests, synthetic data is in essence, ‘fake’ data that is artificially or programmatically generated, as opposed to ‘real’ data that is collected through real-world surveys or events. The creation of synthetic data stems from real data, and a good synthetic dataset is able to capture the underlying structure and display the same statistical distributions as the original data, rendering it indistinguishable from the real one.

The first major benefit of synthetic data is its ability to support machine learning/deep learning model development. Often, developers need the flexibility to quickly test an idea or experiment with building a new model. However, sometimes it takes weeks to acquire and prepare sufficient amounts of data. Synthetic data opens the gateway for faster iteration of model training and experimenting, as it provides a blueprint of how models can be built on real data. Additionally, with synthetic data, ML practitioners gain complete sovereignty over the dataset. This includes, controlling the degree of class separations, sampling size, and degree of noise of the dataset. In this article, we will show you how to improve an imbalanced dataset for machine learning with synthetic data.

The second major benefit of synthetic data is that it can protect data privacy. Real data contains sensitive and private user information that cannot be freely shared and is legally constrained. Approaches to preserve data privacy such as the k-anonymity model³ involve omitting data records to a certain extent. This results in an overall loss of information and data utility. In such cases, synthetic data serves as an excellent alternative to these data anonymization techniques. Synthetic datasets can be more openly published, shared, analyzed, without revealing actual individual information.","['user', 'information', 'privacy', 'data', 'model', 'learning', 'need', 'applications', 'machine', 'dataset', 'synthetic', 'real']","Synthetic Data: Applications in Data Privacy and Machine Learning“Data is the new oil in the digital era”¹.
As the name suggests, synthetic data is in essence, ‘fake’ data that is artificially or programmatically generated, as opposed to ‘real’ data that is collected through real-world surveys or events.
The second major benefit of synthetic data is that it can protect data privacy.
Approaches to preserve data privacy such as the k-anonymity model³ involve omitting data records to a certain extent.
In such cases, synthetic data serves as an excellent alternative to these data anonymization techniques."
39,https://www.analyticsinsight.net/a-new-ai-algorithm-detecting-asymptomatic-carriers-of-covid-19/,,,,,
40,https://venturebeat.com/2021/02/12/ai-weekly-techno-utopianism-in-the-workplace-and-the-threat-of-excessive-automation/,AI Weekly: Techno-utopianism in the workplace and the threat of excessive automation,2021-02-12 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Every so often, VentureBeat writes a story about something that needs to go away. A few years back, my colleague Blair Hanley Frank argued that AI systems like Einstein, Sensei, and Watson must go because corporations tend to overpromise results for their products and services. I’ve taken runs at charlatan AI and white supremacy.

This week, a series of events at the intersection of the workplace and AI lent support to the argument that techno-utopianism has no place in the modern world. Among the warning signs in headlines was a widely circulated piece by a Financial Times journalist who said she was wrong to be optimistic about robots.

The reporter describes how she used to be a techno-optimist but in the course of her reporting found that robots can crunch people into their system and force them to work at a robot’s pace. In the article, she cites the Center for Investigative Reporting’s analysis of internal Amazon records that found instances of human injury were higher in Amazon facilities with robots than in facilities without robots.

“Dehumanization and intensification of work is not inevitable,” wrote the journalist, who’s quite literally named Sarah O’Connor. Fill in your choice of Terminator joke here.

Also this week: The BBC quoted HireVue CEO Kevin Parker as saying AI is more impartial than a human interviewer. After facing opposition on multiple fronts, HireVue announced last month it would no longer use facial analysis in its AI-powered video interview analysis of job candidates. Microsoft Teams got similar tech this week to recognize and highlight who’s enjoying a video call.

External auditors have examined the Al used by HireVue and hiring software company Pymetrics, which refers to its AI as “entirely bias free,” but the processes seem to have raised more questions than they’ve answered.

And VentureBeat published an article about a research paper with a warning: Companies like Google and OpenAI have a matter of months to confront negative societal consequences of the large language models they release before they perpetuate stereotypes, replace jobs, or are used to spread disinformation.

What’s important to understand about that paper, written by researchers at OpenAI and Stanford, is that before criticism of large language models became widespread, research and dataset audits found major flaws in large computer vision datasets that were over a decade old, like ImageNet and 80 Million Tiny Images. An analysis of face datasets dating back four decades also found ethically questionable practices.

A day after that article was published, OpenAI cofounder Greg Brockman tweeted what looked like an endorsement of a 90-hour work week. Run the math on that. If you slept seven hours a night, you would have about four hours a day to do anything that is not work — like exercise, eating, resting, or spending time with your family.

An end to techno-utopianism doesn’t have to mean the death of optimistic views about ways technology can improve human lives. There are still plenty of people who believe that indoor farming can change lives for the better or that machine learning can accelerate efforts to address climate change.

Google AI ethics co-lead Margaret Mitchell recently made a case for AI design that keeps the bigger picture in mind. In an email sent to company leaders before she was placed under investigation, she said consideration of ethics and inclusion is part of long-term thinking for long-term beneficial outcomes.

“The idea is that, to define AI research now, we must look to where we want to be in the future, working backwards from ideal futures to this moment, right now, in order to figure out what to work on today,” Mitchell said. “When you can ground your research thinking in both foresight and an understanding of society, then the research questions to currently focus on fall out from there.”

With that kind of long-term thinking in mind, Google’s Ethical AI team and Google DeepMind researchers have produced a framework for carrying out internal algorithm audits, questioned the wisdom of scale when addressing societal issues, and called for a culture change in the machine learning community. Google researchers have also advocated rebuilding the AI industry according to principles of anticolonialism and queer AI and evaluating fairness using sociology and critical race theory. And ethical AI researchers recently asserted that algorithmic fairness cannot simply be transferred from Western nations to non-Western nations or those in the Global South, like India.

The death of techno-utopia could entail creators of AI systems recognizing that they may need to work with the communities their technology impacts and do more than simply abide by the scant regulations currently in place. This could benefit tech companies as well as the general public. As Parity CEO Rumman Chowdhury told VentureBeat in a recent story about what algorithmic auditing startups need to succeed, unethical behavior can have reputation and financial costs that stretch beyond any legal ramifications.

Computer scientists trying to find a path to 'Ethical AI' but refusing to learn anything about white supremacy, heteropatriarchy, capitalism, ablism, or settler colonialism pic.twitter.com/vPf0aa4gLa — Sasha Costanza-Chock (@schock) February 8, 2021

The lack of comprehensive regulation may be why some national governments and groups like Data & Society and the OECD are building algorithmic assessment tools to diagnose risk levels for AI systems.

Numerous reports and surveys have found automation on the rise during the pandemic, and events of the past week remind me of the work of MIT professor and economist Daron Acemoglu, whose research has found one robot can replace 3.3 human jobs.

In testimony before Congress last fall about the role AI will play in the economic recovery in the United States, Acemoglu warned the committee about the dangers of excessive automation. A 2018 National Bureau of Economic Research (NBER) paper coauthored by Acemoglu says automation can create new jobs and tasks, as it has done in the past, but says excessive automation is capable of constraining labor market growth and has potentially acted as a drag on productivity growth for decades.

“AI is a broad technological platform with great promise. It can be used for helping human productivity and creating new human tasks, but it could exacerbate the same trends if we use it just for automation,” he told the House Budget committee. “Excessive automation is not an inexorable development. It is a result of choices, and we can make different choices.”

To avoid excessive automation, in that 2018 NBER paper Acemoglu and his coauthor, Boston University research fellow Pascual Restrepo, call for reforms of the U.S. tax code, because it currently favors capital over human labor. They also call for new or strengthened institutions or policy to ensure shared prosperity, writing, “If we do not find a way of creating shared prosperity from the productivity gains generated by AI, there is a danger that the political reaction to these new technologies may slow down or even completely stop their adoption and development.”

This week’s events involve complexities like robots and humans working together and language models with billions of parameters, but they all seem to raise a simple question: “What is intelligence?” To me, working 90 hours a week is not intelligent. Neither is perpetuating bias or stereotypes with language models or failing to consider the impact of excessive automation. True intelligence takes into account long-term costs and consequences, historical and social context, and, as Sarah O’Connor put it, makes sure “the robots work for us, and not the other way around.”

For AI coverage, send news tips to Khari Johnson and Kyle Wiggers and AI editor Seth Colaner — and be sure to subscribe to the AI Weekly newsletter and bookmark our AI channel, The Machine.

Thanks for reading,

Khari Johnson

Senior AI Staff Writer","['automation', 'research', 'used', 'researchers', 'ai', 'excessive', 'weekly', 'human', 'work', 'workplace', 'threat', 'week', 'robots', 'technoutopianism']","A day after that article was published, OpenAI cofounder Greg Brockman tweeted what looked like an endorsement of a 90-hour work week.
And ethical AI researchers recently asserted that algorithmic fairness cannot simply be transferred from Western nations to non-Western nations or those in the Global South, like India.
In testimony before Congress last fall about the role AI will play in the economic recovery in the United States, Acemoglu warned the committee about the dangers of excessive automation.
“Excessive automation is not an inexorable development.
Neither is perpetuating bias or stereotypes with language models or failing to consider the impact of excessive automation."
41,https://towardsdatascience.com/facebook-detr-transformers-dive-into-the-object-detection-world-39d8422b53fa,Facebook DETR: Transformers dive into the Object Detection World,2021-02-12 14:18:53.386000+00:00,"Facebook DETR: Transformers dive into the Object Detection World

Photo by Samule Sun on Unsplash

Most of the great recent machine learning papers are based on transformers. They are powerful effective machine learning models that have proven they are worth the time and effort to optimize. Recently, Facebook published a new paper that uses transformers to outperform state-of-the-art Faster RCNNs in object detection.

Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components.The new model is conceptually simple and does not require a specialized library. We show that it significantly outperforms com- petitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.

Source: arxiv

The paper examines the weakness of current object detection solutions such as post-processing steps to collapse near-duplicate predictions [1] and propose novel solutions using the attention mechanism offered by transformers. The paper also introduces an end-to-end framework without any customizable layers and with code for ease of utilization and reproduction.

We have all seen the impact transformers have had on NLP and image classification and so it's exciting to see it being applied to object detection, which is generally more challenging than classic image recognition. The power of the self-attention mechanism helps in relaxing the constraints of current object detection solutions. Without further ado, let's start diving into how this model works!

Pre-requisites

Before we start, I wanted to cover some fundamental ideas that I think are quite interesting, essential to understanding this model, and are also quite useful in general.

A bipartite graph (or bigraph) is a graph whose vertices can be divided into two disjoint and independent sets U and V such that every edge connects a vertex in U to one in V. Vertex sets and are usually called the parts of the graph.

Source: Wikipedia

The first one is bipartite matching. This stems from graph theory which is one of the fundamental building blocks of computer science. Bipartite matching is where a set of edges in a bipartite graph is chosen such that no 2 edges share a node. If you are not that interested in graph theory, just understand that graph matching algorithms can be quite powerful in Neural networks.

The second interesting concept is anchor boxes. In object detection, there are mainly 2 types of models: single-stage and 2-stage. 2-stage models produce a set of a proposal from an image (essentially sub-images) using an algorithm called Selective Search and then attempt to classify each image using a classic CNN. However, single-stage models (which are more effective) use a more novel concept called anchor boxes.

Anchor boxes are kind of the opposite of region proposals, they are a set of predefined bounding boxes of a certain height and width that are used by the network to approximate the objects it is looking for. Although this might sound a bit more constrained, it has been shown that anchor boxes are much more effective and quicker than region proposals.

The final concept is non-maximum suppression (NMS) and this one is quite fundamental to object detection. NMS is used to select one bounding box from many overlapping boxes. Because think about it, if you are trying to find a cat, there are going to be several boxes covering that cat from multiple angles. However, a high-quality object detection model will use NMS which calculates the Intersection Over Union of the boxes and chooses the optimal box from a defined threshold.

Okay now that we are done with the theory, let's get to it.

The end-to-end pipeline

Photo by JJ Ying on Unsplash

The model starts off with a CNN to extract features, then feeds those features to a transformer, and the output of the transformer is then fed into a Feedforward network that makes the final prediction. I have been reviewing a lot of recent popular papers and this pipeline is being adapted quite frequently, for instance, it was also recently used here:

Our loss produces an optimal bipartite matching between predicted and ground truth objects, and then optimize object-specific (bounding box) losses.

Source: arxiv

Their main loss function is based on bipartite matching and more specifically the Hungarian algorithm which is a “combinatorial optimization algorithm that solves the assignment problem in polynomial time” [2]. I could write a full post about the Hungarian algorithm (but I am not going to). One of the main benefits of this approach is that it simply produces a set of predictions rather than a list, meaning that the produced boxes are unique (which saves a lot of post-processing time). It also allows them to do box predictions directly rather than doing those predictions with respect to some initial guesses (with the addition of some regularisation of course).

As for the first step, a CNN is used to drive down the dimensions of the images to the most essential ones, which is called an activation map. Then a 1 x 1 convolution further drives down those dimensions and after collapsing the feature map we get a sequence which the transformer expects as input. But before moving to the transformer, the image is also supplemented with positional encodings (to preserve the structure of the image).

As for the transformer, for the sake of efficiency, they adjust the classic transformer decoder module to decode the objects in parallel rather than sequentially. This is possible because the transformer is permutation-invariant and because we already passed in the positional encodings so the image structure can be memorized safely even within a parallel processing pipeline.

Using self- and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using pair-wise relations between them, while being able to use the whole image as context.

Source: arxiv

This is one of my favorite optimizations introduced in this paper. Not only are they using a modern model such as the transformer, but they also managed to improve such that they can break down the image, process it in parallel without losing its structure.

Results

I don’t want to talk too much about the results since it’s all data and can be easily checked in the paper, but I think it’s safe to say that the results are quite good. They have also implemented the paper using Pytorch and provided the code here. The code doesn’t seem to be long and complicated, they actually introduce a demo here in less than 100 lines of code!

Final thoughts

One of the best things about recent object detection models is that they don’t require you to write code (kind of) to train your models, you can just run “python train.py <dataset>” and it starts training. You probably need to process your dataset first, but that's quite minimalistic work. You can find their main python training file here. I was thinking about making a tutorial using this code in a real Kaggle challenge (I am doing the VinBiGdata x-ray object detection challenge). Let me know down in the comments if that would be interesting to you.

References:

[1] End-to-End Object Detection with Transformers. Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko.

[2] https://en.wikipedia.org/wiki/Hungarian_algorithm","['models', 'world', 'dive', 'using', 'transformers', 'model', 'detr', 'image', 'object', 'detection', 'transformer', 'facebook', 'quite', 'boxes']","Facebook DETR: Transformers dive into the Object Detection WorldPhoto by Samule Sun on UnsplashMost of the great recent machine learning papers are based on transformers.
Recently, Facebook published a new paper that uses transformers to outperform state-of-the-art Faster RCNNs in object detection.
The power of the self-attention mechanism helps in relaxing the constraints of current object detection solutions.
The final concept is non-maximum suppression (NMS) and this one is quite fundamental to object detection.
References:[1] End-to-End Object Detection with Transformers."
42,https://www.mcknights.com/blogs/guest-columns/how-we-used-machine-learning-to-reduce-rehospitalizations/,How we used machine learning to reduce rehospitalizations,2021-02-12 17:00:00+00:00,"Mike Logan

In late 2020, I applied for a technology grant through a machine-learning technology company that builds customized, machine-learning tools based on a facility’s retrospective EHR and rehospitalization data.

My organization, Masonic Pathways, a Life Plan Community in the heart of Michigan, is no stranger to understanding the importance of reducing resident rehospitalization rates.

We currently participate as one of 11 post-acute care providers within the Mid-Michigan Health Performance Network. We are proud to be a member of the performance network, and we do not want to miss out on any opportunity to support our local acute care hospital system and improve patient care.

As a network member, we receive monthly quality scorecard ratings and training on how to decrease overall length of stay and reduce our rehospitalization rates. The network is managed by Signify Health, one of the largest Bundled Payment for Care Improvement Advance conveners in the United States.

As post-acute care organizations grapple with a multitude of strategic challenges, those conveners and acute care hospital systems continue to put the pressure on post-acute care providers to reduce overall rehospitalizations within their communities.

At the same time, the COVID-19 pandemic has plagued our community with low occupancy and a multitude of financial challenges, and, as a result, rehospitalizations just have not risen to the same degree of urgency as other important matters.

To be honest, I understand the importance of managing rehospitalizations. But without an affordable technological solution to assist our team in managing our residents, we continued to “dig into” resident charts in the hope of finding some trends or clues to assist us in improving our rates.

Given my challenges and our desire to strengthen our relationship with not only Mid-Michigan Health Performance Network, but also with Signify Health, I realized our organization needed a technological solution.

Finding the risk

Through the grant program, we gained access to SAIVA Healthcare’s machine learning approach. They build solutions specific to each partnering organization, with the system pulling daily data updates and prospectively running them through the model.

The output is a daily list of residents who are at high risk of rehospitalizations over the next several days. Essentially, the machine-learning technology does the work of pouring through the resident’s chart to find trends and behaviors that predict when a resident is at risk of being rehospitalized.

Now, just because we have machine learning scrubbing our EMR does not mean that we do not have to think. On the contrary, SAIVA reports help us identify early signs of imminent decline and rehospitalization risk, but organizations must still have the human critical thinking skills and clinical acumen to optimize the predictive model.

What we learned through utilizing machine learning is that the charge nurse and care staff MUST document well and provide as many details within the resident chart, so that the machine learning can accurately predict who may rise on the daily ranking report and who may be most at risk for rehospitalization.

The non-tech factor

While predicting imminent decline and reducing rehospitalization risk are the primary focal points of the machine-learning technology, the interdisciplinary team must ask themselves these questions when reviewing their daily ranking reports:

• Do I agree with the SAIVA assessment for this resident? Why or why not?

• If a resident has been on the list previously, is risk going up or down?

• Why might this resident be on the SAIVA list?

• What can I do to reduce the risk to this resident?

• Who can help me reduce the risk to this resident?

I must admit: I am not always a trailblazer for new technology. However, I also must tell you that through machine learning, we can save time and effort and strengthen our overall clinical acumen by analyzing the daily reports. I am pleased to say that, based upon our first couple of months using machine learning, we improved our overall percentage of new resident admission rehospitalizations from 20% to 10.5%. That was in a three-month period.

More importantly, while machine learning is becoming more predominant within healthcare, organizations still need to rely on the human intuition and instincts as well.

Sometimes healthcare professionals are concerned that machine technology might replace them. They need not worry. The only way that we can meaningfully reduce the rehospitalization rate is through close cooperation between machine technology and healthcare professionals.

Mike Logan has a senior living operations management background that spans over 20 years. He is the CEO of Michigan Masonic Home in Alma, Michigan, and a board member of LeadingAge Michigan, PACE Central Michigan and the National PACE Association Finance Committee.","['reduce', 'network', 'rehospitalization', 'used', 'care', 'learning', 'rehospitalizations', 'machine', 'resident', 'technology', 'risk']","Finding the riskThrough the grant program, we gained access to SAIVA Healthcare’s machine learning approach.
Now, just because we have machine learning scrubbing our EMR does not mean that we do not have to think.
However, I also must tell you that through machine learning, we can save time and effort and strengthen our overall clinical acumen by analyzing the daily reports.
More importantly, while machine learning is becoming more predominant within healthcare, organizations still need to rely on the human intuition and instincts as well.
The only way that we can meaningfully reduce the rehospitalization rate is through close cooperation between machine technology and healthcare professionals."
43,https://www.wired.com/story/microsoft-win-quantum-computing-error/,Microsoft’s Big Win in Quantum Computing Was an ‘Error’ After All,,"In March 2018, Dutch physicist and Microsoft employee Leo Kouwenhoven published headline-grabbing new evidence that he had observed an elusive particle called a Majorana fermion.

Microsoft hoped to harness Majorana particles to build a quantum computer, which promises unprecedented power by tapping quirky physics. Rivals IBM and Google had already built impressive prototypes using more established technology. Kouwenhoven’s discovery buoyed Microsoft’s chance to catch up. The company’s director of quantum computing business development, Julie Love, told the BBC that Microsoft would have a commercial quantum computer “within five years.”

Three years later, Microsoft’s 2018 physics fillip has fizzled. Late last month, Kouwenhoven and his 21 coauthors released a new paper including more data from their experiments. It concludes that they did not find the prized particle after all. An attached note from the authors said the original paper, in the prestigious journal Nature, would be retracted, citing “technical errors.”

Two physicists in the field say extra data Kouwenhoven’s group provided them after they questioned the 2018 results shows the team had originally excluded data points that undermined its news-making claims. “I don’t know for sure what was in their heads,” says Sergey Frolov, a professor at the University of Pittsburgh, “but they skipped some data that contradicts directly what was in the paper. From the fuller data, there’s no doubt that there’s no Majorana.”

The 2018 paper claimed to show firmer evidence for Majorana particles than a 2012 study with more ambiguous results that nevertheless won fame for Kouwenhoven and his lab at Delft Technical University. That project was partly funded by Microsoft, and the company hired Kouwenhoven to work on Majoranas in 2016.

The 2018 paper reported seeing telltale signatures of the Majorana particles, termed “zero-bias peaks,” in electric current passing through a tiny, supercold wire of semiconductor. One chart in the paper showed dots tracing a plateau at exactly the electrical conductance value that theory predicted.

Frolov says he saw multiple problems in the unpublished data, including data points that strayed from the line but were omitted from the published paper. If included, those data points suggested Majorana particles could not be present. Observations flagged by Frolov are visible in the charts in the new paper released last month, but the text does not explain why they were previously excluded. It acknowledges that trying to experimentally validate specific theoretical predictions “has the potential to lead to confirmation bias and effectively yield false-positive evidence.”

Tweets by Sergey Frolov, a physicist who questioned missing data in the 2018 paper.

Microsoft provided a statement attributed to Kouwenhoven saying he could not comment, because the new paper that reinterprets his group’s results is undergoing peer review. “We are confident that scaled quantum computing will help solve some of humanity’s greatest challenges, and we remain committed to our investments in quantum computing,” he said. Nature added an “editorial expression of concern” to the 2018 paper in April last year, and a spokesperson said this week that the journal is “working with the authors to resolve the matter.” A spokesperson for Delft Technical University said an investigation by its research integrity committee, started in May 2020, is not complete. A person familiar with the process says the final report will likely find that researchers at Delft made mistakes but did not intend to mislead.","['university', 'big', 'majorana', 'data', 'technical', 'error', 'computing', '2018', 'particles', 'microsofts', 'results', 'kouwenhoven', 'paper', 'win', 'quantum']","Microsoft hoped to harness Majorana particles to build a quantum computer, which promises unprecedented power by tapping quirky physics.
The company’s director of quantum computing business development, Julie Love, told the BBC that Microsoft would have a commercial quantum computer “within five years.”Three years later, Microsoft’s 2018 physics fillip has fizzled.
The 2018 paper reported seeing telltale signatures of the Majorana particles, termed “zero-bias peaks,” in electric current passing through a tiny, supercold wire of semiconductor.
If included, those data points suggested Majorana particles could not be present.
“We are confident that scaled quantum computing will help solve some of humanity’s greatest challenges, and we remain committed to our investments in quantum computing,” he said."
44,https://phys.org/news/2021-02-tackles-central-powerful-quantum.html,New research tackles a central challenge of powerful quantum computing,,"Credit: CC0 Public Domain

To build a universal quantum computer from fragile quantum components, effective implementation of quantum error correction (QEC) is an essential requirement and a central challenge. QEC is used in quantum computing, which has the potential to solve scientific problems beyond the scope of supercomputers, to protect quantum information from errors due to various noise.

Published by the journal Nature, research co-authored by University of Massachusetts Amherst physicist Chen Wang, graduate students Jeffrey Gertler and Shruti Shirol, and postdoctoral researcher Juliang Li takes a step toward building a fault-tolerant quantum computer. They have realized a novel type of QEC where the quantum errors are spontaneously corrected.

Today's computers are built with transistors representing classical bits (0's or 1's). Quantum computing is an exciting new paradigm of computation using quantum bits (qubits) where quantum superposition can be exploited for exponential gains in processing power. Fault-tolerant quantum computing may immensely advance new materials discovery, artificial intelligence, biochemical engineering and many other disciplines.

Since qubits are intrinsically fragile, the most outstanding challenge of building such powerful quantum computers is efficient implementation of quantum error correction. Existing demonstrations of QEC are active, meaning that they require periodically checking for errors and immediately fixing them, which is very demanding in hardware resources and hence hinders the scaling of quantum computers.

In contrast, the researchers' experiment achieves passive QEC by tailoring the friction (or dissipation) experienced by the qubit. Because friction is commonly considered the nemesis of quantum coherence, this result may appear quite surprising. The trick is that the dissipation has to be designed specifically in a quantum manner. This general strategy has been known in theory for about two decades, but a practical way to obtain such dissipation and put it in use for QEC has been a challenge.

""Although our experiment is still a rather rudimentary demonstration, we have finally fulfilled this counterintuitive theoretical possibility of dissipative QEC,"" says Chen. ""Looking forward, the implication is that there may be more avenues to protect our qubits from errors and do so less expensively. Therefore, this experiment raises the outlook of potentially building a useful fault-tolerant quantum computer in the mid to long run.""

Chen describes in layman's terms how strange the quantum world can be. ""As in German physicist Erwin Schrödinger's famous (or infamous) example, a cat packed in a closed box can be dead or alive at the same time. Each logical qubit in our quantum processor is very much like a mini-Schrödinger's cat. In fact, we quite literally call it a `cat qubit.' Having lots of such cats can help us solve some of the world's most difficult problems.

""Unfortunately, it is very difficult to keep a cat staying that way since any gas, light, or anything leaking into box will destroy the magic: The cat will become either dead or just a regular live cat,"" explains Chen. ""The most straightforward strategy to protect a Schrodinger's cat is to make the box as tight as possible, but that also makes it harder to use it for computation. What we just demonstrated was akin to painting the inside of the box in a special way and that somehow helps the cat better survive the inevitable harm of the outside world.""

Explore further Error-protected quantum bits entangled for the first time

More information: Jeffrey M. Gertler et al, Protecting a bosonic qubit with autonomous quantum error correction, Nature (2021). Journal information: Nature Jeffrey M. Gertler et al, Protecting a bosonic qubit with autonomous quantum error correction,(2021). DOI: 10.1038/s41586-021-03257-0","['box', 'research', 'protect', 'errors', 'tackles', 'powerful', 'computing', 'error', 'central', 'qubit', 'qec', 'way', 'cat', 'challenge', 'qubits', 'quantum']","Credit: CC0 Public DomainTo build a universal quantum computer from fragile quantum components, effective implementation of quantum error correction (QEC) is an essential requirement and a central challenge.
They have realized a novel type of QEC where the quantum errors are spontaneously corrected.
Quantum computing is an exciting new paradigm of computation using quantum bits (qubits) where quantum superposition can be exploited for exponential gains in processing power.
Since qubits are intrinsically fragile, the most outstanding challenge of building such powerful quantum computers is efficient implementation of quantum error correction.
Journal information: Nature Jeffrey M. Gertler et al, Protecting a bosonic qubit with autonomous quantum error correction,(2021)."
45,https://www.smartcitiesworld.net/opinions/opinions/reviving-smart-cities-with-edge-computing-and-5g,Reviving smart cities with edge computing and 5G,,"As we recover from Covid-19, we have the opportunity to rethink our cities. During the pandemic we’ve been more reliant on our local communities as well as on technology – and these two things will come together to create new smarter cities. With more people becoming aware of the realities of climate change, future cities are likely to be set up very differently when it comes to both energy and transport.



All autonomous transport, and much of the technology future smart cities will depend on 5G and edge computing. The latter is essentially a technological version of what we’ve been doing throughout the pandemic - relying on what’s nearby. This enables greater resiliency as well as more information to be added into the system to create ever smarter cities.

5G networks offer increased cell density, higher data speed and lower network latency. In 5G, more processing is being pushed to the edge of the network, enabling the implementation of low latency applications. In addition, cell site densification provides increased network capacity, more data bandwidth and higher mobile data speeds to the consumers. This network densification will enable advanced analytics for real time decision-making. The applications of 5G technology can help cities save money, resources and create cleaner, safer and healthier places for people to live.

5G and edge computing go hand in hand. 5G increases the amount of data that can be communicated, while edge computing uses data to run calculations locally as opposed to sending it elsewhere to be analysed and acted upon. This is often faster and more resilient to disruption. Combined, these two technologies hold massive promise and are the preeminent emerging technologies of today.

According to studies by IDC, worldwide 5G connections are set to grow to 1.01bn in 2023 and worldwide spending on edge computing will reach $250bn in 2024. This presents an enormous ecosystem opportunity to transform cities by infusing next generation technology.

Smart cities aim to improve the quality of life for residents. Key technologies like Internet of Things (IoT), blockchain, artificial intelligence and analytics can be leveraged to cover a whole gamut including waste management, smart parking, e-governance, electricity and public lighting, education, health, traffic management, and smart buildings. It is the combination of edge computing, 5G capabilities and industrial Internet of Things devices which underpins the efficient use of tech, has the potential to enable smarter supply chains, and better equip us to handle disruption.

Utilities and infrastructure

Edge analytics can enable a smart city municipality to better manage and conserve precious resources including energy, water and fresh air. Analytics on top of IoT sensors in water systems and waste management systems enable better monitoring and management while innovative electric grids increase energy efficiency for businesses and consumers alike. Edge analytics also help in the monitoring and controlling of building operations such as heating, ventilation, air conditioning, lighting, and security to enable the best possible living environment virtually and automatically.

Economic development and civic management

Traffic flow, parking space availability, utility usage and public streetlight management can be monitored by using IoT sensors on a 5G network. Authorities can leverage edge analytics to find practical solutions to conserve energy, optimise water and power resources, and reduce environmental impact. During the pandemic, we have seen some trends of people moving out of congested cities and into areas which are less crowded and with better services. Using technology, minimum traffic congestions and improved waste management can help to entice new residents and increase economic opportunities within the community.","['reviving', 'network', 'management', 'enable', 'data', 'computing', 'analytics', 'cities', 'technology', 'smart', '5g', 'edge']","All autonomous transport, and much of the technology future smart cities will depend on 5G and edge computing.
The applications of 5G technology can help cities save money, resources and create cleaner, safer and healthier places for people to live.
5G and edge computing go hand in hand.
According to studies by IDC, worldwide 5G connections are set to grow to 1.01bn in 2023 and worldwide spending on edge computing will reach $250bn in 2024.
Authorities can leverage edge analytics to find practical solutions to conserve energy, optimise water and power resources, and reduce environmental impact."
46,https://towardsdatascience.com/a-primer-on-the-sources-of-biases-in-data-mining-for-machine-learning-d82e89604693,Sources of biases in data mining for machine learning,2021-02-13 17:56:44.329000+00:00,"Despite rising levels of automation through big-data, much of the data-mining and machine learning process still relies on human intervention, introducing different biases.

Photo by: JESHOOTS.COM on Unsplash

The amount of structured and unstructured data generated has grown exponentially over the last few decades and will continue to do so for years to come. The ‘big data’ analytics could potentially overcome numerous challenges that corporations and governments have faced for centuries while making decisions: the lack of adequate data for formulating policies (e.g., targeting policies for a particular social group) or examining market or consumer expectations (e.g., recommendation system). The descriptive as well as predictive modeling that is driven by the big data paradigm can help decision-makers derive valuable insights for personal, commercial, or collective gains.

However, the modern data collection process and algorithms remain susceptible to data mining biases. Without taking appropriate measures, the big data can amplify the negative effect of the existing social issues (e.g., racial discrimination) and render the findings worthless or even counterproductive [1], [2]. The purpose of this blog is to explore the potential sources of biases that can be introduced during data-mining and possibly the feature engineering stages of a big-data project.

Garbage in garbage out — true for big data too

The big-data has potential to overcome the limitations of data availability, an issue that has plagued the traditional statistical analysis for decades, but the value of such analytics is also vulnerable to the quality of data as well as the poor application of the data. With growing penetration of digital gadgets, it is increasingly becoming easier than ever before to collect more data of all kinds. For example, a smart phone has ability to record in real-time where you are, what you are doing, how you are spending your time and money, who you are meeting and for how long, and many such personal details. Notwithstanding the privacy and surveillance concerns, such information has already transformed the economy and the way we live.

Nonetheless, just like the old-school data analytics, the success of the new data paradigm also depends on the validity of the same assumptions — the target population include all members associated with the problem being investigated; the sample reflects the statistical population; and the choice of the modeling variables is appropriate. The violation of any of the assumptions can result in biased conclusions, thereby lowering the value of machine learning for solving many of the real-world problems.

Figure 1 depicts a typical sequence of steps in a machine learning exercise and shows how different mistakes can reinforce the existing problems. While careful use will provide effective solutions and reduce our problems, any mistake can perpetuate or worsen the existing problems (disparities, discrimination, or something else).

Image by author based on information in [2]

Part I: Identifying target population

One of the early steps, if not the first step, in a data analytics exercise is to ask who all or what members would most likely have the information we need or are familiar with the issue we trying to address. This group of members is our desired population (let’s call it problem population) to considered for the sample collection. The representative-ness of the sample for training data must be measured only against problem population that has a direct connection to the problem.

For example, say a road maintenance department wants to use crowdsourcing (via smart-phones) to monitor road conditions across a city [1]. While it may seem that all people living in the area should be the target population, it is only the people traveling on the roads (as passengers or drivers) in the area that would form the actual population of interest (our problem population). The accelerometer data from the devices of people who do not travel or use cars will not enhance any value. Similarly, for a hiring goal, all potentially qualified (meeting mandatory qualification requirements) employees are the sampling population rather than all graduates or people looking for employment. Another obvious illustration is the sampling population for gauging who would win an election. The opinions of most teenagers do not matter in an opinion poll because only voters (people over the age of 18 years in most countries) constitute the population for an election survey. It does not matter how many younger people are surveyed; the survey outcome or any prediction model based on such information will be useless.

While completing this step may be trivial in most cases with an appropriately described problem, any unintended mistakes could have huge implications for the accuracy and relevance of our work. In our case, after defining the target population of all motorists, the road department now has to devise a strategy to gather data from a representative, even if small, number of roadies (excluding cyclists and walkers for the moment).

Part II: Ensuring representative training data

The value of any small or big-data applications depends on how accurately the training data describes the problem population. While the big data implies a huge dataset, it rarely represents the entire problem population and captures only a portion of it, which increase the importance of representative sampling. Creating a training dataset that does not reflect the composition of the underlying problem population will emphasize certain members and under-represent others. The Wikipedia has a long list of sampling biases, including selection bias, exclusion bias, reporting bias, detection bias, and the list goes on. In fact, a small, but representative sample can produce more reliable results that a large, biased dataset [3].

For example, in case of the road monitoring project mentioned above, if the department relies on accelerometers in the smart-phones to collect the information, it is possible that low-income group lacking such high-tech gadgets or adequate internet access fails to be part of the sample [1]. The biased data could result in unfair allocation of tax-payers money for road development and repairs, leading to systemic disparities in the quality of road infrastructure within a city. Poor access to the internet can also limit what applicants are shortlisted for a position, or the choice of words in a resume can cause gender-biased hiring [4].

Part III: Choosing representative features

The choice of the predictors also affects how different members of the population are considered in the model. Even when we correctly identify problem population and draw a representative sample for training, certain disparities are embedded in the input variables. There could be many features that capture an incomplete picture of certain members of the population, and assigning greater weight to the biased predictors will produced an undesirable outcome. So, although all population strata are well described by the sample, the feature engineering can indirectly under- or over-represent certain groups and influence the fairness of the model.

For example, a small correlation between apparently neutral features can produce unfair results. If aggressive drivers and elderly both preferred red cars, a decision to charge higher insurance premium for red cars to punish the bad drivers would indirectly be biased against the elderly [2].

As quoted in [2], “mis-represented groups coincide with social groups against which there already exists social bias such as prejudice or discrimination, even \unbiased computational processes can lead to discriminating decision procedures”. Such risks are valid for sampling as well as feature selection steps.

For example, hiring good employees depends on the definition of what counts as ‘good’, which can be based on the average length of past employment or proven achievement record. Using the tenure length as a feature can disproportionately exclude certain people working in an industry with high hiring turn-over (or attrition) rates [1]. While it is a desirable proxy for measuring employee loyalty, its unnecessary inclusion in the model may have dominating influence and perpetuate the very disparities that big-data is believed to be reducing. Therefore, the despite the best efforts to define population and gather good data, selecting wrong features can introduce systemic biases in the results and contributes to the problem.

Final thoughts

The big data analytics has potential to overcome many biases resulting from individuals and human decision-making and present a rational, representative picture of the population. The purpose of this blog was to present an introductory overview of how biases can become part of a machine learning modeling exercise. While performing data-mining and doing a machine learning project, the analysts must be aware of the following three causes that can dilute the efficacy of a machine learning project:

· Failure to correctly identify the statistical (problem) population relating to the problem

· Using unrepresentative training data for predictive modeling

· Biased choice of features for predicting a target variable

As the common saying goes ‘with great power comes great responsibility’, any carelessness in data mining and feature engineering could worsen the existing problems and the efforts would become counter-productive. Only the combined effect of big data, its ease of access, and sensible data mining can potentially revolutionize our daily lives towards an environmentally sustainable, socially equitable, and economically vibrant world.

References

[1] S. Barocas and A. D. Selbst, “Big data’s disparate impact,” Calif. Law Rev., vol. 104, no. 3, pp. 671–732, Feb. 2016.

[2] E. Ntoutsi et al., “Bias in data-driven artificial intelligence systems — An introductory survey,” WIREs Data Min. Knowl. Discov. , vol. 10, no. 3, p. e1356, May 2020.

[3] J. J. Faraway and N. H. Augustin, “When small data beats big data,” Stat. Probab. Lett., vol. 136, pp. 142–145, 2018.

[4] J. Manyika, J. Silberg, and B. Presten, “What do we do about the biases in AI?,” 2019. [Online]. Available: https://hbr.org/2019/10/what-do-we-do-about-the-biases-in-ai.","['biases', 'big', 'data', 'sources', 'sample', 'learning', 'road', 'problem', 'representative', 'machine', 'members', 'training', 'mining', 'population']","However, the modern data collection process and algorithms remain susceptible to data mining biases.
The representative-ness of the sample for training data must be measured only against problem population that has a direct connection to the problem.
Part II: Ensuring representative training dataThe value of any small or big-data applications depends on how accurately the training data describes the problem population.
Only the combined effect of big data, its ease of access, and sensible data mining can potentially revolutionize our daily lives towards an environmentally sustainable, socially equitable, and economically vibrant world.
[3] J. J. Faraway and N. H. Augustin, “When small data beats big data,” Stat."
47,https://www.miragenews.com/artificial-intelligence-can-boost-disaster-513730/,Artificial intelligence can boost disaster management,2021-02-13 01:34:12+11:00,"The World Meteorological Organization is participating in a new interdisciplinary Focus Group to contend with the increasing prevalence and severity of natural hazards with the help of artificial intelligence (AI).

The International Telecommunications Union Focus Group on ‘AI for natural disaster management’ will support global efforts to improve our understanding and modelling of natural hazards and disasters. Led by ITU in partnership with WMO and the UN Environment Programme, it will distill emerging best practices to develop a roadmap for international action in AI for natural disaster management.

The group’s first meeting is scheduled for 15-17 March 2021.

“With new data and new insight come new powers of prediction able to save countless numbers of lives,” says ITU Secretary-General Houlin Zhao. “This new Focus Group is the latest ITU initiative to ensure that AI fulfils its extraordinary potential to accelerate the innovation required to address the greatest challenges facing humanity.”

Over the past 50 years, more than 11,000 disasters have been attributed to weather, climate and water-related hazards, involving 2 million deaths and US$ 3.6 trillion in economic losses. While the average number of deaths recorded for each disaster has fallen by a third during this period, the number of recorded disasters has increased five times and the economic losses have increased by a factor of seven, according to WMO’s State of Climate Services 2020 report.

Extreme weather and climate events have increased in frequency, intensity and severity as result of climate change and hit vulnerable communities disproportionately hard. Yet one in three people are still not adequately covered by early warning systems.

In 2018, globally, around 108 million people required help from the international humanitarian system as a result of storms, floods, droughts and wildfires. By 2030, it is estimated that this number could increase by almost 50% at a cost of around US$ 20 billion a year, says the WMO report.

Although the global death toll has fallen, the poor remain disproportionately exposed.

“AI has the potential to help all countries to achieve major advances in disaster management that will leave no one behind,” according to Jürg Luterbacher, Chief Scientist and Director of Science and Innovation at WMO.

“The WMO Disaster Risk Reduction Programme assists countries in protecting lives, livelihoods and property from natural hazards, and it is strengthening meteorological support to humanitarian operations for disaster preparedness through the development of a WMO Coordination Mechanism and Global Multi-Hazard Alert System. Complementary to the Focus Group, we aim to advance knowledge transfer, communication and education – all with a focus on regions where resources are limited.”

The Focus Group’s work will pay particular attention to the needs of vulnerable and resource-constrained regions. It will make special effort to support the participation of the countries shown to be most acutely impacted by natural disasters, notably Small Island Developing States and Least Developed Countries.

AI is becoming increasingly important to WMO’s work. Supercomputers crunch petabytes of data to forecast weather around the world. The WMO also coordinates a global programme of surface-based and satellite observations. Their models merge data from more than 30 satellite sensors, weather stations and ocean-observing platforms all over the planet, explains Anthony Rea, Director of the Infrastructure Department at WMO.

The WMO Information System (WIS) acts as a one-stop shop for all activities related to data management. AI can help interpret resulting data and help with decision support for forecasters who receive an overwhelming amount of data. AI can help recognize where there might be a severe event or a risk of it happening.

“AI is not, however, a magic bullet which will replace the models built on physical understanding and decades of research into interactions between the atmosphere and oceans. And in order for AI to thrive, data needs to be open, available and interoperable,” says Rea.","['artificial', 'hazards', 'management', 'data', 'weather', 'support', 'ai', 'wmo', 'natural', 'help', 'boost', 'focus', 'disaster', 'intelligence']","The World Meteorological Organization is participating in a new interdisciplinary Focus Group to contend with the increasing prevalence and severity of natural hazards with the help of artificial intelligence (AI).
The International Telecommunications Union Focus Group on ‘AI for natural disaster management’ will support global efforts to improve our understanding and modelling of natural hazards and disasters.
Led by ITU in partnership with WMO and the UN Environment Programme, it will distill emerging best practices to develop a roadmap for international action in AI for natural disaster management.
AI can help interpret resulting data and help with decision support for forecasters who receive an overwhelming amount of data.
AI can help recognize where there might be a severe event or a risk of it happening."
48,https://www.prweb.com/releases/quantiphi_named_as_an_idc_innovator_in_artificial_intelligence_services/prweb17721733.htm,Quantiphi Named as an IDC Innovator in Artificial Intelligence Services,,"“We are proud to be named as an IDC Innovator for our approach to building AI-First Digital Transformation Engineering solutions and solving complex business problems for our clients across industries,” said Asif Hasan, Co-Founder, Quantiphi.

Quantiphi, an applied AI and data science software and services company, today announced that it has been named an IDC Innovator in the IDC Innovators: Artificial Intelligence Services, 2020 ( Doc # US45733220, December 2020) report. Quantiphi is one of just four companies featured in the report, which covers a selection of vendors that offer an innovative new technology or a groundbreaking business model, or both in artificial intelligence (AI) services.

""AI has quickly evolved from a 'nice-to-have technology' to a business imperative, driving enterprise demand for expertise from solution design through production at scale,"" said Jennifer Hamel, Research Manager for IDC's Worldwide Services team. ""Quantiphi approaches the AI services market in distinct ways, partnering with its clients to apply complex AI techniques to solve real business problems.""

The report acknowledges Quantiphi's broad portfolio of repeatable IP and accelerators, and strong partnerships with major AI technology providers (e.g., Google, AWS, and NVIDIA) to assemble and scale AI solutions for clients in a variety of industries, leveraging a talent pool of industry analysts, cloud/data engineers, and ML engineers.

“We are proud to be named as an IDC Innovator for our approach to building AI-First Digital Transformation Engineering solutions and solving complex business problems for our clients across industries,” said Asif Hasan, Co-Founder, Quantiphi. “With a broad range of industry-focused solutions and end-to-end deployment capabilities, we continue to empower enterprises to manage the process of assembling, training, validating, operating and monitoring AI solutions at scale

You can access the report excerpt here

According to the IDC Innovator Assessment:

“Quantiphi's AI solutions target horizontal workloads, such as contact center and document processing, as well as packaged solutions that leverage repeatable patterns and use cases across a broad range of industry verticals.”

“The company's engagement model takes clients from ideation (""hack it"") to pilot (""prove it"") to production applications (""nail it"") to a factory-based model for AI solutions (""scale it"").”

“Quantiphi aims to deconstruct the ""mythical data science unicorn"" by combining competency in three areas to solve business problems: industry analysts, who orchestrate people, processes, and technologies; cloud/data engineers, who organize data, create data pipelines, and prepare data for modeling; and ML engineers, who provide deep learning and statistical ML modeling expertise.”

About IDC Innovators

IDC Innovators reports present a set of vendors – under $100M in revenue at time of selection -- chosen by an IDC analyst within a specific market that offer an innovative new technology, a groundbreaking approach to an existing issue, and/or an interesting new business model. It is not an exhaustive evaluation of all companies in a segment or a comparative ranking of the companies. Vendors in the process of being acquired by a larger company may be included in the report provided the acquisition is not finalized at the time of publication of the report. Vendors funded by venture capital firms may also be included in the report even if the venture capital firm has a financial stake in the vendor’s company. IDC INNOVATOR and IDC INNOVATORS are trademarks of International Data Group, Inc.

About Quantiphi

Quantiphi is an award-winning applied AI and data science software and services company driven by the desire to solve transformational problems at the heart of business. Quantiphi solves the toughest and complex business problems by combining deep industry experience, disciplined cloud and data engineering practices, and cutting-edge artificial intelligence research to achieve quantifiable business impact at unprecedented speed. We are passionate about our customers and obsessed with problem-solving to make products smarter, customer experiences frictionless, processes autonomous and businesses safer by detecting risks, threats and anomalies. For more on Quantiphi’s capabilities, visit http://www.quantiphi.com.","['artificial', 'data', 'idc', 'services', 'ai', 'innovator', 'business', 'solutions', 'report', 'vendors', 'problems', 'named', 'quantiphi', 'intelligence']","“We are proud to be named as an IDC Innovator for our approach to building AI-First Digital Transformation Engineering solutions and solving complex business problems for our clients across industries,” said Asif Hasan, Co-Founder, Quantiphi.
Quantiphi, an applied AI and data science software and services company, today announced that it has been named an IDC Innovator in the IDC Innovators: Artificial Intelligence Services, 2020 ( Doc # US45733220, December 2020) report.
Quantiphi is one of just four companies featured in the report, which covers a selection of vendors that offer an innovative new technology or a groundbreaking business model, or both in artificial intelligence (AI) services.
""Quantiphi approaches the AI services market in distinct ways, partnering with its clients to apply complex AI techniques to solve real business problems.""
“We are proud to be named as an IDC Innovator for our approach to building AI-First Digital Transformation Engineering solutions and solving complex business problems for our clients across industries,” said Asif Hasan, Co-Founder, Quantiphi."
49,https://www.iotforall.com/automotive-manufacturing-and-advanced-artificial-intelligence,Automotive Manufacturing and Advanced Artificial Intelligence,2021-02-12 13:00:00+00:00,"Over the last several weeks, there have been a couple of major announcements from two of the world’s largest automotive brands that show how IoT can help them avoid recalls and warranty claims.

Announcements

After conducting several years of its own research and testing, the first came from GM, who announced that it recalls six million vehicles in the U.S. after the National Highway Traffic Safety Administration denied its recall appeal, saying the carmaker had not established the recall was unnecessary.

The most recent came from Ford, which announced it took steps to rein in rising warranty costs. Part of the new plan to offset these costs involves the company charging suppliers upfront for half of the cost of warranty-related issues.

In both these cases, there’s obviously a lot at stake. First and foremost, it’s the safety of drivers and passengers in the vehicles. Second, it’s the damage to the OEMs’ brands when massive recalls or spikes in warranty claims occur. And finally, it’s the financial hit that these companies take––which Ford is now saying will be shared with its suppliers––a reality that will undoubtedly have a severe impact on valuation and shareholder return.

In both of these cases, there’s an important part of the story that’s missing. What are automotive manufacturers, OEMs, and suppliers, doing to prevent issues like recalls and warranty claims from happening at all?

Have airbag manufacturers developed new testing protocols? Are manufacturers and OEMs using the latest technology to detect and avoid these issues, or are they helping them limit the scope of this type of recall? And for Ford, how are they calculating the distribution in the responsibility of the costs? Does pushing prices back to part manufacturers truly represent a renewed focus on quality?

Proactive Over Reactive

How about getting proactive instead of being reactive?

On top of the cost of recalls, automotive brands and parts suppliers undoubtedly invest a significant amount of money conducting their own tests, not to mention legal fees. And rather than investing time and resources to create and manage a complicated warranty cost-sharing program, could manufacturers work with suppliers to test new technologies to eliminate issues during manufacturing and assembly that could massively reduce warranty claims?

Instead of stop-gap measures, the adoption of AI and Machine Learning technologies could help manufacturers shift from being reactive to proactive––an investment that could save significant money over the short and long terms.

Let’s explore some of the features and benefits that current technology and an advanced vision solution can provide.

Data Capture for Every Part

With critical parts like those manufactured for vehicles, inspection and data on every aspect are essential. An advanced AI vision platform features the latest camera/imaging technology to capture each part’s high-resolution images and collects other critical sensor and machine data.

All of the image and process data collected for each part is assigned a unique identifier and available via our online platform for complete traceability. That means manufacturers can execute root cause analysis when an issue occurs because the data was at their fingertips instead of before when they had to access various data sources, and pulling exactly what they needed was cumbersome and highly manual.

Truly Meaningful Data

As raw data doesn’t lend itself to scalable AI deployments within industrial manufacturing applications, advanced vision platforms are designed to allow the end-users to focus on the most important data, or put simply, to see what was impossible to see before. Advanced vision platforms also normalize the data, allowing the end-users to compare apples to apples across multiple machines and lines. This is critical when they look to scale the technology across multiple sites.

A seamless feedback loop accelerates the process for AI model training, eliminating the requirement for large datasets. Once ready, the user is in control. They can unlock the value of AI at their pace and integrate the solution to their industrial platform of choice, automating the process’s decision-making.

Continuous Learning & Improvement

Advanced AI Vision platforms don’t stop there. They serve as a collaborative monitoring and improvement tool that provides users with the ability to track and share the models’ results in production.

Manufacturers also use the toolset to find the sweet spot between producing acceptable products vs. maintaining a baseline of throughput regardless of the quality output. They continuously learn about their process, allowing them to get better always, and in the event there is an issue, get to resolution much faster. Previously unavailable machine and process data is a click of a mouse or tap of the screen away, empowering manufacturers to solve some of their most complex problems. While pushing technology adoption may appear self-serving, the ongoing pandemic has been a wake-up call for manufacturers––especially the laggards. Various studies find that companies adopting tech more quickly are bouncing back faster and not just returning to previous revenue levels but achieving revenue growth.

For automotive manufacturers (all large-scale manufacturers), it’s time to shift into high gear and get proactive about making recalls and warranty issues a fading image in their rear view mirrors.","['artificial', 'issues', 'advanced', 'data', 'warranty', 'process', 'manufacturers', 'ai', 'recalls', 'vision', 'suppliers', 'technology', 'manufacturing', 'intelligence', 'automotive']","What are automotive manufacturers, OEMs, and suppliers, doing to prevent issues like recalls and warranty claims from happening at all?
An advanced AI vision platform features the latest camera/imaging technology to capture each part’s high-resolution images and collects other critical sensor and machine data.
All of the image and process data collected for each part is assigned a unique identifier and available via our online platform for complete traceability.
Continuous Learning & ImprovementAdvanced AI Vision platforms don’t stop there.
For automotive manufacturers (all large-scale manufacturers), it’s time to shift into high gear and get proactive about making recalls and warranty issues a fading image in their rear view mirrors."
50,http://www.hedgeweek.com/2021/02/12/295869/ex-jp-morgan-strategist-unveils-machine-learning-us-equity-hedge-fund-falcon,Ex-JP Morgan strategist unveils machine-learning US equity hedge fund on Falcon platform,2021-02-12 00:00:00,"Falcon Investment Management and ex-JP Morgan strategist Arman Salavitabar are launching a US equity-focused long/short hedge fund on the London-based firm’s multi-manager investment platform.

The Sala Vita L/S Equity Fund brings together human analysis and machine learning algorithms, combining systematic equity selection and market timing signals to manage a long/short portfolio of single stocks.

The new fund, which targets uncorrelated, absolute returns of more than 10 per annum through trading S&P 500 names, will be positioned to outperform in high-volatility market environments in order to complement investors’ broader equity portfolios.

“Capital markets are just starting to see the type of innovation and disruption that other industries have enjoyed for years,” said Arman Salavitabar, a former EMEA head of FICC structuring at JP Morgan.

Salavitabar, who spent eight years at JPM before managing portfolios at a family office, received initial seed funding in late 2020.

The new fund is designed to merge the long-term benefits of being invested in equities with the security of portfolio protection against market drawdowns.

Commenting on the launch, Benny Menashe, managing director of Falcon, described the strategic partnership between Falcon and Sala Vita as “a natural fit”.

“Our platform allows sophisticated investors to access Sala Vita’s niche strategy through a centralized operational and risk management framework,” Menashe said.

“For years institutional investors could not invest in early-stage talents due to operational risks. We want to change that.”","['equity', 'portfolio', 'strategist', 'morgan', 'investors', 'machinelearning', 'unveils', 'falcon', 'exjp', 'vita', 'salavitabar', 'operational', 'hedge', 'platform', 'fund', 'market', 'sala']","Falcon Investment Management and ex-JP Morgan strategist Arman Salavitabar are launching a US equity-focused long/short hedge fund on the London-based firm’s multi-manager investment platform.
The Sala Vita L/S Equity Fund brings together human analysis and machine learning algorithms, combining systematic equity selection and market timing signals to manage a long/short portfolio of single stocks.
The new fund is designed to merge the long-term benefits of being invested in equities with the security of portfolio protection against market drawdowns.
Commenting on the launch, Benny Menashe, managing director of Falcon, described the strategic partnership between Falcon and Sala Vita as “a natural fit”.
“Our platform allows sophisticated investors to access Sala Vita’s niche strategy through a centralized operational and risk management framework,” Menashe said."
51,https://www.smartcitiesworld.net/news/news/start-up-helps-turin-to-optimise-healthcare-and-covid-19-vaccine-spaces-6092,Start-up helps Turin to optimise healthcare and Covid-19 vaccine spaces,,"ASL Torino 4, the local public health company serving the Italian Metropolitan city of Turin’s northern area, has signed a deal with Hynnova to deploy its products to help streamline home care activities and Covid-19 vaccination programmes.

Hynnova, a start-up launched within the EIT Digital Innovation Factory, developed a platform that uses advanced mathematics and machine learning to dynamically optimise the use of healthcare spaces and resources.

Local vaccination campaign

As part of the contract, ASL Torino 4 will use both Hynnova solutions: ASC Home Healthcare to manage its day-to-day home assistance and care activities operations; and ASC Smart Booking to optimise the local vaccination campaign for the 520,000 citizens it serves.

The platform integrates information coming from three separate datasets:

the activities to be carried out

the available resources

and the constraints to be respected.

It then generates optimised work-programmes and optimal paths, maximising the time spent on productive activities and minimising downtime, the company reports.

According to Hynnova, its mission is particularly relevant in the current context with the spread of Covid-19 pushing healthcare systems to operate as much as possible in a decentralised manner.

The solution allocates available resources in the “best possible way” and is specifically tailored to address the specific constraints of vaccination campaigns

In addition, the start-up points out its goal is consistent with the World Health Organisation’s guidelines on the Ageing Society, that advocate a gradual transition from a centralised care model (hospital and nursing homes) to a decentralised one.

Hynnova is one of the activities selected by the EIT Digital Innovation Factory in mid-2020 within Data Against Covid-19 initiative, an effort to develop digital tools to fight back against the pandemic in less than six months. With the support of EIT Digital, the Hynnova start-up was launched by Italian company Hypermynds and the Dutch Kinetic Analysis.

Vivisol, one of the main European groups operating in home care, was Hynnova’s first customer, purchasing from Hynnova in December 2020. In the meantime, Hynnova developed the ASC Smart Booking solution, to help manage and optimise vaccination campaigns.

Hynnova claims the solution allocates available resources in the “best possible way” and is specifically tailored to address the specific constraints of vaccination campaigns.

The company reports it is discussing possible deals with several other Italian local public health companies of which there are 110. “If just a few of these deals are finalised, we will become profitable, skipping the seed phase, and scaling the company,” Luca Calvetti, founder and chairman of Hypermynds, and CEO of Hynnova.

You might also like:","['helps', 'hynnova', 'company', 'vaccine', 'possible', 'turin', 'optimise', 'vaccination', 'care', 'covid19', 'spaces', 'solution', 'digital', 'healthcare', 'activities', 'startup']","Hynnova, a start-up launched within the EIT Digital Innovation Factory, developed a platform that uses advanced mathematics and machine learning to dynamically optimise the use of healthcare spaces and resources.
The platform integrates information coming from three separate datasets:the activities to be carried outthe available resourcesand the constraints to be respected.
It then generates optimised work-programmes and optimal paths, maximising the time spent on productive activities and minimising downtime, the company reports.
With the support of EIT Digital, the Hynnova start-up was launched by Italian company Hypermynds and the Dutch Kinetic Analysis.
In the meantime, Hynnova developed the ASC Smart Booking solution, to help manage and optimise vaccination campaigns."
52,https://techcrunch.com/2021/02/12/swedens-data-watchdog-slaps-police-for-unlawful-use-of-clearview-ai/,TechCrunch is now a part of Verizon Media,2021-02-12 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
53,https://www.analyticsinsight.net/artificial-intelligence-redefining-and-innovating-the-textile-industry/,,,,,
54,https://pulse2.com/machine-learning-cloud-infrastructure-company-pinecone-raises-10-million/,Machine Learning Cloud Infrastructure Company Pinecone Raises $10 Million,2021-02-11 21:44:21+00:00,"Pinecone — a machine learning cloud infrastructure company — recently left stealth with $10 million in seed funding. These are the details.

Pinecone — a machine learning cloud infrastructure company — recently left stealth with $10 million in seed funding. And the investment was led by Wing Venture Capital, and Wing’s Founding Partner, Peter Wagner, will be joining the Pinecone board. Launched by the team behind Amazon SageMaker and founded by scientist and former AWS Director Edo Liberty, Pinecone makes large-scale real-time inference as simple as querying a database. And it is available for self-onboarding now.

Pinecone builds infrastructure for enabling the next wave of artificial intelligence (AI) applications in the cloud. And the company’s vector database supports large scale production deployments of real-time applications like personalization, semantic text search, image retrieval, data fusion, deduplication, recommendation, and anomaly detection. Plus each of these is a multi-billion dollar market today and projected to grow 30%+ YoY for the foreseeable future as more and more companies adopt ML technologies.

Modern ML and AI use vectors (aka embeddings) for representing data like documents, videos, and user behaviors. And applications that need to accurately filter and rank large collections of such vectors in real time require a highly specialized data infrastructure.

Existing databases or search engines are not a good fit as they are designed for tables and documents, not vectors. And in-house systems that use open source libraries are expensive to build and hard to maintain. This forces developers to constantly compromise between speed, accuracy, stability, and scale.

With Pinecone’s unique vector database, ML and data infrastructure engineers can dynamically transform and index billions of high-dimensional vectors. And they can answer queries like nearest neighbor and max-dot-product search accurately and in milliseconds.

Pinecone’s database is 100% serverless and API-driven — which means customers always have the computing resources they need, when they need them, without having to worry about infrastructure or maintenance. And simple self-onboarding and consumption-based pricing let companies build proofs of concept with very little overhead and then scale effortlessly.

One of the largest retailers in the world reports using Pinecone to serve real-time shopping recommendations based on their own deep learning models. And they saw an immediate 18.5% lift in revenue per recommendation compared with their previous solution.

KEY QUOTES:

“Visionary leaders in many companies are working hard to transform their business with machine learning. Pinecone gives them technology they need which, until today, was reserved to a few tech giants.”

— Edo Liberty, Founder and CEO of Pinecone

“The modern Enterprise is built on data and powered by AI. The Data Cloud has emerged as its foundation with the ascendance of Snowflake. Pinecone is poised to unleash data teams and their ML-based applications in a similar fashion.”

— Peter Wagner, Founding Partner, Wing Venture Capital.","['pinecone', 'data', 'search', 'vectors', 'learning', 'raises', 'need', 'applications', 'database', 'machine', 'cloud', 'infrastructure', 'million', 'company']","Pinecone — a machine learning cloud infrastructure company — recently left stealth with $10 million in seed funding.
Pinecone — a machine learning cloud infrastructure company — recently left stealth with $10 million in seed funding.
Pinecone builds infrastructure for enabling the next wave of artificial intelligence (AI) applications in the cloud.
With Pinecone’s unique vector database, ML and data infrastructure engineers can dynamically transform and index billions of high-dimensional vectors.
The Data Cloud has emerged as its foundation with the ascendance of Snowflake."
55,https://venturebeat.com/2021/02/11/quantum-venture-funding-dipped-12-in-2020-but-quantum-investments-rose-46/,"Quantum venture funding dipped 12% in 2020, but quantum investments rose 46%",2021-02-11 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Sorting through the hype surrounding quantum computing these days isn’t easy for enterprises trying to figure out the right time to jump in. Skeptics say any real impact is still years away, and yet quantum startups continue to seduce venture capitalists in search of the next big thing.

A new report from CB Insights may not resolve this debate, but it does add some interesting nuance. While the number of venture capital deals for quantum computing startups rose 46% to 37 in 2020 compared to 2019, the total amount raised in this sector fell 12% to $365 million.

Looking at just the number of deals, the annual tally has ticked up steadily from just 6 deals in 2015. As for the funding total, while it was down from $417 million in 2019, it remains well above the $73 million raised in 2015.

There’s a couple of conclusions to draw from this.

First, the number of startups being drawn into this space is clearly rising. As research has advanced, more entrepreneurs with the right technical chops feel the time is now to start building their startup.

Second, the average deal size for 2020 was just under $10 million. And if you include the $46 million IQM raised, that squeezes the average for most other deals down even further. That certainly demonstrates optimism, but it’s far from the kind of financial gusher or valuations that would indicate any kind of quantum bubble.

Finally, it’s important to remember that startups are likely a tiny slice of what’s happening in quantum these days. A leading indicator? Perhaps. But a large part of the agenda is still being driven by tech giants who have massive resources to invest in a technology that may have a long horizon and could be years away from generating sufficient revenues. That includes Intel, IBM, Google, Microsoft, and Amazon.

Indeed, Amazon just rolled out a new blog dedicated to quantum computing. Last year, Amazon Web Services launched Amazon Braket, a product that lets enterprises start experimenting with quantum computing. Even so, AWS quantum computing director Simone Severini wrote in the inaugural blog post that business customers are still scratching their heads over the whole phenomenon.

“We heard a recurring question, ‘When will quantum computing reach its true potential?’ My answer was, ‘I don’t know.'” he wrote. “No one does. It’s a difficult question because there are still fundamental scientific and engineering problems to be solved. The uncertainty makes this area so fascinating, but it also makes it difficult to plan. For some customers, that’s a real issue. They want to know if and when they should focus on quantum computing, but struggle to get the facts, to discern the signal from all the noises.”","['startups', '46', 'raised', 'number', '2020', 'deals', 'rose', 'investments', 'computing', 'amazon', 'venture', 'funding', 'dipped', 'wrote', '12', 'million', 'quantum']","Sorting through the hype surrounding quantum computing these days isn’t easy for enterprises trying to figure out the right time to jump in.
Skeptics say any real impact is still years away, and yet quantum startups continue to seduce venture capitalists in search of the next big thing.
While the number of venture capital deals for quantum computing startups rose 46% to 37 in 2020 compared to 2019, the total amount raised in this sector fell 12% to $365 million.
As for the funding total, while it was down from $417 million in 2019, it remains well above the $73 million raised in 2015.
“We heard a recurring question, ‘When will quantum computing reach its true potential?’ My answer was, ‘I don’t know."
56,https://www.discovermagazine.com/the-sciences/quantum-computer-chips-manufactured-using-mass-market-industrial-fabrication,Quantum Computer Chips Manufactured Using Mass-Market Industrial Fabrication Techniques,,"The quantum computing revolution is upon us. Well, almost. It’s hard to have missed the headlines proclaiming the great power of the latest generation of quantum, their ability to outperform conventional computers , a property called quantum supremacy, and the huge promise of the years ahead.

But an important question remains — how are we going to build these devices? Quantum computers variously rely on photons and/or exotic states of matter trapped in magnetic fields at mind-numbingly cold temperatures. So it’s easy to imagine that quantum computing will require an entirely new industrial base founded on novel technologies.

But there is another possibility: that quantum computers can work with electrons passing through transistor-like devices called quantum dots carved out of silicon. If that’s the case, the entire revolution can piggyback on the industrial base that supports current chip-manufacture.

Now this option looks a step closer thanks to the work of Anne-Marije Zwerver at Delft University of Technology in Denmark and colleagues, many at the research labs at U.S. chipmaker Intel, based in Hillsboro, Oregon. This group has fabricated nanoscale silicon transistors that can reliably process quantum information in ways that match specialist devices.

But the key breakthrough is that they have done this using industrial chip fabrication processes with a yield that is high enough to allow significant scalability. That paves the way for industrial-scale fabrication of quantum computing chips. “The feasibility of high-quality qubits made with fully-industrial techniques strongly enhances the prospects of a large-scale quantum computer,” says the team.

Quantum Dots

Academic labs have been making these kinds of quantum dot devices for some time. A quantum dot is similar in essence to a transistor that operates at the level of single electrons. It consists of a channel through which electrons can flow with a gate above that limits the flow to one electron at a time.

This control is a delicate balancing act. The gate shapes the electric potential in the channel into wells that trap single electrons — hence the term quantum dots. These wells often sit next to other components that manipulate or read the spin of the electron.

But the electric fields are so delicate that they are influenced by any stray field from other components, from contaminants or even defects in the silicon. So any tiny variation in the manufacturing conditions can ruin the quantum properties of the electrons.

That’s why it has only been possible to make these devices in special facilities and even then in small numbers. By contrast, industrial chip fabrication aims to carve billions of transistors onto a single chip. A key question is whether these manufacturing conditions can be made uniform enough to make quantum dots.

The answer according to Zwerver and colleagues is affirmative. Their chip wafers are 300 millimeters in diameter — about the size of an Oreo biscuit — and each contains some 10,000 quantum dots. The team's measurements show that the dots perform as well as those created in specialist facilities. “We achieve nanoscale gate patterns with remarkable homogeneity,” they say.

In more detail: “single-spin qubit operation using magnetic resonance reveals relaxation times of over 1 s at 1 Tesla and coherence times of over 3 milliseconds, matching the quality of silicon spin qubits reported to date,” say Zwerver and colleagues.

That’s interesting work that paves the way for the manufacture of quantum computer chips on an industrial scale. Of course, these will still be exotic devices. They will require huge magnetic fields tens of thousands of times stronger than the Earth’s field. And they will need to be cooled to superconducting temperature.

So nobody will be carrying these in their back pockets any time soon. But with large scale manufacture, quantum computers may soon become standard accessories at supercomputing facilities around the world, where complex calculations can be divided into classical and quantum components and then passed to the relevant processing machine for crunching.

Of course, what this will make possible is a topic of significant debate. But the prospect of Industrial fabrication should help to sharpen everybody’s thinking.

Ref: arxiv.org/abs/2101.12650: Qubits Made By Advanced Semiconductor Manufacturing","['using', 'devices', 'electrons', 'techniques', 'chip', 'industrial', 'massmarket', 'silicon', 'zwerver', 'computer', 'dots', 'manufactured', 'computers', 'chips', 'fabrication', 'quantum']","Quantum computers variously rely on photons and/or exotic states of matter trapped in magnetic fields at mind-numbingly cold temperatures.
But there is another possibility: that quantum computers can work with electrons passing through transistor-like devices called quantum dots carved out of silicon.
But the key breakthrough is that they have done this using industrial chip fabrication processes with a yield that is high enough to allow significant scalability.
By contrast, industrial chip fabrication aims to carve billions of transistors onto a single chip.
But the prospect of Industrial fabrication should help to sharpen everybody’s thinking."
57,http://www.nextplatform.com/2021/02/11/the-billion-dollar-ai-problem-that-just-keeps-scaling/,The Billion Dollar AI Problem That Just Keeps Scaling,2021-02-11 00:00:00,"There is a new challenge workload on the horizon, one where few can afford to compete. But for those who can, it will spark a rethink in what is possible from even the most powerful traditional supercomputers.

It might sound odd that it can be collected under the banner of language modeling since that invokes speech and text analysis and generation. But emerging workloads and research show how far this is from traditional natural language processing. Over the next several years, language models will likely become far more general purpose, encompassing an unimaginable range of problem types.

Being able to have a world described through language and rendered as an image or video, or even asking text-based questions about the world with answers based on a system’s understanding of our nuanced reality sounds like science fiction. But we are inching toward this future. The problem with making this a practical reality, however, is that the economics of a near infinitely scalable model are incredibly prohibitive.

Bryan Catanzaro, VP of Applied Deep Learning Research at NVIDIA put this into staggering context when he told us that he thinks it is entirely possible that in five years a company could invest one billion dollars in compute time to train a single language model. Think about that for a moment.

“With models like GPT-3 we are starting to see models that can go beyond, that can actually become more general purpose tools for solving real-world problems. It’s a step toward a more general form of artificial intelligence and that justifies the investment in training these enormous language models on clusters like Selene.” Recall that #5 ranked Selene is NVIDIA’s own AI supercomputer used for experimental and production operations. This includes everything from dense model experimentation like Catanzaro’s team does to providing the backbone for initiatives at the graphics giant like those wrapped into the host of Maxine services.

“We’re going to see these models push the economic limit. Technology has always been constrained by economics, even Moore’s Law is an economic law as much as a physics law. These models are so adaptable and flexible and their capabilities have been so correlated with scale we may actually see them providing several billions of dollars worth of value from a single model, so in the next five years, spending a billion in compute to train those could make sense,” Catanzaro says.

These services have been valuable for NVIDIA, he says, but when he looks ahead to areas first in entertainment and gaming where text can be used to feed into image or video synthesis—to create worlds from description—those future investments seem more plausible. Where those fit into large enterprise is still up in the air but Catanzaro says with language models as a general purpose platform to develop new problems and applications, smart people at companies globally will find ways to make up the billion-plus investments.

“Language modeling is one of the most expensive done in ML now,” Catanzaro explains. “Vast language models need enormous amounts of compute with GPT-3 for example and its zero-shot learning approach where we ask the language model to solve a new problem that’s never been seen and it often comes up with the right answer without being taught the task simply because the language model is powerful enough to understand the question and provide an answer that makes sense in context.”

Chances are, as you read this, you keep going back to that whole billion dollar idea. That may sound a little crazy, but it is not hard to get to $1 billion, either on premises and particularly on the public cloud.

This is some approximate math but it shows, for companies that see the value and want to sign up for a long-term commitment to a continuously training/retraining language modeling adventure, getting to a billion dollars doesn’t take long.

The initial configuration of Selene had 280 DGX-A100 systems, which cost $199,000 each approximately. If networking is around 15 percent of the total cost of the system and proper storage is around 20 percent, at list price the machine would cost on the order of $85 million. After volume discounts or by going with a third party making a similar server using HGX-A100 motherboards and their own compute engines, maybe you could get the price down to $75 million.

Over a three-year period, that works out to something like $2,852 per hour for the entire Selene system, not including power, cooling, operations, and other costs. A cloud builder with lots of volume might be able to get the components for a little less. The p4d instances at Amazon Web Services cost $32.77 per hour for a system with two processors and eight A100s. And for an equal amount of capacity as Selene, you are talking $9,176 per hour for 280 instances, and over three years that is $241.3 million on demand; if you do three year reserved instances the price to rent a Selene-like system on AWS for three years comes down to – you guessed it – $85.2 million.

To get to $1 billion, you are going to need to spend for 12X the capacity of Selene. So maybe that is not so easy, after all, or maybe it will be because these models are going to be truly enormous.

(Editor’s note: Nvidia says that Selene has been recently upgraded to 560 DGX-A100 systems, which doubles its size and cost, and means you only need 6X more compute — something we did not know when we wrote this story. The idea holds just the same.)

The math puts this in perspective, even if the ROI and real-world use cases that can make this a suitable investment are still a bit nebulous.

All of the economic scale means figuring out how to keep scaling the model itself is critical but Catanzaro and team have shown some impressive results over time. Below see the scaling results from current GPT-3 models on Selene. Notice the jump from 8 billion parameters on the original Megatron model from over a year ago to 175 billion parameters (the size of GPT-3) and the relative performance. The team went from 200 GPUs to 3,000 A100 GPUs to train a single model with sustained 400 petaflops (at FP16, not the double-precision we’re used to).

But if companies are ever going to invest a billion in compute time to use these models what challenges are on the roadmap in terms of scalability of the models?

Looking at Catanzaro and team’s evolving work shows where things break and what continues to work as they keep scaling. Much of the footwork has been on the software size exploring different ways to handle model parallelism. The starting point is data parallelism, which is the “easiest” but is limited from an algorithmic perspective because the bigger the batch, the more time is spent optimizing, eventually leading to diminishing returns. Still, to get their results they pushed data parallelism to the upper limit before looking at other methods, including “tensor parallelism” to get maximum GPU utilization and pipeline parallelism, something already well understood in HPC.

Tensor scaling is key to the linear scaling shown above. In this case, each layer of the neural network is split into pieces without any changes to semantics of the computation; it’s just direct parallelization. There are limits to how tiny the slices to be without incurring heavy communication overhead. Pipeline parallelism, the other key to linear scaling of these models across the 2,240 A100 GPUs (and 280 DGX machines) benefits from the structure of neural networks with multiple layers that are identical from a computational perspective. The team can chop up these layers and define different layers to different processors. This is effective but semantics matter—unlike with tensor parallelism they change with pipelining and even with cleverness in minimizing the bubbles that come during pipeline filling and draining it has its limits as well. Ultimately, it’s pushing all of these ways to address model parallelism to their max before the law of diminishing returns kicks in, a fine balance, Catanzaro says.

On the hardware side, the advantages of the GPU on these problems are well known but it’s the interconnect and network architecture that delivers the linear scaling Catanzaro cites. The Selene system is basically 280 DGX A100 machines joined externally with Mellanox Infiniband but in each node is the all-important NVSwitch. “The thing about NVSwitch is that it has all-to-all communication abilities so the software has total flexibility about how we’ll perform model parallelism, both tensor and pipeline. There are tradeoffs we can make that have different impacts on the interconnect, some more favorable for compute (helping us get max utilization out of the GPUs) but others involve increasing demands on the interconnect,” Catanzaro explains.

The models need all-to-all connectivity but as one might imagine, the communication patterns are deeply complex compared to a more traditional nearest-neighbor mesh architecture, which is needed on systems that don’t have NVSwitch. Using that, he says they can explore far more combinations of model parallelism to increase compute throughput significantly.

It’s true that all of these capabilities are available on the cloud, although there are some hits in terms of getting that model out (for instance, Azure has NVSwitch). What will be most interesting over time is how many massive-scale system sales this could lead to among the majors. Although the ones we can think about–the only ones who might do this–own their cloud anyway.

If we move some of these insights about work being done to continue the linear scalability of these models back to the economic conversation, that billion dollar investment figure might not be as wild as it sounds on the surface. If five years of continued scalability of the model, even if not perfectly linear, means increasing complexity of problems that can be addressed, it’s not inconceivable to think of what it would take to build a “super” Selene—and getting to a billion dollars in hardware alone isn’t as inconceivable as it sounds.","['models', 'dollar', 'model', 'selene', 'catanzaro', 'problem', 'ai', 'language', 'keeps', 'compute', 'parallelism', 'system', 'scaling', 'billion']","Over the next several years, language models will likely become far more general purpose, encompassing an unimaginable range of problem types.
Much of the footwork has been on the software size exploring different ways to handle model parallelism.
On the hardware side, the advantages of the GPU on these problems are well known but it’s the interconnect and network architecture that delivers the linear scaling Catanzaro cites.
The Selene system is basically 280 DGX A100 machines joined externally with Mellanox Infiniband but in each node is the all-important NVSwitch.
Using that, he says they can explore far more combinations of model parallelism to increase compute throughput significantly."
58,https://phys.org/news/2021-02-machine-stellar-tess.html,Researcher uses machine learning to classify stellar objects from TESS data,,"This illustration depicts light curves for a representative eclipsing binary (top) and one of the candidate eclipsing quadruple star systems identified by Adam Friedman. The extra dips caused by additional eclipses in the quadruple system result in a more complicated pattern. Credit: NASA’s Goddard Space Flight Center

A game of chess has 20 possible opening moves. Imagine being asked to start a game with tens of millions of openings instead. That was the task assigned to Adam Friedman, a 2020 summer intern at NASA's Goddard Space Flight Center in Greenbelt, Maryland. A chess champion in high school, Friedman analyzed his opponent—a deluge of data on the brightness changes of over 70 million stars.

Using traditional computational approaches, the task of sifting through and classifying these measurements could have taken months. With the use of machine learning, a form of artificial intelligence, this can be accomplished in seconds. Working with Brian Powell, a data scientist in the High Energy Astrophysics Science Archive Research Center at Goddard, Friedman trained a computer system to identify an important class of variable stars without explicitly programming it do so.

Machine learning allows computers to process and sort immense amounts of data automatically—just what was needed to sift through the torrent of stellar data. To do this, Powell created a neural network—a series of mathematical rules that attempt to recognize underlying relationships in data through a process that mimics, in a greatly simplified way, how the human brain works. For a neural network to function, though, it must be trained.

""The internship was about collecting training data,"" Friedman said, ""because machine learning works by collecting an incredibly large number of examples to train the model.""

NASA's Transiting Exoplanet Survey Satellite (TESS) launched in April 2018 to find new worlds beyond our solar system, or exoplanets, by monitoring brightness changes in nearby stars. Since its launch, TESS has observed nearly the entire sky. Biweekly, the satellite beams back several thousand large pictures called full-frame images of a pre-planned section of the sky.

Astronomers use the data to construct light curves, graphs that show how a star's brightness shifts over time. From the raw TESS data, Powell used the 129,000-core Discover supercomputer at NASA's Center for Climate Simulation (NCCS) at Goddard to build millions of light curves.

""Thanks to support from NCCS, we were able to start building light curves in massive quantities. We have approximately 70 million now, with more on the way. Data science and machine learning can help drive these discoveries, allowing for high volumes of data to be sorted and processed faster and more accurately than ever before,"" Powell said.

Out of this enormous stack, Friedman wanted to identify eclipsing binaries, paired stars that alternately pass in front of, or transit, each other every orbit as seen from Earth. During each eclipse, the system dims as one star passes in front of the other, which produces a dip in its light curve. ""The really useful feature of eclipsing binaries, and the reason that they are the backbone of astrophysics, is that they give us direct measurements of their fundamental properties, such as their mass and size,"" said Veselin Kostov, a research scientist at Goddard and the SETI Institute in Mountain View, California. ""And through these properties, we can directly measure distances to these systems. They afford us one of the very few opportunities to measure direct distances in the universe.""

NCCS also provided their Advanced Data Analytics PlaTform Graphics Processing Unit Cluster for running the neural network that Powell coded and Friedman trained.

Friedman could input a light curve and instruct the neural network to assign it to a particular category. After repeating this action thousands of times, the neural network began to recognize groups of light curves and suggest classifications based on the probability that a given curve fits into a given group. Friedman found example light curves for a broad range of star systems and input them until the network learned what each looked like and could identify new light curves autonomously. This allowed for a task that would have taken months on a modern desktop computer to be completed in a few seconds.

Machine learning vastly improves the efficiency of finding these star systems in tens of millions of TESS images by learning to identify the features of an eclipse and labeling the light curve accordingly. But Friedman soon noticed a quirk in some of the light curves the network had claimed were eclipsing binary candidates. They had extra dips.

Occasionally, star systems can have more than two components. If these stars eclipse each other, then the light curve will have additional dimmings that, at first glance, will appear at irregular intervals. Friedman discovered they were candidates for multistar systems and then began an exhaustive search for similar systems among the eclipsing binaries identified by the neural network. In total, Friedman found eight new candidate quadruple star systems. These cases are interesting because they provide insight into how multistar systems form and evolve.

Friedman had just finished his freshman year as a computer science major at the University of Michigan, and, at the start of the summer, had no background in astronomy, high-performance computing, data science, or machine learning. Compounding the complexity of the task at hand, Friedman performed his internship from home due to COVID-19, but despite these challenges, Powell said he caught on quickly.

""He is nothing short of brilliant,"" Powell said. ""Adam has an uncanny ability to see deviations from periodicity in light curves."" With such a cosmically profound outcome from his internship, it is easy to forget the course of Friedman's progress. ""It's not like he was an astronomer and machine learning expert at the start of the summer,"" he added. ""His ability to master extremely complex concepts and skill sets in such a short time is astounding.""

Friedman was grateful for his time with Powell over the summer. He said: ""I have to give huge credit to Brian. He was an incredible mentor; he was definitely the best supervisor I've ever met. He met with me every single day, just to teach me how to do the project. He really was a great teacher.""

Explore further Sextuply-eclipsing sextuple star system uncovered in TESS data with an assist from AI","['classify', 'tess', 'researcher', 'network', 'powell', 'data', 'star', 'stellar', 'learning', 'friedman', 'neural', 'uses', 'objects', 'machine', 'systems', 'light', 'curves']","Machine learning allows computers to process and sort immense amounts of data automatically—just what was needed to sift through the torrent of stellar data.
""The internship was about collecting training data,"" Friedman said, ""because machine learning works by collecting an incredibly large number of examples to train the model.""
Astronomers use the data to construct light curves, graphs that show how a star's brightness shifts over time.
From the raw TESS data, Powell used the 129,000-core Discover supercomputer at NASA's Center for Climate Simulation (NCCS) at Goddard to build millions of light curves.
""Thanks to support from NCCS, we were able to start building light curves in massive quantities."
59,https://www.businessinsider.com/labelbox-40m-seriesc-funding-data-labeling-machine-learning-2021-2,Data training startup Labelbox raised a $40 million Series C after getting emails from investors 'every day',2021-02-11 00:00:00,"On Thursday, Labelbox announced a $40 million Series C round of funding led by B Capital Group.

The startup didn't need capital but was getting constant emails from investors and was won over by B Capital's Rashmi Gopinath.

The startup is taking on a large competitor in the data labeling space: Amazon's SageMaker.

Read more on the Insider homepage.

The data training startup Labelbox announced a $40 million Series C round of funding on Thursday led by B Capital Group, with participation from Andreessen Horowitz, First Round Capital, Kleiner Perkins, and others.

Labelbox's platform manages and annotates data to prepare it for use in machine learning applications. Labeling data — like indicating whether a photo contains a traffic light or a plant — helps machines find patterns and know what to look for. Once trained, the system can find similar trends in new data sets. Building useful algorithms like this requires vast amounts of training data and Labelbox's platform essentially helps data science teams quickly and easily label that data to ensure that it's accurate and high-quality.

The raise comes just about a year after LabelBox's $25 million Series B, which valued it at $135 million according to PitchBook, and this latest round — for which the firm declined to share valuation — brings the total funding to $79 million.

Labelbox software detecting cell anomalies. Labelbox

Cofounder and CEO Manu Sharma told Insider that the San Francisco-based startup, which was founded in 2018, was getting cold emails from interested investors ""every day"" in mid-2020. Even though the startup still had at least two years of runway left at the time, the team began having conversations and was very impressed with the insights of B Capital partner Rashmi Gopinath. Ultimately, that relationship bloomed into B Capital leading the oversubscribed round.

Data labeling and management can be labor-intensive for companies, Gopinath said, citing a 2019 Cognilytica report that enterprises adopting machine learning spend over 80% of their time on those activities alone.

""If we believe that data is the new oil, then labeling and annotation becomes like the essential parts of the refinery,"" she told Insider. ""Saving the amount of time that data scientists and engineers are spending labeling that data becomes additional time that they could spend on actually building the most predictive models.""

From the same Cognilytica report, the market for third-party data labeling solutions was $150 million in 2018, and is projected to grow to six times that by 2023, to over $1 billion.

The firm is taking on at least one aggressive competitor though: Amazon's SageMaker platform. SageMaker is an admittedly obvious option for many businesses because their applications are already built on the AWS cloud platform, Gopinath said. Still, Labelbox maintains that it has several advantages — including better automated labeling — and that its customers spend less time curating data sets with its platform over Amazon's.

""[Labelbox has] hundreds of customers today on the platform, including some of the Fortune 500 enterprises, federal customers, many of who run on AWS or Azure today,"" Gopinath said. ""But they're really looking for that best-in-class product and have mentioned that Labelbox is several years ahead of where the cloud native tool capabilities are at today.""","['labelbox', 'getting', 'series', 'day', 'data', 'raised', 'investors', 'labeling', 'emails', 'capital', 'platform', 'gopinath', 'training', 'b', 'million', 'round', 'startup']","On Thursday, Labelbox announced a $40 million Series C round of funding led by B Capital Group.
The startup is taking on a large competitor in the data labeling space: Amazon's SageMaker.
The data training startup Labelbox announced a $40 million Series C round of funding on Thursday led by B Capital Group, with participation from Andreessen Horowitz, First Round Capital, Kleiner Perkins, and others.
Labeling data — like indicating whether a photo contains a traffic light or a plant — helps machines find patterns and know what to look for.
SageMaker is an admittedly obvious option for many businesses because their applications are already built on the AWS cloud platform, Gopinath said."
60,https://aithority.com/technology/analytics/piano-software-elevates-flagship-product-composer-with-new-data-and-machine-learning-capabilities-for-more-powerful-customer-journey-design/,Piano Software Elevates Flagship Product Composer with New Data and Machine Learning Capabilities for More Powerful Customer Journey Design,2021-02-11 12:15:16+00:00,"Integrated capabilities enable publishers to leverage out-of-the-box and bespoke customer journey segments to cut churn, boost loyalty and increase acquisition

Piano, the leading subscription commerce and customer experience platform, announced extensive enhancements to its flagship product, Composer, which empowers publishers and brands to orchestrate and optimize customer journeys across paywalls, content and personalized experiences. Piano’s data collection, segment creation and activation capabilities are now fully integrated with Composer to provide industry-first customer journey design powered by machine learning, allowing clients to influence behaviors using zero- and first-party data.

“Piano’s mission has always been to enable our clients to create the most relevant and satisfying customer experiences possible using data and machine learning, and we are constantly improving how we do that,” said Trevor Kaufman, CEO of Piano. “We believe our integrated data and customer experience platform has unlocked the industry’s most powerful targeting capabilities to improve business performance throughout the entire customer journey.”

Recommended AI News: TradeStation Supports Trading of Ether Futures from CME Group on Day One

The latest upgrades to Composer allow organizations to capitalize on Piano’s data, as well as their own, in order to boost retention, subscriptions and customer satisfaction in a number of ways:

Prevent accidental churn: Refine your targeting and messaging strategies to avoid lost users due to failure to resubscribe.

Refine your targeting and messaging strategies to avoid lost users due to failure to resubscribe. Proactively message dormant users: Leverage out-of-the-box segments to serve trending content to users who have been dormant for two or more weeks in order to increase engagement.

Leverage out-of-the-box segments to serve trending content to users who have been dormant for two or more weeks in order to increase engagement. Recommend content based on performance: Fuel sophisticated site personalization with Piano’s machine learning content recommendations, which programmatically collect reader data and pair it with Composer segments to target quality content.

Fuel sophisticated site personalization with Piano’s machine learning content recommendations, which programmatically collect reader data and pair it with Composer segments to target quality content. Apply real-time propensity models: Utilize propensity-based, machine learning segments for launching or fine-tuning subscription strategies—based on real-time subscription scoring and a view into users in each segment.

Utilize propensity-based, machine learning segments for launching or fine-tuning subscription strategies—based on real-time subscription scoring and a view into users in each segment. Flourish in a cookieless world: Collect zero- and first-party data, build bespoke segments and customer profiles and use Composer to orchestrate personalized pricing, content and messaging strategies.

Recommended AI News: DVP Launches World’s First Decentralized Security Crowd-testing Platform to Solve Security Issues of DeFi Ecosystem

For most publishers, 2020 offered a surge in paid media subscriptions, largely attributed to COVID-19. The influx of new subscribers also brings common audience challenges that every media company should understand and solve. In fact, Piano benchmark data has found that more than half of subscribers will churn from a monthly subscription within their first year. By enabling media companies to apply real-time data to complex life cycles for the “next best action” for all users, Piano is helping publishers of all sizes and structures maintain their subscription success well into 2021.

Recommended AI News: Seed Health Announces Acquisition of Digital Health Company Auggi","['segments', 'subscription', 'realtime', 'elevates', 'data', 'users', 'customer', 'learning', 'piano', 'product', 'powerful', 'journey', 'flagship', 'composer', 'machine', 'design', 'software', 'content']","Piano’s data collection, segment creation and activation capabilities are now fully integrated with Composer to provide industry-first customer journey design powered by machine learning, allowing clients to influence behaviors using zero- and first-party data.
Recommend content based on performance: Fuel sophisticated site personalization with Piano’s machine learning content recommendations, which programmatically collect reader data and pair it with Composer segments to target quality content.
Fuel sophisticated site personalization with Piano’s machine learning content recommendations, which programmatically collect reader data and pair it with Composer segments to target quality content.
Apply real-time propensity models: Utilize propensity-based, machine learning segments for launching or fine-tuning subscription strategies—based on real-time subscription scoring and a view into users in each segment.
Utilize propensity-based, machine learning segments for launching or fine-tuning subscription strategies—based on real-time subscription scoring and a view into users in each segment."
61,https://screenrant.com/star-wars-episode-10-artificial-intellegence-parody-video/,,,,,
62,https://analyticsindiamag.com/how-genpact-uses-ai-to-automate-vehicle-insurance-claims-process/,How Genpact Uses AI To Automate Vehicle Insurance Claims Process,2021-02-12 05:30:00+00:00,"Damage assessment is a critical component in the automobile insurance claim process. Currently, it’s manually executed by third parties and entails considerable cost and resources. The inherent difficulty in capturing images in an uncontrolled environment is the biggest challenge in automating the process,

In his talk at MLDS 2021, Chirag Jain, AVP, Data Science and Insights – Augmented Intelligence Practice at Genpact, elaborated on using AI state-of-the-art deep learning architectures and data enhancement approaches to generate millions of images to train the system. He dilated on the lessons learnt and how their solution was able to segment out body parts and damaged areas at the pixel level to generate robust insight for decision-making in multiple scenarios including total loss, repair-replace, cost estimates etc.

Manual Intervention Vs AI

Manual auto claim processes take anywhere between 5 to 10 days to review the claim. The process involves steps such as the customer care agent registering and sending a claim request to the adjuster, assigning the case to the mobility inspection team, analysing images captured through self serve or field representatives, reviewing the estimate with loss details, and more. AI can streamline the entire process as the computer vision classifies total loss, detects damage automatically and provides inputs for line-item estimates such as part prices, vehicle history and overall estimate cost — all of which could be done in 24 hours.

Genpact’s Claims AI gets First Notice of Loss (FNOL), followed by images of damaged vehicles being analysed by computer vision to identify attributes such as part analysis, part damage likelihood, the severity of the damage, total loss and cost estimates for the vehicle’s repair.

Genpact’s Claims AI is trained on millions of images supported by auto-labelling and synthetic data generation and can detect six damage types such as scratch, dent, crush, missing parts, misalignment and tear. It can further handle the detection of 38 parts in SUVs, Sedans, trucks, hatchbacks, minivans and coups.

Pixel-level Detection

While the techniques mentioned above cover the surface-level detection, Jain said they are exploring pixel-level part detection to extract detailed information on the damage. “Pixel level detection is critical for finer part boundaries,” said Jain.

Some of the techniques Genpact uses to understand the pixels include class activation, training data augmentation such as synthetic data generation, transfer learning, etc.

Challenges

While the state of the art technology has been able to help detect damage type, damaged parts, and make a decision on repair and replace, it is not without challenges. Jain laid out the obstacles they face while using AI in claims settlement including:

Zoomed in images which makes it difficult to distinguish between parts

High reflection of surroundings making it difficult for damage detection

Random angles of capture which might lead to ambiguity for part detection

Exposed engine parts or interiors coming in the way of precise damage detection

Low-quality images

Uncontrolled count of captured images

Jain said the decisions are not solely based on AI. There is a fair amount of human-intervention involved. “AI-powered line estimation is definitely our focus, but if in certain instances AI is found to be making incorrect decisions, humans take over,” he concluded.

Subscribe to our Newsletter

You can write for us and be one of the 500+ experts who have contributed stories at AIM. Share your nominations here.

Get the latest updates and relevant offers by sharing your email.","['genpact', 'insurance', 'cost', 'data', 'total', 'claims', 'vehicle', 'process', 'automate', 'images', 'ai', 'uses', 'loss', 'claim', 'detection', 'damage', 'parts']","Damage assessment is a critical component in the automobile insurance claim process.
Manual Intervention Vs AIManual auto claim processes take anywhere between 5 to 10 days to review the claim.
AI can streamline the entire process as the computer vision classifies total loss, detects damage automatically and provides inputs for line-item estimates such as part prices, vehicle history and overall estimate cost — all of which could be done in 24 hours.
It can further handle the detection of 38 parts in SUVs, Sedans, trucks, hatchbacks, minivans and coups.
Some of the techniques Genpact uses to understand the pixels include class activation, training data augmentation such as synthetic data generation, transfer learning, etc."
63,https://venturebeat.com/2021/02/11/researchers-propose-platform-for-evaluating-ai-disease-forecasting-methods/,Researchers propose platform for evaluating disease-forecasting AI methods,2021-02-11 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Since the start of the pandemic, there’s been an influx of papers on epidemic forecasting. Indeed, as of February, a search for “COVID forecasting” on Google Scholar yields over 14,000 results. But while many researchers compare their approaches against traditional modeling strategies, forecasts are highly sensitive to the implementation, which requires a well-defined benchmark.

Researchers at the University of Southern California propose a benchmark in EpiBench, which focuses on retrospective forecasting — i.e., forecasting when ground truth is already available. While the platform is in the preliminary stages, the researchers believe that it could benefit real-time forecasting efforts by the U.S. Centers for Disease Control and other agencies around the world that drive governments’ responses.

Modeling is only a part of the forecasting process and can’t be considered a method to compare against. It describes only the epidemic portion rather than data preprocessing and model architecture development. Therefore, the researchers say, without a platform like EpiBench, claiming a new AI forecasting approach performs better than another isn’t possible.

As a proof of concept, the researchers developed a prototype of EpiBench called “COVID-19 forecast-bench,” which keeps a daily record of COVID-19 cases and deaths as reported by Johns Hopkins University. Researchers are required to provide details of their methodologies regarding data preprocessing, modeling techniques, and learning strategy, and these methodologies are evaluated for 1-, 2-, 3-, and 4-week-ahead forecasts.

In an experiment, the researchers compared 3 AI and machine learning forecasting methods and 30 methodologies pulled from published research using EpiBench. They found that while many of the forecasts reportedly used the same model (SEIR), they predicted “drastically” different outcomes. Moreover, two methodologies identical except that one smoothed data over 14 days versus the other’s 7 days varied “significantly” in their performance, suggesting that data preprocessing played a nontrivial role.

The researchers believe that EpiBench will help the research community in making decisions by analyzing methodologies based on how they arrive at their forecasts. Expanding from the prototype, the researchers hope to launch a website, a GitHub repository, and a Slack channel for discussion. If all goes according to plan, developers will be able to view evaluations for submitted models and upload sets of forecasts for benchmarking, as well as upload code to reproduce results and run code on more datasets.

“EpiBench can help us identify which decisions (data preprocessing, modeling choice, learning strategy, hyper-parameter tuning, etc.) in the forecasting approach are critical in epidemic forecasting and help direct the research accordingly,” the researchers wrote in a paper describing EpiBench. “In the future, we wish to expand EpiBench to various epidemic forecasting tasks. The platform will be available to the AI and machine learning researchers and epidemiologists.”","['preprocessing', 'research', 'epibench', 'evaluating', 'data', 'researchers', 'learning', 'forecasts', 'ai', 'platform', 'methods', 'methodologies', 'epidemic', 'diseaseforecasting', 'propose', 'forecasting']","Since the start of the pandemic, there’s been an influx of papers on epidemic forecasting.
Therefore, the researchers say, without a platform like EpiBench, claiming a new AI forecasting approach performs better than another isn’t possible.
In an experiment, the researchers compared 3 AI and machine learning forecasting methods and 30 methodologies pulled from published research using EpiBench.
in the forecasting approach are critical in epidemic forecasting and help direct the research accordingly,” the researchers wrote in a paper describing EpiBench.
The platform will be available to the AI and machine learning researchers and epidemiologists.”"
64,https://siliconangle.com/2021/02/11/labelbox-raises-40m-automate-data-labeling-ai-model-development/,,,,,
65,https://www.techrepublic.com/article/aws-ibm-google-and-microsoft-are-taking-ai-from-1-0-to-2-0-according-to-forrester/,"AWS, IBM, Google, and Microsoft are taking AI from 1.0 to 2.0, according to Forrester",,"A new report says that the hyperscalers are using reinforcement learning and transformer networks to make AI smarter and more mobile.

Image: Forrester

While many companies are still in the early stages of implementing artificial intelligence platforms, the early adopters are moving on to AI 2.0. A new report from Forrester, ""AI 2.0: Upgrade Your Enterprise With Five Next-Generation AI Advances,"" explains what these changes are and why they are important.

These new capabilities include:

Transformer networks

Synthetic data

Reinforcement learning

Federated learning

Causal inference

Authors Kjell Carlsson, Brandon Purcell, and Mike Gualtieri describe how these advances impact AI in terms of technical feasibility and business applications.

These changes address some of the limitations of AI 1.0, such as data, accuracy, speed, and security limitations that have made it hard for businesses to develop robust use cases. The authors describe AI 1.0 as focused on pattern recognition task-specific models, and centralized training and deployment, while AI 2.0 is characterized by language, vision, and other general data generation models and it is embedded everywhere. This is a discontinuous change for AI, meaning that these new capabilities are a significant break with the history of AI to date, according to the report.

SEE: Natural language processing: A cheat sheet (TechRepublic)

In addition to AI 2.0 being able to automatically generate content and software code and summarize articles and generate questions, these capabilities can be deployed anywhere. The authors state that AI models can be placed and trained at the edge, meaning that new applications need to be cheaper, faster, and more secure.

According to the authors, companies already have access to most of the tools and services needed to start building AI 2.0 solutions from hyperscalers such as Amazon Web Services, Google, IBM, and Microsoft. Here's a look at each of the five technologies.

Transformer networks

These networks can handle tasks with a time or context element, such as natural language processing and generation. This advancement makes it possible to train giant models to conduct multiple tasks at once with higher accuracy and less data than individual models operating separately. According to the report, Microsoft uses these transformer networks in business applications such as natural language search, auto-captioning of images, moderating inappropriate gamer language, and automated customer support. Photon from Salesforce Research uses these networks to turn questions from business users into automatically generated SQL queries.

Synthetic data

AI runs on data and it's not easy or cheap to get the volume of data needed to train models and build enterprise use cases. Synthetic data solves that problem and improves accuracy, robustness, and generalizability of models, according to the report. Companies like MDClone are using synthetic data in healthcare settings to fill data gaps and protect patient privacy. This is one example of the new ecosystem of vendors providing this service to companies that don't want to create synthetic data in-house.

Reinforcement learning

This new functionality makes it easier for companies to react swiftly to changes in data. Reinforcement learning learns from interacting with a real or simulated environment through trial and error instead of relying on historical data. An oil and gas exploration company is using Microsoft's Project Bonsai to find the most promising paths for horizontal drilling underground, the report authors said.

Federated learning

One barrier to wider distribution of learnings from AI is the need to transfer data from multiple sources. Transferring this data can be ""costly, difficult, and often risky from a security, privacy, or competitiveness perspective."" Federated learning allows separate AI models to share models instead of the underlying data. This means intelligence can be shared ""quickly, cheaply, and more securely"" within a single organization and across several organizations. The report authors state that Google's Android 11 uses federated learning to generate smart replies and suggest emojis.

Causal inference

This technique can identify cause-and-effect relationships between variables which can suggest relationships supported by data. This can't prove causality but it can make it easier to avoid faulty business decisions based on poorly performing models. This capability is in an early stage of development compared to the other four factors.

Forrester recommends that companies take these steps to incorporate these new capabilities into existing AI effort:

Continue the AI 1.0 journey while laying the groundwork for 2.0 functionality. Invest in training existing staff because people with AI 2.0 expertise don't exist yet. Look for use cases that score highly on both business value and technical feasibility. Look for AI 2.0 offerings from Amazon Web Services, Google, IBM, and Microsoft. Watch for a killer use case or technological breakthrough.

Data, Analytics and AI Newsletter Learn the latest news and best practices about data science, big data analytics, and artificial intelligence. Delivered Mondays Sign up today

Also see","['according', 'models', 'authors', 'data', 'ibm', 'taking', 'ai', 'companies', 'language', 'business', 'networks', 'google', 'report', 'microsoft', 'forrester', 'aws', '20']","Image: ForresterWhile many companies are still in the early stages of implementing artificial intelligence platforms, the early adopters are moving on to AI 2.0.
A new report from Forrester, ""AI 2.0: Upgrade Your Enterprise With Five Next-Generation AI Advances,"" explains what these changes are and why they are important.
According to the authors, companies already have access to most of the tools and services needed to start building AI 2.0 solutions from hyperscalers such as Amazon Web Services, Google, IBM, and Microsoft.
Invest in training existing staff because people with AI 2.0 expertise don't exist yet.
Look for AI 2.0 offerings from Amazon Web Services, Google, IBM, and Microsoft."
66,https://www.aitrends.com/ai-trends-insider-on-executive-leadership/ai-holistic-adoption-for-manufacturing-and-operations-ethics/,AI Holistic Adoption for Manufacturing and Operations: Ethics,2021-02-11 20:54:21+00:00,"By Dawn Fitzgerald, the AI Executive Leadership Insider

Part Four of a Four Part Series: “AI Holistic Adoption for Manufacturing and Operations” is a four-part series which focuses on the executive leadership perspective including key execution topics required for the enterprise digital transformation journey and AI Holistic Adoption. Planned topics include: Value, Program, Data and Ethics. Here we address Ethics.

The Executive Leadership Perspective

Executive leaders have the responsibility to guide their organization’s AI Holistic Adoption journey. In previous articles of this series, we began with the foundation of Value, moved to the framework of the Program, and then addressed some specific AI Holistic Adoption aspects of Data. We now discuss the most complex topic of all, the executive leader’s response to the Ethics of AI.

AI Trust Triad: Ethics, Security and Privacy

AI Holistic Adoption means that we are taking a holistic view of the multi-faceted aspects involved in bringing our organizations through Digital Transformation and subsequent AI solution execution. On our AI Holistic Adoption journey thus far, we looked not only at creating high value AI Solutions (Value Analytics) by defining and measuring their value, but also at life cycle maintenance. We looked at the needs and contributions of all stakeholders plus the visibility and access from both corporate and business points of view. In addition, we addressed multi-sourcing of Value Analytics and security from a system and design component perspective. As we embark on the topic of Ethics our view must broaden again to include the intertwined topics of Security and Privacy.

Ethics, Security, and Privacy are indeed the AI Trust Triad. It is imperative to understand that these topics are inseparable, and ALL must be addressed as we architect our AI Solutions. In an AI system, when we address one, we must touch all three. To ignore any of the Triad will, at best, lead to no adoption of our AI solutions, and at worst, lead to catastrophic unintended consequences (the kind of adoption you do NOT want). Although the AI Trust Triad is everyone’s responsibility, the executive leader is in the unique position to lead this awareness and provide the framework, governance, and guidance to enable implementation.

An IDC 2020 survey conducted with Microsoft found, “Trustworthy AI is fast becoming a business imperative. Fairness, explainability, robustness, data lineage, and transparency, including disclosures, are critical requirements that need to be addressed now.” Also, IDC forecasted that “Lack of Trust in AI/ML will inhibit Adoption”. Trust affects stakeholders at every level, “most sophisticated users trust AI-based recommendations the least” and “trust is paramount in expanding adoption of AI/ML-infused technologies”.

The common questions of Ethics, such as gender bias, ethnic bias, and human condition impact, also have Privacy components. Individuals may not want to be associated with a category or have their human characteristics shared. Privacy regulations like the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) of 2018 are helping to improve personal data protection. We must, however, go beyond basic legal compliance by considering the ethical implications and fundamental challenges to the human condition when algorithmically driven behavioral tracking or persuasive computing are used. Finally, we must ensure the security of our data and AI Systems from unauthorized access, use, and cyber-crimes. All three components of the AI Trust Triad are required to achieve Trustworthy AI.

The Challenge of Digitizing Ethics

As executive leaders driving digital transformation and AI Holistic Adoption into our organizations, we find ourselves with the challenge of digitizing our corporation’s Code of Ethics. In the digital era, executive leaders have the responsibility to drive both corporate ethics policies and privacy policies into all digital aspects of our corporations. The power of AI solutions makes this obligation all the more imperative. Unlike Security, the third component of the AI Trust Triad, incorporating both Privacy and Ethics into our corporation’s digital aspects (both operations and offer related) is a new concept for most organizations. To complicate things further, Ethics especially can be susceptible to subjective interpretation.

As society evolves, ethics evolve, varying by time, geography, culture, political influence, and age demographics. Our definition of what is ethical evolves, as humans evolve. Just look at the recent rise in diversity and inclusion movements across employers. These changes must be reflected in our corporate code of ethics and must translate to a digital interpretation.

As leaders, it is our business and moral obligation to exercise the process of digitizing the AI Trust Triad. We must build the ability for our organizations to frequently define, digitize, and mature our ethical and privacy stance. We must synchronize the technical implementation of our ethics with our policy evolution. Once digitized and coded into our technology, we must judge its quality, protect it, and mature it. To do this we must introduce design for controlled and ethical AI practices into our organizations.

Design for Controlled and Ethical AI (DCE_AI)

Just as we have had “Design for Test”, “Design for Manufacturing” and “Design for Security”, AI Holistic Adoption requires that teams engage in “Design for Controlled and Ethical AI” (DCE_AI). Since Security is part of the AI Trust Triad and the most aligned, we can leverage experiences from Design for Security.

Like security initiatives, DCE_AI requires a focused effort and will impact development timelines. And just like the Design for Security discipline, adopting DCE_AI will become an offer acceptance criteria with a specified level of mandatory adoption.

Since DCE_AI is just as mandatory and as extensive as Design for Security we have the benefit of leveraging the systems of governance used for our security execution.

The basic components of DCE_AI initiatives include:

Early inclusion of e thics and p rivacy criteria as requirements in the d esign c ycle.

Transparency for stakeholders with r ole -b ased v isibility (see the intent of the AI solution) and c ontrol p oints (stop it/ redirect it) built into solutions.

Metrics and management of the Analytics Design Package associated with each Value Analytic (AI Solution) in our Value Analytics Library. (as defined in the column on Program in AI Tr ends in August 2020)

Certification governance with evidence-based test criteria.

Periodic Ethics and Privacy Policy Alignment Reviews to synchronize evolution.

Role-Based Visibility and Control Points

Trustworthy AI requires transparency, accountability and explainability. In AI Holistic Adoption, this is achieved with roles-based visibility and control points. The concept of Visibility and Control Points, introduced in the Program column, aligns with the concept of the Human-in-the-Loop. For Ethics and Privacy, Human-in-the-Loop is fundamental.

The importance of the Human-in-the-Loop was also emphasized in the AI Trends interview with John Havens, Executive Director of the IEEE, on Ethics by Design in May 2019. As stated by Havens, “…technology needs to be human-centric. That often means a Human-in -the-Loop (HITL) mentality is used in the technology, which means there can always be some form of intervention in a system where humans maintain control”.

The Visibility and Control Points in AI Holistic Adoption give humans an intervention mechanism based on role defined access and control privileges. The corporation’s Analytics Library consists of Value Analytics (VA) which must be designed with stakeholder Role-Based Visibility and Control Points. These are required to ensure that AI evolution stays on track with business objectives including those requirements derived from Ethics and Privacy Policies.

Management of Visibility and Control Points will be key and must be part of the AI System Architecture and design from the beginning. The definition of the roles, and their corresponding Visibility and Control Points, are highly sensitive and key as they influence the direction of the AI evolution.

Design teams must determine where Role-Based Visibility and Control Points will reside and the associated governance mechanism in the AI System. A straightforward solution is to maintain Role-Based Visibility and Control Point data in the Analytics Design Package of each Value Analytic and have a platform-based delivery mechanism via an API. The API provides the roles-based access for visibility and for the Analytics Design Package changes.

The platform API must have access to historical trend data associated with the AI solution. Standard Identity Access Management (IAM) is engaged for visibility and control access to both the Analytics Design Package elements and historic data trends database. Control Point adjustments will alter the Analytics Design Package components, most likely the algorithm code, training model, baseline dataset and user value configuration.

Visibility and control over Ethics and Privacy concerns can be enabled for stakeholders both in the corporation and at the consumer level. Some examples of role-based visibility and control point are below:

Role Scenario Example Visibility Point Control Point Technician on manufacturing floor AI predicts and adjusts equipment air flow based on seasonal temperature fluctuations and current manufacturing floor layout. The technician knows of new neighboring equipment arriving which will change the temperature and air flow. The technician wants to use visibility and control points to adjust and aid new predictions of the equipment performance. See machinery status and predictive trends. Adjust air cooling parameters based on site changes. Manufacturing Site Manager A manager has unexpected expenses which have hit the site budget. AI predicts and schedules maintenance of the equipment on the floor based on equipment parameters and manufacturer’s recommended time window. The manager wants to use visibility and control points to avoid the calendar Quarter boundary while keeping in the recommended window to optimize budget management. See the predicted equipment maintenance window schedule for optimal equipment lifetime. Adjust the expenditure of the equipment maintenance within the window. HR An HR manager is rolling out new diversity hiring guidelines. Algorithms have learned the characteristics of historic hiring successes based on past demographics. The new corporate diversity hiring program aims to increase diversity from past trends thus the learned ethnic parameters must be adjusted to account for new policies. See the decision criteria, resumes & resulting hiring trends based on the existing AI algorithms. Adjust to incorporate new diversity hiring program guidelines. Consumer The consumer is going out for the evening to an establishment that requires age ID to enter. They wish to only provide the relevant age data but not their address. See personal data provided. Select that age data alone is shared and it is only used for the purpose of entry. ( ie . Personal Data and Individual Agency).

With AI Holistic Adoption’s implementation of DCE_AI with role-based visibility and control points, humans maintain authority in how data is used, how models are evolving, and how influence is being applied, ensuring that all 3 aspects of the AI Trust Triad are incorporated.

Stakeholder roles and their corresponding visibility and control points may be managed by Identity Access Management (IAM) techniques for less complex systems or more sophisticated techniques might be necessary such as blockchain for highly expansive systems.

Certification

For decades, we have seen the Design for Security movement with corporate security initiatives include programs, certifications, penetration (pen) testing, audits with mitigation and remediation plans etc.

Just as with security criteria, the execution of ethical and privacy designs will be governed through certification. The goal is to evolve standard corporate security certification to a Trust Triad Certification scope. The certification criteria must be defined by the corporation’s Security Policies, Ethics Policies and Privacy Policies. When designing solutions, the checklist must include mandatory compliance with evidence, addressing all requirements in each category.

AI Trust Triad Certification Policy Example Policy Question Evidence Security Firewall implementation? Pen Test Results Privacy Data Agency rules implemented? Data Privacy Test Results Ethics Complies with gender inclusion guidelines? Gender Inclusion Test Results

Just as with security policies, standards for what are to be included in privacy and ethical policies must be defined by the corporation: independently or by leveraging international body works such as IEEE or ISO.

The Un-Asked Questions and Deeper Implications

Just taking an organization’s high level Ethics policies, unpacking and digitizing them is not enough. The executive leader driving AI Holistic Adoption needs to look deeper and dig out the not-so-obvious and unknown questions using the AI Trust Triad as the guide.

For example, in the AI Holistic Adoption: Data column in AI Trends, article, we explored the Baseline Data Set of a Value Analytic and the use of smaller sized Data Sets to achieve the first mover advantage of our AI solutions. This is not only good for business, but recently a potentially deeper implication of Data Set size was brought forward by AI Ethicist Timnit Gebru, formerly of Google, (See AI Trends, Dec. 10, 2020) speculating that avoiding Large Language Models is more ethical due to environmental impact and inequitable access to resources.

An additional example is seen in the question regarding the common concern, ‘Will AI replace the workforce?’ If the answer unfolds, ‘No, but those who know how to work with AI will replace those that do not,’ the deeper implication is that the corporation bears the responsibility in part, to teach the existing workforce how to work with AI prior to the AI solution launch.

To unearth these questions and implications, the organization must bring diversity of thinking to their teams, master the AI Trust Triad and execute Design for Controlled and Ethical AI. In driving these, executive leaders will ensure the success of their organization’s Digital Transformation and Holistic AI Adoption journey.

Dawn Fitzgerald is VP of Engineering and Technical Operations at Homesite, an American Family Insurance company, where she is focused on Digital Transformation. Prior to this role, Dawn was a Digital Transformation & Analytics executive at Schneider Electric for 11 years. She is also currently the Chair of the Advisory Board for MIT’s Machine Intelligence for Manufacturing and Operations program. All opinions in this article are solely her own and are not reflective of any organization.","['data', 'holistic', 'trust', 'privacy', 'ai', 'control', 'security', 'adoption', 'visibility', 'manufacturing', 'design', 'operations', 'ethics']","By Dawn Fitzgerald, the AI Executive Leadership InsiderPart Four of a Four Part Series: “AI Holistic Adoption for Manufacturing and Operations” is a four-part series which focuses on the executive leadership perspective including key execution topics required for the enterprise digital transformation journey and AI Holistic Adoption.
AI Trust Triad: Ethics, Security and PrivacyAI Holistic Adoption means that we are taking a holistic view of the multi-faceted aspects involved in bringing our organizations through Digital Transformation and subsequent AI solution execution.
Design for Controlled and Ethical AI (DCE_AI)Just as we have had “Design for Test”, “Design for Manufacturing” and “Design for Security”, AI Holistic Adoption requires that teams engage in “Design for Controlled and Ethical AI” (DCE_AI).
The executive leader driving AI Holistic Adoption needs to look deeper and dig out the not-so-obvious and unknown questions using the AI Trust Triad as the guide.
In driving these, executive leaders will ensure the success of their organization’s Digital Transformation and Holistic AI Adoption journey."
67,https://www.sciencedaily.com/releases/2021/02/210210133407.htm,A language learning system that pays attention -- more efficiently than ever before: Researchers' new hardware and software system streamlines state-of-the-art sentence analysis,2021-02-21 00:00:00,"Human language can be inefficient. Some words are vital. Others, expendable.

Reread the first sentence of this story. Just two words, ""language"" and ""inefficient,"" convey almost the entire meaning of the sentence. The importance of key words underlies a popular new tool for natural language processing (NLP) by computers: the attention mechanism. When coded into a broader NLP algorithm, the attention mechanism homes in on key words rather than treating every word with equal importance. That yields better results in NLP tasks like detecting positive or negative sentiment or predicting which words should come next in a sentence.

The attention mechanism's accuracy often comes at the expense of speed and computing power, however. It runs slowly on general-purpose processors like you might find in consumer-grade computers. So, MIT researchers have designed a combined software-hardware system, dubbed SpAtten, specialized to run the attention mechanism. SpAtten enables more streamlined NLP with less computing power.

""Our system is similar to how the human brain processes language,"" says Hanrui Wang. ""We read very fast and just focus on key words. That's the idea with SpAtten.""

The research will be presented this month at the IEEE International Symposium on High-Performance Computer Architecture. Wang is the paper's lead author and a PhD student in the Department of Electrical Engineering and Computer Science. Co-authors include Zhekai Zhang and their advisor, Assistant Professor Song Han.

Since its introduction in 2015, the attention mechanism has been a boon for NLP. It's built into state-of-the-art NLP models like Google's BERT and OpenAI's GPT-3. The attention mechanism's key innovation is selectivity -- it can infer which words or phrases in a sentence are most important, based on comparisons with word patterns the algorithm has previously encountered in a training phase. Despite the attention mechanism's rapid adoption into NLP models, it's not without cost.

advertisement

NLP models require a hefty load of computer power, thanks in part to the high memory demands of the attention mechanism. ""This part is actually the bottleneck for NLP models,"" says Wang. One challenge he points to is the lack of specialized hardware to run NLP models with the attention mechanism. General-purpose processors, like CPUs and GPUs, have trouble with the attention mechanism's complicated sequence of data movement and arithmetic. And the problem will get worse as NLP models grow more complex, especially for long sentences. ""We need algorithmic optimizations and dedicated hardware to process the ever-increasing computational demand,"" says Wang.

The researchers developed a system called SpAtten to run the attention mechanism more efficiently. Their design encompasses both specialized software and hardware. One key software advance is SpAtten's use of ""cascade pruning,"" or eliminating unnecessary data from the calculations. Once the attention mechanism helps pick a sentence's key words (called tokens), SpAtten prunes away unimportant tokens and eliminates the corresponding computations and data movements. The attention mechanism also includes multiple computation branches (called heads). Similar to tokens, the unimportant heads are identified and pruned away. Once dispatched, the extraneous tokens and heads don't factor into the algorithm's downstream calculations, reducing both computational load and memory access.

To further trim memory use, the researchers also developed a technique called ""progressive quantization."" The method allows the algorithm to wield data in smaller bitwidth chunks and fetch as few as possible from memory. Lower data precision, corresponding to smaller bitwidth, is used for simple sentences, and higher precision is used for complicated ones. Intuitively it's like fetching the phrase ""cmptr progm"" as the low-precision version of ""computer program.""

Alongside these software advances, the researchers also developed a hardware architecture specialized to run SpAtten and the attention mechanism while minimizing memory access. Their architecture design employs a high degree of ""parallelism,"" meaning multiple operations are processed simultaneously on multiple processing elements, which is useful because the attention mechanism analyzes every word of a sentence at once. The design enables SpAtten to rank the importance of tokens and heads (for potential pruning) in a small number of computer clock cycles. Overall, the software and hardware components of SpAtten combine to eliminate unnecessary or inefficient data manipulation, focusing only on the tasks needed to complete the user's goal.

The philosophy behind the system is captured in its name. SpAtten is a portmanteau of ""sparse attention,"" and the researchers note in the paper that SpAtten is ""homophonic with 'spartan,' meaning simple and frugal."" Wang says, ""that's just like our technique here: making the sentence more concise."" That concision was borne out in testing.

advertisement

The researchers coded a simulation of SpAtten's hardware design -- they haven't fabricated a physical chip yet -- and tested it against competing general-purposes processors. SpAtten ran more than 100 times faster than the next best competitor (a TITAN Xp GPU). Further, SpAtten was more than 1,000 times more energy efficient than competitors, indicating that SpAtten could help trim NLP's substantial electricity demands.

The researchers also integrated SpAtten into their previous work, to help validate their philosophy that hardware and software are best designed in tandem. They built a specialized NLP model architecture for SpAtten, using their Hardware-Aware Transformer (HAT) framework, and achieved a roughly two times speedup over a more general model.

The researchers think SpAtten could be useful to companies that employ NLP models for the majority of their artificial intelligence workloads. ""Our vision for the future is that new algorithms and hardware that remove the redundancy in languages will reduce cost and save on the power budget for data center NLP workloads"" says Wang.

On the opposite end of the spectrum, SpAtten could bring NLP to smaller, personal devices. ""We can improve the battery life for mobile phone or IoT devices,"" says Wang, referring to internet-connected ""things"" -- televisions, smart speakers, and the like. ""That's especially important because in the future, numerous IoT devices will interact with humans by voice and natural language, so NLP will be the first application we want to employ.""

Han says SpAtten's focus on efficiency and redundancy removal is the way forward in NLP research. ""Human brains are sparsely activated [by key words]. NLP models that are sparsely activated will be promising in the future,"" he says. ""Not all words are equal -- pay attention only to the important ones.""","['models', 'hardware', 'data', 'spatten', 'researchers', 'learning', 'sentence', 'attention', 'key', 'system', 'language', 'nlp', 'software', 'streamlines', 'stateoftheart', 'pays', 'words', 'mechanism']","The importance of key words underlies a popular new tool for natural language processing (NLP) by computers: the attention mechanism.
When coded into a broader NLP algorithm, the attention mechanism homes in on key words rather than treating every word with equal importance.
So, MIT researchers have designed a combined software-hardware system, dubbed SpAtten, specialized to run the attention mechanism.
One challenge he points to is the lack of specialized hardware to run NLP models with the attention mechanism.
Alongside these software advances, the researchers also developed a hardware architecture specialized to run SpAtten and the attention mechanism while minimizing memory access."
68,https://www.aitrends.com/ai-and-space-exploration/scientists-pursue-a-range-of-projects-employing-ai-for-space-exploration/,Scientists Pursue a Range of Projects Employing AI for Space Exploration,2021-02-11 19:31:37+00:00,"By AI Trends Staff

Scientists are engaged in a range of efforts to study how AI can help in space exploration.

Space Debris: The European Space Agency reports nearly 34,000 objects bigger than 10 cm (4 inches) which pose serious threats to existing space infrastructure, according to an account in The Conversation. Innovative approaches to addressing the issue include designing satellites to reenter Earth’s atmosphere if they are deployed in low Earth orbit, making them disintegrate in a controlled way.

Another approach is to avoid collisions in space, thus preventing the creation of more debris. Scientists at Aerospace Research Central recently issued a paper on an analytical framework for a collision avoidance maneuver (CAM) technique design, using machine learning techniques.

“The need for and complexity of collision avoidance activities between active spacecraft and debris (or other spacecraft) has experienced a notable increase in the last couple of decades, due to the growing number of satellites in orbit and significant fragmentation events,” stated the authors, led by Juan Luis Gonzalo of the Department of Aerospace Engineering, Polytechnic University of Madrid.

“The proliferation of objects in Earth orbit already poses a critical threat to the safe and sustainable use of space,” he noted. The threat is expected to keep increasing as a result of new launch companies driving down the cost of access to space, the popularization of small cube satellites as affordable and flexible platforms, and large constellations of satellites being proposed by incumbent companies and startups. International efforts to tackle the issue include the Inter-Agency Space Debris Coordination Committee, which has issued space debris mitigation guidelines.

Mission Design and Planning: Planning a mission to Mars is not easy, and AI can help. Researchers are studying the idea of a design engineering assistant that would reduce the time required for initial mission design. A report entitled, “Artificial Intelligence for the Early Design Phases of Space Missions,” was recently published by IEEE Xplore.

The paper explores how AI and in particular, Knowledge Representation and Reasoning (KRR), can be employed at the start of the space mission life cycle via an Expert System (ES) used as a Design Engineering Assistant (DEA).

“Applied to space mission design, and in particular, in the context of concurrent engineering sessions, an ES could serve as a knowledge engine and support the generation of the initial design inputs, provide easy and quick access to previous design decisions or push to explore new design options,” state the paper’s authors, led by Audrey Berquand, a PhD student at the University of Strathclyde in Glasgow, Scotland.

Today space mission design experts apply methods of concurrent engineering and Model-Based System Engineering, relying on their implicit knowledge and available knowledge in past reports, publications and data sheets. “Searching for information through this data is highly time-consuming,” the authors state, citing the opportunity for a more digitized solution to enhance the productivity of the experts.

Astronaut Assistants: Researchers are working to develop intelligent assistants to help astronauts. NASA’s Johnson Space Center, in collaboration with General Motors and Oceaneering, are working on Robonaut. Also called R2, it is a highly dexterous, humanoid robot made up of multiple component technologies and systems.

First released in 2000, Robonaut has gone through a series of design developments, including legs, added in 2014. Today R2 is made up of multiple component technologies and systems including vision, image recognition, sensor integrations, tendon hands and control algorithms. It has 50 patented and patent-pending technologies that NASA makes available for licensing.

The technologies “have the potential to be game-changers in multiple industries, including logistics and distribution, medical and industrial robotics,” states the R2 website

Using its Active Reduced Gravity Offload System (ARGOS), the Robonaut team on the ground continues to develop technologies in a testbed using similar robots located at the Johnson Space Center. Once vetted, the software can be implemented and tested on the R2 unit aboard the International Space Station. “The goal of this work is to create a fully-featured robotics research platform on board the ISS… to aid in future exploration missions,” the researchers state.

Astrobee is a newer robotic assistant system for astronauts, intended to focus on routine work to free up time. The Astrobee is rectangular, 12.5in x 12.5in, designed to complete tasks such as taking inventory, documenting experiments conducted by astronauts with their built-in cameras, or working together to move cargo throughout the station. In addition, the Astrobee can serve as a research platform to be outfitted and programmed to carry out experiments in microgravity, which can benefit astronauts in space.

The robots use electric fans as a propulsion system that allows them to fly freely through the microgravity environment of the station. Cameras and sensors help them to “see” and navigate their surroundings. The robots also carry a perching arm that allows them to grasp station handrails in order to conserve energy or to grab and hold items.

Once the Astrobee system has been fully commissioned, it will take over for the predecessor SPHERES system, for Synchronized Position Hold, Engage, Reorient, Experimental Satellite, implemented over a decade ago. Astrobee will become the space station’s robotic test platform. The three Astrobee robots are named Honey, Queen, and Bumble.

Read the source articles and information in The Conversation, from Aerospace Research Central, at IEEE Xplore, at the Robonaut site and at Astrobee.","['astrobee', 'mission', 'range', 'employing', 'satellites', 'ai', 'exploration', 'system', 'scientists', 'engineering', 'technologies', 'design', 'debris', 'pursue', 'space', 'projects', 'station']","By AI Trends StaffScientists are engaged in a range of efforts to study how AI can help in space exploration.
Space Debris: The European Space Agency reports nearly 34,000 objects bigger than 10 cm (4 inches) which pose serious threats to existing space infrastructure, according to an account in The Conversation.
International efforts to tackle the issue include the Inter-Agency Space Debris Coordination Committee, which has issued space debris mitigation guidelines.
Mission Design and Planning: Planning a mission to Mars is not easy, and AI can help.
Researchers are studying the idea of a design engineering assistant that would reduce the time required for initial mission design."
69,https://venturebeat.com/2021/02/11/bowery-cto-injong-rhee-on-the-grand-challenge-of-ai-for-indoor-farming/,Bowery CTO Injong Rhee on the grand challenge of AI for indoor farming,2021-02-11 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

In recent years, AI leaders have urged machine learning experts to consider understanding the world’s oceans and tackling climate change grand challenges on par with building autonomous vehicles, beating a computer at a game of chess, or robotic grasping.

Combining computer vision, logistics, robotics, and the science of botany, indoor farming has the potential to change human lives. But innovation in this area requires considering dozens of variants and doing more with less. These challenges are likely among the reasons companies like Intel, Microsoft, and Tencent have participated in experiments to automate greenhouses.

Bowery Farming may be the largest vertical farming company operating in the U.S. today. Founded in 2015, the company has introduced a number of major changes in recent weeks. These are part of its largest expansion since raising more than $170 million from investors including GV (formerly Google Ventures) and individuals like Uber CEO Dara Khosrowshahi.

One of those changes was giving the CTO role to former Google VP Injong Rhee, who had worked on IoT platforms for Google like the edge TPU AI chip and software and services for Samsung. Last month, Bowery also opened what it calls a “center of excellence” in New Jersey. Called Farm X, the center will focus on raising not just leafy greens like the kind Bowery sells today, but also cucumbers, root vegetables, strawberries, and tomatoes. This initiative will focus on research and development, functioning as a sandbox for considering the possible blends of seeds and conditions required to grow produce indoors. A Bowery spokesperson declined to share how much square footage is devoted to the project, but a statement from the company describes Farm X as “one of the largest and most sophisticated vertical farming R&D facilities in the world.”

The first two Bowery farms are located outside Baltimore and New York City. An additional commercial farm is scheduled to open in late 2021 in Bethlehem, Pennsylvania, about 70 miles from Philadelphia. Bowery claims its facilities are currently 100 times more productive than traditional outdoor farming methods. By adding operations in the Pennsylvania area, the company plans to serve a population of nearly 50 million along the Eastern seaboard of the United States. The goal, Bowery said, is to build indoor farms near every major city in the United States and the world.

A number of companies want to crack the code of supplying vegetables to people in urban environments, reducing the need to truck produce into cities. Growing Underground, for example, occupies a World War II-era bunker in London. In Singapore, a company is exploring how to create indoor farming operations that can fit inside a shipping container as the country seeks greater food independence in the face of accelerating climate change and reduced global food supplies.

Scientific progress in indoor plant cultivation could play a key role in addressing food deserts and food security as climate change intensifies. Such knowledge could also advance further exploration of the Moon and Mars. But Bowery chief science officer Henry Sztul says his company is focused on growing produce at scale in monoculture environments in larger facilities.

Several indoor farming startups in the U.S. are currently dedicated to providing premium organic lettuce for customers at Whole Foods and other high-end grocery stores. Bowery, for example, sells to nearly 1,000 stores on the East Coast of the United States, including Amazon Fresh, Walmart, and Whole Foods, as well as ecommerce vendors. Bowery didn’t share specifics when we asked how sales are currently split between Whole Foods and Walmart. But Rhee said the goal is for advances in efficiency to result in more affordable, high-quality produce.

“The AI is still in its infancy, and there’s a lot of human touch that’s still needed to get it mature. But I think with the help of AI, what we get out of this is so amazing that we can actually drive what perceivably in the past was not economically viable,” Rhee said. “Now we’re bringing it to be economically viable, and that’s really the power of the AI and machine learning that makes that happen.”

VentureBeat sat down with Rhee and Sztul to talk about what they consider the holy grail of machine learning challenges for indoor farming, the specific challenges smart indoor farming companies encounter, and the idea of polyculture gardens with multiple types of plants growing in the same place.

This interview has been edited for brevity and clarity.

VentureBeat: In 2019, while speaking to reporters, Amazon VP of devices David Limp called a particular advance in Alexa tech a “holy grail of voice science.” What’s the holy grail of indoor farming, in terms of the machine learning in this space?

Injong Rhee: I think the scale of doing this is one thing. Another thing is this whole process of growing crops from seed all the way to the harvest, packing them, and then delivering them to the store. That entire life cycle of crop and then supply chain presents so many opportunities for optimization.

And really making this indoor farming and vertical farming popular or economically viable is one optimization, but there are so many different dials and levers that we have to optimize, and AI is the best method to do this multi-variable optimization across the space. It’s really emulating what farmers do, and then what the trucking companies do, a combination of all of them, and then making the machine actually do all of this in a much more optimized way to make this really economically viable to provide it to people who need [food] and mass produce it at a low cost. So that’s what’s the holy grail of [indoor farming].

It’s not one thing. I’ve developed voice assistants before. You can actually say language understanding could be a holy grail, but in this case, everything that you know about IoT, cloud AI, machine learning, and robotics all comes into the picture, being orchestrated to find the economical way to produce vegetables on a large scale.

Henry Sztul: We have camera coverage of every crop that grows in a Bowery farm, and we’re constantly taking pictures. We do use computer vision algorithms, deep learning algorithms, to understand things like growth rates over time. To understand, not just when we see something like a stress response, which with something like arugula could be something like purpling, or something like butterhead [lettuce] could be like a yellowing at the edges. And we can observe those things. But what we can also start to do is predict what the conditions are that create that response. And we can be triggered — not just when does it happen, but also [what are] the leading indicators? And so what we’re doing now is we’re starting to look at not just [being] told when we see a stress response, but when we start to see the conditions that we predict to impact that, to create that stress response. And so all of these things are like parts of a puzzle, like the holy grail. But the holy grail is also solving the challenge of how to do this at an immense scale.

VentureBeat: Injong, could you talk a bit about your background and how your past experience informs this work in indoor farming?

Rhee: Yeah, so I did work, especially what I did at Google is building the cloud IoT platform for developers. I developed an end-to-end software and hardware stack of cloud IoT platform, including IoT Core, and then the edge TPU, which is a purpose-built AI chip that you can embed at the edge … so that you can make faster decisions to do control. And so while I’m working on this IoT problem, developing a platform for the IoT developers, I find this smart farming so interesting. It’s a full combination of all the things that I love, like my background in IoT or background in computer networks and background in distributed systems and building software and hardware and sensor networks and all of that and AI coming into good use. And so that’s how a whole thing actually plays. It’s not just one thing that’s going to contribute. It’s many, many things that I have worked on, they’ve become so much in use, and this is amazing. That’s why I was looking for a smart farming opportunity, and Bowery was just presenting itself to me as a perfect opportunity to use what I’ve learned in academia and industry.

VentureBeat: To what degree do people play a role in growing operations today? Is part of the goal for Bowery to create fully autonomous indoor farms?

Sztul: I think the goal of Bowery is to build farms that can deliver more, healthier produce to more people at the right price point. And so one of the ways to do that is with automation in areas. I think there might be people out there that will say “Yeah, our goal is to totally automate everything.” I think we come into the space with an open mind, which I think [is] a little different. And so that’s why I was saying we may get there, but we’re really more focused on how do we put out the best, the highest quality product consistently?

VentureBeat: What are some challenges associated with machine learning systems for indoor farming?

Sztul: There’s a basic machine learning example called the multi-armed bandit problem. And if you think about an octopus in a casino, sitting at a slot machine, the octopus is exploring, pulling different slot machine handles until it finds one that it can start to take advantage of, it can exploit. This is a classic problem of exploration versus exploitation.

A recipe [for growing produce] at Bowery includes things like light intensity, photo period, spectrum, different types of concentrations of nutrients, water temperature, air temperature, and humidity. There’s dozens of components that come into a Bowery recipe. If you were to try and tweak all of those combinations, all of those recipe components to make different combinations, that would take forever, and so we do the same thing with recipes. We have dozens and dozens of recipes in our farm; actually, I believe now we have over 50 recipes currently active across 10 products.

Rhee: Another area to add is the ability to forecast. Obviously, mass will be based on, you take a picture, and millions and millions of pictures, and then throughout the life cycle … until it gets to the harvest. And so, if I take a picture, we can actually figure out … growth rate, and then what the height of the plant is going to be and how much it’s going to produce in a future harvest. And that’s really driven by computer vision, as well as the sensing technology, and then adding all of that into a machine learning model to predict the mass. So that’s an interesting problem that is also fairly challenging because you know, different plants have different patterns, right? And the different colors and density. And so that’s quite a challenging problem.

Sztul: That’s actually a problem similar to some of the problems that self-driving cars have in detecting cars, distinguishing between cars when they’re overlapping. And so, as leaves grow, it’s a problem. It’s called occlusion. As leaves grow, how do you know that you know this is a leaf, and that’s a leaf in a different part of different plants? So it’s an incredibly challenging space. And the better we do there — Injong’s totally right — the better we can understand how much do we have in our farms? And how are we doing today versus yesterday?

Another one — and you would never think about this as a problem, well, simple but challenging — is where do you put things? How do you fill up your farm? If you have thousands of discrete locations to put your crops, how do you decide where things go? And that comes back to science at scale and recipe optimization.

One of the things we’ve done is used machine learning to optimize based on what a basil wants versus a butterhead, as an example — one wanting a cooler, drier climate and one wanting a warmer, more humid environment. We can set preferences and place these crops in different locations in the farm based off of those preferences. And so that’s an area that’s ripe for machine learning because [for] a person to make those decisions, I tell you firsthand, is impossible. And especially as you’re adding more complexity in terms of what the rules are. So that’s an area that I think seems straightforward but is complex and rewarding for us to spend time in.

VentureBeat: I spoke with Ken Goldberg at UC Berkeley last year about a project to create a fully autonomous polyculture garden and computer vision systems to monitor diverse groups of plants growing together. Is Bowery doing any experiments with polyculture growing?

Sztul: We’ve started growing some things that way, where we grow different types of — like our spring blend. So you can go buy our spring blend, and it’s got a whole bunch of different types of lettuces in it, and we used to grow it all together. And actually, we’ve gone away from polyculture over time. We’ve actually moved away from that, and we think that’s right now a better model because we can target what the individual crop needs versus another. But we could go back to that one day, I don’t know.","['think', 'rhee', 'actually', 'things', 'grand', 'bowery', 'different', 'ai', 'learning', 'farming', 'machine', 'cto', 'injong', 'thats', 'challenge', 'indoor']","Combining computer vision, logistics, robotics, and the science of botany, indoor farming has the potential to change human lives.
Bowery Farming may be the largest vertical farming company operating in the U.S. today.
Several indoor farming startups in the U.S. are currently dedicated to providing premium organic lettuce for customers at Whole Foods and other high-end grocery stores.
VentureBeat: Injong, could you talk a bit about your background and how your past experience informs this work in indoor farming?
VentureBeat: What are some challenges associated with machine learning systems for indoor farming?"
70,https://venturebeat.com/2021/02/11/nvidia-researchers-train-ai-to-reward-dogs-for-responding-to-commands/,Nvidia: Researchers train AI to reward dogs for responding to commands,2021-02-11 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Colorado State University researchers Jason Stock and Tom Cavey have published a paper on an AI system that rewards dogs for doing tricks.

The computer science grad students trained image classification networks to determine whether a dog is sitting, standing, or lying down. If a dog responds to a command by adopting the correct posture, the machine dispenses a treat.

The students used an Nvidia Jetson edge AI platform for real-time trick recognition and treats. Stock and Cavey see their prototype system as a dog trainer’s aid — it handles the treats — or a way to school dogs on better behavior at home.

“We’ve demonstrated the potential for a future product to come out of this,” Stock said in a statement.

Fetching dog training data

Image Credit: Nvidia

The researchers needed dog images that exhibited the three specified postures. They found the Stanford Dogs datasets, with more than 20,000 images of various sizes depicting dogs in many positions. The images required preprocessing so they wrote a program to help quickly label them.

In an email to VentureBeat, Nvidia said, “It doesn’t yet work remotely; it’s currently for in-person. But that would be an easy setup to make it a remote system. You might think of it as a system, or IP, to license for devices like the Furbo. The researchers see many possible applications but haven’t committed to anything yet.”

To refine the model, the researchers applied features of dogs from ImageNet to enable transfer learning. Next, they applied post-training and optimization techniques to boost speed and reduce model size.

For optimizations, they tapped into Nvidia’s Jetpack software development kit on Jetson, which is a lightweight AI platform for drones and other systems. It offers an easy way to get things up and running quickly and to access the TensorRT and cuDNN libraries, Stock said. Nvidia TensorRT optimization libraries offered “significant improvements in speed,” he added.

Tapping into the university’s computing system, Stock trained the model overnight on two 24GB Nvidia RTX 6000 graphics processing units (GPUs).

Deployed models on Henry

The researchers tested their models on Henry, Cavey’s Australian Shepherd. The model achieved accuracy of up to 92% in tests and demonstrated an ability to make split-second inference at nearly 40 frames per second.

Using the Jetson Nano, the system makes real-time decisions about dog behaviors and reinforces positive actions with a treat, transmitting a signal to release a reward.

“We looked at Raspberry Pi and Coral, but neither was adequate, and the choice was obvious for us to use Jetson Nano,” Cavey said.

Explainable AI helps provide transparency about the makeup of neural networks. It’s becoming more common in the financial services industry as a way to understand fintech models. Stock and Cavey included model interpretation in their paper to provide explainable AI for the pet industry.

They do this with images of the videos that show the posture analysis. One set of images relies on GradCAM — a common technique for displaying where a convolutional neural network model is focused. Another set of images explains the model by tapping into Integrated Gradients, which helps analyze pixels.

The researchers said it was important to create a trustworthy and ethical component of the AI system for trainers and general users. Otherwise, there’s no way to explain the methodology, should it come into question.

“We can explain what our model is doing, and that might be helpful to certain stakeholders — otherwise how can you back up what your model is really learning?” Cavey said.","['researchers', 'nvidia', 'model', 'commands', 'ai', 'images', 'stock', 'train', 'reward', 'dogs', 'system', 'dog', 'cavey', 'way', 'responding']","Colorado State University researchers Jason Stock and Tom Cavey have published a paper on an AI system that rewards dogs for doing tricks.
Fetching dog training dataImage Credit: NvidiaThe researchers needed dog images that exhibited the three specified postures.
They found the Stanford Dogs datasets, with more than 20,000 images of various sizes depicting dogs in many positions.
Tapping into the university’s computing system, Stock trained the model overnight on two 24GB Nvidia RTX 6000 graphics processing units (GPUs).
The researchers said it was important to create a trustworthy and ethical component of the AI system for trainers and general users."
71,https://petapixel.com/2021/02/11/canons-new-app-culls-photos-with-artificial-intelligence/,Canon’s New App Culls Photos with Artificial Intelligence,2021-02-11 00:00:00,"Canon has announced the Photo Culling app, the company’s new software for iOS that is built on a newly-announced proprietary artificial intelligence. Canon advertises its new app as a “digital photo assistant” to help select your best images based on four key parameters.

Starting with the core of its new app, Canon announced that it has built a Computer Vision Artificial Intelligence Engine named PHIL, which is abbreviated from “Photography Intelligence Learning.” This is the debut of the company’s AI engine which it has chosen to deploy first as the base of its new culling application.

The app, which analyzes images based on sharpness, noise, emotions, and closed eyes using the power of PHIL, was created to address the perceived problem of too many images.

“According to a recent report from Keypoint Intelligence, 1.4 trillion photos were taken worldwide in 2020 and 7.4 trillion images were stored,” Canon writes. “With these staggering numbers, it could be overwhelming for the average consumer to decide what photos are best to keep and store.”

The Photo Culling app has two culling options: Whole Culling and Similar Culling.

Whole Culling determines the best photos based on what Canon defines as the “absolute best scores” that the AI grants based on the four aforementioned models. If the score is over the threshold the user sets, the photo is considered to be the best. The remaining images would be suggested as deletions.

Similar Culling determines the best photos based on a score comparison among a group of images. The highest-scoring photos within that pre-determined group are considered “best” and users can tell the app to select and display the “second best” as well. Mirroring the Whole Culling system, the remaining images would be suggested as deletions. The app is also able to find similar photos and group them together.

“For example,” Canon explains. “If a user selects 10 photos of a dog and 10 photos of a sunset, it will break the photos into two groups and find the best photo of each; one of the dog and one of the sunset.”

The app does more than just cull and features a couple of other notable features.

First, it can show the number of photos a user has as well as the amount of phone storage that is being used and available on the app’s home screen. It can also categorize and place photos in albums, also displayed on the Home Screen, which are dynamically based on events and dates throughout the year. The app will pull events that have a large number of photos and suggest users review and delete less appealing ones to save storage space.

The app will also display the number of photos that it has helped you choose to delete and also give you the ability to set parameters around how it scores sharpness, noise, emotion, and closed yees. You can adjust how much of an impact noise has on a photo versus closed eyes, for example.

The Canon Photo Culling app is available from the iOS app store (no mention of Google Play availability was noted) and is supported by subscription options: $2.99 per month or $14.99 for the year. You can try the app out for free for three days ahead of making any purchasing decisions.

(via Canon Watch)","['artificial', 'user', 'canons', 'number', 'images', 'app', 'culls', 'based', 'canon', 'best', 'photos', 'culling', 'intelligence']","Canon has announced the Photo Culling app, the company’s new software for iOS that is built on a newly-announced proprietary artificial intelligence.
Canon advertises its new app as a “digital photo assistant” to help select your best images based on four key parameters.
Whole Culling determines the best photos based on what Canon defines as the “absolute best scores” that the AI grants based on the four aforementioned models.
Similar Culling determines the best photos based on a score comparison among a group of images.
The Canon Photo Culling app is available from the iOS app store (no mention of Google Play availability was noted) and is supported by subscription options: $2.99 per month or $14.99 for the year."
72,https://www.privateinternetaccess.com/blog/code-is-law-why-software-openness-and-algorithmic-transparency-are-vital-for-privacy/,Code is law: why software openness and algorithmic transparency are vital for privacy,2021-02-10 19:27:24+00:00,"This blog has written a number of times about the growing threat that low-cost, rapid DNA sequencing represents for privacy. The increased use of genetic material by the police to identify suspects poses particular problems. A recent case in the US involving a DNA sample raises a new issue. Because of its importance, both the EFF and ACLU have been actively involved. An EFF blog post explains the background:

The case of New Jersey v. Pickett involves complex DNA analysis using TrueAllele software. The software analyzed a DNA sample obtained by swabbing a weapon, a sample that likely contained the DNA of multiple people. It then asserted that it was likely that the defendant, Corey Pickett, had contributed DNA to that sample, implicating him in the crime.

That might look like a routine application of DNA matching in order to pinpoint an individual allegedly involved in a crime. But in this case, something interesting happened. The legal defense team wanted to analyze how the TrueAllele software had arrived at the conclusion that Pickett’s DNA was present in the sample. The reasoning was that without checking the underlying software code, it was impossible to know whether that implicit accusation was valid. However, both the prosecutors and the software vendor claimed this code was a trade secret. The vendor had a commercial interest in preventing competitors from understanding and copying its approach, and claimed that this outweighed the right of the accused to check the inner logic of the program. Fortunately, an appeals court in New Jersey agreed with the defendant:

As technology proliferates, so does its use in criminal prosecutions. Courts must endeavor to understand new technology – here, probabilistic genotyping – and allow the defense a meaningful opportunity to examine it. Without scrutinizing its software’s source code – a human-made set of instructions that may contain bugs, glitches, and defects – in the context of an adversarial system, no finding that it properly implements the underlying science could realistically be made. Consequently, affording meaningful examination of the source code, which compels the critical independent analysis necessary for a judge to make a threshold determination as to reliability at a Frye hearing, is imperative.

The “Frye hearing” refers to the process of determining whether scientific evidence is admissible. In this case, the issue is whether the TrueAllele software provides valid evidence. The judge ruled that defense expert witnesses need access to the software code in order to offer their opinions on the matter.

For readers of this blog, this will be an obvious conclusion. After all, it was over 20 years ago that Lawrence Lessig pointed out in his seminal book, Code and Other Law of Cyberspace, that “code is law”. That is, the details of how software is coded effectively create laws of their own, separate from traditional ones. Without access to the underlying code of the TrueAllele program, it is hard, or even impossible, to establish the assumptions – and errors – that have been written into the code. And yet those assumptions and errors may play a crucial role in the case of programs that are used to identify suspects, falsely accusing the innocent, and absolving the guilty.

In the current case, the code will not be publicly disclosed, but made available to the defense team only. Arguably any code that has such important consequences for people’s lives should be publicly accessible to allow the fullest possible scrutiny. That’s true not just for specialised programs analyzing genetic material, but also for the important new class of systems that involve automated decision making (ADM). These typically use some form of artificial intelligence, for example machine learning. As a major new report on the area from Algorithm Watch underlines, the biggest problem with such systems is the fact that it is extremely hard to scrutinize their inner workings:

The message for policy-makers couldn’t be clearer. If we truly want to make the most of their potential, while at the same time respecting human rights and democracy, the time to step up, make those systems transparent, and put ADM wrongs right, is now.

The report makes a number of policy recommendations designed to increase the transparency of ADM systems. Although they apply specifically to the EU legal context, there are general points that are relevant globally. For example, Algorithm Watch calls for a public register of ADM systems used by the public sector. It wants a legal obligation to explain the purpose of the system, its underlying logic, and information about who developed it. Since many of these ADM systems rely on training data, Algorithm Watch suggests that this should be made available to researchers, journalists and civil society organizations so that they can check that there are no hidden biases there that might skew the results. It would also like to see a more thoroughgoing approach to auditing ADM systems, but admits that is doesn’t have any easy answers to the question of how that should be done.

As ADM systems are increasingly applied to personal data, with inevitable implications for privacy, the importance of transparency will increase. Opening up the black boxes of ADM is not going to be easy, but the case of the TrueAllele software shows why it is something that must be done sooner or later. Thinking about the issues now will make it easier to come up with solutions as the need grows more urgent.

Featured image by JJ Jones.","['vital', 'defense', 'case', 'privacy', 'openness', 'algorithmic', 'adm', 'sample', 'dna', 'code', 'transparency', 'law', 'trueallele', 'software', 'systems', 'underlying']","An EFF blog post explains the background:The case of New Jersey v. Pickett involves complex DNA analysis using TrueAllele software.
The software analyzed a DNA sample obtained by swabbing a weapon, a sample that likely contained the DNA of multiple people.
The reasoning was that without checking the underlying software code, it was impossible to know whether that implicit accusation was valid.
The report makes a number of policy recommendations designed to increase the transparency of ADM systems.
As ADM systems are increasingly applied to personal data, with inevitable implications for privacy, the importance of transparency will increase."
73,https://www.itproportal.com/features/key-data-trends-for-2021-and-beyond/,Key data trends for 2021 and beyond,2021-02-15 00:00:00,"The pandemic has been instrumental in accelerating our economy's digitalization and our transition towards a digital marketplace. At the height of the UK pandemic in April, close to half of the employed population worked from home. Almost every single Briton – 96 percent – ordered a parcel online.

Businesses had to adapt. Fast. With continuity dependent on their ability to operate and transact virtually, digital transformation was expedited. Advancements previously scheduled to take years were condensed into a few months.

If businesses are to survive the immediate fallout from the pandemic and thrive in the future, they have to understand their new, post-Covid landscape. And this is where data is key. Data can help businesses identify and comprehend shifts in customer behavior, and better understand the rapidly evolving marketplace in which they operate.

But for this vision to materialize, business leaders need to get a decisive data strategy in place – and ensure all employees are on board with this. Experian's new report ""The Data Debate: A forward view of key trends for 2021 and beyond"" highlights four key areas that businesses should focus on in 2021 to create a pathway to success:

Consistent data standards

Developing better data skills

Improving the availability of data

Demonstrating responsible data use

Remember, there is no one-size-fits-all approach to achieving your data-informed objectives. But as we start seeing consistent data standards, data skills built, people better motivated to share their data, as well as safeguards put in place to maintain their long-term confidence, businesses will be able to deploy data in new, innovative ways that gives them the best chance of succeeding in this new data-driven world.

Building greater collaboration

At a time of great uncertainty, what is clear is that we are not going back to the way we were all doing business before. Just a few decades ago, businesses would have slowly picked themselves up and gradually established the pandemic's long-term implications for their operating models. In 2021, data can fast-track this process by delivering the insights businesses need to adapt and succeed in their new landscape.

However, accessing these benefits will demand close collaboration between government, industry, and regulators. These parties will need to work together to develop a data regulation framework that encourages innovation and data sharing while building people's trust that their data is being used responsibly. It can also allow organizations to get better access to the next wave of data-enabled technologies and unlock data's full potential to drive business and economic growth.

Looking further ahead

Once organizations have established a strategy that allows them to use data more effectively, what do the near-term opportunities look like in 2021 and beyond?

The following are three of the future trends in data we believe are closest to fruition, which would significantly affect how all sectors can and should use data to the benefit of their organization, but more importantly, people and society.

Standardization will drive the growth of data marketplaces and data as a service (DaaS), and in doing so, improve data availability - Data marketplaces and exchanges provide single platforms to consolidate third-party data offerings, and it is a booming area. Since the data resides in the cloud, these marketplaces and exchanges provide centralized availability and access (to analytics and other data sets, for example) that create economies of scale and reduce third-party data costs. Gartner predicts that by 2022, 35% of large organizations will be either sellers or buyers of data via formal online data marketplaces, up from 25% in 2020.

The key to accelerating these marketplaces will be standardization. Whether the scale of this materializes will depend on establishing a fair and transparent methodology by defining a data governance principle that ecosystem partners can rely on.

It's sensible that marketplaces focus initially on non-personal data, which has a less inherent risk associated with it. The governance experience of how open data is handled and shared can be usefully drawn upon in this instance.

With ever-increasing regulation around the use of personal data, it remains to be seen whether marketplaces can be equipped to handle the risks, transparency, and due diligence that organizations need to assess before using these data types. Companies also need to ensure that data traded in this way still maintains its integrity. The ""understanding"" of the data and its value is not divorced from the data itself. If this can be achieved, then data marketplaces will significantly impact efforts to combat ongoing issues around the availability of data.

Responsible use of data at the heart of the operationalization of artificial intelligence (AI), machine learning (ML) and natural language processing (NLP) – Techniques such as ML, optimization and NLP have demonstrated their worth during the Covid-19 crisis. They provide vital insights and predictions about the spread of the virus and the effectiveness and impact of countermeasures. The way these technologies have been applied during the pandemic has demonstrated their potential on the world stage and will undoubtedly accelerate their adoption.

Yet these solutions are only as good as the data they're fed. Standardization will help ensure data is digestible, while advancements in synthetic data could help remove bias from datasets by ensuring they are adequately diverse. Along with this will come a focus on the data governance and transparency aspects of AI. There will be a move towards Responsible AI – which enables model transparency as an essential mechanism for protecting against poor decisions. It results in better human-machine collaboration and greater trust, driving adoption and alignment with the organizations’ decisions.

The return of the business as data steward – Legislation and regulation are better connecting individuals to their personal information. It is now up to organizations to use this information and turn it into value for their customers easily and safely. Suppose organizations can demonstrate that they are able to store and use information responsibly, in a way that is accessible, easy to share and simple to withdraw at any point. In that case, they will win people's trust. An opportunity then arises for these businesses to be a repository for people's data.

Jonathan Westley, Chief Data Officer, Experian","['businesses', 'data', 'need', 'key', '2021', 'marketplaces', 'organizations', 'way', 'pandemic', 'business', 'trends', 'better']","Experian's new report ""The Data Debate: A forward view of key trends for 2021 and beyond"" highlights four key areas that businesses should focus on in 2021 to create a pathway to success:Consistent data standardsDeveloping better data skillsImproving the availability of dataDemonstrating responsible data useRemember, there is no one-size-fits-all approach to achieving your data-informed objectives.
In 2021, data can fast-track this process by delivering the insights businesses need to adapt and succeed in their new landscape.
Standardization will drive the growth of data marketplaces and data as a service (DaaS), and in doing so, improve data availability - Data marketplaces and exchanges provide single platforms to consolidate third-party data offerings, and it is a booming area.
Gartner predicts that by 2022, 35% of large organizations will be either sellers or buyers of data via formal online data marketplaces, up from 25% in 2020.
If this can be achieved, then data marketplaces will significantly impact efforts to combat ongoing issues around the availability of data."
74,https://www.information-age.com/qa-helping-government-singapore-improve-virtual-agents-123493801/,Q&A: Improving access to Government services through virtual agents,2021-02-11 13:05:50+00:00,"Q&A: Improving access to Government services through inclusive virtual agents

Speech analytics have been leveraged in order to improve access to public services.

Dr Lea El Samarji, intelligent industry solutions lead for EMEA at Avanade, spoke to Information Age about the company’s current project to improve virtual agents to help citizens gain better access to public services

Dr Lea El Samarji, who has led intelligent industry solutions for EMEA at Avanade since November 2019, has been overseeing a project currently in development that will improve access to government services. The pilot initiative has entailed the improvement of virtual agents’ understanding of the local dialects, using Singlish as a start, through the use of speech analytics.

This Q&A will explore how accessibility has increased due to ethical speech analytics capabilities, how social nuances can best be represented by AI, and managing bias in models.

Could you please provide an example of how ethical speech analytics can be put to good use?

For background, the national language of Singapore is Malay. The four commonly used languages of Singapore are English, Chinese, Malay, and Tamil. Singapore English is the set of varieties of the English language native to Singapore. The English language is spoken in Singapore, of which there are two main forms, are Standard Singapore English and Singapore Colloquial English (better known as Singlish).

Whereas commonly used languages in Singapore can be supported by today’s advanced cognitive services frameworks and tools, Singlish remains a big challenge for a machine to understand and support. We’ve trained the machine to understand Singlish terms and definitions; we trained the speech-to-text component for cognitive service with a data set of over 300+ different words relating to people, places, and languages.

In Singapore, there is a need to develop more virtual assistants based on Microsoft Azure, that have been trained and customised with the Singlish dialect, reducing the bias for greater inclusion and better performance. The project now is in pilot mode, and the plan is to go for production very soon. We started with Singapore, and the plan is to expand to the rest of Asia (Japan and China), and also test across Europe.

Can we automate data quality to support artificial intelligence and machine learning? Clint Hook, director of Data Governance at Experian, looks at how organisations can automate data quality to support artificial intelligence and machine learning. Read here

Why does bias relating to company culture sometimes end up being included in models?

When it comes to machine learning solutions, it is teams of people, data scientists, natural language processing specialists, engineers, developers, business people, and others who are designing machines to be intelligent and to fulfil the business need. These are the people who must identify the data sets used to train the machines and customise the variables that we need to pay attention to if we are to reduce bias.

There are two important things here to consider. First, we are creating the features that the machine should consider when learning, and secondly, the data sets on which the machine learning algorithm will learn; and bias can exist in both. Consider the first one: the features, also called machine learning variables, represent the business and the data science expertise. Together, it is the expertise from people working in and for the organisation, who develop the training and feature sets, so it is important to have a diverse team made up of males and females, different cultural and social backgrounds, ages, and possibly different countries. The more diverse the team that is building the solution is, the more diverse the solution and features will be as well.

The second point is the data set, and this is a key topic that we’re working on with the Universidad Francisco de Vitoria (UFV) in Madrid. We are running a research collaboration project with the university, where we are developing training algorithms using multiple datasets, different languages, and different points of view. We are developing an asset that searches and scrapes the website to get more data from diverse points of view. We are scraping articles from multiple journals, because we are convinced that the more diverse the data sets used to train AI, the more inclusive the AI solution will be.

How can societal nuances be best represented by artificial intelligence?

Societal nuances can be best represented when using AI by having more diverse data sets, by considering all points of view, and opening possibilities to search and collect data sets from outside the organisation. The Internet is a good place to mine data because you can find diverse perspectives. Another way is to have a diverse team working together to develop a less biased AI solution.

Is graph technology the fuel that’s missing for data-based government? Alicia Frame, lead project manager, data science at Neo4j, explains why the future of data-based government lies with a new way of working with complex data. Read here

How can bias best be removed from models, and in turn, how can a more diverse training data set be ensured?

If an organisation has an existing model that contains bias, we will start by a deep dive into the model and the organisation to understand how the bias is generated. We will then inspect the features that were implemented in the algorithm, then determine which features are creating the bias, and try to replace or remove those or add others. Then, we could add more diverse data sets in the training phase that we can find inside and outside the organisation. And third, we’d study the organisation and the team who built this solution, and recommend bringing more diversity to the team if possible. These three factors — adding new data sets, inspecting the features in place to modify as needed, and recommending a more diverse team, will help reduce bias in AI models.

This article is tagged with:","['sets', 'virtual', 'access', 'team', 'data', 'organisation', 'diverse', 'learning', 'improving', 'services', 'agents', 'singapore', 'bias', 'machine', 'features', 'qa']","Q&A: Improving access to Government services through inclusive virtual agentsSpeech analytics have been leveraged in order to improve access to public services.
Clint Hook, director of Data Governance at Experian, looks at how organisations can automate data quality to support artificial intelligence and machine learning.
Consider the first one: the features, also called machine learning variables, represent the business and the data science expertise.
Another way is to have a diverse team working together to develop a less biased AI solution.
Then, we could add more diverse data sets in the training phase that we can find inside and outside the organisation."
75,https://www.zdnet.com/article/ibm-and-exxonmobil-are-building-quantum-algorithms-to-solve-this-giant-optimization-problem/,IBM and ExxonMobil are building quantum algorithms to solve this giant computing problem,,"Quantum computers are coming Watch Now

Research teams from energy giant ExxonMobil and IBM have been working together to find quantum solutions to one of the most complex problems of our time: managing the tens of thousands of merchant ships crossing the oceans to deliver the goods that we use every day.

The scientists lifted the lid on the progress that they have made so far and presented the different strategies that they have been using to model maritime routing on existing quantum devices, with the ultimate goal of optimizing the management of fleets.

ExxonMobil was the first energy company to join IBM's Quantum Network in 2019, and has expressed a keen interest in using the technology to explore various applications, ranging from the simulation of new materials to solving optimization problems.

SEE: Research: Why Industrial IoT deployments are on the rise (TechRepublic Premium)

Now, it appears that part of the energy company's work was dedicated to tapping quantum capabilities to calculate journeys that minimize the distance and time traveled by merchant ships across the globe.

On a worldwide scale, the equation is immense – intractable, in fact, for classical computers. About 90% of world trade relies on maritime shipping, with more than 50,000 ships, themselves carrying up to 200,000 containers each, moving around every day to transport goods with a total value of $14 trillion.

The more the number of ships and journeys increase, the bigger the problem becomes. As IBM and ExxonMobil's teams put it in a blog post detailing their research: ""Logistically speaking, this isn't the 'traveling salesperson problem.'""

While this type of exponentially growing problem can only be solved with simplifications and approximations on classical computers, the challenge is well-suited to quantum technologies. Quantum computers can effectively leverage a special dual state that is taken on by quantum bits, or qubits, to run many calculations at once; meaning that even the largest problems could be resolved in much less time than is possible on a classical computer.

""We wanted to see whether quantum computers could transform how we solve such complex optimization problems and provide more accurate solutions in less computational times,"" said the researchers.

Although the theory behind the potential of quantum computing is well-established, it remains to be found how quantum devices can be used in practice to solve a real-world problem such as the global routing of merchant ships. In mathematical terms, this means finding the right quantum algorithms that could be used to most effectively model the industry's routing problems, on current or near-term devices.

To do so, IBM and ExxonMobil's teams started with widely-used mathematical representations of the problem, which account for factors such as the routes traveled, the potential movements between port locations and the order in which each location is visited on a particular route. There are many existing ways to formulate the equation, one of which is called the quadratic unconstrained binary optimization (QUBO) technique, and which is often used in classical computer science.

The next question was to find out whether well-known models like QUBO can be solved with quantum algorithms – and if so, which solvers work better. Using IBM's Qiskit optimization module, which was released last year to assist developers in building quantum optimization algorithms, the team tested various quantum algorithms labeled with unbeatably exotic names: the Variational Quantum Eigensolver (VQE), the Quantum Approximate Optimization Algorithm (QAOA), and Alternating Direction Method of Multiplier (ADMM) solvers.

After running the algorithms on a simulated quantum device, the researchers found that models like QUBO could effectively be solved by quantum algorithms, and that depending on the size of the problem, some solvers showed better results than others.

In another promising finding, the team said that the experiment showed some degree of inexactness in solving QUBOs is tolerable. ""This is a promising feature to handle the inherent noise affecting the quantum algorithms on real devices,"" said the researchers.

SEE: BMW explores quantum computing to boost supply chain efficiencies

Of course, while the results suggest that quantum algorithms could provide real-world value, the research was carried out on devices that are still technically limited, and the experiments can only remain small-scale. The idea, however, is to develop working algorithms now, to be ready to harness the power of a fully fledged quantum computer when the technology develops.

""As a result of our joint research, ExxonMobil now has a greater understanding of the modelling possibilities, quantum solvers available, and potential alternatives for routing problems in any industry,"" said the researchers.

What applies to merchant ships, in effect, can also work in other settings. Routing problems are not inherent to the shipping industry, and the scientists confirmed that their findings could easily be transferred to any vehicle optimization problem that has time constraints, such as goods delivery, ride-sharing services or urban waste management.

In fact, ExxonMobil is not the first company to look at ways to use quantum computing techniques to solve optimization problems. Electronics manufacturer OTI Lumionics, for example, has been using QUBO representations to find the most optimal simulation of next-generation OLED materials. Instead of using gate-based quantum computers to run the problem, however, the company has been developing quantum-inspired algorithms to solve calculations on classical Microsoft Azure hardware, with encouraging results.

The mathematical formulas and solution algorithms are described in detail in the research paper, and the ExxonMobil/IBM team stressed that their use is not restricted. The researchers encouraged their colleagues to reproduce their findings to advance the global field of quantum solvers.","['research', 'exxonmobil', 'using', 'ships', 'routing', 'building', 'ibm', 'algorithms', 'optimization', 'problem', 'computing', 'computers', 'giant', 'solve', 'problems', 'quantum']","In mathematical terms, this means finding the right quantum algorithms that could be used to most effectively model the industry's routing problems, on current or near-term devices.
The next question was to find out whether well-known models like QUBO can be solved with quantum algorithms – and if so, which solvers work better.
Using IBM's Qiskit optimization module, which was released last year to assist developers in building quantum optimization algorithms, the team tested various quantum algorithms labeled with unbeatably exotic names: the Variational Quantum Eigensolver (VQE), the Quantum Approximate Optimization Algorithm (QAOA), and Alternating Direction Method of Multiplier (ADMM) solvers.
""This is a promising feature to handle the inherent noise affecting the quantum algorithms on real devices,"" said the researchers.
In fact, ExxonMobil is not the first company to look at ways to use quantum computing techniques to solve optimization problems."
76,https://www.forbes.com/sites/sap/2021/02/11/how-big-data-predictive-analytics-and-machine-learning-changed-the-railway-industry/,,,,,
77,https://www.eureporter.co/general/2021/02/11/how-artificial-intelligence-is-used-in-online-casinos/,,,,,
78,https://techobserver.in/2021/02/11/sebi-to-bank-on-ai-based-technology-for-inspections-and-surveillance-of-mutual-funds/,Sebi to bank on AI based technology for inspections and surveillance of mutual funds – Tech Observer,2021-02-11 00:00:00,"Bringing better technology to use, the markets regulator, Securities and Exchange Board of India (Sebi) is set to roll-out new technology to automate inspections and surveillance of mutual funds. This is in line with the regulator’s future tech adoption plan to reduce the lag in recognition of violations.

According to Sebi chief Ajay Tyagi, the Sebi is eyeing a major technological leap in its surveillance and investigation functions. “Sebi is in the process of implementing a project on automation of inspections and surveillance of mutual fund,” Tyagi said in the regulator’s annual report for 2019-20.

Under the new automation project, mutual fund-related data would be ingested in the Sebi database and use of algorithms in order to generate instances of breaches of regulatory guidelines by mutual funds in respect of several important guidelines where quantitative analysis is possible.

With the new technology in place, Sebi will be able to move towards automation of inspections of mutual funds, 100% inspection of data in place of samples and to reduce the lag in recognition of violation, said the annual report.

Building on better technology, Sebi has also developed an in-house automated system by which it is able to do system based reconciliation of client level securities holdings by brokers thus detecting client securities misuse by brokers if any.

Further, in order to detect possible mark manipulation and strengthen market supervision through technology solutions, Sebi has initiated a “data analytics and data models project.”

Apart from taking steps towards increasing technological sophistication of the markets, Sebi said it has continuously upgraded technology in its own regulatory functions.

The regulator has started work on a data lake project which would have the capability to store and retrieve quickly, a large amount of structured, semi-structured and unstructured data.

The data lake project will support advanced analytical tools, such as artificial intelligence and machine learning (AI/ML), deep learning, big data analytics, pattern recognition, processing of structured and unstructured data, text mining and natural language processing thereby significantly augmenting surveillance capabilities.

Sebi has also implemented its Security and Network Operations Center project which adheres to global security standards to safeguard against the associated risks including cyber frauds in newer technologies.","['project', 'sebi', 'mutual', 'securities', 'data', 'bank', 'based', 'tech', 'recognition', 'observer', 'technology', 'inspections', 'funds', 'surveillance']","Bringing better technology to use, the markets regulator, Securities and Exchange Board of India (Sebi) is set to roll-out new technology to automate inspections and surveillance of mutual funds.
This is in line with the regulator’s future tech adoption plan to reduce the lag in recognition of violations.
According to Sebi chief Ajay Tyagi, the Sebi is eyeing a major technological leap in its surveillance and investigation functions.
“Sebi is in the process of implementing a project on automation of inspections and surveillance of mutual fund,” Tyagi said in the regulator’s annual report for 2019-20.
Building on better technology, Sebi has also developed an in-house automated system by which it is able to do system based reconciliation of client level securities holdings by brokers thus detecting client securities misuse by brokers if any."
79,https://venturebeat.com/2021/02/11/immunai-raises-60-million-to-analyze-the-immune-system-with-ai/,Immunai raises $60 million to analyze the immune system with AI,2021-02-11 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Immunai, a startup developing an AI platform to analyze the human immune system, today announced that it raised $60 million. The company says it will use the funds to broaden its functional genomics capabilities and help its partners prioritize, discover, and develop new therapies and drug combinations.

Emerging treatments like gene cell therapies and cancer immunotherapies promise to revolutionize the field of medicine. But the immune system’s complexity — trillions of cells partitioned into hundreds of types and states that interplay with various systems and proteins — threatens to stymie research. In 1999, a patient in a trial died after an immune system attack likely resulting from preexisting antibodies against a virus used as part of gene therapy — a death that experts believe led to years lost in gene therapy development. Immunai aims to prevent such mistakes with immune profile-generating AI.

Immunai was founded in 2018 by Noam Solomon, an ex-Harvard and -MIT postdoctoral researcher, and Luis Voloch, an MIT graduate and former machine learning engineer at Palantir. The two teamed up with members of the Parker Institute, which works with researchers to accelerate the development of immune therapies, to pursue a platform that sheds light on cell populations post- and pre-treatments.

“When I met my cofounder Luis, I was a math postdoc at MIT and Luis was working to apply machine learning to biology. Together, we wanted to bring ‘transfer learning’ AI methods to what we believe would solve the biggest problem in society today — disease,” Solomon told VentureBeat via email. “All disease can be traced back to the immune system. But what Luis and I realized is that pharmaceutical companies don’t have access to any comprehensive, granular insight into how the immune system works, how it responds to the drugs or therapies they’re developing, and what patients are most likely to benefit.”

Immunai’s tech records over a terabyte of data from a blood sample, profiling cells at what the company characterizes as “unprecedented” depth. Samples are compared with a database using AI that maps data to hundreds of cell types and states, creating immune profiles.

It’s an approach similar to that of scientists affiliated with the Human Vaccines Project, who are working to identify biomarkers — i.e., indicators of particular disease states — that predict immune responses to vaccines and cell therapies. Microsoft and startup Adaptive Biotechnologies are also collaborating to develop algorithms that create a “translation map” for cell receptors to antigens, or pathogen molecules that trigger an immune response, and map those antigens back to diseases.

Clinical studies have traditionally focused on testing thousands or even tens of thousands of subjects and collecting a limited amount of data on each. But massive corpora and AI enable millions of data points to be collected about a single individual.

“The immune system is implicated in nearly every illness, making our technology critical for identifying, diagnosing, and treating disease, from cancer to autoimmune disorders,” Solomon said in a statement. “Our expansion into functional genomics will help our partners tackle their most pressing questions in therapy development, and will ultimately improve the lives of many patients.”

Immunai’s immune profiles could support the discovery of biomarkers by spotting changes in cell type and expression. For example, the Immunai team characterized a CAR-Natural Killer T (NKT) infusion cell therapy product developed at the Baylor College of Medicine for use in neuroblastoma patients. Baylor researchers and Immunai identified a gene potentially involved in CAR-NKT-mediated killing of tumor cells and are working to validate it. Elsewhere, Immunai says it’s engaging with commercial partners to develop cell therapy candidates in solid tumors.

Voloch says that Immunai is working with 5 of the world’s largest pharma companies in addition to institutions including Stanford, Harvard, Memorial Sloan Kettering, and the University of Pennsylvania. “We’ve developed a novel platform to reprogram immunity by mining AMICA, our proprietary harmonized single-cell immunology database, with cutting-edge transfer and multi-task learning algorithms,” he added. “Our vertically integrated functional genomics and AI capabilities allow us to prioritize and validate targets more accurately.”

Seventy-employee Immunai is headquartered in New York City, with offices in San Francisco and Tel Aviv. The series A round announced today was led by the Schusterman Foundation, the Duquesne Family, Catalio Capital Management, and Dexcel Pharma. Existing investors Viola Ventures and TLV Partners also participated, bringing Immunai’s total raised to date to $80 million.","['immunai', 'cell', 'data', 'raises', '60', 'analyze', 'ai', 'therapy', 'system', 'therapies', 'immune', 'working', 'partners', 'million']","Immunai, a startup developing an AI platform to analyze the human immune system, today announced that it raised $60 million.
Emerging treatments like gene cell therapies and cancer immunotherapies promise to revolutionize the field of medicine.
“All disease can be traced back to the immune system.
“The immune system is implicated in nearly every illness, making our technology critical for identifying, diagnosing, and treating disease, from cancer to autoimmune disorders,” Solomon said in a statement.
Elsewhere, Immunai says it’s engaging with commercial partners to develop cell therapy candidates in solid tumors."
80,https://venturebeat.com/2021/02/11/labelbox-raises-40-million-for-its-data-labeling-and-annotation-tools/,Labelbox raises $40 million for its data labeling and annotation tools,2021-02-11 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Labelbox, a startup developing a data annotation and labeling platform, today announced it has raised $40 million, bringing its total raised to $79 million. The company says the funds will be used to acquire new customers, expand its solutions, and grow its workforce around the globe.

Training AI and machine learning algorithms requires plenty of annotated data. But data rarely comes with annotations. The bulk of the work often falls to human labelers, whose efforts tend to be expensive, imperfect, and slow. It’s estimated most enterprises that adopt machine learning spend over 80% of their time on data labeling and management.

Labelbox was founded in 2018 by Manu Sharma and Brian Rieger, who both worked in the aeronautics industry, designing and testing flight control systems and experimenting with machine learning models. The San Francisco-based company offers a web service and API that allows data science teams to work with annotation teams from a single dashboard. Users can customize the tools to support specific use cases, including instances, custom attributes, and more, and label directly on photos, text strings, conversations, paragraphs, documents, and videos.

Using Labelbox, admins can manage access to data and projects for team members, ensuring access controls when working with a labeling service. They also get labeler performance metrics and a catalog of available labeling services, in addition to feature counts and object analytics to improve model capabilities.

Labelbox is in a category adjacent to companies like Scale AI, which has raised over $100 million for its suite of data labeling services, and CloudFactory, which says it offers labelers growth opportunities and “metric-driven” bonuses. That’s not to mention Hive, Alegion, Appen, SuperAnnotate, Dataloop, and Cognizant.

But Labelbox, which has 150 customers and just over 100 employees, says it reduces the time and cost associated with annotation through pre-labeling, where unlabeled data is initially seeded with machine learning model predictions. The company also claims to employ active learning, which dynamically prioritizes data labeling queues. From Labelbox, customers can search, browse, and curate training data to investigate poor or inconsistent labels.

When these tools are leveraged in conjunction with each other, Labelbox asserts they enable customers to automate labeling where confidence is high and spotlight assets where performance remains low. This ostensibly lets labelers pre-label assets to confirm, reject, or edit annotations, rather than labeling from scratch.

“While software is built with code, AI is built with data. Algorithms and compute power have now been commoditized, which means the way to differentiate your AI in the market is via your training data,” Rieger told VentureBeat via email. “But converting your proprietary data into revenue-generating AI has been a difficult process, full of delays and false starts. Our training data platform allows organizations to build their own AI ‘data engine’ extremely quickly at significant cost savings.”

B Capital Group led the series C investment Labelbox announced today. Previous investors Andreessen Horowitz, First Round Capital, Gradient Ventures (Google’s AI venture fund), Kleiner Perkins, and ARK Invest CEO Catherine Wood also participated.","['customers', 'labelbox', 'labeling', 'data', 'raised', '40', 'raises', 'learning', 'ai', 'annotation', 'machine', 'tools', 'training', 'million']","Labelbox, a startup developing a data annotation and labeling platform, today announced it has raised $40 million, bringing its total raised to $79 million.
Training AI and machine learning algorithms requires plenty of annotated data.
It’s estimated most enterprises that adopt machine learning spend over 80% of their time on data labeling and management.
From Labelbox, customers can search, browse, and curate training data to investigate poor or inconsistent labels.
Our training data platform allows organizations to build their own AI ‘data engine’ extremely quickly at significant cost savings.”B Capital Group led the series C investment Labelbox announced today."
81,https://www.marktechpost.com/2021/02/11/fda-gives-landmark-clearance-to-clews-ai-driven-icu-predictive-tool/,FDA Gives Landmark Clearance to CLEW’S AI-Driven ICU Predictive Tool,2021-02-11 00:00:00,"CLEW has come up with a Medical Intense Care Unit tool that uses machine learning (ML) models to identify patients’ conditions. The United States Food and Drug Administration (FDA) has permitted (510k Clearance) using the above AI medical tool. This is the first time for FDA to give clearance to such a tool.

The tool predicts hemodynamic instability in adult patients in ICU. The tool is named CLEWICU, and it uses Artificial Intelligence algorithms and ML models to identify the likelihood of significant clinical events for ICU patients.

According to Dr. David Bates, CLEW Advisory Board member, AI can be a powerful healthcare change force. It can enable assessment of time-critical patient information and predict deterioration health that could help better clinical decisions and improve the ICU outcomes.

Hemodynamic instability is a condition when there is abnormal blood pressure, which can cause inadequate blood flow to your organs. The instability symptoms may include

Decreased urine output

Low blood pressure (hypotension)

Loss of consciousness

Abnormal heart rate and Chest pain

Cold hands, arms, legs, or feet, or bluish discoloration of these areas

Confusion, Restlessness, and Breath shortness

Hemodynamic instability is a common COVID-19 complication. CLEWICU’s predictive ability could prove useful during the ongoing Covid pandemic. CLEWICU provides a picture of the overall unit status by analyzing patient data from various sources and identifying individuals with deteriorating health conditions. The system notifies users of deterioration condition in advance by eight years. The above enables early intervention.

The emergency use authorization was provided by FDA to CLEWICU back this past June. The tool was one of the several AI-powered innovations in technology that was built, or modified, in response to the ongoing pandemic.

According to Cris Ross, Mayo Clinic Chief Information Officer, Artificial Intelligence has been crucial in understanding the Covid pandemic.

The model is already proven successful in the ICU and is now being extended to all care settings. The model is designed in a way to guide the healthcare workers predict the patient’s health state. CLEW helps make informed clinical decisions, improves outcomes, and better deals with increased regulation. The tool also lowers the cost of care.

Source: https://clewmed.com/clew-medical-receives-fda-clearance-for-ai-based-predictive-analytics-technology-to-support-adult-icu-patient-assessment/

Suggested","['predictive', 'clinical', 'clews', 'patients', 'health', 'clew', 'gives', 'icu', 'instability', 'blood', 'clewicu', 'aidriven', 'clearance', 'landmark', 'tool', 'fda']","The United States Food and Drug Administration (FDA) has permitted (510k Clearance) using the above AI medical tool.
This is the first time for FDA to give clearance to such a tool.
The tool predicts hemodynamic instability in adult patients in ICU.
The tool is named CLEWICU, and it uses Artificial Intelligence algorithms and ML models to identify the likelihood of significant clinical events for ICU patients.
The model is designed in a way to guide the healthcare workers predict the patient’s health state."
82,https://www.analyticsinsight.net/a-look-at-5-ai-startups-empowering-defence-and-security-industry/,,,,,
83,http://www.medicaldesignandoutsourcing.com/cardiologs-touts-results-of-afib-study/,Cardiologs touts results of AFib study,2021-02-10 19:43:25+00:00,"Artificial intelligence diagnostics company Cardiologs today announced the results of a clinical study showing its AI-based ECG analysis software “significantly” reduces the rate of false-positive atrial fibrillation (AFib) detection in remote cardiac monitoring of certain patients.

The company, which has offices in Paris and Boston, worked with Valley Health System of northern New Jersey to track a cohort of 425 patients who each received a Medtronic Reveal LINQ implantable loop recorder (ILR) for known AFib or cryptogenic stroke.

Researchers evaluated 1,500 AF episodes and annotated each as either true or false, according to the study, published in the Journal of the American College of Cardiology Clinical Electrophysiology. All applicable ECG readings were uploaded into Cardiologs’ cloud-based solution for analysis by its deep-learning algorithm. The research showed Cardiologs’ AI model eliminated the incidence of false-positives by as much as 66% and had a positive predictive value (PPV) as high as 75%.

According to the American Heart Association, more than 2.7 million Americans are living with AFib, and that number continues to grow. ILRs play a critical role in the management of these patients. Devices with wireless capabilities are programmed to automatically transmit ECG data to a secure website for review by cardiology staff. However, the large volume of recordings can present challenges — in particular, the substantial number of false-positives alerts that electrophysiologists must review to avoid misdiagnosis and potential errors in clinical management.

The high false-positive rate of AFib detected by ILRs has created a significant clinical burden, according to lead investigator Dr. Suneet Mittal, director of electrophysiology and the Snyder Center for Comprehensive Atrial Fibrillation at Valley Health System.

“Since ILRs transmit data daily, these false positives are one of the Achilles heels of remote cardiac monitoring,” Mittal added in a news release. “This study validates that Cardiologs’ advanced AI can filter 2/3 of false-positive AF episodes, which should improve clinical efficiency.”

“The need for reliable and efficient remote patient management is stronger than ever before and will only increase in the future,” added Cardiologs CEO Yann Fleureau. “Publication of our successful trial results in JACC EP further validates the considerable promise of our solution for the clinical community. We have demonstrated that we can offer an extremely reliable solution that will reduce the clinical burden of managing ILR patients remotely.”","['afib', 'study', 'cardiologs', 'touts', 'ilrs', 'clinical', 'patients', 'falsepositive', 'remote', 'results', 'solution', 'ecg']","Artificial intelligence diagnostics company Cardiologs today announced the results of a clinical study showing its AI-based ECG analysis software “significantly” reduces the rate of false-positive atrial fibrillation (AFib) detection in remote cardiac monitoring of certain patients.
All applicable ECG readings were uploaded into Cardiologs’ cloud-based solution for analysis by its deep-learning algorithm.
According to the American Heart Association, more than 2.7 million Americans are living with AFib, and that number continues to grow.
“Publication of our successful trial results in JACC EP further validates the considerable promise of our solution for the clinical community.
We have demonstrated that we can offer an extremely reliable solution that will reduce the clinical burden of managing ILR patients remotely.”"
84,https://techcrunch.com/2021/02/11/base-operations-raises-2-2-million-to-modernize-physical-enterprise-security/,TechCrunch is now a part of Verizon Media,2021-02-11 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
85,http://aiweekly.co/issues/200,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Ethics

Artificial Intelligence: The Class Struggle’s Next Frontier It’s also the focus of the first annual ‘International Conference on AI in Work, Innovation, Productivity and Skills’, a conference organised by the OECD, a powerful intergovernmental organisation made up predominantly of rich countries and intent on finding ways to ratchet economic growth.

Tech and Ethics: The World Economic Forum’s Kay Firth-Butterfield on Doing the Right Thing in AI In 2017, Kay became head of AI and machine learning at the World Economic Forum, where her team develops tools and on-the-ground programs to improve AI understanding and governance across the globe.","['artificial', 'conference', 'world', 'understanding', 'learning', 'ai', 'leading', 'deep', 'kay', 'weekly', 'ways', 'thing', 'work', 'tools', 'economic', 'intelligence', 'newsletter']","EthicsArtificial Intelligence: The Class Struggle’s Next Frontier It’s also the focus of the first annual ‘International Conference on AI in Work, Innovation, Productivity and Skills’, a conference organised by the OECD, a powerful intergovernmental organisation made up predominantly of rich countries and intent on finding ways to ratchet economic growth.
Tech and Ethics: The World Economic Forum’s Kay Firth-Butterfield on Doing the Right Thing in AI In 2017, Kay became head of AI and machine learning at the World Economic Forum, where her team develops tools and on-the-ground programs to improve AI understanding and governance across the globe."
86,https://techcrunch.com/2021/02/11/intenseye-raises-4m-to-boost-workplace-safety-through-computer-vision/,TechCrunch is now a part of Verizon Media,2021-02-11 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
87,https://www.sciencedaily.com/releases/2021/02/210210142049.htm,New wearable device turns the body into a battery,2021-02-21 00:00:00,"Researchers at the University of Colorado Boulder have developed a new, low-cost wearable device that transforms the human body into a biological battery.

The device, described today in the journal Science Advances, is stretchy enough that you can wear it like a ring, a bracelet or any other accessory that touches your skin. It also taps into a person's natural heat -- employing thermoelectric generators to convert the body's internal temperature into electricity.

""In the future, we want to be able to power your wearable electronics without having to include a battery,"" said Jianliang Xiao, senior author of the new paper and an associate professor in the Paul M. Rady Department of Mechanical Engineering at CU Boulder.

The concept may sound like something out of The Matrix film series, in which a race of robots have enslaved humans to harvest their precious organic energy. Xiao and his colleagues aren't that ambitious: Their devices can generate about 1 volt of energy for every square centimeter of skin space -- less voltage per area than what most existing batteries provide but still enough to power electronics like watches or fitness trackers.

Scientists have previously experimented with similar thermoelectric wearable devices, but Xiao's is stretchy, can heal itself when damaged and is fully recyclable -- making it a cleaner alternative to traditional electronics.

""Whenever you use a battery, you're depleting that battery and will, eventually, need to replace it,"" Xiao said. ""The nice thing about our thermoelectric device is that you can wear it, and it provides you with constant power.""

High-tech bling

advertisement

The project isn't Xiao's first attempt to meld human with robot. He and his colleagues previously experimented with designing ""electronic skin,"" wearable devices that look, and behave, much like real human skin. That android epidermis, however, has to be connected to an external power source to work.

Until now. The group's latest innovation begins with a base made out of a stretchy material called polyimine. The scientists then stick a series of thin thermoelectric chips into that base, connecting them all with liquid metal wires. The final product looks like a cross between a plastic bracelet and a miniature computer motherboard or maybe a techy diamond ring.

""Our design makes the whole system stretchable without introducing much strain to the thermoelectric material, which can be really brittle,"" Xiao said.

Just pretend that you're out for a jog. As you exercise, your body heats up, and that heat will radiate out to the cool air around you. Xiao's device captures that flow of energy rather than letting it go to waste.

""The thermoelectric generators are in close contact with the human body, and they can use the heat that would normally be dissipated into the environment,"" he said.

advertisement

Lego blocks

He added that you can easily boost that power by adding in more blocks of generators. In that sense, he compares his design to a popular children's toy.

""What I can do is combine these smaller units to get a bigger unit,"" he said. ""It's like putting together a bunch of small Lego pieces to make a large structure. It gives you a lot of options for customization.""

Xiao and his colleagues calculated, for example, that a person taking a brisk walk could use a device the size of a typical sports wristband to generate about 5 volts of electricity -- which is more than what many watch batteries can muster.

Like Xiao's electronic skin, the new devices are as resilient as biological tissue. If your device tears, for example, you can pinch together the broken ends, and they'll seal back up in just a few minutes. And when you're done with the device, you can dunk it into a special solution that will separate out the electronic components and dissolve the polyimine base -- each and every one of those ingredients can then be reused.

""We're trying to make our devices as cheap and reliable as possible, while also having as close to zero impact on the environment as possible,"" Xiao said.

While there are still kinks to work out in the design, he thinks that his group's devices could appear on the market in five to 10 years. Just don't tell the robots. We don't want them getting any ideas.

Coauthors on the new paper include researchers from China's Harbin Institute of Technology, Southeast University, Zhejiang University, Tongji University and Huazhong University of Science and Technology.

Video: https://www.youtube.com/watch?v=hexScHvEFwQ&feature=emb_logo","['battery', 'university', 'devices', 'turns', 'body', 'wearable', 'device', 'xiaos', 'thermoelectric', 'human', 'skin', 'power', 'xiao']","Researchers at the University of Colorado Boulder have developed a new, low-cost wearable device that transforms the human body into a biological battery.
""The nice thing about our thermoelectric device is that you can wear it, and it provides you with constant power.""
He and his colleagues previously experimented with designing ""electronic skin,"" wearable devices that look, and behave, much like real human skin.
Xiao's device captures that flow of energy rather than letting it go to waste.
Coauthors on the new paper include researchers from China's Harbin Institute of Technology, Southeast University, Zhejiang University, Tongji University and Huazhong University of Science and Technology."
88,https://venturebeat.com/2021/02/10/neureality-emerges-from-stealth-to-accelerate-ai-workloads-at-scale/,NeuReality emerges from stealth to accelerate AI workloads at scale,2021-02-10 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

NeuReality, a Caesarea, Israel-based startup developing high-performance AI hardware for cloud datacenters and edge nodes, today emerged from stealth with $8 million. The company, which counts among its board of directors Naveen Rao, former GM of Intel’s AI product group, says the funding will lay the groundwork for the launch of its first product later in 2021.

Machine learning deployments have historically been constrained by the size and speed of algorithms and the need for costly hardware. In fact, a report from MIT found that machine learning might be approaching computational limits. A separate Synced study estimated that the University of Washington’s Grover fake news detection model cost $25,000 to train in about two weeks. OpenAI reportedly racked up a whopping $12 million to train its GPT-3 language model, and Google spent an estimated $6,912 training BERT, a bidirectional transformer model that redefined the state of the art for 11 natural language processing tasks.

NeuReality aims to solve these scalability challenges with purpose-built computing platforms for recommender systems, classifiers, digital assistants, language-based applications, and computer vision. The company claims its products, which will be made available as a service, can enable customers to scale AI utilization while cutting costs, lowering energy consumption, and shrinking their infrastructure footprint. In fact, NeuReality claims it can deliver 30 times the system cost benefit over today’s state-of-the-art, CPU-centric servers.

“Our mission is to deliver AI users best in class system performance while significantly reducing cost and power,” CEO and cofounder Moshe Tanach told VentureBeat via email. “In order to make AI accessible to every organization, we must build affordable infrastructure that will allow innovators to deploy AI-based applications that cure diseases, improve public safety, and enhance education. NeuReality’s technology will support that growth while making the world smarter, cleaner, and safer for everyone. The cost of the AI infrastructure and AI-as-a-service will no longer be limiting factors.”

NeuReality was cofounded in 2019 by Tanach, Tzvika Shmueli, and Yossi Kasus. Tanach previously served as director of engineering at Marvell and Intel and AVP of R&D at DesignArt-Networks, which was acquired by Qualcomm in 2012. Shmueli is the former VP of backend at Mellanox Technologies and VP of engineering at Habana Labs. And Kasus held a senior director of engineering role at Mellanox and was head of very large-scale integrations at EZChip.

NeuReality has competition in OctoML, a startup that similarly purports to automate machine learning optimization with proprietary tools and processes. Other competitors include Deci and DeepCube, which describe their solutions as “software-based inference accelerators,” and Neural Magic, which redesigns AI algorithms to run more efficiently on off-the-shelf processors by leveraging the chips’ available memory. Yet another rival, DarwinAI, uses what it calls generative synthesis to ingest models and spit out highly optimized versions.

But Tanach says the company is currently active in three main lanes: (1) Public and private cloud datacenter companies, (2) solution providers that build datacenter solutions and large-scale software solutions for enterprises, financial institutions, and government organizations, and (3) OEMs and ODMs that build servers and edge node solutions.

“There are no such solutions in the market today. The competition is split between various silicon and system products. The most obvious ones are the inference deep-learning accelerators [from] companies such as Nvidia, Intel, and startups that are competing in that market. However, these competitors have only part of the solution both from a system perspective and from an AI compute capabilities standpoint,” Tanach said. “[We] will release more information about the solution later this year when its first platform is ready. For now, the company can only share that the total cost of ownership of its AI compute service will be more efficient by an order of magnitude compared to existing solutions.”

Cardumen Capital, OurCrowd, and Varana Capital led today’s seed round, the company’s first public investment. NeuReality has 18 employees.","['emerges', 'scale', 'tanach', 'cost', 'public', 'model', 'stealth', 'learning', 'ai', 'system', 'neureality', 'solutions', 'accelerate', 'solution', 'workloads', 'company']","NeuReality, a Caesarea, Israel-based startup developing high-performance AI hardware for cloud datacenters and edge nodes, today emerged from stealth with $8 million.
A separate Synced study estimated that the University of Washington’s Grover fake news detection model cost $25,000 to train in about two weeks.
In fact, NeuReality claims it can deliver 30 times the system cost benefit over today’s state-of-the-art, CPU-centric servers.
NeuReality has competition in OctoML, a startup that similarly purports to automate machine learning optimization with proprietary tools and processes.
However, these competitors have only part of the solution both from a system perspective and from an AI compute capabilities standpoint,” Tanach said."
89,https://www.medtechintelligence.com/column/quantum-computing-makes-inroads-in-life-sciences/,Quantum Computing Makes Inroads in Life Sciences,2021-02-11 00:44:32-04:00,"Quantum computing has spurred the development of new breakthroughs in life sciences from using machine learning methods to diagnose illnesses sooner to identifying materials to make more efficient devices.

Quantum computing enables industries to tackle problems they never would have attempted to solve with a classical computer. In 2016, IBM was the first company to put a quantum computer on the cloud, signalling a move from theoretical mathematics to real-world applications and discovering solutions.

A few years ago we developed a forecasting tool called a Life-Science Radar to identify key areas in the markets we serve and share our predictions of when these trends will become mainstream. One of the trends identified was Quantum Computing. In 2019 we placed this technology at the very outer section of our Life Sciences Radar diagram as we considered that it would take some years before this technology became “mainstream”.

However, technology developments have moved quickly over the last few years, benefiting from faster and more sophisticated Internet technology, faster computer systems and advanced Cloud technology. Now, using these new technologies as a basis, we see a growing need for better value and better analysis of various types of data: From business-data gathering and analytics to artificial intelligence, including machine learning and deep learning. Quantum computing makes that possible. As a result, life science companies are evaluating and adopting quantum computing.

A good example of this application is Boehringer Ingelheim’s recently announced partnership with Google Quantum AI, which focuses on “research and implementing cutting-edge use cases for quantum computing in pharmaceutical research and development (R&D), including molecular dynamics simulations”. Boehringer Ingelheim is the first pharmaceutical company to join forces with Google in quantum computing.

“We are really excited about joining forces with Google, the leading tech company when it comes to quantum computing,” said Michael Schmelmer, member of the board of managing directors of Boehringer Ingelheim in a press release. “Quantum computing has the potential to significantly accelerate and enhance R&D processes in our industry. Quantum computing is still very much an emerging technology. However, we are convinced that this technology could help us to provide even more humans and animals with innovative and ground-breaking medicines in the future.”

It is the goal of Boehringer Ingelheim to take advantage of the partnership and new technology to spur innovation and provide a competitive edge in developing medical and therapeutic breakthroughs for diseases with unmet medical needs.

In the above referenced press release, Ryan Babbush, head of quantum algorithms at Google noted that “Extremely accurate modelling of molecular systems is widely anticipated as among the most natural and potentially transformative applications of quantum computing.”

Another major life sciences company, Merck, is using quantum computing technology from Honeywell. “Quantum Computing is poised to disrupt classical computing and enable a variety of unprecedented opportunities,” said Philipp Harbach, head of in-silico research at the chief digital organization of Merck, in a 2019 press release. “The applications touch upon many fields with direct relevance to Merck and to our customers, for example materials research, drug discovery, artificial intelligence, and e-commerce.”

In January 2021, Cambridge Quantum Computing (CQC) formed a partnership with Roche to design and implement algorithms for early-stage drug discovery and development, including for Alzheimer’s disease. In addition to Google, Honeywell and CQC, several major technology players are using quantum computing technology, including Microsoft, Intel, IBM and a growing number of vendors that develop solutions for specific applications.

What makes this technology different and valuable for the industry? Quantum computing differs from classical computing in three ways:

How information is represented: In classical computing, a computer runs on bits that have a value of either 0 or 1. Quantum bits or “qubits” are similar but they can also hold much more complex information, or even be negative values.

How information is processed: In classical computing, at the fundamental level, bits are processed sequentially, one step at a time. In quantum computation, qubits work together to find the optimal solution. This allows quantum computers to converge on the right answer to a problem very quickly.

How results are interpreted: In classical computing, only specifically defined results are available, based on the algorithm’s design. Quantum answers are probabilistic, with multiple possible answers considered, in a certain-given computation.

Innovative life science companies are leveraging the value from quantum technology to accelerate discovery, research and development of new therapies and target specific complex patient treatments. In addition, they are investigating the use of quantum computing to facilitate prediction and simulation of expected outcomes, which will help to achieve improved patient outcomes in a much shorter timeframe and more cost effectively.

Considering the early adoption of quantum computing by leading life science companies, we believe it has the potential to positively disrupt research and development. Therefore, we moved quantum computing closer to the horizon in the 2021 Life Sciences Radar and expect it to become widely adopted across life sciences in the next three years and become a mainstream technology in five years.","['research', 'using', 'life', 'development', 'classical', 'computing', 'makes', 'sciences', 'google', 'technology', 'inroads', 'quantum']","Quantum computing has spurred the development of new breakthroughs in life sciences from using machine learning methods to diagnose illnesses sooner to identifying materials to make more efficient devices.
Quantum computing makes that possible.
“Quantum computing has the potential to significantly accelerate and enhance R&D processes in our industry.
Quantum computing differs from classical computing in three ways:How information is represented: In classical computing, a computer runs on bits that have a value of either 0 or 1.
Therefore, we moved quantum computing closer to the horizon in the 2021 Life Sciences Radar and expect it to become widely adopted across life sciences in the next three years and become a mainstream technology in five years."
90,https://metrology.news/machine-learning-fault-detection-to-deliver-on-smart-factory-aerospace-fasteners-production-line/,Machine Learning Fault Detection to Deliver on Smart Factory Aerospace Fasteners Production Line – Metrology and Quality News,2021-02-11 06:12:08+00:00,"LISI Aerospace (BAI UK) and the University of Sheffield Advanced Manufacturing Research Centre (AMRC) have jointly secured £975,000 ($1,365,000) in funding from the Aerospace Technology Institute (ATI) to unlock productivity gains, new markets and reduce waste in the fastener industry through the integration of Industry 4.0 technologies.

The pilot production line for high-precision aerospace fasteners, at BAI UK’s Rugby facility, will help define the ‘smart factory’ by pioneering the use of machine learning, data analytics, indirect fault detection, and other cutting-edge digital technologies. The aim is to deliver a smart, dynamic manufacturing line that minimises waste, improves worker safety and efficiency, and sets a new benchmark for productivity in the aerospace fastener industry.

“The investment shows that BAI UK are committed to advancing the fastener industry with a unique and impactful approach; the award of ATI funding adds credibility to that ethos,” said Mark Capell, General Manager of BAI UK.

The company is part of LISI Aerospace, the third largest supplier of aerospace fasteners globally, who are searching for proactive steps they can take to lead their industry through innovation in the face of international competition, tighter margins and an ageing workforce.

Mr Capell said there is an enormous diversity in the applicability of the investment: “The project goes far beyond the technology, it serves as an investment in the remarkable manufacturers in the UK, the fantastic workers we employ, and does so in a more environmentally sensitive way. The project will reduce wastage and operator interaction; ultimately aiming to provide an all-round better manufacturing process.”

The AMRC, part of the High Value Manufacturing (HVM) Catapult, is an internationally recognised leader in manufacturing research, with specific expertise around Industry 4.0 technologies at its flagship Factory 2050 in Sheffield, the place where digital meets manufacturing.

Gavin Hill, Project Manager at Factory 2050, said: “This project shows the value of Industry 4.0 to all levels of the UK supply chain and has the potential to break down barriers to technology adoption which can push UK manufacturing into a new era.

“Implementing these technologies is also a chance to reduce environmental impact by reducing waste and providing the data for LISI to understand where they are wasting energy and where they can get better life out of tooling and consumables in a more sustainable way.”

BAI UK and the AMRC will work closely with machine builders and tooling providers throughout the 27-month programme of work to help streamline the technology introduction process, as well as investigate how these technologies can add value to both aerospace and the wider manufacturing world.

For more information: www.amrc.co.uk





HOME PAGE LINK","['waste', 'value', 'aerospace', 'fault', 'smart', 'factory', 'fasteners', 'industry', 'production', 'bai', 'project', 'machine', 'manufacturing', 'technology', 'metrology', 'line', 'learning', 'uk', 'technologies', 'quality']","LISI Aerospace (BAI UK) and the University of Sheffield Advanced Manufacturing Research Centre (AMRC) have jointly secured £975,000 ($1,365,000) in funding from the Aerospace Technology Institute (ATI) to unlock productivity gains, new markets and reduce waste in the fastener industry through the integration of Industry 4.0 technologies.
The pilot production line for high-precision aerospace fasteners, at BAI UK’s Rugby facility, will help define the ‘smart factory’ by pioneering the use of machine learning, data analytics, indirect fault detection, and other cutting-edge digital technologies.
The aim is to deliver a smart, dynamic manufacturing line that minimises waste, improves worker safety and efficiency, and sets a new benchmark for productivity in the aerospace fastener industry.
“The investment shows that BAI UK are committed to advancing the fastener industry with a unique and impactful approach; the award of ATI funding adds credibility to that ethos,” said Mark Capell, General Manager of BAI UK.
For more information: www.amrc.co.ukHOME PAGE LINK"
91,https://www.rtinsights.com/6q4-nlp-expert-neta-snir-on-how-ai-can-save-lives/,"6Q4: NLP Expert Neta Snir, on How AI Can Save Lives",2021-02-10 08:33:38+00:00,"NLP Expert Neta Snir explains how NLP can be used to detect high-risk situations online and save lives.

Our “6Q4” series features six questions for the leaders, innovators, and operators in the real-time analytics arena who are leveraging big data to transform the world as we know it.



RTInsights recently asked NLP expert Neta Snir, who is a speaker at the Women On Stage Conference taking place February 10-11th, about how AI can be used for good, her journey, and more.



Q1: When did you become interested in tech and what is your technical background?

Neta Snir

During the second year studying for my degree in mathematics I noticed two of my fellow students were occasionally disappearing for long days off campus. I had to ask, and they told me about this other subject that they majored in called generative linguistics. The following semester I was registered at the linguistics department. I would always try to take the classes that were more about logic in language, and cognitive science, but I thought I was going to become a professor. Until I hit “NLP 101” with my dear teacher Dr. Mori Rimon who got me hooked, and I never looked back since.



Q2: What is your current position and how did you get there?

I’m an NLP Expert, I work with early-stage startups on building their NLP and data science technology, usually from scratch. My expertise is in bringing tech into entrepreneurship and helping C-level management to think data.

I’ve always been a tech geek and I find that being a freelancer is a great way for me to continuously engage with various technologies, trends and innovative companies, and simply to stay in sync with the ecosystem.



Q3: You’ve developed technology for detection of suicidal posts over the Internet. Can you describe the application and how it is used?

For seven years, I have volunteered in a helpline NGO as part of the “detection force,”a team of trained volunteers who would scan the internet and read hundreds of web posts daily, in order to find cries for help and reach out to them on time. It was natural for me to seek for an automated way to do it.

There’s a lot of knowledge that you get seeing all those cases – the language, the patterns, the colors… Something there tells you that there’s more than the text uncovers. But it takes years to train a human to pick up the intuition. It’s not always the explicit words that describe despair. Sometimes silence is also informative, as people choose when to respond to their commenters and what to respond to. My work was to take this “gut feeling” and turn it into measurable features that are machine readable. There’s a whole book of “web emotional behaviors” to be written, and I believe that AI is necessary for writing at least some of its chapters.



Q4: What does the application do when it detects something? For example, does it escalate the situation, alert a human, or something else?

Using technologies like mine, the detection process of emotional posts and high-risk situations becomes fast and efficient, but that’s not enough if you’re not connecting the ends: Any detection technology needs an alerting system that interacts with emergency care or help centers, or monetization apps. The goal is to achieve traceability and auditability in streamlining the info, but there needs to be a human there to receive it, reach out to the person at risk and provide them with the required help.



Q5: How was it before and why does a solution like this do a better job than humans?

The internet is where you exist, now more than ever with Covid-19 and its global effect on social disassociation. But when you turn to the internet, you never know what you’re going to find. Since trained human resources are not necessarily accessible to everyone, and many people don’t actually turn to get professional help, anyone is basically a helper (or not). There becomes a need for a positive online presence that can be this connection between people who are seeking for help, evidently or implicitly, and the actual help they deserve to get. Algorithms that are trained to be that, would be able to scale, avoid cognitive bias, adapt quickly to new trends and slang, and of course respond faster than any human.



Q6: What other high-risk situations can it be applied to?

Similar technologies could be applicable for detection of bullying, sexual harassment, loneliness, and any other emotional behaviors that you’re interested in indicating. Each has its own map of features and needs to be developed independently. I’m now working with a pioneering startup called MoodKnight that develops a distress detection tool, and I see a great future of using AI for Impact.



Bonus Question: What advice, comments, or tips do you have for other women in tech?



Everyone, in any industry: Find your squad. Collaborate with peers and colleagues that are sharing your journey. It’s reassuring to know where you stand, it’s crucial to see where you can get, and it’s good karma to show others they can get there too.","['save', 'youre', 'neta', 'expert', '6q4', 'turn', 'ai', 'help', 'detection', 'snir', 'nlp', 'technologies', 'human', 'internet', 'trained', 'technology', 'lives']","NLP Expert Neta Snir explains how NLP can be used to detect high-risk situations online and save lives.
RTInsights recently asked NLP expert Neta Snir, who is a speaker at the Women On Stage Conference taking place February 10-11th, about how AI can be used for good, her journey, and more.
Until I hit “NLP 101” with my dear teacher Dr. Mori Rimon who got me hooked, and I never looked back since.
I’m an NLP Expert, I work with early-stage startups on building their NLP and data science technology, usually from scratch.
Since trained human resources are not necessarily accessible to everyone, and many people don’t actually turn to get professional help, anyone is basically a helper (or not)."
92,https://www.theengineer.co.uk/new-study-uses-wireless-signals-for-emotion-detection/,New study uses wireless signals for emotion detection | The Engineer,2021-02-10 12:12:29+00:00,"New research from Queen Mary University of London explores how an AI approach based on wireless signals could lead to new methods of emotion detection.

Published in the journal PLOS ONE, the study demonstrates the use of radio waves to measure heart rate and breathing signals to predict how someone is feeling in the absence of other visual cues such as facial expressions.

PhD student at QMUL, Achintha Avin Ihalage, said their proposed deep learning approach is a novel neural architecture that can process time-dependent wireless signal and frequency-domain wavelet transformation images simultaneously while preserving temporal and spatial relationships.

While the ‘basic building blocks’ used to implement the neural network are well known and widely adopted, added first author and PhD student Ahsan Noor Khan, the method of evoking emotions and its combination with wireless signals for unobtrusive sensing of breathing and heart rate is potentially new and could open doors for new opportunities in wireless emotion detection.

“Being able to detect emotions using wireless systems is a topic of increasing interest for researchers as it offers an alternative to bulky sensors and could be directly applicable in future ‘smart’ home and building environments,” said Noor Khan. “In this study, we’ve built on existing work using radio waves to detect emotions and show that the use of deep learning techniques can improve the accuracy of our results.”

Comment: How AI and robotics are transforming healthcare

During the study, a group of participants were asked to watch a video selected by researchers for its ability to evoke anger, sadness, joy or pleasure. Whilst the individual watched the video, researchers then emitted harmless radio signals toward the individual and measured the signals that bounced off them. By analysing the changes to these signals, caused by slight body movements, researchers said they could reveal ‘hidden’ information about an individual’s heart and breathing rates.

Previous research has used similar non-invasive or wireless methods of emotion detection, however in these studies data analysis has depended on the use of classical machine learning approaches, whereby an algorithm is used to identify and classify emotional states within the data.

“Based on our results, this technology can detect emotions at an accuracy of 71 per cent,” Avin Ihalage commented. “The experiment was conducted on 15 participants and no female participants were involved. Ideally, a widespread study involving more participants should be performed to evaluate the generalisability of this method. We note that increasing the number of emotions considered could be useful for future applications – again, this requires more data.”

The team believes the technology could have significant implications for the management of health and wellbeing and plan to work with healthcare professionals and social scientists on public acceptance.

Noor Khan said that the current focus is to use the technology primarily for the healthcare sector, adding that while it has potential applications in many other social sectors, the long-term commercialisation of the technology needs to consider the associated ethical concerns, such as privacy breach and data protection.

For future work, he explained that the team aims to evaluate their methods in an uncontrolled environment, where there may be multiple people or moving objects, to realise more practical and real life scenario based emotion detection schemes using deep learning.

Professor Yang Hao, project lead, said that the research could also open up opportunities in areas such as human/robot interaction.","['wireless', 'participants', 'using', 'study', 'researchers', 'signals', 'emotion', 'engineer', 'uses', 'detection', 'emotions', 'work', 'technology']","New research from Queen Mary University of London explores how an AI approach based on wireless signals could lead to new methods of emotion detection.
Whilst the individual watched the video, researchers then emitted harmless radio signals toward the individual and measured the signals that bounced off them.
“Based on our results, this technology can detect emotions at an accuracy of 71 per cent,” Avin Ihalage commented.
Ideally, a widespread study involving more participants should be performed to evaluate the generalisability of this method.
Professor Yang Hao, project lead, said that the research could also open up opportunities in areas such as human/robot interaction."
93,https://www.techgenyz.com/2021/02/10/google-cloud-finds-verloop-ios/,Google Cloud finds Verloop.io’s conversational AI provides end-to-end robotic customer support,2021-02-10 00:00:00,"Google Cloud finds Verloop.io’s conversational AI provides end-to-end robotic customer support

Google Cloud results in Verloop.io provides 24/7, fully automated online customer support for 15,000 companies globally.

Support brands with 10+ languages such English, Hindi, Arabic, Bahasa, Konkani, Tamil, Telugu, Kannada, Hinglish, etc., with multiple dialects AI chatbot developed on Google Cloud.

Enables instant delivery of AI model updates to client platforms with Cloud SQL and Cloud CDN Guarantees seamless global customer support with sub-millisecond latency INDIA, 10, February 2021, Verloop.io, the world’s leading customer support automation platform, has been a pioneer in automating customer support with its deep and insightful machine learning patterns formed across languages and dialects since 2015.

This startup, which now supports over 15,000 companies in over ten languages such as English, Hindi, Arabic, Konkani, Tamil, Telugu, Kannada, Hinglish, and more, has developed its product on Google Cloud. In a detailed case study on Google Cloud, Gaurav Singh, the CEO & Founder of Verloop.io, elucidates how the platform helped them concentrate on core resources allowing time and mind space for achievable innovations.

Google Kubernetes Engine, Google AI, and data storage tools give us the scaling power, low-latency, and creative freedom to turn our vision into reality, – said Gaurav Singh.

Verloop.io’s chatbot devised a solution specific to a few key industries in a one-size-fits-all world: e-commerce, banking, real estate, and a deep insight into the end-to-end journey of these sectors. The advanced chatbot can now provide end-to-end robotic support in 92% of cases without human intervention.

The Google case study finds Verloop.io also depends on Cloud Logging and Cloud Monitoring to offer 24/7 customer support even while running major system upgrades and unexpected demand spikes. The startup once saw peak loads of six to ten times its usual load over two quarters, but Google Cloud had seamlessly accommodated the excess traffic, without the team having to worry about it.

Added to that, being a part of the cloud infrastructure helps Verloop.io’s engineers solve problems as they happen. “An integrated logging, monitoring, and alerting system allow our engineers to trade issues across the microservice landscape and identify and resolve problems quickly,” said Singh. Verloop.io innovated an Oracle-like chatbot that can speak multiple languages; it uses various Google technologies and its natural language processing stack (NLP), including TensorFlow. “More than 80% of our natural language queries are answered using transformer-based models, a key contribution from Google Research,” said Singh.

The full article on the Google Cloud case study is available here: About Verloop.io: Verloop.io is the world’s leading customer support automation platform that enables businesses to deliver delightful support experiences to their customers across channels.

Verloop.io helps brands securely, effortlessly, and accurately scale up their customer support and is used by over 5,000 brands globally, including Decathlon, Cleartrip, Dar Al Arkan, Fetchr, Livpure, Adani Capital, DSP Mutual Fund, Rentomojo, Scripbox, and many more. For more information, visit Verloop.io or follow them on LinkedIn and Twitter.

About Google Cloud: Google Cloud provides organizations with leading infrastructure, platform capabilities, and industry solutions. We deliver enterprise-grade cloud solutions that leverage Google’s cutting-edge technology to help companies operate more efficiently and adapt to changing needs, giving customers a foundation for the future. Customers in more than 150 countries turn to Google Cloud as their trusted partner to solve their most critical business problems.","['verloopio', 'endtoend', 'conversational', 'study', 'chatbot', 'customer', 'finds', 'support', 'verloopios', 'ai', 'provides', 'platform', 'google', 'cloud', 'robotic', 'languages']","Google Cloud finds Verloop.io’s conversational AI provides end-to-end robotic customer supportGoogle Cloud results in Verloop.io provides 24/7, fully automated online customer support for 15,000 companies globally.
Support brands with 10+ languages such English, Hindi, Arabic, Bahasa, Konkani, Tamil, Telugu, Kannada, Hinglish, etc., with multiple dialects AI chatbot developed on Google Cloud.
Google Kubernetes Engine, Google AI, and data storage tools give us the scaling power, low-latency, and creative freedom to turn our vision into reality, – said Gaurav Singh.
About Google Cloud: Google Cloud provides organizations with leading infrastructure, platform capabilities, and industry solutions.
Customers in more than 150 countries turn to Google Cloud as their trusted partner to solve their most critical business problems."
94,https://economictimes.indiatimes.com/tech/startups/vernacular-ai-makes-key-hires-to-leadership-team-ramps-up-headcount/articleshow/80830313.cms,"Vernacular.ai makes key hires to leadership team, ramps up headcount",,"Find this comment offensive?

Choose your reason below and click on the Report button. This will alert our moderators to take action

Name

Reason for reporting:

Foul language

Slanderous

Inciting hatred against a certain community","['moderators', 'community', 'click', 'team', 'leadership', 'comment', 'languageslanderousinciting', 'ramps', 'key', 'reportingfoul', 'reason', 'makes', 'headcount', 'vernacularai', 'hires', 'report', 'offensivechoose', 'hatred']","Find this comment offensive?
Choose your reason below and click on the Report button.
This will alert our moderators to take actionNameReason for reporting:Foul languageSlanderousInciting hatred against a certain community"
95,https://techcrunch.com/2021/02/10/scalarr-series-a/,TechCrunch is now a part of Verizon Media,2021-02-10 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
96,https://customerthink.com/2021-emerging-ai-trends-in-the-telecom-industry/,2021: Emerging AI trends in the telecom industry,,"Tweet

No longer limited to providing basic phone and Internet service, the telecom industry is at the epicenter of technological growth, led by its mobile and broadband services in the Internet of Things (IoT) era. This growth is expected to continue: the IoT telecom services market was estimated to grow from $2.90 billion in 2016 to $17.67 billion in 2021, at a CAGR of 43.6%. The driver for this growth? Artificial intelligence (AI).

Emerging trends in telecom sector

AI applications are revolutionizing the way telecoms operate, optimize and provide service to their customers

Today’s communications service providers (CSPs) face increasing customer demands for higher quality services and better customer experiences (CX). Companies are addressing these opportunities by leveraging the vast amounts of data collected over the years from their massive customer base. This data is culled from devices, networks, mobile applications, geolocations, detailed customer profiles, services usage and billing data. Telecoms are harnessing the power of AI to process and analyze these huge volumes of Big Data in order to extract actionable insights to provide better customer experiences, improve operations, and increase revenue through new products and services.

Let’s take a look at some of the industry trends that are being driven by the expansion into AI: Network optimization, preventive maintenance, Virtual Assistants, and robotic process automation (RPA).

Network optimization

AI is essential for helping CSPs build self-optimizing networks (SONs), where operators have the ability to automatically optimize network quality based on traffic information by region and time zone. AI applications trending in the telecommunications industry use advanced algorithms to look for patterns within the data, enabling them to both detect and predict network anomalies, and allowing operators to proactively fix problems before customers are negatively impacted.

According to Gartner, the number of CSPs investing in artificial intelligence (AI) technologies for improving their infrastructure planning, operation, and products will rise from 30% in 2020 to 70% in 2025. Some popular AI solutions are ZeroStack’s ZBrain Cloud Management, which analyzes private cloud telemetry storage and use for improved capacity planning, upgrades and general management; Aria Networks, an AI-based network optimization solution that counts a growing number of Tier-1 companies as customers, and Sedona Systems’ NetFusion, which optimizes the routing of traffic and speed delivery of 5G-enabled services like AR/VR. Nokia launched its own machine learning-based AVA platform, a cloud-based network management solution to help CSPs automate network operations and deliver service assurance.

Predictive maintenance

AI-driven predictive analytics are one of the latest trends helping telecoms provide better services by utilizing data, sophisticated algorithms and machine learning techniques to predict future results based on historical data. This means telecoms can use data-driven insights to monitor the state of equipment, predict failure based on patterns, and proactively fix problems with communications hardware, such as cell towers, power lines, data center servers, and even set-top boxes in customers’ homes.

In the short-term, network automation and intelligence will enable better root cause analysis and prediction of issues. Long term, these technologies will underpin more strategic goals, such as creating new customer experiences and dealing efficiently with business demands.

AT&T is using machine learning to enhance their end-to-end incident management process by detecting network issues in real-time. The technology can address 15 million alarms per day, restoring service before the customers notice an outage. The company is also relying on AI to support its maintenance procedures: the telecom giant is using drones to expand its LTE network coverage and to utilize the analysis of video data captured by drones for tech support and infrastructure maintenance of its cell towers. Dutch telecom KPN – in partnership with Accenture – is using ultra-high-definition cameras that leverage 5G to scan and analyze wide areas of connected piping in real-time, identifying high-risk corrosion areas and determining the best corrective actions.

Virtual Assistants

Conversational AI platforms – known as virtual assistants – have learned to automate and scale one-on-one conversations so efficiently that the market is expected to grow to $13.9 billion by 2025, a CAGR of 21.9% from 2020-2025. Virtual assistants are an emerging trend in this sector, tapped to help contend with the massive number of support requests for installation, set up, troubleshooting and maintenance, which often overwhelm customer support centers. Using AI, telecoms can implement self-service capabilities that instruct customers how to install and operate their own devices.

Vodafone’s website-located AI assistant, Julia, can assist customers with a range of tasks from technical support to invoicing queries, and then feeds critical, insightful data back to Vodafone to aid in future decision-making. Another Vodafone chatbot — TOBi – has already launched in 11 markets and handles a range of customer service-type questions. The chatbot scales responses to simple customer queries, thereby delivering the speed that customers demand. Deutsche Telekom’s chatbot, Tinka, acts as a search engine by providing targeted assistance to customers in Austria (see her at the bottom right of screen). The 20% of queries Tinka is unable to handle gets passed to a human agent for follow up. Deutsche Telekom also relies on recruitment chatbot Hub:rom, a conversational AI assistant that fields questions about job offers and other personnel-related issues.

Voice assistants, such as Telefónica’s Aura, are designed to reduce customer service costs generated by phone inquiries. Comcast has also introduced a voice remote that allows customers to interact with their Comcast system through natural speech. Similarly, DISH Network’s partnership with Amazon’s Alexa allows customers to search or buy media content by spoken word rather than remote control. Integrating visual support within IVR further delivers an efficient usage of time – reducing average handling times (AHT) and customer hold times, and ultimately driving a better CX.

Robotic process automation (RPA)

CSPs all have vast numbers of customers and an endless volume of daily transactions, each susceptible to human error. Robotic Process Automation (RPA) is a form of business process automation technology based on AI that is one of the latest technology trends. RPA can bring greater efficiency to telecommunications functions by allowing easier management of their back-office operations and the large volumes of repetitive and rules-based processes. By streamlining execution of once complex, labor-intensive and time-consuming processes such as billing, data entry, workforce management and order fulfillment, RPA frees CSP staff for higher value-add work.

CSPs in 2021 are recognizing that applying RPA to alleviate even some of the staff’s workload will have a major impact on streamlining processes and improving profitability. AT Kearney reports that RPA costs 1/3 as much as an offshore employee and 1/5 the cost of on-site staff, and can cut costs by 25-50%. With these numbers, it’s no surprise that Forrester data shows that over 44% of customer service organizations are already using RPA to help them gain a competitive advantage.

Celaton helps companies streamline inbound data, such as emails, web forms and posts, extracts key data for the correspondence, validates it and presents a suggested response to a service rep, who then amends the message before responding to the customer. Kryon assists telecoms with identifying key processes to automate in support of both digital and human workforces for optimal process efficiency. AutomationEdge helps network providers automate data entry, invoice processing, and responses to queries.

Top AI Trends in the Telecom Industry

Artificial intelligence applications are among the latest trends in the telecom industry, increasingly helping CSPs manage, optimize and maintain not only their infrastructure, but their customer support operations as well. Network optimization, predictive maintenance, virtual assistants and RPA are examples of use cases where AI has impacted the industry, delivering an enhanced CX and added value for the enterprise overall. Technology is already a big part of the telecommunications industry, and as Big Data tools and applications become more available and sophisticated, AI can be expected to continue to grow in this space into 2021 and beyond.

This article was first published on the TechSee blog.","['customers', 'network', 'data', 'telecom', 'customer', 'emerging', 'industry', 'ai', 'support', '2021', 'rpa', 'process', 'service', 'trends']","Let’s take a look at some of the industry trends that are being driven by the expansion into AI: Network optimization, preventive maintenance, Virtual Assistants, and robotic process automation (RPA).
Virtual assistants are an emerging trend in this sector, tapped to help contend with the massive number of support requests for installation, set up, troubleshooting and maintenance, which often overwhelm customer support centers.
Voice assistants, such as Telefónica’s Aura, are designed to reduce customer service costs generated by phone inquiries.
Robotic Process Automation (RPA) is a form of business process automation technology based on AI that is one of the latest technology trends.
Top AI Trends in the Telecom IndustryArtificial intelligence applications are among the latest trends in the telecom industry, increasingly helping CSPs manage, optimize and maintain not only their infrastructure, but their customer support operations as well."
97,https://www.information-age.com/the-3-ai-startups-revolutionising-the-defence-sector-123493776/,The 3 AI startups revolutionising the defence sector,2021-02-10 10:57:26+00:00,"The 3 AI startups revolutionising the defence sector

AI is helping revolutionise the defence and security sector.

This article explores three AI startups that are helping redefine the defence and security sector

The birth of artificial intelligence arguably took place during the Second World War when the Allies constructed the then-revolutionary decipherment machines that were designed to crack the Nazis’ Enigma code. These rudimentary computers were based on the theoretical foundations laid by Alan Turing, himself an integral part of the Allied code-breaking efforts at Bletchley Park. Indeed, throughout history, military and defence organisations have leveraged intelligence and technology to their strategic advantage, as the Allies did with Turing and his ground-breaking insights into the theory of computation.

This radical technological development usually filters back into wider society. The armed forces have long had a reputation for turning their R&D into successful industries in the civilian world — many technologies we use today have emerged from military laboratories: GPS, for example, was developed during navy experiments in the 1960s, while the US Defence Advanced Research Projects Agency (DARPA) — created after the unexpected Soviet launch of Sputnik — has been a factory of innovation and an accelerator for progress in applied sciences.

And yet, this military resourcefulness seems to have dried up — at least in the UK. The Ministry of Defence’s annual expenditure for R&D is around £1.3 billion, far less than what many private companies are willing to spend. This can be explained by a shortage of funding: the MoD is unwilling to bear the costs and risks of developing new equipment. New projects are expensive and often take a long time to get off the ground. The Royal Navy’s plan to build two new aircraft carriers, for example, has been a protracted affair, and one which has seen costs (over £6.4 billion) ballooning out of control.

In response to these shortcomings, small private companies from the UK’s world-leading defence industry are now beginning to supply the innovative research so desperately needed by the army. A new generation of venture-backed tech start-ups believe that they can revolutionise traditional defence procurement — long thought to be obsolete — by bringing advanced civilian technologies into the defence and security sector. The adoption of this cutting-edge research will drive modernisation and transform military facilities using home-grown technology.

The government’s recent increase in defence spending is likely to accelerate this boom in military tech companies. Last month, Britain approved the largest rise to the defence budget since the Cold War, pledging an additional £16.5 billion for defence over the next four years. The prime minister said the extra cash would go towards the creation of a new space command and artificial intelligence agency. This is good news for AI startups which are expected to contribute to these nascent areas of military capability.

Indeed, the long-term strategic value of AI has been continuously reiterated by the intelligence and defence communities in the UK; they believe that technological superiority in emerging arenas of conflict is a necessity for this country. The four-star generals and chiefs of staff see artificial intelligence as having a multi-faceted applicability across theatres of war, whether that be on land, sea, or air.

In the future, they argue, AI could identify enemy submarines from complex sonar data. It could run automated computer-network defences that detect cyber-attacks and then patch up vulnerabilities ad hoc. It could use machine learning algorithms to filter through hours of video footage and then isolate suspected terrorists and insurgents. It could shoot down drones, aim tank guns, plan artillery barrages, and piece together difference sensor feeds into a more complete picture.

Some of these innovations are already in development. Here are three AI startups redefining the defence sector:

1. Adarga

Founded in 2006, British company Adarga — the etymology of which comes from the word for an old Moorish shield — has developed a proprietary AI engine that allows its clients to analyse large, unstructured datasets with the detection of meaningful patterns and anomalies. Put simply, it compresses ever-growing mountains of data into manageable and evaluable insights. This automated analysis is fast and efficient and has the potential to unlock hidden value in data. It also frees up human expertise from time-intensive tasks and allows them to concentrate on more complex assignments.

The company was set up by Rob Bassett Cross, a former army officer and special operations expert, who spotted AI’s potential for military intelligence-gathering while fighting in Iraq and Afghanistan. The problem he faced there — and afterwards as an investment banker at JP Morgan — was a simple one: too much information and not enough time to properly scrutinise it. This had serious repercussions: in a warzone, overlooked data can cost lives.

The UK government is a client: Adarga currently supplies its services to Strategic Command — the organisation which oversees the UK’s armed forces — as well as the finance and insurance sectors.

AI analytics is a potentially lucrative market. The dark arts of data science have yielded considerable financial success for Adarga’s US counterparts. The secretive Palantir — a big data company founded by tech billionaire Peter Thiel — is thriving for example; its data-sieving platform is used by the US military (as well as financial corporations and the NHS, among others).

AI predictions: how AI is transforming five key industries What are the key industries that are ripe for disruption as AI becomes more pervasive? Here, we explore the biggest AI predictions. Read here

2. Rebellion Defense

The Washington DC-based software startup Rebellion Defense offers customers AI and machine learning tools related to data analysis, cyber security, and communications. Although its AI capabilities are still shrouded in secrecy, the company sells its software to governmental clients including the US Navy and the UK’s MoD.

One area Rebellion has focussed on is the analysis of video filmed via drone. Here, there are striking similarities to the ill-fated Project Maven, a Google-backed project undertaken with the Pentagon to develop AI for the US drone programme. Not everybody in the Silicon Valley behemoth was happy about the prospects of working on defence contracts and Google eventually mothballed its plans after more than 3,000 employees protested in 2018.

Rebellion is led by co-founder and CEO Chris Lynch, who was integral in launching the Pentagon’s Defence Digital Service (DDS). As director of the DDS, he also helped set up the $10 billion Joint Enterprise Defence Infrastructure (JEDI) contract for cloud computing. Lynch’s strong connections with both the Pentagon and individual start-ups points to the ever-growing relationship between governmental departments and the technology sector.

3. Improbable Defence

A subdivision of Improbable Worlds — the gaming technology business — the defence side of this business offers customers a war-gaming service. This enables the construction of simulated environments, within which clients can play out military scenarios, investigating how factors like troop morale and ammunition supplies affect the probability of a successful operation.

The logic behind Improbable’s modelling platform is that their virtually-created worlds can be used to realistically simulate the complexity of the modern world, without any of the risks. This allows defence clients to experiment, test plans, conduct training and improve overall organisational preparedness before taking real-world decisions. According to Improbable’s website, synthetic environments provides insight to “those asking questions of complex systems”.

The start-up has just signed another 12-month contract with the UK’s Strategic Command while the MoD has already spent more than £8.3 million on Improbable’s software, a figure that will increase significantly over the next year. The British company has also just expanded into the US, hiring a number of Pentagon officials as advisors.

AI: redefining defence and security

In a rapidly evolving world, AI is redefining the defence and security landscape. A growing number of AI startups are seeking new commercial opportunities for their technology in defence contracting. They see lucrative room for their software-as-a-service products in a sector that has traditionally been dominated by established companies like Lockheed Martin and Northrop Grumman.

This is important because state-level adversaries — such as Russia and China — are already embracing the proliferating developments in the technology of war, and successful data breaches like the recent ransomware hack of CD Projekt are becoming increasingly common.

The UK’s forces cannot match these adversaries in terms of quantity. Rather, superiority will be achieved by greater quality. Just as the Allies utilised Turing’s ingenuity to decipher the Enigma code and turn the tide against the Axis powers in the Second World War, so Britain today has again begun to realise the value of technological dominance in warfare. AI is one part of this.

Written by Archie Phillpotts, freelance journalist

This article is tagged with:","['startups', 'world', 'uks', 'war', 'data', 'military', 'ai', 'revolutionising', 'security', 'technology', 'sector', 'defence']","The 3 AI startups revolutionising the defence sectorAI is helping revolutionise the defence and security sector.
This is good news for AI startups which are expected to contribute to these nascent areas of military capability.
Here are three AI startups redefining the defence sector:1.
AI: redefining defence and securityIn a rapidly evolving world, AI is redefining the defence and security landscape.
A growing number of AI startups are seeking new commercial opportunities for their technology in defence contracting."
98,https://www.medianama.com/2021/02/223-sebi-upgrading-it-infrastructure/,"SEBI to focus on IT infrastructure, data analytics to improve supervision",2021-02-22 00:00:00,"The Securities and Exchange Board of India (SEBI) is upgrading its Information Technology (IT) resources with a particular focus on data analytics to improve its supervision of the securities market, it said in its annual report for 2019-20. Over the course of the coming year, the markets regulator will build a new data centre, which will host a dedicated private cloud infrastructure as well.

“SEBI has started work on a data lake project which would have the capability to store and retrieve quickly, a large amount of structured, semi structured and unstructured data. This data lake project will support advanced analytical tools, such as artificial intelligence and machine learning (AI/ML), deep learning, big data analytics, pattern recognition, processing of structured and unstructured data, text mining and natural language processing thereby significantly augmenting surveillance capabilities,” said Ajay Tyagi, chairman, SEBI.

As part of its supervisory functions, SEBI has developed an in–house automated system to detect misuse of client securities by brokers, and is in the process of implementing a project to automatically inspect and surveil mutual fund activity, Tyagi said. “Further, in order to detect possible market manipulation and strengthen market supervision through technology solutions, SEBI has initiated a Data Analytics and Data Models Project,” he added.

Cloud Strategy and Data Centre

SEBI has adopted private cloud strategy for a large part of its infrastructure. “The proposed private cloud solution will be the enabling technology which will be scalable, inter-operable, and elastic and support big data analytics. Going forward, the private cloud solution will provide infrastructure, storage and computing capacity to many existing and upcoming projects,” it said.

The private cloud strategy will help the markets regulator optimise its resource utilisation and provide flexibility across various hardware devices and softwares. It will also allow the markets regulator to integrate, host and scale new or existing applications much faster, it said. This cloud strategy will have the highest level of security and will be integrated to SEBI’s SOC-NOC, the regulator added.

SEBI will implement a large scale tier 3+ data centre this year, at its Bandra Kurla Complex office in Mumbai. As part of building the data centre, existing infrastructure from other data centres in Mumbai will be consolidated to the new data centre, SEBI said. It added that the new data centre will also host the regulator’s private cloud infrastructure.

Building Data Analytics

“Given the increasing volume and complexity of activity in the securities market and the resultant explosion in volume and complexity of data, SEBI has felt an increasing need to develop data analytics capability within the organization,” SEBI said. The regulator hopes this initiative will help in policy formulation, surveillance and investigations and automating various functions going forward.

The markets regulator said that it has adopted a ‘Hub and Spoke’ model as part of its data analytics initiative. While the data warehouse team of the IT department will act as a Hub for data residing with the regulator, individual data analytics teams in each of SEBI’s various departments will be the Spokes. It plans to use this model to implement two projects, namely off-site inspection of brokers and offsite inspection of mutual funds.

SEBI also plans to implement a data lake in 2021, which will leverage artificial intelligence, machine learning and deep learning that will analyse and process vast amounts of structured, semi structured and unstructured data.

In June 2019, the Ministry of Corporate Affairs and SEBI signed an Memorandum of Understanding (MoU) to create a digital mechanism to share information bi-laterally. The regulator will sign the next MoU with the Central Board of Direct Taxes in the coming months, it said.

Cyber Security Initiatives

“Considering the increasing trend of cyber frauds and attacks across the globe especially in financial institutions, it was decided to strengthen further, the cyber security controls and processes in SEBI,” the markets regulator said. It has created a Security Operations Center/ Network Operations Center (SOC-NOC), which will function 24×7 to give real-time visibility on SEBI’s entire IT system. The SOC also creates automated alerts against suspicious or malicious activity on SEBI’s computers, as well as on sensitive data residing in databases when hackers attempt to attack it.

SEBI also implemented intrusion prevention systems, encrypted traffic management systems in addition to end-point protection solutions on its computers, servers, email system and for its staff’s mobile phones. The markets regulator now has an advanced anti-malware software as well as anti-phishing software. It also implemented new solutions to prevent Distributed Denial of Service and Domain Name Server attacks, it said.

“Having consolidated and integrated security systems for threat feed, forensic analysis, malware analysis, and so on, SEBI can make better security decisions with intelligence-driven analytics available with SOC… SOC at SEBI consists of multiple cyber security technologies and services. It is designed to secure processes and technologies and will help SEBI to protect confidentiality and privacy of regulatory and market data at various end points in SEBI… Cyber-attacks on such applications can be detected and mitigated proactively in an efficient and effective way through the SOC,” SEBI Annual Report 2019-20

Also Read","['structured', 'sebi', 'supervision', 'regulator', 'data', 'markets', 'cloud', 'analytics', 'security', 'focus', 'private', 'market', 'infrastructure', 'improve']","“Further, in order to detect possible market manipulation and strengthen market supervision through technology solutions, SEBI has initiated a Data Analytics and Data Models Project,” he added.
Cloud Strategy and Data CentreSEBI has adopted private cloud strategy for a large part of its infrastructure.
“The proposed private cloud solution will be the enabling technology which will be scalable, inter-operable, and elastic and support big data analytics.
As part of building the data centre, existing infrastructure from other data centres in Mumbai will be consolidated to the new data centre, SEBI said.
The markets regulator said that it has adopted a ‘Hub and Spoke’ model as part of its data analytics initiative."
99,https://www.businessinsider.in/science/health/news/ai-in-healthcare-is-set-to-open-up-a-range-of-futuristic-job-profiles-in-healthcare/articleshow/80833859.cms,AI in healthcare is set to open up a range of futuristic job profiles in healthcare,,"Artificial intelligence (AI) in healthcare will change how the industry operates in India provided it can find the talent to support this shift.

(AI) in healthcare will change how the industry operates in India provided it can find the talent to support this shift. According to a survey conducted by BML Munjal University , deep learning experts and AI experts will be one of the most in-demand skills for the healthcare sector as it goes through a digital transformation.

, deep learning experts and AI experts will be one of the most in-demand skills for the as it goes through a digital transformation. Vishal Talwar, the Dean of BML Munjal University, tells Business Insider what’s needed to make the shift in the healthcare sector a potential game-changer in the Indian context.

BI India

Advertisement

BI India

Advertisement

BML Munjal University/BI India

Advertisement

Advertisement

BI India

Advertisement

BI India

Advertisement

Advertisement

BI India

Artificial intelligence (AI) is bringing about a wave of change in nearly every sector of the economy, and healthcare is no exception to that rule. A new survey conducted by BML Munjal University shows that most executives across the country already have an AI strategy in place for their organisations. Execution of these strategies remains to be seen.“Public and private sector investments, government push, health startup boom etc. could provide the necessary fillip required to make this a potential gamechanger in the Indian context,” Vishal Talwar, the Dean of School of Management, — BML Munjal University, told Business Insider.High capital requirement is a key challenge in the implementation of AI. Cultural considerations, such as trust in AI technologies, is also a barrier that needs to be addressed. While consumers are open to use of AI, it cannot be considered a replacement for the soft skills that healthcare staff has to offer.However, before any of this can be addressed, the primary challenge for executives is finding the right talent. As the nature of work changes and the workforce needs to be reskilled, more executives are investing heavily in upgradation to keep up with their competition.Using AI is more efficient in diagnosing patients and reducing the rate of error. A 2020 study found that AI algorithms and deeping learning were able to diagnose breast cancer at a higher rate than 11 pathologists.While the demand for AI-related skills is set to boom, a whopping 96% of executives say that current healthcare education needs significant upgradation in India to keep pace with the rapid change in technology.“This gap will need to be addressed aggressively and the search will not be easy. Talent will need to be sought both within and outside the sector and thus approach to hiring as well as processes will be impacted,” said Talwar.Talwar proposes that, in the long run, the healthcare education sector will have to work on this along with the industry to expand the talent base and offer a more integrated approach to learning.“With higher infrastructure development costs, there may be a possibility of higher expenses in some quarters but organisations would be better off taking a long term view,” said Talwar.Executives also feel that the potential impact on jobs — reduction due to automation — is a big challenge given that low-skilled jobs will likely become redundant in the coming decade. But, it will also open up a whole new set of employment opportunities for the coming generation.“As AI continues to evolve in healthcare, there would be more jobs created for new skill sets, it is being integrated in healthcare organisations to assist with care provision, not replace it. Moreover, experts believe that such technologies will be an important contributor to better decision making and not necessarily replacement,” explained Talwar.Meanwhile, there will be a greater impetus on reskilling and upskilling of the current skill base across all ages. This is already a phenomenon across many sectors and is becoming imperative. “Continuing education will become rather relevant in all sectors including healthcare,” said Talwar.India’s healthcare system is one of the biggest in the world given that it caters to a population of 1.4 billion. But, it has a long way to go in terms of improving the quality of healthcare and making sure that everyone has access. And, that’s where AI comes in.Using AI, people in the most remote locations of India will be able to access medical healthcare from top professionals. Mundane tasks like scheduling or tracking medical inventory can be left up to algorithms, reducing the costs of running day to day business.In addition to operational efficiency, the use of chatbots and personalised solutions that can provide better patient experience in the healthcare space, which is highly competitive. But, these chatbots cannot be considered a replacement for actual human interaction. Soft skills will always have a role to play in patient care.More than half of the executives surveyed say that they are already seeing significant benefits from AI adoption.The recent experience in vaccine distribution in the US has thrown up challenges on inequality. Black and brown Americans are receiving fewer coronavirus shots , despite dying at higher rates. The logistical challenge is something that AI can help solve. It can forecast demand, clear out bottlenecks and give quality assurance that the right dose is administered.AI can also help monitor for any unusual side effects, especially with the new strains of COVID-19 popping up globally.This is also an opportunity in terms of jobs and long-lasting changes to the healthcare sector that can be applied to other challenges like the distribution of the polio vaccine and ensuring that children and pregnant mothers get the right level of nutrition.","['open', 'university', 'range', 'futuristic', 'skills', 'set', 'ai', 'experts', 'profiles', 'munjal', 'executives', 'jobs', 'sector', 'talent', 'healthcare', 'job']","Artificial intelligence (AI) in healthcare will change how the industry operates in India provided it can find the talent to support this shift.
(AI) in healthcare will change how the industry operates in India provided it can find the talent to support this shift.
According to a survey conducted by BML Munjal University , deep learning experts and AI experts will be one of the most in-demand skills for the healthcare sector as it goes through a digital transformation.
, deep learning experts and AI experts will be one of the most in-demand skills for the as it goes through a digital transformation.
Vishal Talwar, the Dean of BML Munjal University, tells Business Insider what’s needed to make the shift in the healthcare sector a potential game-changer in the Indian context."
100,https://www.cnet.com/news/experience-synesthesia-google-tool-lets-you-hear-colors-and-shapes/,Experience synesthesia: Google tool lets you 'hear' colors and shapes,,"Vassily Kandinsky/Google

When painter Vassily Kandinsky saw yellow, he heard trumpets. Looking at the color red made restless violins play in his head. Blue brought him the sounds of a calming organ.

The pioneering Russian abstract artist and art theorist had the neurological condition synesthesia, which allows some people to hear colors and shapes. Just imagine the singular symphonies that played as he splashed paint onto his vibrant multicolored canvases.

A new project from Google Arts & Culture called Play a Kandinsky lets you go beyond imagining to hear what Kandinsky might have heard as he looked at color. The interactive tool even lets you experience his abstract 1925 masterpiece Yellow Red Blue through sound by clicking around the artwork to hear a seven-movement composition that travels through colors and moods as Kandinsky described them.

Google

Experimental musicians Antoine Bertin and NSDOS worked with Google to study Kandinsky's writings detailing his multisensory perception. Google then applied machine learning to create a tool that simulates what Kandinsky might have heard as he created Yellow Red Blue. Depending on the colors and shapes you're hovering over, the piece can sound soothing or cacophonous, simple or complex.

The painting is part of Sounds like Kandinsky, an extensive online effort by The Centre Pompidou in Paris and Google Arts & Culture to preserve the artist's life, work and legacy by immersing you in his world. A tour of his 1940 oil painting Sky Blue lets you zoom in on the details, and photos and descriptions of his Paris studio on the banks of the Seine take you inside the space where he painted his final works.

But Play a Kandinsky brings the artist to life in a whole new way by immersing you in the synesthesia that played such a defining role in his creative process. Other artists such as Rimbaud, Billie Eilish and Pharrell Williams also say they have the condition.

""Color is the keyboard, the eyes are the harmonies, the soul is the piano with many strings,"" Kandinsky said. ""The artist is the hand that plays, touching one key or another, to cause vibrations in the soul.""","['experience', 'synesthesia', 'blue', 'colors', 'heard', 'lets', 'google', 'kandinsky', 'yellow', 'play', 'red', 'shapes', 'tool', 'hear']","The pioneering Russian abstract artist and art theorist had the neurological condition synesthesia, which allows some people to hear colors and shapes.
A new project from Google Arts & Culture called Play a Kandinsky lets you go beyond imagining to hear what Kandinsky might have heard as he looked at color.
The interactive tool even lets you experience his abstract 1925 masterpiece Yellow Red Blue through sound by clicking around the artwork to hear a seven-movement composition that travels through colors and moods as Kandinsky described them.
Google then applied machine learning to create a tool that simulates what Kandinsky might have heard as he created Yellow Red Blue.
Depending on the colors and shapes you're hovering over, the piece can sound soothing or cacophonous, simple or complex."
101,https://www.news-medical.net/news/20210210/Metabolomics-and-machine-learning-used-to-identify-possible-COVID-19-biomarkers.aspx,Metabolomics and machine learning used to identify possible COVID-19 biomarkers,2021-02-10 00:00:00,"One of the many mysteries still surrounding COVID-19 is why some people experience only mild, flu-like symptoms, whereas others suffer life-threatening respiratory problems, vascular dysfunction and tissue damage.

Now, researchers reporting in ACS' Analytical Chemistry have used a combination of metabolomics and machine learning to identify possible biomarkers that could both help diagnose COVID-19 and assess the risk of developing severe illness.

Although some pre-existing conditions, such as diabetes or obesity, can increase the risk of hospitalization and death from COVID-19, some otherwise healthy people have also experienced severe symptoms. As most of the world's population awaits vaccination, the ability to simultaneously diagnose a patient and estimate their risk level could allow better medical decision-making, such as how closely to monitor a particular patient or where to allocate resources.

Therefore, Anderson Rocha, Rodrigo Ramos Catharino and colleagues wanted to use mass spectrometry combined with an artificial intelligence technique called machine learning to identify a panel of metabolites that could do just that.

The cross-sectional study included 442 patients who had different severities of COVID-19 symptoms and tested positive by a reverse transcriptase-polymerase chain reaction (RT-PCR) test, 350 controls who tested negative for COVID-19 and 23 people who were suspected of having the virus despite a negative RT-PCR test.

The researchers analyzed blood plasma samples from the participants with mass spectrometry and machine learning algorithms, identifying 19 potential biomarkers for COVID-19 diagnosis and 26 biomarkers that differed between mild and severe illnesses. Of the COVID-19-suspected patients, 78.3% tested positive with the new approach, possibly indicating these were RT-PCR false negatives.

Although the identified biomarkers, which included metabolites involved in viral recognition, inflammation, lipid remodeling and cholesterol homeostasis, need to be further verified, they could reveal new clues to how SARS-CoV-2 affects the body and causes severe illness, the researchers say.","['used', 'tested', 'severe', 'possible', 'researchers', 'learning', 'symptoms', 'rtpcr', 'covid19', 'machine', 'biomarkers', 'metabolomics', 'identify', 'risk']","One of the many mysteries still surrounding COVID-19 is why some people experience only mild, flu-like symptoms, whereas others suffer life-threatening respiratory problems, vascular dysfunction and tissue damage.
Now, researchers reporting in ACS' Analytical Chemistry have used a combination of metabolomics and machine learning to identify possible biomarkers that could both help diagnose COVID-19 and assess the risk of developing severe illness.
Although some pre-existing conditions, such as diabetes or obesity, can increase the risk of hospitalization and death from COVID-19, some otherwise healthy people have also experienced severe symptoms.
Therefore, Anderson Rocha, Rodrigo Ramos Catharino and colleagues wanted to use mass spectrometry combined with an artificial intelligence technique called machine learning to identify a panel of metabolites that could do just that.
The researchers analyzed blood plasma samples from the participants with mass spectrometry and machine learning algorithms, identifying 19 potential biomarkers for COVID-19 diagnosis and 26 biomarkers that differed between mild and severe illnesses."
102,https://www.enterpriseai.news/2021/02/10/where-to-expect-enterprise-ai-growth-in-2021-more-predictions-from-our-ai-experts/,Where to Expect Enterprise AI Growth in 2021: More Predictions from Our AI Experts,2021-02-10 00:00:00,"Where to Expect Enterprise AI Growth in 2021: More Predictions from Our AI Experts

So many industry predictions for how AI use in business will change and grow in 2021 have come in to EnterpriseAI that we didn’t have room to share them all in our first predictions story on Feb. 1.

Now we are expanding that bounty of AI predictions riches with a second installment of 2021 predictions from our AI experts across industries, verticals and the world of IT.

Included are a wide range of opinions and insights from IT executives, leaders, experts and AI users in the field of AI, on everything from the AI marketplace to coming innovations over the next year. Don’t forget to read our earlier predictions roundup as well to get the full flavor of where AI could be heading in 2021.

Clement Farabet, vice president of AI infrastructure for GPU and AI chipmaker Nvidia, said he expects AI in 2021 to take on the role of a compiler. “As AI training algorithms get faster, more robust and with richer tooling, AI will become equivalent to a compiler — developers will organize their datasets as code, and use AI to compile them into models,” said Farabet. “The end state of this is a large ecosystem of tooling/platforms, just like today’s tools for regular software, to enable more and more non-experts to ‘program’ AIs. We’re partially there, but I think the end state will look very different than where we are today — think compilation in seconds to minutes instead of days of training. And we’ll have very efficient tools to organize data, like we do for code via Git today.

Another Nvidia executive, Charlie Boyle, a vice president and general manager of Nvidia DGX systems, said he sees the issue of shadow AI coming to the forefront inside companies that are experimenting with AI. “Managing AI across an organization will be a hot-button internal issue if data science teams implement their own AI platforms and infrastructure without IT involvement,” said Boyle. “Avoiding shadow AI requires a centralized enterprise IT approach to infrastructure, tools and workflow, which ultimately enables faster, more successful deployments of AI applications.”

At graph database vendor Neo4j, lead data science product manager Alicia Frame said she expects 2021 to shorten the timespan from when AI breakthroughs arrive in academia to when they show up in real-world use inside enterprises. This trend will translate into faster innovation cycles where new techniques discovered by researchers are rapidly scaled up and democratized for the masses, said Frame.

“With the computation power to crunch data getting cheaper and easier to access, and serverless technology making it easier to develop and deploy code, we’ll see data scientists getting back to focusing on the basics: solving big problems more effectively than anyone else,” she said. “In 2021, we’ll see a growing appreciation for the relationships between our data points – and the use of those relationships to make better insights – becoming more mainstream. Skills like graph algorithms, embeddings, and using graph convolutional neural networks will come into maturity.”

Dipti Borkar, the co-founder and chief product officer for data analytics vendor Ahana, expects AI to lean more on the promise of open source platforms to propel AI further over the year.

“We’ll see more data-driven companies leverage open source for analytics and AI in 2021,” said Borkar. “Open source analytics technologies like Presto and Apache Spark power AI platforms and are much more flexible and cost effective than their traditional enterprise data warehouse counterparts that rely on consolidating data in one place–a time-consuming and costly endeavor that usually requires vendor lock-in. We will see a rise in usage of analytic engines like Presto for AI applications because of its open nature - open source license, open format, open interfaces, and open cloud.”

Joanna Lowry-Duda, a machine learning research scientist for text analytics vendor Luminoso, said she believes that a trend of democratization of AI will be ever more prevalent in 2021. “There will be more toolkits, pre-trained models and datasets available for general consumption,” she said. “I hope that this will result in more understanding, specifically from business users, of what problems machine learning can solve (and how well) but also what its limitations are.”

At chipmaker Intel Corp., Christine Boles, the vice president of the internet of things group and general manager of the industrial solutions division, said she predicts the continuing acceleration of industrial transformation through the use of AI in 2021.

""The pandemic has greatly accelerated the need for companies to complete their Industry 4.0 transformations with solutions that allow them to have more flexibility, visibility and efficiency in their operations,” said Boles. “We'll see an acceleration of adoption of solutions that help address that need, ranging from AI including machine learning, machine vision and advanced analytics. As the economy bounces back, we'll continue to see investment in the foundational operational technology infrastructure with more IT capabilities to allow the broad ecosystem of players to deploy these solutions and we’ll see Industry 4.0 adoption significantly ramp up in 2021.""

Lomax Ward, a co-founder and partner with venture capital firm Luminous Ventures, said he expects AI research to gain deeper insights. “It is possible in 2021 we will see more breakthroughs with impact on the same orders of magnitude as GPT-3 or DeepMind’s AlphaFold – giant leaps of historical importance that shape industries in a profound way,” said Ward. “Increasing number of organizations will continue the adoption of AI and we will see a transition from doing pilot studies to creating real organizational changes to integrate AI into their processes. The AI integration does not happen overnight, and more work is needed on the organizational level to achieve ‘organizational learning’ on top of machine learning.”

At cloud software vendor Infor, Rick Rider, the vice president of product management, said he sees AI in 2021 transforming the hiring process.

“In the unpredictable job market of 2021, it will be critical for organizations to leverage AI to ensure they find the right candidate for the job,” said Rider. “AI will enable HR departments to become more proactive in their hiring and help them determine a candidate’s cultural fit by using data to measure the quality of a hire. Innovations such as intelligent screening software that automates resume screening, recruiter chat bots that engage candidates in real-time, and digitized interviews that help assess a candidate’s fit, will start becoming commonplace in HR departments. AI also holds great promise for creating more diverse and inclusive workplaces, given its ability to reduce biases and add objectivity into employment decision-making through AI-powered algorithms that will identify the unique qualities of candidates.”

In addition, AI will be pivotal for real-time supply and demand matching, said Rider. “As the incredible supply chain disruptions of 2020 unfolded, it became clear that managing real-time supply and demand matching and forecasting were no longer tasks humans can take on alone. It’s no longer reasonable to expect a supply chain leader to predict when one country’s market will suddenly close and another’s will open, or account for ever-shifting materials and costs — especially as government restrictions on transportation and travel change rapidly. In 2021, we will see supply chain managers accelerating their adoption of AI to augment workers’ instincts and experiences and provide them with intelligent insights into changing market conditions, letting them accurately forecast supply and demand in real-time.”

Ramprakash Ramamoorthy, director of AI research at cloud software vendor Zoho, said he sees AI continuing to help businesses morph and survive through the pandemic in 2021. “This pandemic year has been a ‘come-to-AI’ moment for many businesses,” said Ramamoorthy. “Now that companies are able to see the various ways AI can assist in business processes, the demand for these tools is high. In 2021, AI will be more commonly embedded in business tools, electronics, cars, and across all areas of business. Businesses will see AI embedded into workflows, analytics, communication tools, and others. We'll see AI augmenting tasks to run more efficiently and effectively.”

George Young, the global managing director of professional IT services firm Kalypso, said he sees artificial intelligence becoming less artificial in 2021. The use of AI will become standard for addressing the challenges of COVID-19, work from home, supply chain disruptions and more, said Young, and will help companies find new ways to continue operations effectively from products to plants and to end users.

“However, without considering how humans will interact with and leverage these new autonomous systems, AI will fail,” added Young. “In 2021, enterprises will take a human-centered approach to AI initiatives, understanding user needs and values, then adapting AI designs and models accordingly, which will in turn, improve adoption. Enterprises must put the same focus on people and culture as the technology itself for AI to be successful. Organizational change management teams will be critical for driving digital transformation and AI forward by bringing people along for the change journey and setting the organization up for measurable results. Proper change management is the most important – yet overlooked – aspect of any digital transformation initiative.”

To Florian Douetteau, the CEO and co-founder of data science software platform vendor Dataiku, AI experimentation will become more strategic in 2021.

“Experimentation takes place throughout the entire model development process – usually every important decision or assumption comes with at least some experiment or previous research to justify those decisions,” said Douetteau. “Experimentation can take many shapes, from building full-fledged predictive ML models to doing statistical tests or charting data. Trying all combinations of every possible hyperparameter, feature handling, etc., quickly becomes untraceable. Therefore, we’ll begin to see organizations define a time and/or computation budget for experiments as well as an acceptability threshold for usefulness of the model.”

A major factor that will influence AI use in 2021 are specific remote work challenges due to the COVID-19 pandemic, said Mike Leach, solution portfolio lead for client AI, professional VR and remote/rack workstation solutions at Lenovo. Typical datasets used by data scientists and other AI researchers are typically large files of 100GB or more, which can create internet connection/bandwidth bottlenecks when trying to upload or download them to and from any work-from-home environment, said Leach. In addition, many datasets cannot be removed from a company’s site, especially in certain industries like healthcare, he said.

As a result, Leach said he sees 2021 as the year of small data.

“Small data will allow businesses to become more agile, taking segments of data and gleaning instant insights that allow data and business analysts to deliver the real-time insights needed to make informed business decisions,” he said. “To achieve this, companies will need to invest in smarter technology solutions such as powerful mobile and desktop workstations to gain an agile and predictive view of their world.”

At data science automation vendor dotData, founder and CEO Ryohei Fujimaki said he sees more AI use in manufacturing in 2021. “We expect to see growing momentum in the adoption of AI and ML automation technologies in the manufacturing industry, as more organizations look to AI to streamline processes, lower operating costs and accelerate time to value,” said Fujimaki. “A big factor driving this momentum is the projected manufacturing workforce shortage due to skilled employees’ looming retirements. AI and automation are key technologies that can address this gap while increasing operational efficiency, improving quality and boosting productivity. The key areas where AI will make the most impact are monitoring and quality control, predictive maintenance, supply chain optimization and robotic process automation and bot programming.”

Sastry Malladi, the CTO of cloud-native applications vendor FogHorn, said he sees AI bring used in 2021 to help solve logistics challenges for many businesses.

“Today, many warehouse and logistics operations are under pressure to significantly reduce order-to-delivery timelines, driven by increasing consumer demand and expectations” due to the pandemic, said Malladi. “In 2021, warehouses will pair the low-latency processing power of the edge with the mobility of handheld devices to enable real-time operational insights on mobile devices unrestricted from fixed locations or even cloud connectivity. This flexibility ensures warehouse workers are kept in the loop of all internal operations and changes at all times and without having to alter their current daily routines. In turn, mobile edge solutions can enable workers to more instantaneously share information and insights across the warehouse, ensuring that every worker is on the same page at all times.”

At the same time, AI capabilities at the edge will help organizations transform video data from IoT connected sensors into actionable insights in real-time, he said. “Edge AI will play an essential role in evaluating and delivering heightened data quality and effectiveness, as edge-enabled solutions will perform real-time analysis of voluminous data streams and identify only the most valuable insights for further processing. We will see increasing adoption of edge AI technology as early adopters reap the benefits of real-time streaming analytics.”","['open', 'insights', 'realtime', 'data', 'experts', 'predictions', 'ai', 'vendor', 'supply', 'solutions', 'growth', 'sees', '2021', 'expect', 'enterprise']","Where to Expect Enterprise AI Growth in 2021: More Predictions from Our AI ExpertsSo many industry predictions for how AI use in business will change and grow in 2021 have come in to EnterpriseAI that we didn’t have room to share them all in our first predictions story on Feb. 1.
Now we are expanding that bounty of AI predictions riches with a second installment of 2021 predictions from our AI experts across industries, verticals and the world of IT.
Don’t forget to read our earlier predictions roundup as well to get the full flavor of where AI could be heading in 2021.
Clement Farabet, vice president of AI infrastructure for GPU and AI chipmaker Nvidia, said he expects AI in 2021 to take on the role of a compiler.
In 2021, AI will be more commonly embedded in business tools, electronics, cars, and across all areas of business."
103,https://www.miragenews.com/lab-researchers-explore-learn-by-calibration-512516/,Lab researchers explore ‘learn-by-calibration’ approach to deep learning to accurately emulate scientific process,2021-02-11 03:08:51+11:00,"Lawrence Livermore National Laboratory (LLNL) computer scientists have developed a new deep learning approach to designing emulators for scientific processes that is more accurate and efficient than existing methods.

In a paper published by Nature Communications, an LLNL team describes a “Learn-by-Calibrating” (LbC) method for creating powerful scientific emulators that could be used as proxies for far more computationally intensive simulators. While it has become common to use deep neural networks to model scientific data, an often overlooked, yet important, problem is choosing the appropriate loss function – measuring the discrepancy between true simulations and a model’s predictions – to produce the best emulator, researchers said. The article was among those featured in the journal’s special AI and machine learning “Focus” collection on Jan. 26, designating it as one that editors found of particular interest or importance.

The LbC approach is based on interval calibration, which has been used traditionally for evaluating uncertainty estimators, as a training objective to build deep neural networks. Through this novel learning strategy, LbC can effectively recover the inherent noise in data without the need for users to pick a loss function, according to the team.

Applying the LbC technique to various science and engineering benchmark problems, the researchers found the approach results in high-quality predictive models that are closer to real-world data and better calibrated than previous state-of-the-art methods. By demonstrating the technique on scenarios with varying data types and dimensionality, including a reservoir modeling simulation code and inertial confinement fusion (ICF) experiments, the team showed it could be broadly applicable to a range of scientific workflows and integrated with existing tools to simplify subsequent analysis.

“This is an extremely easy-to-use principle that can be added as the loss function for any neural network that we currently use, and make the emulators significantly more accurate,” said lead author Jay Thiagarajan. “We considered different types of scientific data – each of these data have completely different assumptions, but LbC could automatically adapt to those use cases. We are using the same exact algorithm to approximate the underlying scientific process in all these problems, and it consistently produces much better results.”

While there has been a surge in using machine learning to build data-driven emulators, the field has lacked an effective method for determining how closely the predictive models reflect physical reality, Thiagarajan explained. In the latest paper, the LLNL team proposes using calibration-driven training to enable models to capture the inherent data characteristics without making assumptions on data distribution, saving time and effort and improving efficiency.

“Learn-by-Calibrating is an approach that eliminates the pain of having to come up with specific loss functions for every problem,” Thiagarajan said. “It automatically can handle both symmetric and asymmetric noise models and can provide robustness to ‘rare’ outlying data. The other interesting thing is that because we are able to better model the observed data, compared with the standard loss functions people use, we are able to use a smaller neural network with reduced parameters to produce the same result as existing methods.”

In the study, the team applied the approach to a variety of scientific and engineering processes: predicting a superconductor’s critical temperature, estimating the noise of an airfoil in aeronautical systems and the compressive strength of concrete, approximating a decentralized smart grid control simulation, mimicking the clinical scoring process from biomedical measurements in Parkinson patients and emulating a one-dimensional simulator for ICF experiments. The researchers found the LbC approach produced better emulators across the board, with significantly improved generalization than the most common techniques in use today, even among scenarios with small datasets.

“It’s very challenging for emulators to accurately capture the underlying physical processes when they are only given access to the simulation codes in the form of input/output pairs, often resulting in subpar predictive capabilities. With the use of interval calibration, LbC goes one step further during training than simply to match the outputs of the simulator,” said co-author and LLNL computer scientist Rushil Anirudh. “When measuring the quality of emulators with mean squared error (MSE), LbC produces better quality models than the ones that have been explicitly trained using MSE, which is a sign that LbC is indeed able to go behind the curtain to capture some essence of the physical process that governs the actual numerical simulator.”

LLNL scientists said the “plug-and-play” approach could prove valuable, not just for ICF reactions, but with a host of Laboratory applications.

“The Lab’s most critical and high-consequence missions need AI methods that can both improve predictions and precisely quantify the uncertainty in those predictions,” said principal investigator and Cognitive Simulation Initiative Director Brian Spears. “LbC is literally tailor-made to do this, allowing it to tackle key problems in ICF, weapons, predictive biology, additive manufacturing and much more.”

Thiagarajan said the team’s immediate next steps are to integrate the approach into the Lab’s scientific workflows and leverage these higher fidelity emulators to solve other challenging design optimization problems.

The work was funded by the Laboratory Directed Research and Development program.

Co-authors included LLNL researchers Bindya Venkatesh, Peer-Timo Bremer, Jim Gaffney and Gemma Anderson.","['lbc', 'lab', 'emulate', 'models', 'using', 'data', 'researchers', 'scientific', 'process', 'learnbycalibration', 'learning', 'deep', 'approach', 'emulators', 'loss', 'better', 'llnl', 'explore']","Lawrence Livermore National Laboratory (LLNL) computer scientists have developed a new deep learning approach to designing emulators for scientific processes that is more accurate and efficient than existing methods.
In a paper published by Nature Communications, an LLNL team describes a “Learn-by-Calibrating” (LbC) method for creating powerful scientific emulators that could be used as proxies for far more computationally intensive simulators.
The LbC approach is based on interval calibration, which has been used traditionally for evaluating uncertainty estimators, as a training objective to build deep neural networks.
“We considered different types of scientific data – each of these data have completely different assumptions, but LbC could automatically adapt to those use cases.
The researchers found the LbC approach produced better emulators across the board, with significantly improved generalization than the most common techniques in use today, even among scenarios with small datasets."
104,https://www.hpcwire.com/2021/02/10/chinese-company-launches-origin-pilot-os-for-quantum-computing/,Chinese Company Launches Origin Pilot (OS) for Quantum Computing,2021-02-10 00:00:00,"Origin Quantum Computing Technology (Origin Tech), based in China, this week launched what’s being called China’s first home-grown operating system for quantum computers. The OS – Origin Pilot – has the potential to improve QC performance efficiency by several-fold according to the company.

Origin Tech is closely aligned with the Key Laboratory of Quantum Information under the Chinese Academy of Sciences. Origin Tech co-founders Guang-can Guo and Guo-ping Guo are also director and deputy director, respectively, of the Key Laboratory.

According the company profile (on its website), “[Origin Tech] is Chinese leading enterprise of Quantum Computing and has developed subsidiaries in Beijing, Shanghai, Chengdu and Shenzhen City. Taking the Key Laboratory of Quantum Information of the USTC and CAS as the technology base, Origin Quantum is devoted to the full-stack development of quantum computing. The technical indicators of Origin Quantum’s software and hardware products are domestically advanced and the company has more than 400 intellectual property achievements.”

Relatively few details about the OS have been discussed in the flurry of news accounts following Origin Tech’s announcement on Monday. Here’s an excerpt from the cnTechPost:

“The team developed a quantum image recognition application using a quantum convolutional neural network model, which can transform the image recognition task into multiple quantum lines that are lined up after being encoded with quantum state data. The system is effective in applications that improve the overall utilization of quantum chips and significantly reduce the runtime of quantum applications such as quantum image recognition, the company said.

“The system also supports a variety of quantum computing systems, including superconducting quantum processors, semiconductor quantum processors, ion trap quantum processors, or hybrid quantum processors, enabling access to high-performance quantum workstations with multiple quantum processor cores for users.”

A brief in the Quantum Computing Report, noted, “[Origin Tech] indicates that the system has a number of unique features including parallel execution of multiple quantum computing tasks, automatic qubit calibration, and unified management of quantum computing resources. The company’s goal with this operating system is to make quantum computer operations more efficient and stable. It is expected that this operating system will be used with the Origin Quantum Cloud Service which they announced last year. Additional information about Origin Pilot is available on the company’s website here and also in a blog posting (in Chinese) that describes the software in more detail.”

While several quantum computing OS efforts are underway worldwide, this is the first developed in China where it is seen as part of China’s broad effort to achieve technology independence generally and its specific effort to achieve technology leadership in quantum computing. Currently all of the various OS development efforts are somewhat nascent in the sense that the state of the underlying hardware is likewise nascent.

Zeng Bei, a professor of physics with Hong Kong University of Science and Technology and an expert in quantum computing, is quoted in the South China Morning post, “Yes, people are trying to make an operating system, borrowing the idea of classical calculating, but actually at this stage we don’t actually need them because the key part is the underlying hardware.”

Link to South China Morning Post: https://www.scmp.com/news/china/science/article/3121248/chinese-company-origin-develops-system-software-quantum

Link to cnTechPost: https://cntechpost.com/2021/02/09/origin-pilot-chinas-first-quantum-computer-operating-system-released/

Link to the Quantum Computing Report: https://quantumcomputingreport.com/chinese-company-origin-quantum-announces-its-own-quantum-operating-system/

Feature art: Pictured is a 16-qubit superconducting quantum chip designed, fabricated, and tested at MIT and Lincoln Laboratory.","['quantum', 'company', 'operating', 'chinese', 'key', 'computing', 'pilot', 'system', 'tech', 'launches', 'processors', 'technology', 'os', 'origin']","Origin Quantum Computing Technology (Origin Tech), based in China, this week launched what’s being called China’s first home-grown operating system for quantum computers.
The OS – Origin Pilot – has the potential to improve QC performance efficiency by several-fold according to the company.
According the company profile (on its website), “[Origin Tech] is Chinese leading enterprise of Quantum Computing and has developed subsidiaries in Beijing, Shanghai, Chengdu and Shenzhen City.
Taking the Key Laboratory of Quantum Information of the USTC and CAS as the technology base, Origin Quantum is devoted to the full-stack development of quantum computing.
“The system also supports a variety of quantum computing systems, including superconducting quantum processors, semiconductor quantum processors, ion trap quantum processors, or hybrid quantum processors, enabling access to high-performance quantum workstations with multiple quantum processor cores for users.”A brief in the Quantum Computing Report, noted, “[Origin Tech] indicates that the system has a number of unique features including parallel execution of multiple quantum computing tasks, automatic qubit calibration, and unified management of quantum computing resources."
105,https://techcrunch.com/2021/02/10/lang-ai-snags-2m-to-remove-technical-burden-of-implementing-ai-for-businesses/,TechCrunch is now a part of Verizon Media,2021-02-10 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
106,https://www.forbes.com/sites/davidteich/2021/02/10/ecommerce-delivery-and-the-gig-economy-create-opportunities-for-both-fraud-and-the-artificial-intelligence-to-detect-it/,,,,,
107,https://www.techrepublic.com/article/study-robots-are-trusted-more-than-people-when-it-comes-to-money-management/,"Study: ""Robots"" are trusted more than people when it comes to money management",,"COVID-19 has accelerated digital transformation across industries. A new study takes a look at the sentiments regarding ""robots"" in the financial services sector.

Image: Oracle

The coronavirus pandemic has accelerated digital transformation across industries as companies look to streamline processes and workflows with artificial intelligence (AI), machine learning, and more. On Wednesday, Oracle released a study focusing on the sentiments regarding the use of algorithms in the financial services sector and the ways in which these technologies could impact the industry in the future.

""Financial processes in our personal and professional worlds have become increasingly digital for many years and the events of 2020 have accelerated that trend,"" said Juergen Lindner, senior vice president of global marketing at Oracle. ""Digital is the new normal and technologies such as artificial intelligence and chatbots play a vital role in managing finance.""

SEE: TechRepublic Premium editorial calendar: IT policies, checklists, toolkits, and research for download (TechRepublic Premium)

Overall, the study--conducted in partnership with personal finance expert Farnoosh Torabi--included responses from over 9,000 business leaders and consumers from 14 countries. One of the most standout key findings states that nearly seven-in-10 respondents (67%) trust robots more than human beings to oversee their finances. However, it's important to clarify these semantics for perspective and clarity.

""We use this term 'robots' pretty loosely. It can be a digital interface. It can be a machine learning algorithm. It can be an application,"" Lindner said.

More than three-quarters of business leaders (77%) said they trusted ""robots"" more than their own finance teams and 73% of these business leaders said they trusted robots more than trusted themselves when it comes to managing finances.

About half of consumers (53%) said they trusted robots more than they trusted themselves when it comes to managing finances and 63% said they trusted these technologies more than personal finance advisors. About one-quarter of consumers (22%) said they thought these technologies could help decrease spending and 15% felt as though ""robots"" could help them invest in the stock market.

A portion of the study also details ways in which increased trust in these technologies among business leaders and consumers could alter roles in the financial sector. For example, the vast majority of business leaders (85%) ""want help from robots for finance tasks"" and about half (56%) believed robots ""will replace corporate finance professionals"" in the next half-decade, according to the release.

Significantly fewer consumers (42%) believe these technologies will replace personal financial advisors over this time period, but many want to seek the advice of ""robots"" with financial decisions.

For example, three-quarters of respondents (76%) want these technologies to help with financial management. About one-third of consumers (31%) want ""robots"" to assist with both finance management by decreasing ""unnecessary spending"" and 25% want these technologies to increase ""on-time payments,"" according to the release.

Interestingly, consumers still want humans to assist with major financial decisions. For example, about half (45%) want humans to offer guidance on buying a home. A similar number of consumers (41%) want human guidance when purchasing a car or to assist with retirement planning (38%).

SEE: NASA taps AI to identify ""fresh craters"" on Mars (TechRepublic)

COVID-19 and digital transformation

More than one year after the first COVID-19 cases, the coronavirus pandemic continues to present both public health and economic challenges worldwide. The report also illustrates various ways the coronavirus pandemic has impacted people's relationship with money, spending habits, and use of technology.

About one-third (29%) of consumers said that ""cash-only is a deal-breaker for doing business"" and 69% of business leaders reported investing in capabilities to support digital payments. About half of companies (51%) and one-quarter of consumers (27%) said they use AI to ""manage financial processes.""

Data, Analytics and AI Newsletter Learn the latest news and best practices about data science, big data analytics, and artificial intelligence. Delivered Mondays Sign up today

Also see","['comes', 'digital', 'study', 'finance', 'management', 'leaders', 'financial', 'trusted', 'consumers', 'business', 'robots', 'personal', 'technologies', 'money']","A new study takes a look at the sentiments regarding ""robots"" in the financial services sector.
More than three-quarters of business leaders (77%) said they trusted ""robots"" more than their own finance teams and 73% of these business leaders said they trusted robots more than trusted themselves when it comes to managing finances.
About half of consumers (53%) said they trusted robots more than they trusted themselves when it comes to managing finances and 63% said they trusted these technologies more than personal finance advisors.
A portion of the study also details ways in which increased trust in these technologies among business leaders and consumers could alter roles in the financial sector.
Significantly fewer consumers (42%) believe these technologies will replace personal financial advisors over this time period, but many want to seek the advice of ""robots"" with financial decisions."
108,https://techcrunch.com/2021/02/09/with-ai-translation-service-that-rivals-professionals-lengoo-attracts-new-20m-round/,TechCrunch is now a part of Verizon Media,2021-02-09 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
109,https://www.sciencedaily.com/releases/2021/02/210210091126.htm,Emerging robotics technology may lead to better buildings in less time,2021-02-21 00:00:00,"Emerging robotics technology may soon help construction companies and contractors create buildings in less time at higher quality and at lower costs.

Purdue University innovators developed and are testing a novel construction robotic system that uses an innovative mechanical design with advances in computer vision sensing technology to work in a construction setting.

The technology was developed with support from the National Science Foundation.

""Our work helps to address workforce shortages in the construction industry by automating key construction operations,"" said Jiansong Zhang, an assistant professor of construction management technology in the Purdue Polytechnic Institute. ""On a construction site, there are many unknown factors that a construction robot must be able to account for effectively. This requires much more advanced sensing and reasoning technologies than those commonly used in a manufacturing environment.""

The Purdue team's custom end effector design allows for material to be both placed and fastened in the same operation using the same arm, limiting the amount of equipment that is required to complete a given task.

Computer vision algorithms developed for the project allow the robotic system to sense building elements and match them to building information modeling (BIM) data in a variety of environments, and keep track of obstacles or safety hazards in the system's operational context.

""By basing the sensing for our robotic arm around computer vision technology, rather than more limited-scope and expensive sensing systems, we have the capability to complete many sensing tasks with a single affordable sensor,"" Zhang said. ""This allows us to implement a more robust and versatile system at a lower cost.""

Undergraduate researchers in Zhang's Automation and Intelligent Construction (AutoIC) Lab helped create this robotic technology.

The innovators worked with the Purdue Research Foundation Office of Technology Commercialization to patent the technology.

This work will be featured at OTC's 2021 Technology Showcase: The State of Innovation. The annual showcase, being held virtually this year Feb. 10-11, will feature novel innovations from inventors at Purdue and across the state of Indiana.","['robotic', 'construction', 'buildings', 'emerging', 'zhang', 'system', 'robotics', 'sensing', 'vision', 'developed', 'work', 'technology', 'lead', 'better', 'purdue']","Emerging robotics technology may soon help construction companies and contractors create buildings in less time at higher quality and at lower costs.
Purdue University innovators developed and are testing a novel construction robotic system that uses an innovative mechanical design with advances in computer vision sensing technology to work in a construction setting.
The technology was developed with support from the National Science Foundation.
Undergraduate researchers in Zhang's Automation and Intelligent Construction (AutoIC) Lab helped create this robotic technology.
The innovators worked with the Purdue Research Foundation Office of Technology Commercialization to patent the technology."
110,https://venturebeat.com/2021/02/10/cye-raises-120-million-for-security-that-uses-hackers-and-ai/,CYE raises $120 million for security that uses hackers and AI,2021-02-10 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

The soaring number of cyberattacks in recent years has propelled a massive expansion of potential solutions as startups and incumbent security providers jostle to address the market opportunity. But Tel Aviv-based CYE believes it has developed a way to cut through that clutter with a solution that uses artificial intelligence to probe for technical weaknesses and human hackers to test a company’s overall security resilience.

This approach allows the company to complement simulated attacks by virtual avatars with its ethical hackers’ efforts to provide a more thorough risk assessment.

CYE (pronounced “sigh”) today announced it has raised $120 million in a round led by EQT, with participation from 83North.

“CYE is a combination of very strong top hackers [on] one side and very strong technology based on artificial intelligence that learns from those guys and mimics them via avatars and robots,” CYE CTO and cofounder Ronen Lago said. “We enable coverage that can scale with robots with the understanding of the human being.”

The startup is part of the dynamic Israeli cybersecurity ecosystem.

The company’s flagship product is called Hyver. This AI-driven platform provides a comprehensive security analysis of potential vulnerabilities that may include internal IT systems and products from other security vendors. This cybersecurity reconnaissance also includes scanning the dark and deep webs for things like executive emails, leaked passwords, or any other propriety information that may have already leaked.

Using that intelligence, the company’s hackers, or “red teams,” launch additional non-simulated attacks to probe the company’s weaknesses. This method also allows CYE to measure the effectiveness of a company’s security policies and the responsiveness of its teams.

“We take cybersecurity from the classic reactive method and perimeter protection into a more proactive mindset by actually attacking the organization,” Lago said.

When the company first launched, about 80% of the attacks were staged by humans and 20% by avatars. But as the company’s AI has improved, the ratio is now 80% avatars and 20% humans, Lago said.

The company says it has customers among Fortune 500 companies and plans to use the funding for product development and sales and marketing. It has about 80 employees.

“We have no sales and marketing teams,” Lago said. “Most of our customers came from personnel referral and things like that, which is great. With the new funding and the expertise of the investors, we want to take a company with nice revenues and strong customers to be a machine that scales with a global footprint.”","['hackers', 'cye', 'companys', '120', 'strong', 'raises', 'ai', 'uses', 'security', 'cybersecurity', 'avatars', 'lago', 'million', 'intelligence', 'company']","This approach allows the company to complement simulated attacks by virtual avatars with its ethical hackers’ efforts to provide a more thorough risk assessment.
CYE (pronounced “sigh”) today announced it has raised $120 million in a round led by EQT, with participation from 83North.
This AI-driven platform provides a comprehensive security analysis of potential vulnerabilities that may include internal IT systems and products from other security vendors.
Using that intelligence, the company’s hackers, or “red teams,” launch additional non-simulated attacks to probe the company’s weaknesses.
This method also allows CYE to measure the effectiveness of a company’s security policies and the responsiveness of its teams."
111,https://venturebeat.com/2021/02/10/bighat-raises-19-million-for-an-ai-powered-antibody-design-platform/,BigHat raises $19 million for an AI-powered antibody design platform,2021-02-10 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

BigHat Biosciences, a protein therapeutics startup and developer of an AI-guided antibody design platform, today announced it has closed a $19 million series A round led by Andreessen Horowitz. A spokesperson told VentureBeat the funding will be used to invest in R&D as BigHat further refines its antibody engineering processes.

The use of biologics — medications made of components like sugars, proteins, DNA, whole cells, and tissues — is on the rise in drug development. Two years ago, seven of the top 10 drugs were biologics. Today, more than 200 biotherapeutics are in use. They are estimated to generate over $100 billion in annual revenue for drug companies, but they’re difficult to design, particularly those that leverage antibodies. While existing technologies can screen for antibody molecules, traditional lab workflows can take weeks to produce an antibody variant and characterize its behavior.

BigHat, which was founded in 2019 by Mark DePristo and Peyton Greenside, with advisor Theresa Tribble, aims to combine a high-speed antibody characterization lab with AI to engineer disease-fighting molecules. DePristo was formerly the head of genomics for Google AI and codirector of medical genetics at the Broad Institute of MIT and Harvard. Greenside, also a computational biologist at the Broad Institute, earned her Ph.D. in biomedical informatics from Stanford.

BigHat claims its wet lab can go from designs to hundreds of “expressed, purified, and characterized” antibodies in days, as opposed to weeks. That’s in part thanks to AI and machine learning technologies that guide the process and improve with each development cycle on the platform. The technologies also learn how mutations affect an antibody’s expression yield, affinity, stability, solubility, downstream function, and other molecular properties, charting a course to rare molecules that might otherwise remain undiscovered.

The BigHat platform can be used to build proteins like monoclonal antibodies, short chains of amino acids called peptides, and more. These proteins can bind to multiple targets on hostile cells and offer better tissue penetration, as well as providing access to sites other antibodies can’t reach and improving safety while lowering production costs. For example, CAR-T therapies use bioengineered antibodies and receptors to stimulate or suppress the immune system and help the body fight cancer.

BigHat claims its wet lab was actively producing and characterizing 100 antibodies per week as early as summer 2020. At the onset of the COVID-19 pandemic, BigHat began a SARS-CoV-2 retargeting program that ultimately led to a viral neutralizing molecule in the lab. More recently, BigHat was awarded Amgen’s Golden Ticket to MBC BioLabs, which entitles the company to lab bench space and access to Amgen’s scientific and business leaders. BigHat was also awarded a Small Business Innovation Research grant from the U.S. National Institute of Standards and Technology (NIST).

According to Greenside, BigHat is collaborating with unnamed strategic partners to leverage its platform and codevelop antibodies and other biotherapeutics. Over the next three years, BigHat plans to triple its headcount for clinical trials in therapeutic programs.

“There are very few biologic applications in which you can pose a hypothesis and have real, lab-produced answers in days. Our wet lab can synthesize and characterize hundreds of antibodies every few days. Coupled with active learning techniques, BigHat molecular engineering platform can optimize antibodies for affinity, stability, solubility, and even performance in a functional assay,” Greenside said. “We’ve just started to explore the potential for novel antibody and protein therapeutics enabled by BigHat’s approach to antibody discovery.”

8VC, AME Cloud Ventures, and Innovation Endeavors also participated in San Carlos, California-based BigHat’s latest round. The startup has raised around $25 million in venture capital.","['antibody', 'lab', 'proteins', 'antibodies', 'greenside', '19', 'bighat', 'raises', 'platform', 'technologies', 'design', 'wet', 'aipowered', 'million', 'molecules']","BigHat Biosciences, a protein therapeutics startup and developer of an AI-guided antibody design platform, today announced it has closed a $19 million series A round led by Andreessen Horowitz.
While existing technologies can screen for antibody molecules, traditional lab workflows can take weeks to produce an antibody variant and characterize its behavior.
The BigHat platform can be used to build proteins like monoclonal antibodies, short chains of amino acids called peptides, and more.
BigHat claims its wet lab was actively producing and characterizing 100 antibodies per week as early as summer 2020.
According to Greenside, BigHat is collaborating with unnamed strategic partners to leverage its platform and codevelop antibodies and other biotherapeutics."
112,https://www.sciencedaily.com/releases/2021/02/210205104219.htm,"AI can make accurate assessment of whether a person will die from COVID-19, study finds",2021-02-21 00:00:00,"Using patient data, artificial intelligence can make a 90 percent accurate assessment of whether a person will die from COVID-19 or not, according to new research at the University of Copenhagen. Body mass index (BMI), gender and high blood pressure are among the most heavily weighted factors. The research can be used to predict the number of patients in hospitals, who will need a respirator and determine who ought to be first in line for a vaccination.

Artificial intelligence is able to predict who is most likely to die from the coronavirus. In doing so, it can also help decide who should be at the front of the line for the precious vaccines now being administered across Denmark.

The result is from a newly published study by researchers at the University of Copenhagen's Department of Computer Science. Since the COVID pandemic's first wave, researchers have been working to develop computer models that can predict, based on disease history and health data, how badly people will be affected by COVID-19.

Based on patient data from the Capital Region of Denmark and Region Zealand, the results of the study demonstrate that artificial intelligence can, with up to 90 percent certainty, determine whether an uninfected person who is not yet infected will die of COVID-19 or not if they are unfortunate enough to become infected. Once admitted to the hospital with COVID-19, the computer can predict with 80 percent accuracy whether the person will need a respirator.

""We began working on the models to assist hospitals, as during the first wave, they feared that they did not have enough respirators for intensive care patients. Our new findings could also be used to carefully identify who needs a vaccine,"" explains Professor Mads Nielsen of the University of Copenhagen's Department of Computer Science.

Older men with high blood pressure are highest at risk

The researchers fed a computer program with health data from 3,944 Danish COVID-19 patients. This trained the computer to recognize patterns and correlations in both patients' prior illnesses and in their bouts against COVID-19.

advertisement

""Our results demonstrate, unsurprisingly, that age and BMI are the most decisive parameters for how severely a person will be affected by COVID-19. But the likelihood of dying or ending up on a respirator is also heightened if you are male, have high blood pressure or a neurological disease,"" explains Mads Nielsen.

The diseases and health factors that, according to the study, have the most influence on whether a patient ends up on a respirator after being infected with COVID-19 are in order of priority: BMI, age, high blood pressure, being male, neurological diseases, COPD, asthma, diabetes and heart disease.

""For those affected by one or more of these parameters, we have found that it may make sense to move them up in the vaccine queue, to avoid any risk of them becoming inflected and eventually ending up on a respirator,"" says Nielsen.

Predicting respiratory needs is a must

Researchers are currently working with the Capital Region of Denmark to take advantage of this fresh batch of results in practice. They hope that artificial intelligence will soon be able to help the country's hospitals by continuously predicting the need for respirators.

""We are working towards a goal that we should be able to predict the need for respirators five days ahead by giving the computer access to health data on all COVID positives in the region,"" says Mads Nielsen, adding:

""The computer will never be able to replace a doctor's assessment, but it can help doctors and hospitals see many COVID-19 infected patients at once and set ongoing priorities.""

However, technical work is still pending to make health data from the region available for the computer and thereafter to calculate the risk to the infected patients. The research was carried out in collaboration with Rigshospitalet and Bispebjerg and Frederiksberg Hospital.","['study', 'data', 'person', 'finds', 'patients', 'ai', 'covid19', 'assessment', 'predict', 'health', 'accurate', 'die', 'region', 'computer', 'working', 'respirator', 'infected']","Using patient data, artificial intelligence can make a 90 percent accurate assessment of whether a person will die from COVID-19 or not, according to new research at the University of Copenhagen.
Once admitted to the hospital with COVID-19, the computer can predict with 80 percent accuracy whether the person will need a respirator.
Older men with high blood pressure are highest at riskThe researchers fed a computer program with health data from 3,944 Danish COVID-19 patients.
This trained the computer to recognize patterns and correlations in both patients' prior illnesses and in their bouts against COVID-19.
However, technical work is still pending to make health data from the region available for the computer and thereafter to calculate the risk to the infected patients."
113,https://venturebeat.com/2021/02/10/salesforce-brings-intelligent-document-automation-to-health-cloud/,Salesforce taps AWS to bring ‘intelligent document automation’ to Health Cloud,2021-02-10 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Salesforce today announced a new product to help health care and life sciences companies digitize their document management processes.

Intelligent document automation (IDA), as it’s called, is designed for use with another new Salesforce tool called intelligent form reader.

With IDA, Salesforce is promising its customers reduced manual data entry while enabling them to manage all patients or members from a single place. So any incoming documents, including typed or handwritten forms such as patient referrals that may have arrived as a digital or hard copy (e.g. by fax or post), can now be automatically analyzed and routed to the right queue for review and processing in Salesforce’s Health Cloud.

The intelligent form reader, which leans on optical character recognition (OCR) technology, is powered by Amazon Web Services’ (AWS) Textract. AWS launched Textract in 2019, leveraging machine learning smarts to enable any business to automatically extract content from tables, forms, pages, and more. A few months back, AWS introduced added support for handwriting recognition and a host of new languages.

Any business wishing to use Salesforce’s intelligent form reader must also have a separate Textract license, which is available through Salesforce.

*Article updated to clarify that the Textract license is available through Salesforce.","['automation', 'textract', 'intelligent', 'health', 'document', 'reader', 'license', 'taps', 'recognition', 'salesforce', 'cloud', 'ida', 'salesforces', 'form', 'aws', 'bring']","Salesforce today announced a new product to help health care and life sciences companies digitize their document management processes.
Intelligent document automation (IDA), as it’s called, is designed for use with another new Salesforce tool called intelligent form reader.
With IDA, Salesforce is promising its customers reduced manual data entry while enabling them to manage all patients or members from a single place.
The intelligent form reader, which leans on optical character recognition (OCR) technology, is powered by Amazon Web Services’ (AWS) Textract.
Any business wishing to use Salesforce’s intelligent form reader must also have a separate Textract license, which is available through Salesforce."
114,https://venturebeat.com/2021/02/10/vivun-raises-35-million-to-advance-presales-engineering-platform/,Vivun raises $35 million to advance presales engineering platform,2021-02-10 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Vivun provides a software-as-a-service (SaaS) platform dubbed Hero that automates the management of presales processes. Today the company revealed it has garnered $35 million in additional funding via a series B round led by Menlo Ventures.

While customer relationship management (CRM) software is widely employed to manage sales processes, applications optimized for presales teams — made up of engineers who often have more insights into which deals are likely to close than other members of the sales team — are not widely deployed, Vivun cofounder and CEO Matt Darrow said.

Vivun is focused on presales processes that are especially critical in IT sales involving software and increasingly complex IT infrastructure platforms. The company counts among its customers Autodesk, Okta, Cloudera, and Dell. But industry segments like aerospace face similar presales management challenges, Darrow noted. “These issues apply to all sorts of industries,” he said.

Presales engineers tend to have a better idea of which deals are likely to close based on the attributes of a product. As that data is captured in Hero, the platform applies AI and other data science techniques. It can, for example, identify when enhancements to a product might address a technical gap that had resulted in an organization losing deals. That expert system is invoked via a natural language processing (NLP) engine Vivun developed.

Hero is designed to enable presales teams to create their own lists to prioritize tasks based on their preferences, as opposed to relying solely on the agenda of a sales team that may not have as deep a technical understanding of a platform’s capabilities. Presales engineers can also receive personalized alerts based on those preferences.

Hero offers a scoring system for rating the likelihood a deal will close using clustering techniques that identify similar sales opportunities that have been closed successfully. Hero can also recommend strategies that have been proven to remedy risky deals or otherwise bolster sales opportunities.

While presales teams have historically been viewed as an adjunct to salespeople, who typically engage the customer first, Darrow said presales teams often have a deeper understanding of each customer’s requirements. Based on that knowledge, they may have a better appreciation for which deals are likely to close based on what they learned during engagements with other customers, Darrow said.

Those insights can be balanced against less technically oriented members of the sales team who often tend to be overly optimistic about the prospects of closing a deal, Darrow said. That doesn’t necessarily mean a deal won’t close. Sometimes salespeople have a deep enough relationship with the customer to enable them to overcome any potential technical obstacles. Nevertheless, the “biggest voice in the room” should not always be allowed to dictate the allocation of limited presales engineering resources, Darrow said.

Organizations have for decades invested billions in platforms to automate sales management processes, with mixed results. A CRM application, for example, is often employed more as a system of record for sales managers and finance executives than as a tool to enable individual salespeople to succeed. Most of those CRM platforms are not designed to maximize the investment in a sales engineering team that needs to support multiple deals before and after they close. In fact, Darrow notes that customers often trust members of sales engineering teams because of their technical acumen.

Regardless of how a deal is closed, the need to manage presale and post-sale engineering processes using a dedicated platform will only continue growing.","['vivun', 'advance', '35', 'team', 'teams', 'presales', 'sales', 'deals', 'raises', 'technical', 'based', 'processes', 'darrow', 'platform', 'close', 'engineering', 'million']","Vivun provides a software-as-a-service (SaaS) platform dubbed Hero that automates the management of presales processes.
Vivun is focused on presales processes that are especially critical in IT sales involving software and increasingly complex IT infrastructure platforms.
Presales engineers tend to have a better idea of which deals are likely to close based on the attributes of a product.
While presales teams have historically been viewed as an adjunct to salespeople, who typically engage the customer first, Darrow said presales teams often have a deeper understanding of each customer’s requirements.
In fact, Darrow notes that customers often trust members of sales engineering teams because of their technical acumen."
115,https://venturebeat.com/2021/02/10/rhino-health-emerges-from-stealth-to-bring-hospital-data-to-federated-learning/,Rhino Health emerges from stealth to bring hospital data to federated learning,2021-02-10 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Rhino Health, a startup leveraging federated learning to connect hospitals and AI developers, today emerged from stealth with $5 million. The company says it will use the funds to further develop its platform, which provides access to distributed datasets from a diverse group of patients.

The global market for big data analytics in health care was valued at $16.87 billion in 2017 and is projected to reach $67.82 billion by 2025, according to a recent report from Allied Market Research. It’s believed that health care organizations’ implementation of big data analytics might reduce annual costs by more than 25% in the coming years. Better diagnosis and disease predictions, enabled by AI and analytics, can lead to cost reduction by decreasing hospital readmission rates, among other factors.

Rhino, which was cofounded by former Mass General Brigham exec Ittai Dayan and ex-Google engineer Yuval Baror, who led the Google Duplex team, aims to power AI models through a federated learning approach that ultimately improves the standard of care. In machine learning, federated learning entails training models across decentralized devices that hold data samples (e.g., imaging data, pathology data, structured clinical data, and clinical notes) without exchanging those samples. A centralized server might be used to orchestrate the steps of the algorithm and act as a reference clock, or it might be a peer-to-peer arrangement. Regardless, local algorithms are trained on local data samples, and weights (the learnable parameters of the algorithms) are exchanged between the algorithms at some frequency to generate a global model.

Federated learning isn’t exactly new to the world of medicine. Last June, major pharmaceutical companies inked an agreement to build federated learning technologies to collectively train drug discovery AI on datasets without having to share proprietary data. Intel is engaged with a National Institutes of Health-funded program that will leverage AI to identify brain tumors while preserving privacy. And Nvidia has begun working with collaborators to release COVID-19-related models trained with federated learning through the company’s Clara Imaging Software platform, following a collaboration with King’s College London on a federated learning neural network for brain tumor segmentation.

Rhino claims its platform enables customers to develop, validate, monitor, and maintain AI models by connecting datasets and developers, ensuring layers of protection. Data is anonymized and remains behind the local firewall. Using Rhino, managers can create “regulatory-grade” data packages from model validation and monitor data streams while identifying opportunities to improve performance and generalizability. Beyond this, Rhino can help transition prototype models from the research phase to the regulatory approval and eventually clinical deployment stages.

Baror notes that as AI solutions proliferate throughout the health industry, their development and maintenance are attracting increasing attention. In January 2021, the U.S. Food and Drug Administration updated its action plan for AI and machine learning in software as a medical device, underscoring the importance of inclusivity across dimensions like sex, gender, age, race, and ethnicity when compiling datasets for training and testing.

“Rhino Health is bringing together foundational learnings and emerging best practices from AI-forward industries to ensure that health care solutions are solving real-world problems and delivering consistent results,” Baror said. He added that Rhino has become a member of Nvidia’s Inception program — in a collaboration with the chipmaker — to bring its federated learning solution to clinics. “With federated learning, we’re able to do this in the privacy-centric manner this industry demands, advancing the interests of patients, hospitals, and technology developers alike.”

LionBird Ventures led the seed round Rhino announced today, with participation from Arkin Holdings and several angel investors. Rhino is headquartered in Cambridge, Massachusetts, with an R&D center in Tel Aviv.","['emerges', 'models', 'federated', 'data', 'rhino', 'care', 'stealth', 'health', 'learning', 'hospital', 'ai', 'datasets', 'samples', 'bring']","Rhino Health, a startup leveraging federated learning to connect hospitals and AI developers, today emerged from stealth with $5 million.
In machine learning, federated learning entails training models across decentralized devices that hold data samples (e.g., imaging data, pathology data, structured clinical data, and clinical notes) without exchanging those samples.
And Nvidia has begun working with collaborators to release COVID-19-related models trained with federated learning through the company’s Clara Imaging Software platform, following a collaboration with King’s College London on a federated learning neural network for brain tumor segmentation.
“Rhino Health is bringing together foundational learnings and emerging best practices from AI-forward industries to ensure that health care solutions are solving real-world problems and delivering consistent results,” Baror said.
He added that Rhino has become a member of Nvidia’s Inception program — in a collaboration with the chipmaker — to bring its federated learning solution to clinics."
116,https://venturebeat.com/2021/02/09/theator-raises-15-5-million-to-analyze-surgical-footage-with-computer-vision/,Theator raises $15.5 million to analyze surgical footage with computer vision,2021-02-09 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

Surgical platform developer Theator today closed a $15.5 million series A round led by Insight Partners, bringing the company’s total raised to date to over $18 million. Theator says the funds will be used to scale its commercial operations and partnerships with U.S. providers as it looks to grow its R&D team.

Some experts assert that surgeons’ efforts to overcome inequalities are stunted by an apprenticeship model that overemphasizes scope of practice, limiting surgeons’ knowledge. In the U.S., Black children are three times as likely as white children to die within a month of surgery, according to a study published in the journal Pediatrics. An estimated 5 billion people lack access to safe surgical care worldwide. And of the at least 7 million people who suffer complications following surgery each year, including more than 1 million who die, at least 50% of these deaths and complications are preventable.

Theator, a two-year-old company based in San Mateo, California, claims its product addresses these challenges by enabling surgeons to view video recordings of their procedures as lists (e.g., “Decompression, Cystic duct leak, Critical view of safety”) and jump to steps from which they wish to learn. Users get annotated summaries of their surgical performance as well as KPIs, analytics, and rankings to identify where skills training might be needed.

Theator was founded in 2018, but the company’s backstory dates back to 2015, when CEO and founder Tamir Wolf’s wife and then-boss both suffered from appendicitis a few months apart. As a physician, Wolf was able to diagnose their condition and took them to the emergency room for treatment. “They were treated at two different hospitals, and I was struck by the wildly different levels of care they experienced at each one, even though they were a mere 7 miles apart,” he told VentureBeat via email. “As I tried to understand why the approach to a seemingly straightforward illness was so different, I began to unearth the widespread disparity and variability that is plaguing the medical field, and surgery specifically.”

The use of AI and machine learning in the operating theater is on the rise. In 2019, McGill University developed an algorithm that was able to tell when trainee surgeons were ready to perform a real operation. And last April, researchers from Stanford created a model to help protect health care professionals from COVID-19 in the operating room.

As for Theator, it leverages computer vision to scan video footage of real-world procedures and identify key moments in order to annotate them with metatags. Through desktop and mobile apps, surgeons gain access to an indexed library of over 400,000 minutes of surgical video encompassing over 80,000 “intraoperative” moments.

“[We use] AI to complete high-granularity indexing of large scale surgical datasets, which enables big data analysis of surgical patterns for the first time, introducing a scientific data-driven understanding of best practices,” Wolf explained. “At Theator, we’re using computer vision, on edge, to detect any frames that might have identifying features (everything that is extra-cavitary), removing these before uploading to our cloud-based system … Theator’s research team [also] recently developed VTN (Video Transfer Network), a first of its kind Transformer-based framework for video action recognition that yields a significantly reduced training runtime (x16.1) and an accelerated inference process (x5.1) while maintaining state-of-the-art performance. VTN has the potential to significantly accelerate the video input and review component of Theator’s platform, in turn improving our ability to roll it out at scale in hospitals worldwide.”

Other investors in Theator’s latest round of fundraising include 23andMe cofounder and CEO Anne Wojcicki, former Netflix chief product officer Neil Hunt, and Zebra Medical Vision’s cofounder Eyal Gura.

“The lack of in-person contact with experienced surgeons has accelerated the need for Theator’s solution, which broadens surgeon expertise, even remotely or asynchronously. The pandemic has highlighted a reality wherein surgeons barely have time for in-person interaction, allowing them a greater degree of freedom to continuously improve performance,” Wolf said. “Surgeons also used to learn new techniques and approaches at conferences, but these have all but been removed from the equation. Theator’s comprehensive surgical video dataset, the ability to upload your own procedures, view others, and receive and provide feedback has made it an increasingly vital tool when more and more continuous development is being carried out remotely.”","['scale', 'footage', 'view', 'raises', 'analyze', 'surgeons', 'theators', 'vision', 'surgery', '155', 'theator', 'computer', 'video', 'wolf', 'surgical', 'million']","Surgical platform developer Theator today closed a $15.5 million series A round led by Insight Partners, bringing the company’s total raised to date to over $18 million.
Theator says the funds will be used to scale its commercial operations and partnerships with U.S. providers as it looks to grow its R&D team.
As for Theator, it leverages computer vision to scan video footage of real-world procedures and identify key moments in order to annotate them with metatags.
Through desktop and mobile apps, surgeons gain access to an indexed library of over 400,000 minutes of surgical video encompassing over 80,000 “intraoperative” moments.
“[We use] AI to complete high-granularity indexing of large scale surgical datasets, which enables big data analysis of surgical patterns for the first time, introducing a scientific data-driven understanding of best practices,” Wolf explained."
117,https://www.enterpriseai.news/2021/02/09/ai-tool-emerges-to-hasten-vaccine-development/,AI Tool Emerges to Accelerate COVID-19 Vaccines that Battle New Virus Mutations,2021-02-09 00:00:00,"AI Tool Emerges to Accelerate COVID-19 Vaccines that Battle New Virus Mutations

A growing list of global COVID-19 variants is prompting disease researchers to employ AI models trained using bioinformatics data to speed up vaccine development in the critical search to find improved vaccines that can effectively fight the virus mutations.

In a paper published in the journal Scientific Reports, researchers from the University of Southern California’s Viterbi School of Engineering developed a machine learning model to speed vaccine analysis. The investigators claimed their model promises to reduce vaccine design cycles from months to minutes.

The USC researchers developed a deep learning framework for the prediction and design of multi-epitope vaccines that could be used to combat COVID-19 variants. Research on multi-epitope vaccines to combat the coronavirus using immuno-informatics techniques have been ongoing since at least last spring. An epitope is the antigen component recognized by the immune system.

The USC investigators claimed their AI-assisted model eliminated up to 95 percent of vaccine approaches, greatly narrowing the field of promising candidates. The method predicted 26 potential vaccines, of which 11 were identified as best suited for developing a multi-epitope vaccine.

“This AI framework, applied to the specifics of this virus, can provide vaccine candidates within seconds and move them to clinical trials quickly to achieve preventive medical therapies without compromising safety,” Paul Bogdan, an associate professor of electrical and computer engineering at USC Viterbi and co-author of the study, told the university’s news service. “Moreover, this can be adapted to help us stay ahead of the coronavirus as it mutates around the world.”

The machine learning model was trained using a bioinformatics data set dubbed the Immune Epitope Database that catalogs experimental data on antibody and T cell epitopes. The database contains more than 600,000 known epitopes. It also hosts tools for predicting and analyzing the antigen-antibody binder.

The USC deep learning framework combines immuno-informatics computer simulations with deep neural network techniques to predict potential vaccine approaches based on the SARS-CoV-2 spike protein sequence. “The 3D structure of the designed vaccine is predicted, refined and validated by in silico tools,” the researchers reported this week.

The AI-based tool addresses the lengthy process of evaluating and selecting vaccine candidates. No such tool previously existed for sorting predictions, then analyzing and selecting the most promising vaccine candidates.

The USC framework called DeepVacPred instead uses a deep neural network architecture to “directly predict a very small number of potential vaccine subunits within a second and start the following evaluation and vaccine construction on a much smaller amount of data,” the investigators reported.

As more coronavirus variants emerge, “DeepVacPred allows us to quickly check for newly emerging threats caused by the RNA mutations of the SARS-CoV-2,” the researcher said. “We prove that our vaccine can tackle the virus RNA mutations.”

The proposed vaccine design framework can tackle the three most frequently observed mutations and be extended to deal with other potentially unknown mutations, Bogdan added.

The AI research was funded by the National Science Foundation, the U.S. Army Research Office, Defense Advanced Research Projects Agency and a grant from Northrop Grumman Corp.

Related","['emerges', 'research', 'vaccine', 'researchers', 'model', 'learning', 'virus', 'ai', 'covid19', 'battle', 'deep', 'accelerate', 'framework', 'mutations', 'tool', 'vaccines', 'usc']","AI Tool Emerges to Accelerate COVID-19 Vaccines that Battle New Virus MutationsA growing list of global COVID-19 variants is prompting disease researchers to employ AI models trained using bioinformatics data to speed up vaccine development in the critical search to find improved vaccines that can effectively fight the virus mutations.
The USC researchers developed a deep learning framework for the prediction and design of multi-epitope vaccines that could be used to combat COVID-19 variants.
Research on multi-epitope vaccines to combat the coronavirus using immuno-informatics techniques have been ongoing since at least last spring.
The method predicted 26 potential vaccines, of which 11 were identified as best suited for developing a multi-epitope vaccine.
The USC deep learning framework combines immuno-informatics computer simulations with deep neural network techniques to predict potential vaccine approaches based on the SARS-CoV-2 spike protein sequence."
118,https://techcrunch.com/2021/02/09/sentinelone-acquires-high-speed-logging-startup-scalyr-for-155m/,TechCrunch is now a part of Verizon Media,2021-02-09 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
119,https://techcrunch.com/2021/02/05/orwellian-ai-lie-detector-project-challenged-in-eu-court/,TechCrunch is now a part of Verizon Media,2021-02-05 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
120,https://research.aimultiple.com/voice-recognition-applications/,Top 9 Voice Recognition Applications in 2021,2021-02-05 12:33:25+00:00,"If you are familiar with virtual assistants such as Alexa, Cortana, Google Assistant, and Siri, you already know about voice recognition and conversational AI. The technology allows users to explain themselves verbally to a computer or a device. It converts users’ verbal queries into text and interprets what the user is saying to respond to their query.

Common Applications

Voice recognition is a maturing technology and users seem to trust it for the most basic functionality like search or playing music. User adoption of voice interfaces is still low for applications with more significant implications like buying things or controlling smart devices,

Voice Search

This is the most common use of speech recognition. In 2019, reports estimate that 112 million people in the US will use a voice assistant at least monthly, up 10% from last year. Another study also reveals, approximately 7 out of 10 consumers (71%) prefer to use voice searches to conduct a query over the traditional method of typing. Thanks to applications such as Siri and Google voice search, voice interfaces have become commonly used.

Voice to text

Thanks to speech recognition, you don’t need to type emails, reports, and other documents. For instance, you can use these voice typing and voice command features in Google Docs if you are using the Google Chrome browser.

Voice commands to smart home devices

Smart home applications are mostly designed to take a certain action after the user gives voice commands. Smart home devices are widely used speech recognition application, specifically when considering these:

the total number of smart devices supported by voice assistants tripled between 2018 and 2019

30% of voice assistant users state smart home devices as their primary reason(s) for investing in an Amazon Echo or Google Home.

Business function applications

Customer service

This is one of the most important AI applications in customer service. Speech recognition is an effective call center service solution that is available 24/7 at a fraction of the cost of a team of customer service representatives. It transcribes thousands of phone calls between customers and agents to identify common call patterns and issues.

Voice Biometrics for Security

Voice biometrics use a person’s voice as a unique identifying biological characteristic in order to authenticate them. Speech recognition can also be used for voice authentication to replace processes where a user has to display her personal information to authenticate herself.

Voice biometrics improves the overall customer experience since it eliminates customer frustration due to cumbersome login processes as well as lost and stolen credentials.

Industry applications

Automotive

In-car speech recognition systems have become a standard feature for most new vehicles. These systems aim to remove the distraction of looking down at your mobile phone while you drive. Thanks to these systems, drivers can use simple voice commands to initiate phone calls, select radio stations or play music.

Academic

80% of sighted children’s learning is through vision and their primary motivator to explore the environment around them. Speech recognition has the potential to minimize the disadvantages of students who are blind or have low vision.

There are also language learning tools such as Duolingo that use speech recognition to evaluate user’s language pronunciation.

Media / Marketing

Tools such as dictation software can enable people to write around 3000-4000 words of content including articles, speeches, books, memos, emails in 30 minutes if they are familiar with the topic. Though these tools still don’t provide 100% accurate results, they are beneficial for first drafts.

Healthcare

During patient examinations, doctors shouldn’t worry about taking notes of patients’ symptoms. Medical transcription software uses speech recognition to capture patient diagnosis notes. Thanks to this technology, doctors can shorten the average appointment which enables doctors to see more patients during their working hours.

If you are looking for a voice bot platform, feel free to check our transparent vendor lists where sorting vendors by AIMultiple score shows the best solutions based on maturity, popularity, and satisfaction.

Voice is one medium for conversations. feel free to check our articles on conversational AI for more:

If you still have questions on speech recognition, don’t hesitate to contact us:","['user', 'voice', 'thanks', 'users', 'customer', 'speech', 'recognition', 'applications', 'google', '2021', 'smart']","If you are familiar with virtual assistants such as Alexa, Cortana, Google Assistant, and Siri, you already know about voice recognition and conversational AI.
Thanks to applications such as Siri and Google voice search, voice interfaces have become commonly used.
Voice to textThanks to speech recognition, you don’t need to type emails, reports, and other documents.
Voice commands to smart home devicesSmart home applications are mostly designed to take a certain action after the user gives voice commands.
feel free to check our articles on conversational AI for more:If you still have questions on speech recognition, don’t hesitate to contact us:"
121,https://www.aitrends.com/security/solarwinds-hack-likely-assisted-by-ai-suggests-microsofts-smith/,"SolarWinds Hack Likely Assisted by AI, Suggests Microsoft’s Smith",2021-02-04 22:07:42+00:00,"By John P. Desmond, AI Trends Editor

The massive attack on US government agencies and US businesses that is a suspected Russian espionage operation was a nation-state attack compounded by the trend of augmenting human intelligence with AI, according to experts.

The attack surfaced in December when security experts discovered hackers had inserted a backdoor into software from SolarWinds called Orion, which was used to update software widely across the federal government and a number of Fortune 500 companies.

Perhaps unfairly, SolarWinds was originally considered to be the hackers’ main avenue of attack. However, close to a third of the victims were later found not to run the SolarWinds software, according to a recent account in The Wall Street Journal.

The attackers “gained access to their targets in a variety of ways. This adversary has been creative,” stated Brandon Wales, acting director of the Cybersecurity and Infrastructure Security Agency, adding, “It is absolutely correct that this campaign should not be thought of as the SolarWinds campaign.”

Similar conclusions have been reached by corporate investigators. The computer security company Malwarebytes has said that a number of its Microsoft cloud email accounts were compromised by the same attackers who pulled off the SolarWinds hack, using that the company called “another attack vector.” The company does not use SolarWinds software.

SolarWinds itself is investigating whether Microsoft cloud was the initial entry point of hackers into its network, one of several theories being pursued, according to a person familiar with the SolarWinds investigation.

John Lambert, the manager of Microsoft’s Threat Intelligence Center, stated, “This is certainly one of the most sophisticated actors that we have ever tracked in terms of their approach, their discipline and range of techniques that they have.”

SolarWinds has said that it first traced activity from the hackers to September 2019, and that the attack gave the intruders a back door into up to 18,000 SolarWinds customers.

The departments of Treasury, Justice, Commerce, State, Homeland Security, Labor and Energy all suffered breaches.

From the point of view of the government, “We continue to maintain that this is an espionage campaign designed for long-term intelligence collection,” stated Wales of CISA. “That said, when you compromise an agency’s authentication infrastructure, there is a lot of damage you could do.”

Microsoft’s Smith Sees Attack Likely Compounded by Use of AI

Brad Smith, President of Microsoft, said in a blog post published on Dec. 17, “The attack unfortunately represents a broad and successful espionage-based assault on both the confidential information of the U.S. Government and the tech tools used by firms to protect them.” Investigations are continuing into the attack, which he said is ongoing, and “is remarkable for its scope, sophistication, and impact.”

He said more than 40 Microsoft business customers were targeted, 80% of them in the US but also in Canada, Mexico, Belgium, Spain, the United Kingdom, Israel, and the UAE in the Middle East.

The attack was not “espionage as usual,” Smith stated. He added, ominously, “These types of sophisticated nation-state attacks are increasingly being compounded by another technology trend, which is the opportunity to augment human capabilities with artificial intelligence (AI). One of the more chilling developments this year has been what appears to be new steps to use AI to weaponize large stolen datasets about individuals and spread targeted disinformation using text messages and encrypted messaging apps. We should all assume that, like the sophisticated attacks from Russia, this too will become a permanent part of the threat landscape.”

He cited a second evolving threat, the growing privatization of cybersecurity attacks through a new general of private companies he calls “private sector offensive actors” (PSOAs).

“This is not an acronym that will make the world a better place,” Smith stated. As an example, he cited NSO Group, an Israeli-based software company now involved in US litigation, accused of violating US anti-hacking laws due to its technique of installing itself on mobile devices without permission of the user. The software company WhatsApp filed the suit, which maintains that Pegasus accessed more than 1,400 mobile devices.

Other companies are rumored to be joining the PSOA market in what Smith said has become a new $12 billion global technology market. “This represents a growing option for nation-states to either build or buy the tools needed for sophisticated cyberattacks,” Smith stated, adding, “An industry segment that aids offensive cyberattacks spells bad news on two fronts. First, it adds even more capability to the leading nation-state attackers, and second, it generates cyberattack proliferation to other governments that have the money but not the people to create their own weapons. In short, it adds another significant element to the cybersecurity threat landscape.”

CISA Pursuing Attack Repercussions

Meanwhile, the CISA is pursuing the repercussions of the massive hack. “An advanced persistent threat (APT) actor is responsible for compromising the SolarWinds Orion software supply chain,” CISA states on its website dedicated to information on the attack.

Following a Presidential policy direction, the FBI and the Office of the Director of National Intelligence have formed a Cyber Unified Coordination Group (UCG) to coordinate a whole-of-government response.

CISA guidance to federal agencies that ran the SolarWinds software is to run forensic analysis and harden platforms still running the Orion software. In a directive issued on Dec. 13, “CISA determined that this exploitation of SolarWinds products poses an unacceptable risk to Federal Civilian Executive Branch agencies and requires emergency action.”

FireEye, the security software company that revealed the theft of some 300 of its proprietary cybersecurity tools five days before SolarWinds announced it had been hacked, posted countermeasures in its GitHub repository.

Read the source articles in The Wall Street Journal, from Brad Smith on the Microsoft blog, and on the CISA website, See countermeasures posted by FireEye in its GitHub repository.","['stated', 'likely', 'attack', 'ai', 'assisted', 'solarwinds', 'hack', 'security', 'microsofts', 'software', 'microsoft', 'suggests', 'smith', 'threat', 'intelligence', 'company']","However, close to a third of the victims were later found not to run the SolarWinds software, according to a recent account in The Wall Street Journal.
The attack was not “espionage as usual,” Smith stated.
“This is not an acronym that will make the world a better place,” Smith stated.
The software company WhatsApp filed the suit, which maintains that Pegasus accessed more than 1,400 mobile devices.
CISA guidance to federal agencies that ran the SolarWinds software is to run forensic analysis and harden platforms still running the Orion software."
122,https://www.aitrends.com/ai-in-government/ai-applied-to-tax-systems-can-help-discover-shelters-support-equality/,"AI Applied to Tax Systems Can Help Discover Shelters, Support Equality",2021-02-04 21:47:31+00:00,"By AI Trends Staff

AI and machine learning represent the new frontier in tax administration.

Since researchers at MIT created an algorithm that could flag a certain type of tax shelter in 2015, the concept of using AI in an effort to find those helping to shield their income from taxation has caught on.

Denmark, which lost nearly US$325 million to tax evasion in 2018, implemented AI tools which have successfully identified 85 of every 100 cases of tax evasion, according to a recent account in The Science Times. France passed a law as part of the country’s 2020 budget that allows tax authorities to deploy algorithms to trawl through social media to detect signs of tax evasion, smuggling, and undeclared income.

“The complexity of tax rules make it a challenge for any organization to stay compliant, much less reduce their tax liabilities. Therefore, artificial intelligence is well suited for tasks that require a deep analysis of the tax codes,” stated Luís Aires, independent VAT consultant and tax advisor, based in Lisbon, Portugal, writing recently in VATupdate. “Using years of previous tax documentation as a foundation for learning, the AI application can provide an in-depth understanding of the tax codes and stays on top of yearly changes. As a result, it’s easier for tax practitioners to identify key areas for possible savings.”

Special methods of intelligent data analysis are needed to detect and prevent losses. Detection logic must recognize complex patterns over periods spanning second to months. The logic must also be easily customizable and ability to be maintained by specialists in a changing business environment. Ensuring compliance and finding fraud requires monitoring millions of daily transactions in real time. Proof of non-compliance that can stand up to audits is critical to tax enforcement.

AI and Machine Learning Can Help Detect Money Laundering

AI and machine learning can also be applied to uncover or detect money laundering. “Tax authorities use AI to predict risk for tax evasion, or to monitor and identify suspicious tenders or bids in public procurement,” Aries states. Some applications of AI and automated decision systems in society remain controversial, he notes. Questions persist on how to handle biased algorithms, on the ability to contest automated decisions, and accountability when machines make the decisions. Also, the right to privacy, the right to explanation, and the “right to be forgotten” remain topics of debate. “Nevertheless, due to the efficiency, apparent neutrality, stable performance, and cost savings associated with AI based processes, such tools are likely to be applied in more and more areas in the future,” he states.

Government of India Using AI to Fight Tax Evasion

The government of India has embarked on an effort to use a machine learning tool to fight tax evasion and identify bogus firms. The AI tool has been researched and developed by two US-based Indian researchers, according to an account in Financial Express. Dr. Aprajit Mahajan, Associate Professor, University of California, Berkeley, and postdoctoral scholar Dr. Shekhar Mittal, will scrutinize a vast dataset of Value Added Tax returns registered in Delhi between 2012 and 2017.

The study, commissioned by the Delhi government, concluded that similar means were used by traders to evade the Goods and Services Tax (GST). “Future versions of machine learning will build on the GST data,” stated Jasmine Shah, vice-chairman of the Delhi Dialogue and Development Commission.

The researchers said that this work is the first-ever systematic study on tax evasion in a country where there is weak tax compliance. “Our results indicate that by using our tool, the tax administration can prevent fraud up to $15-45 million,” the researchers wrote in a paper. “Anecdotal evidence suggests that such false paper trails are a common problem. Our work should have high policy relevance both within India and elsewhere,” the researchers stated.

Salesforce Researchers Studying Whether AI Can Make Tax Policy More Fair

Whether AI can be used to create a fair and equitable tax policy is the focus of research at the Salesforce Research team, which recently released a simulation tool called the AI Economist.

According to an account in BrinkNews, the AI Economist uses a two-level reinforcement learning framework and is highly flexible, designed to optimize for equality, productivity or sustainability, which can be set by the user.

The researchers compared the AI Economist with three other baseline tax methods: the free market with no taxation or redistribution; a progressive tax mirroring the 2018 United States federal tax schedule (i.e., marginal tax rates increase with income); and an analytical tax model proposed by economist Emmanuel Saez, which results in a regressive tax schedule in this case.

In simulations, the AI Economist achieved a 16% gain in the tradeoff between equality and productivity compared to the next best framework, the Saez model. Compared to the free market, the AI Economist improves equality by 47%, with an 11% decrease in productivity, the researchers said.

“We believe these initial results demonstrate the potential of applying a data and simulation-driven approach to quickly create equitable and effective economic policies,” stated Stephan Zheng, lead research scientist and senior manager at Salesforce.

Reinforcement learning algorithms use smart trial-and-error strategies to optimize policy models for a specified goal. During this process, the learning algorithm continuously uses feedback it receives to improve the policy models. High-profile applications of RL enabled AI to compete and win against human players in popular games including Go, Dota 2 and Starcraft.

Taxes were chosen as a focus for the model, since they are a near-universal part of society, used by local, state, and national governments. “But no one has truly determined now tax policy can be feasibly optimized in complex, dynamic economies,” Zheng stated. The number of contingencies to consider, he said is “near-infinite.” He credited Prof. David Parkes, head of the Economics and Computer Science Group at Harvard University, with assisting in the research.

Read the source articles in The Science Times, VATupdate, the Financial Express and in BrinkNews.","['using', 'researchers', 'learning', 'support', 'tax', 'shelters', 'ai', 'help', 'policy', 'evasion', 'machine', 'equality', 'economist', 'systems', 'applied', 'tool', 'discover']","Denmark, which lost nearly US$325 million to tax evasion in 2018, implemented AI tools which have successfully identified 85 of every 100 cases of tax evasion, according to a recent account in The Science Times.
AI and Machine Learning Can Help Detect Money LaunderingAI and machine learning can also be applied to uncover or detect money laundering.
“Tax authorities use AI to predict risk for tax evasion, or to monitor and identify suspicious tenders or bids in public procurement,” Aries states.
In simulations, the AI Economist achieved a 16% gain in the tradeoff between equality and productivity compared to the next best framework, the Saez model.
“But no one has truly determined now tax policy can be feasibly optimized in complex, dynamic economies,” Zheng stated."
123,https://www.aitrends.com/startups/startup-mojo-vision-eyes-a-future-in-smart-contact-lenses/,Startup: Mojo Vision Eyes a Future in Smart Contact Lenses,2021-02-04 21:28:44+00:00,"Wearable technology, including smart electronic devices in clothing or implanted in a user’s body, is a rising market first spurred by fitness tracking and more and more now by health concerns. The market is expected to reach $17.8 billion in 2021, with a penetration of 5.3%, according to estimates from Statista, researchers offering market and consumer data.

Mojo Vision is an example of a company with an innovative wearable technology focused on eyesight, that it is working to get to the market, including by getting FDA approval as a medical device. Steve Sinclair, SVP of Product and Marketing, Mojo Vision, responded by email to queries from AI Trends as part of our startup coverage. Here are his responses:

AI Trends: Describe your team

Steve Sinclair: Mojo Vision is led by a team of Silicon Valley veterans from companies including Apple, Amazon, Samsung, HP, Microsoft, Google, Motorola and others. Together, the senior team at Mojo holds more than 100 patents and has developed numerous revolutionary technologies.

What business problem are you trying to solve?

Mojo Vision is working to build the world’s first true smart contact lens, Mojo Lens. The goal of Mojo Lens is to provide people with immediate access to information that they need without the visual distraction of a device. We believe that information should be there when you need or want it, and then disappear when you don’t. Whether it’s remembering a multi-step procedure, monitoring your heart rate on a run, getting walking directions from a meeting to your hotel, checking on your house remotely, or seeing in a dark garage, we envision people using this most when they need information or answers but want to stay present and “eyes up” in the world around them.

How does your solution address the problem?

Mojo Lens has a built-in display that gives people the useful and timely information they want without forcing them to look down at a screen or losing focus on the people and the world around them. Mojo calls this eyes-up experience Invisible Computing, a platform that enables information to be instantaneous, unobtrusive and available hands-free, and will allow people to interact with each other more freely and genuinely.

Mojo Lens incorporates a number of breakthroughs and proprietary technologies, including the smallest and densest dynamic display ever made, an ultra power-efficient image sensor built for computer vision, a high-bandwidth, low-power wireless radio, and motion sensors for exceptionally precise eye tracking and image stabilization.

How are you getting to the market?

Mojo intends for the lens to expand availability worldwide over time. In the United States, contact lenses are typically a Class II medical device regulated by the FDA. Mojo is working with the FDA through its Breakthrough Device Program to ensure we comply with all safety regulations and standards. Our goal is to focus on FDA certification in the US first and then expand to other markets, but the exact timeline depends on regulatory approval.

Do you have any users or customers?

We ultimately expect this product to benefit consumers, but one of the early applications being developed for Mojo Lens is helping people with impaired vision better navigate the world around them. We’re working with Vista Center for the Blind and Visually Impaired and have been accepted into the FDA’s Breakthrough Device Program, which will help us bring our solution to market.

In parallel with our efforts around helping people with impaired vision, we also anticipate early adoption of Mojo Lens by “prosumer” sports enthusiasts that embrace technology to monitor and improve their performance. Examples of these prosumers could include athletes such as golfers, runners, cyclists, and skiers.

Mojo Lens has the potential for many applications in the enterprise as well. In industries like retail, customer service, and other patient/customer-facing roles like healthcare and hospitality, access to information is essential, but personal interactions and relationships are what they’re built on. By providing access to real-time information in a heads up, hands-free form factor, Mojo Lens can greatly improve productivity, increase quality, and allow users to build relationships without distracting devices in the way. Additionally, in roles that require compliance with technical or industry standards, the Mojo Lens can provide information to workers who need to complete tasks accurately and safely.

Any anecdotes/stories?

Every milestone that the Mojo team has crossed has been a major breakthrough in physics and technology. And we celebrate each one along the way. Our attitude going into the founding of this company is that there is no problem we can’t solve—as long as it doesn’t break the laws of physics.

One major milestone was when our CEO Drew Perkins became the first person to get fitted for and wear a prototype of Mojo Lens. A number of us gathered around the office to see his reaction of wearing the lenses. Imagine being able to see augmented reality images with your eyes even when they are closed! It was very exciting for the entire team.

How is the company funded?

To date, we’ve been able to raise more than $159M over four rounds of funding to develop our technology. These include investments from NEA, Shanda Group, Khosla Ventures, Advantech, Gradient Ventures, HP Tech Ventures, Motorola Solutions, LG Electronics, Liberty Global, Fusion Fund, Struck Capital, Dolby Family Ventures, Motorola Solutions Venture Capital, Intellectus Partners, KDDI Open Innovation Fund, Numbase, InFocus Capital Partners and more.

To learn more, visit Mojo Vision.","['mojo', 'future', 'information', 'ventures', 'market', 'contact', 'need', 'lenses', 'eyes', 'device', 'vision', 'working', 'technology', 'smart', 'lens', 'startup']","Steve Sinclair, SVP of Product and Marketing, Mojo Vision, responded by email to queries from AI Trends as part of our startup coverage.
Here are his responses:AI Trends: Describe your teamSteve Sinclair: Mojo Vision is led by a team of Silicon Valley veterans from companies including Apple, Amazon, Samsung, HP, Microsoft, Google, Motorola and others.
Mojo Vision is working to build the world’s first true smart contact lens, Mojo Lens.
One major milestone was when our CEO Drew Perkins became the first person to get fitted for and wear a prototype of Mojo Lens.
To learn more, visit Mojo Vision."
124,https://techcrunch.com/2021/02/03/evinced-raises-17m-to-speed-up-accessibility-testing-for-the-web/,TechCrunch is now a part of Verizon Media,2021-02-03 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
125,https://techcrunch.com/2021/02/02/trustlayer-raises-6m-seed-to-become-the-carta-for-insurance/,TechCrunch is now a part of Verizon Media,2021-02-02 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
126,https://research.aimultiple.com/self-driving-cars-stats/,35 Statistics about Self-Driving Vehicles Market in 2021,2021-02-02 07:28:43+00:00,"With Amazon acquiring Zoox, a self-driving company, in June 2020, competition among self driving companies will get even more fierce.

We compiled 35 up-to-date autonomous vehicle statistics from the surveys and researches of reputable sources. In this list, you will find self-driving stats that will help you understand the present and future of self-driving market.

Market Forecasts

55% of small businesses believe fleets is expected to be fully autonomous within 20 years. (Nissan News) The global autonomous car market is expected to grow from $5.6 billion in 2018 to $60 billion in 2030. The market definition includes component production and investments. (Businesswire) Annual production levels of robo-cars are expected to reach 800,000 units worldwide by 2030. (Statista*)

Adoption

More than 1,400 self-driving cars, trucks, and other vehicles are currently in testing by more than 80 companies across 36 U.S. states and Washington DC. ( TechCrunch ) More than 1.59 million registered drones are currently in the U.S., of which more than 372,000 are classified as commercial, with more than 136,000 registered as commercial. (TechCrunch) China has the potential to become the world’s largest market for autonomous vehicles. According to Mckinsey forecast, such vehicles could account for as much as 66% of the passenger-kilometers traveled in 2040. (Mckinsey)

How do people feel about self-driving vehicles?

Only 16% of respondents are “very likely” to ride as a passenger in an autonomous vehicle, while 28% of respondents state that they “not likely at all”. (Morning Consult) 22% of respondents believe self-driving cars are safer than the average human driver, while 35% of them believing self-driving cars are less safe than the average human driver. (Morning Consult) 51% of US citizens are interested about laws to make sure self-driving cars are safe. ( American Automobile Association ) 49% of US citizens want to know how vulnerable they will be to hackers. (American Automobile Association) 72% of U.S. adults would feel safer riding in a self-driving car if they had the ability to take over control if something goes wrong. (American Automobile Association) 57% of US citizens say they would like to have a clear understanding of who will be legally responsible in the event of a crash with a self-driving vehicle. (American Automobile Association) According to consumers, main barriers preventing consumers worldwide from adopting self-driving vehicles are Vehicle security against hackers (73%)

System security against hackers (72%)

Self-driving vehicles getting confused by unexpected situations (71%) (Statista**)

How COVID-19 changed public opinion on self driving cars

26% of consumers now view autonomous delivery technologies more favorably than before the coronavirus health crisis. (Consumer Technology Association) 49% of US consumers rate autonomous delivery technologies as somewhat or very favorable. (Consumer Technology Association) Lyft and Aptiv has successfully provided 100,000 commercial robotaxi rides in Las Vegas. 98% of these paying passengers have given 5 starts to their self-driving ride experience, with most stating this first-of-a-kind experience is something they are eager to try again. (Aptiv)

Facts & Stats on self-driving vehicle accidents

On November 20, 2018, the Uber crash in Tempe is the first recorded case of a pedestrian fatality involving a self-driving (autonomous) car. Uber self-driving cars were involved in 37 minor crashes before the crash that led to the death of Elaine Hezerberg. (Wikipedia) Google’s autonomous car, Lexus SUV, crashed with a bus in February 2016. (The Verge) Google’s Waymo had 18 minor accidents from 2019 through the first nine months of 2020. (Venture Beat) Tesla Model S and Tesla Model 3 caused fatal accidents in June 2016 and March 2019 while in autopilot mode. In both scenarios, the driver died. (Wired*) On average, a fatality occurs every 94 million miles in the US and every 60 million miles worldwide. According to Tesla, its Autopilot had been used for more than 130 million miles until its first fatal accident. However, this is not an apples-to-apples comparison as autopilot is used more often on highways and we couldn’t find more detailed data on crashes due to human error to do a better comparison. (The Verge*) Autonomous Vehicles (AVs) got into more crashes overall: 9.1 crashes per million miles driven, compared with 4.1 crashes per million miles for conventional vehicles. However, the ones involving injury were minor compared with the injuries sustained during conventional vehicle crashes. (GovTech)

Interesting self-driving statistic

Waymo cars hold the most autonomous miles. In 2020, Waymo has driven 20 million miles on public roads in 25 US cities. (VentureBeat) Waymo has conducted over 20 billion miles of simulation testing using synthetically generated data. (Waymo) Japan-based Tsukuba Mechanical produced the first autonomous passenger vehicle in 1977 that could recognize street markings while traveling at nearly 20 miles per hour, thanks to two vehicle-mounted cameras. (Wired)

Investments in Self Driving Vehicles

Tech giants

Waymo has raised a total of $3B in funding over 2 rounds. (Crunchbase) Tesla has raised a total of $20.2B in funding over 35 rounds. (Crunchbase) Audi plans to spend close to $16B on self-driving and sustainable tech by 2023. (CBInsights) Uber Advanced Technologies Group has raised a total of $1B in funding to further their research and make self-driving technology safer. (Crunchbase)

Key startups

Rivian has raised a total of $8.2B in funding over 9 rounds. (Crunchbase) Cruise has raised a total of $7.3B in funding over 8 rounds. (Crunchbase) Nuro has raised a total of $1.5B in funding over 4 rounds. (Crunchbase) Aurora has raised a total of $1.1B in funding over 3 rounds. (Crunchbase) Zoox has raised a total of $1B in funding over 6 rounds (Crunchbase) AIWAYS has raised a total of CN¥5.2B in funding over 4 rounds (Crunchbase)

If you want to learn more on autonomous technology, feel free to check our autonomous things article.

If you still have questions on self driving cars, don’t hesitate to contact us:","['vehicles', 'autonomous', '35', 'total', 'raised', 'vehicle', 'miles', 'statistics', 'rounds', 'selfdriving', 'funding', '2021', 'market', 'million']","We compiled 35 up-to-date autonomous vehicle statistics from the surveys and researches of reputable sources.
(TechCrunch) China has the potential to become the world’s largest market for autonomous vehicles.
(Aptiv)Facts & Stats on self-driving vehicle accidentsOn November 20, 2018, the Uber crash in Tempe is the first recorded case of a pedestrian fatality involving a self-driving (autonomous) car.
(Wired*) On average, a fatality occurs every 94 million miles in the US and every 60 million miles worldwide.
(The Verge*) Autonomous Vehicles (AVs) got into more crashes overall: 9.1 crashes per million miles driven, compared with 4.1 crashes per million miles for conventional vehicles."
127,https://www.sciencedaily.com/releases/2021/01/210130092754.htm,Photonics for artificial intelligence and neuromorphic computing,2021-01-21 00:00:00,"Scientists have given a fascinating new insight into the next steps to develop fast, energy-efficient, future computing systems that use light instead of electrons to process and store information -- incorporating hardware inspired directly by the functioning of the human brain.

A team of scientists, including Professor C. David Wright from the University of Exeter, has explored the future potential for computer systems by using photonics in place of conventional electronics.

The article is published today (January 29th 2021) in the journal Nature Photonics.

The study focuses on potential solutions to one of the world's most pressing computing problems -- how to develop computing technologies to process this data in a fast and energy efficient way.

Contemporary computers are based on the von Neumann architecture in which the fast Central Processing Unit (CPU) is physically separated from the much slower program and data memory.

This means computing speed is limited and power is wasted by the need to continuously transfer data to and from the memory and processor over bandwidth-limited and energy-inefficient electrical interconnects -- known as the von Neumann bottleneck.

As a result, it has been estimated that more than 50 % of the power of modern computing systems is wasted simply in this moving around of data.

Professor C David Wright, from the University of Exeter's Department of Engineering, and one of the co-authors of the study explains ""Clearly, a new approach is needed -- one that can fuse together the core information processing tasks of computing and memory, one that can incorporate directly in hardware the ability to learn, adapt and evolve, and one that does away with energy-sapping and speed-limiting electrical interconnects.""

Photonic neuromorphic computing is one such approach. Here, signals are communicated and processed using light rather than electrons, giving access to much higher bandwidths (processor speeds) and vastly reducing energy losses.

Moreover, the researchers try to make the computing hardware itself isomorphic with biological processing system (brains), by developing devices to directly mimic the basic functions of brain neurons and synapses, then connecting these together in networks that can offer fast, parallelised, adaptive processing for artificial intelligence and machine learning applications.","['artificial', 'hardware', 'neuromorphic', 'wasted', 'data', 'photonics', 'fast', 'computing', 'processing', 'directly', 'wright', 'systems', 'intelligence', 'von']","A team of scientists, including Professor C. David Wright from the University of Exeter, has explored the future potential for computer systems by using photonics in place of conventional electronics.
The article is published today (January 29th 2021) in the journal Nature Photonics.
The study focuses on potential solutions to one of the world's most pressing computing problems -- how to develop computing technologies to process this data in a fast and energy efficient way.
As a result, it has been estimated that more than 50 % of the power of modern computing systems is wasted simply in this moving around of data.
Photonic neuromorphic computing is one such approach."
128,https://www.aitrends.com/security/cybersecurity-tools-gaining-an-edge-from-ai/,Cybersecurity Tools Gaining an Edge from AI,2021-01-28 21:57:12+00:00,"By AI Trends Staff

In 2021, more firms will employ AI to battle cyberattacks, trying to gain an edge in a game of one-upmanship with hackers and attackers. A survey of 20 cybersecurity experts recently surveyed by Forbes showed some patterns.

For example, open source software can be an easy way into organizations. Gaining more visibility into open source contributions is possible with the use of AI and machine learning, according to Maty Siman, CTO of Checkmarx, a software security company based in Ramat Gan, Israel. “Rarely does a week go by without the discovery of malicious open source packages,” Siman stated.

Many developers are good at scanning open source components to identify known defects, “But they are still blind to instances where adversaries maliciously push tainted code into packages,” he stated. AI and ML can be used to help detect malicious open-source contributors with greater accuracy and efficiency at scale.

AI and ML comes into play—making it possible to detect malicious open-source contributors and packages with greater accuracy and efficiency and at scale. For example, the AI and ML algorithms can identify a scenario in which it’s the first open source project a user has contributed to, or whether the use is active in any public-facing social channel, or whether the user alters code in sensitive areas of the system, to help verify their credibility.

“This approach can essentially give open source contributors a ‘reputation score,’ making it easier for developers to vet both who they’re trusting and the packages they’re leveraging,” Siman stated. The company uses the term “DevSecOps” to refer to development, security, and operations, a commitment to software security at every step of the software development process.

Ivanti Offers “Zero Trust” Authentication

To combat password-related cyberattacks that continue in every industry, organizations need to adopt a “mobile-centric zero trust” security approach, recommends Bill Harrod, a vice president at Ivanti, an IT security company based in South Jordan, Utah. Applying AI and ML to authentication, companies can use a “more comprehensive set of attributes to determine compliance before granting access.” For example, the system can validate devices, establish user context, check app authorization, verify the network and detect and remediate threats before granting secure access to a device or user.

Ivanti’s Neurons for Patching Intelligence, for example, help fulfill service level agreements by achieving faster vulnerability remediation using supervised and unsupervised machine learning algorithms.

Privitar Offers Enterprise Data Privacy Software

Many organizations pursuing AI in 2021 will focus on privacy and security as critical elements of their data protection strategies, in the view of Steve Totman, Chief Product Officer at Privitar, a supplier of enterprise data privacy software based in London.

“Our digital dependence accelerated throughout 2020 and heightened the need for embracing data privacy as a core element of business dataops [data operations], especially where AI and ML is being embraced,” Totman stated.

In Privitar’s view, privacy technologies must provide multi-level controls automatically to ensure data is protected, usability is preserved and remediation happens in the event of a breach.

The latest version of the company’s Data Privacy Platform includes Right to be Forgotten functionality consistent with European General Data Protection Regulation (GDPR) provisions.

AI Assistance for Humans May Ease Cybersecurity Skills Challenge

The shortage of IT staff skilled in cybersecurity is driving the need to rely more on AI software, while understanding the proper role of the humans involved. “This is why the focus in 2021 is not on which AI/ML engine has the most features or the lowest error rate—it’s moving over to which AI approaches integrate humans into the process in the best way,” stated Mike Lloyd, CTO at RedSeal, a cloud security provider based in San Jose.

Today’s AI is still short-sighted, easily fooled and unable to grasp the human motivations of bad actors, in Dr. Lloyd’s view. Going forward, the company sees that the focus will increasingly shift away from black boxes—inscrutable engines that compute correlations that nobody can understand and which are often biased in significant ways—and toward more transparent reasoning approaches. This is where AI present presents its recommendations along with reasoning that humans can follow, to understand why a given conclusion is important.

Writing in RedSeal’s blog on the SolarWinds attack discovered in December that gave hackers access to potentially hundreds of targets, Lloyd stated, “The attack is extremely sophisticated, and quite alarming—it’s a supply chain attack, involving compromise of a widely used and trusted monitoring product.”

For companies scrambling to respond, RedSeal advised determining whether the SolarWinds Orion is being used in the company, and if so where, and what type of access it has. The company suggests taking the product offline, blocking unwanted access to it to the extent possible, and reset all assets it could have reached to a known good state. RedSeal’s software can be helpful in executing these steps, Lloyd suggested.

Remote Workforce Increasing Cybersecurity Risks for Organizations

In the first six months of the pandemic, 48% of US knowledge workers said they had experienced targeted phishing emails, calls, or texts in a professional capacity, according to a survey from GreatHorn, which offers protection from advanced threats.

SailPoint of Austin, Texas, takes an identity management approach to security. “A well-managed identity governance program can thus be costly and out of reach for many organizations, yet AI is already starting to change this and the trend will accelerate in 2021,” stated Grady Summers, EVP, Solutions and Technology at SailPoint.

AI applied to identity management will enable detection of more risky users, patterns, and anomalies in access requests, and reduction in cumbersome re-certification processes, the company suggests.

“Regulators will start to become comfortable with AI-driven decisions as they realize that machines will deliver smarter and faster results vs. overwhelmed humans trying to determine who can access what and when,” stated Summers.

General Electric is using SailPoint for identity management for 1.8 million employees and over 1,800 business applications. “SailPoint’s identity platform scaled at the rate that our growing, global organization required,” stated Eric Schwab, principal technologist at GE, in a customer story on SailPoint’s website.

Read the source article in Forbes; for more information, visit Checkmarx, Ivanti, Privitar, RedSeal and SailPoint.","['open', 'access', 'data', 'privacy', 'stated', 'ai', 'security', 'cybersecurity', 'software', 'tools', 'source', 'gaining', 'edge', 'company']","By AI Trends StaffIn 2021, more firms will employ AI to battle cyberattacks, trying to gain an edge in a game of one-upmanship with hackers and attackers.
For example, open source software can be an easy way into organizations.
Gaining more visibility into open source contributions is possible with the use of AI and machine learning, according to Maty Siman, CTO of Checkmarx, a software security company based in Ramat Gan, Israel.
The latest version of the company’s Data Privacy Platform includes Right to be Forgotten functionality consistent with European General Data Protection Regulation (GDPR) provisions.
AI Assistance for Humans May Ease Cybersecurity Skills ChallengeThe shortage of IT staff skilled in cybersecurity is driving the need to rely more on AI software, while understanding the proper role of the humans involved."
129,https://www.aitrends.com/financial-services/financial-firms-turning-to-ai-to-fight-fraud-in-2021/,Financial Firms Turning to AI to Fight Fraud in 2021,2021-01-28 21:37:26+00:00,"By AI Trends Staff

The use of AI to fight financial fraud—internally and externally—is a hot topic.

“AI is the future of fraud management, irrespective of the system you are using,” stated Svetlana Belyalova, head of operational risk management at Rosbank, Societe Generale Group, during a recent webcast hosted by Risk.net. “It brings a lot of value in both data management and decision-making.”

A firm’s maturity and operational processes for fraud management are key to selecting the technology that will be right for it, suggested Belyalova. Firms that had taken a more siloed approach by fitting technology to a certain type of fraud, now want to take a more holistic approach and tap the AI capabilities of the fraud systems.

“What we really need to know better is how to manage these AI capabilities in our real-time environment—how to make them more effective, and how to make these systems learn from our [ever-evolving] day-to-day situations,” she stated.

Whereas AI capabilities might have been “nice to have” among the tools financial institutions use to fight fraud, today, “AI is becoming a must-have for analysts to decide whether transactions are fraudulent, stated Amir Shachar, lead fraud research data scientist at NICE Actimize of Raanana, Israel, a supplier of software to combat financial crime and ensure compliance. NICE, for Neptune Intelligence Computer Engineering, was founded by seven former Israeli army colleagues.

It is early days in the banking industry for fighting fraud with new technologies. Some early adopters have implemented advanced platforms incorporating AI, and others are still depending on older systems and existing processes. The group head of operational risk at Allied Irish Bank, Charles Forde, encouraged early adopters to talk about what is and is not working, so other banks can learn and derive best practices.

This would not be only about what technologies are being used, but also the approaches and operating models employed. “I think there’s still a big variance in different firms in how the technologies are being applied, and in the operating model,” he stated. “In some firms it’s primarily all in the first line. In some, the concentration of knowledge is in the second line. Ultimately, this activity should sit next to the business that it is supporting, regardless of what type of business you’re in.”

Bank Fraud Seen Costing At Least $7.1 Billion Annually

Sizing the cost of bank fraud is challenging. The Association of Certified Fraud Examiners’ (ACFE) 2018 Report to the Nations has found that the total losses caused by fraud exceed $7.1billion. However, this is only known losses. The ACFE claims that this figure does not come close to representing the total amount of fraud losses, and the true global cost of fraud is probably “magnitudes higher” due to undetected and indirect costs.

KPMG’s 2019 Global Banking Fraud Survey, with responses from 43 banks worldwide, found that 52% of banks were not monitoring the total cost of fraud risk management, according to a recent report from fcase, a data aggregation hub supporting fraud management services, based in London.

A fraud risk management model is a framework outlining all processes related to how fraud can be identified, assessed, mitigated, monitored, and reported to senior management.

An effective fraud risk management model needs to build risk awareness, accountability, and transparency into how fraud is being actively managed by banks and financial institutions, the report suggests. According to Deloitte, it enables organizations to have controls which initially prevent fraud from taking place, detects the fraud as soon as it occurs and finally responds effectively to fraud incidents.

The Association of Certified Fraud Examiners (ACFE) states that for a fraud risk management approach to function well, it must be proactive rather than reactive.

KPMG found major differences in which internal parties were responsible for setting the fraud risk tolerance for the organization, with 52% saying it was done by their Board/Risk Committee. “This shows there is still a lot to work on,” the report states. “With fraud activity increasing at a rapid pace costing banks and financial institutions billions every year, the right fraud risk management operating models can help manage the damage created by fraudsters.”

AI Seen As Worthwhile Investment for Combating Fraud by Surveyed Banks

The use of AI and machine learning to combat fraud and money laundering was seen as a worthwhile investment in a survey of banks that invested in AI conducted by analyst company Ovum. Over 80% believed the investment in AI generated a return on investment, according to a report on the blog of FICO, the data analytics company based in San Jose.

AI is being employed by the attackers also. “While we’re meeting to discuss how to tackle fraud and financial crime, elsewhere the criminals are holding their own conferences to plan their attacks,” stated Julie Conroy, director of the Fraud and AML practice at Aite Group, market researchers based in Boston, at a recent conference from Finovate, a conference company focused on banking and financial technology.

Conroy pointed out that fraud and money laundering are financing some of the worst crimes society faces, including human trafficking, terrorism, and the operations of drug cartels.

Banks investing in data science teams need to also provide them with the tools to operationalize the work they have done, suggested Doug Clare, who oversees FICO’s fraud and compliance solutions, at the Finovate conference, “Banks need to pivot quickly on their experience of the financial crime they are seeing and get the models they develop into operation fast,” he said. “Without investment in the right platforms they can’t do that.”

The AI in use by banks must be explainable as well. “Organizations that deploy AI and machine learning to detect fraud and money laundering must therefore take care that the models they use are not ‘black box’,” stated Sarah Rutherford, Senior Director, FICO, and author of the recent blog post.

AI models are not infallible. As FICO Chief Analytics Officer Scott Zoldi stated in his post ‘Bank of England Validates Need for Explainable AI’ the sheer size and complexity of these models make it difficult to explain their operating processes to people. Zoldi outlined techniques being developed to make AI explainable, for those using the right models.

Read the source articles in Risk.net, from fcase and the FICO blog.","['fraud', 'models', 'investment', 'management', 'banks', 'stated', 'financial', 'ai', 'firms', 'turning', '2021', 'fight', 'report', 'risk']","By AI Trends StaffThe use of AI to fight financial fraud—internally and externally—is a hot topic.
“It brings a lot of value in both data management and decision-making.”A firm’s maturity and operational processes for fraud management are key to selecting the technology that will be right for it, suggested Belyalova.
A fraud risk management model is a framework outlining all processes related to how fraud can be identified, assessed, mitigated, monitored, and reported to senior management.
An effective fraud risk management model needs to build risk awareness, accountability, and transparency into how fraud is being actively managed by banks and financial institutions, the report suggests.
The Association of Certified Fraud Examiners (ACFE) states that for a fraud risk management approach to function well, it must be proactive rather than reactive."
130,https://www.aitrends.com/transportation/ai-seen-helping-to-reduce-pollution-save-fuel-ease-traffic/,"AI Seen Helping to Reduce Pollution, Save Fuel, Ease Traffic",2021-01-28 21:28:07+00:00,"By AI Trends Staff

Vehicles stopping for red lights, idling as they wait for the signal lights to change and accelerating to get back up to speed wastes fuel and adds pollutants to the air. Idling vehicles waste more than 6 billion gallons of gasoline and diesel combined every year, according to Department of Energy (DOE) estimates.

Seeking a better way, the DOE last year awarded $1.89 million to researchers at the University of Tennessee-Chattanooga, the University of Pittsburgh, Georgia Institute of Technology, Oak Ridge National Laboratory and the City of Chattanooga to create a new model for traffic intersections that reduces energy consumption and improves the flow of traffic.

The goal of the project is to develop an automated traffic control system that would reduce corridor-level fuel consumption by 20%, while maintaining a safe and efficient transportation environment. The researchers intend to apply AI and machine learning to support a number of smart transportation applications, including emergency vehicle preemption, transit signal priority and pedestrian safety, according to officials at Pitt quoted in an account from GCN.

“Our vehicles and phones have combined to make driving safer while nascent intelligent transportation systems have improved traffic congestion in some cities. The next step in their evolution is the merging of these systems through AI,” stated Aleksandar Stevanovic, director of the Pittsburgh Intelligent Transportation Systems Lab. “Creation of such a system, especially for dense urban corridors and sprawling exurbs, can greatly improve energy and sustainability impacts,” he said, noting that transportation will rely heavily on gasoline-powered vehicles for some time.

Oak Ridge National Lab is working on part of the problem, in a project using overhead cameras and roadway sensors to identify gas guzzling commercial trucks in traffic. AI and machine learning algorithms identify the least-efficient vehicles, then track their path and speed in order to change the traffic signals up ahead. This eliminates some degree of the inefficient starting and stopping at intersections and minimizes fuel consumption.

The testing is being conducted on an existing smart corridor built from a 2014 partnership between the Oak Ridge National Laboratory and the Electric Power Board (EPB) of Chattanooga as part of an effort to develop new energy technologies. The corridor employs cameras, LIDAR, radar, software-defined radios, wireless communications and sensors for air quality and audio. These collect information from their spots on poles along a 10-block section of Martin Luther King Boulevard in the city’s downtown. A 10 Gbps fiber network underlies the smart city testbed, enabling real time data transmission.

Smart AI Cameras Sees as Becoming More Widespread

Smart AI cameras will transform traffic management by 2025, according to a new report from ABI Research, technology analysts, described in an account in SmartCitiesWorld. The cameras will enable machine vision applications such as pedestrian detection and alerting.

The company projects that more than 155,000 AI-based cameras will be in use by 2025, up from 33,000 in 2020. In the Edge Analytics Cloud Use Cases in Smart Cities and Intelligent Transportation research report, traffic management applications include adaptive traffic lights, vehicle prioritization and preemption, parking access and detection, and electronic tolling.

Camera system revenue will grow from $46M in 2020 to $189M in 2025, according to Dominique Bonte, a vice president at ABI Research. ”Advanced AI-capable processors featuring hardware acceleration for high-performance neural net software frameworks from silicon vendors like Intel, Nvidia, and Qualcomm are propelling smart cameras into the mainstream, offering more features and flexibility at lower price points compared with legacy traffic and electronic toll collection (ETC) sensors like magnetic loops and radio frequency identification (RFID),” he stated.

A low latency computer network is one that is optimized to process a high volume of data messages with minimal delay or latency. The deployment of 5G and vehicle-to-everything (V2X) connectivity will allow moving low latency analytics to the edge of telco networks—referred to as edge cloud, network cloud, multi-access edge computing (MEC) or distributed cloud—will enable a new range of application categories across larger geographical areas, ABI foresees.

These will include:

road intersection management: cooperative adaptive traffic lights and remote traffic management;

safety and security operations: crowdsourced hazard and security alerts and remotely controlled response management systems installed on light poles, buildings and other street furniture; and

autonomous asset management: remote control and operation of driverless vehicles, drones and robots.

“In most cases the edge cloud will not replace the roadside edge but rather complement and enhance local safety and security systems into more aggregated, collective, cooperative, and holistic solutions including feeding urban digital twins with actionable local intelligence,” stated Bonte.

Texas A&M Team Using Deep Neural Network for Signal Controller

Researchers at Texas A&M University are applying reinforcement learning to the study of traffic management. The team is applying learning algorithms that reward favorable outcomes in an effort to optimize the signal controller to make decisions that improve operations, in this case, a reduction in the buildup of traffic delays.

The model is using a deep neural network (DNN) machine-learning algorithm, which tend to be unpredictable and inconsistent in their decision-making, making it challenging to work with them, said Guni Sharon, professor in the Department of Computer Science and Engineering at Texas A&M, in an account from Futurity. To overcome this, Sharon and his team defined and validated an approach that can successfully train a DNN in real time while transferring what it has learned from observing the real world to a different control function that can be better understood and regulated by engineers.

Using a simulation of a real intersection, the team found that their approach was effective for optimizing their interpretable controller, resulting in up to a 19.4% reduction in vehicle delay in comparison to commonly deployed signal controllers. The researchers said it took about two days for the controller to understand what actions help to mitigate traffic congestion.

“Our future work will examine techniques for jump-starting the controller’s learning process by observing the operation of a currently deployed controller while guaranteeing a baseline level of performance and learning from that,” Sharon stated.

Read the source articles at GCN, SmartCitiesWorld and Futurity.","['vehicles', 'save', 'reduce', 'helping', 'pollution', 'transportation', 'management', 'learning', 'ai', 'systems', 'seen', 'cameras', 'ease', 'fuel', 'smart', 'traffic', 'edge']","“Our vehicles and phones have combined to make driving safer while nascent intelligent transportation systems have improved traffic congestion in some cities.
The next step in their evolution is the merging of these systems through AI,” stated Aleksandar Stevanovic, director of the Pittsburgh Intelligent Transportation Systems Lab.
AI and machine learning algorithms identify the least-efficient vehicles, then track their path and speed in order to change the traffic signals up ahead.
Smart AI Cameras Sees as Becoming More WidespreadSmart AI cameras will transform traffic management by 2025, according to a new report from ABI Research, technology analysts, described in an account in SmartCitiesWorld.
Texas A&M Team Using Deep Neural Network for Signal ControllerResearchers at Texas A&M University are applying reinforcement learning to the study of traffic management."
131,https://www.aitrends.com/ai-insider/ai-autonomous-cars-as-drug-mules-for-narco-dealers-is-a-bad-prescription/,AI Autonomous Cars As Drug Mules For Narco Dealers Is A Bad Prescription,2021-01-28 21:15:24+00:00,"By Lance Eliot, the AI Trends Insider

The number of ways to transport illegal drugs seems to be nearly endless. We all have heard about the use of airplanes to smuggle in illicit drugs. There are also tales aplenty about motorboats and sailboats loaded with banned narcotics that try to reach land.

Here’s a twist that you might not have considered. A recent news story described a narco mini-submarine that was scuttled in shallow waters after three men operating the vessel opted to evacuate and escape as authorities were closing in on them. Reportedly containing an estimated $100 million in cocaine, speculation is that the crew might not have known for sure that the gig was up, and chose to open the valves to sink the narco-sub as a precaution to try and hide the three metric tons of illegal drugs (figuring they could always come back to retrieve the loot, plus they might save their own lives by being able to show to the drug lord that the stuff was still intact and had not been siphoned off or pilfered).

I was asked by some readers whether we might ultimately have Autonomous Vehicles (AVs) that will be employed for criminal trafficking acts, doing so by removing the human element involved in driving or otherwise guiding a craft that contains an illicit drug shipment.

Sadly, yes, this can be expected and to some degree is already underway, including the nefarious use of autonomous submersibles, autonomous water surface craft such as sailboats and motorized ships, autonomous drones that fly in the air, and of course the use of ground-based autonomous transportation such as self-driving cars.

Not only will a particular mode of AV be used, such as in the air, in the water, or on the ground, you can bet your bottom dollar that there will be devious efforts combining those avenues, a trifecta as it were.

Though we all prefer to think about technological innovations such as self-driving cars as being built and fielded for beneficial purposes and striving toward the good of humanity, there is no sense in hiding from the inevitable fact that these marvels will be stridently used for untoward aims too.

Bad people will do bad things, even with the greatest advances in AI.

Let’s consider why the use of self-driving cars would be alluring to drug trafficking and then ponder ways that this might be mitigated or defeated.

On a related note, some argue that no one should discuss these matters as it will merely give dreadful thieves some new ideas of what to do. This is a classic dilemma that confronts the computer field (and other realms) all the time. For example, in the cybersecurity realm, some try to suggest that research about cracking computer systems should not be published and nor discussed at conferences. Keep it all under tight wraps, they say.

But the proverbial head-in-the-sand approach is essentially fatally flawed in that the crooks will one way or another discover or find out the latest break-in practices, and then we will all be caught flat footed by not having properly prepared for that eventuality. Generally, within reasonable limits, it tends to make sense to bring these topics into the open and thus increase awareness overall, aiming to effectively handle criminal behavior.

Here, then, is today’s intriguing question: Will the advent of AI-based true self-driving cars potentially be used in adverse ways including becoming narco mules in the illicit drug trade?

Let’s unpack the matter and see.

For my framework about AI autonomous cars, see the link here: https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/

Why this is a moonshot effort, see my explanation here: https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/

For more about the levels as a type of Richter scale, see my discussion here: https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/

For the argument about bifurcating the levels, see my explanation here: https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/

Understanding The Levels Of Self-Driving Cars

As a clarification, true self-driving cars are ones that the AI drives the car entirely on its own and there isn’t any human assistance during the driving task. These driverless vehicles are considered a Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at a Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems). There is not yet a true self-driving car at Level 5. We don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.

Meanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).

Since semi-autonomous cars require a human driver, the adoption of those types of cars won’t be markedly different than driving conventional vehicles, so there’s not much new to cover about them on this topic (though, as you’ll see in a moment, the points next made are generally applicable).

For semi-autonomous cars, it is important that the public needs to be forewarned about a disturbing aspect that’s been arising lately, namely that despite those human drivers that keep posting videos of themselves falling asleep at the wheel of a Level 2 or Level 3 car, we all need to avoid being misled into believing that the driver can take away their attention from the driving task while driving a semi-autonomous car.

You are the responsible party for the driving actions of the vehicle, regardless of how much automation might be tossed into a Level 2 or Level 3.

For why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here: https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/

To be wary of fake news about self-driving cars, see my tips here: https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/

The ethical implications of AI driving systems are significant, see my indication here: https://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/

Be aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms: https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/

Self-Driving Cars And Drug Trafficking Mules

For Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task. All occupants will be passengers. The AI is doing the driving.

First, contemplate the presumed advantage of not having a human driver when attempting to transport illegal drugs.

When using human drivers, there is always the possibility that the driver will decide to abscond with the loot. They might drive the car to a secret rendezvous and sell the drugs to someone other than the intended receiver of the cache. Or maybe the driver does the big switch, whereby they replace the real drugs with some other fakery, and then appear to deliver the intended drugs to the designated destination.

Another possibility is that the driver gets taken over by someone else that wishes to steal the drugs, getting killed, or kidnapped in the process. Additionally, an alternative driver might take their place and either ditch the vehicle after removing the drugs or could pretend to be the original driver and go to the delivery location as planned. There are lots of double-crossing tricks that can be played.

There is also the everyday kind of chance that a driver might do something unwisely out-of-step. Suppose the driver decides to get drunk and then smashes the car into a telephone pole, bringing forth the cops and getting stupidly caught. One supposes that a driver could be sober and nonetheless still get into a car accident, once again likely exposing the plot. It seems like we frequently hear reports of drivers that roll through a stop sign, ignoring the lawful rules of driving, and end-up with a police officer pulling them over, leading to the discovery of tons of prohibited drugs in the vehicle.

In short, human drivers for partaking in illegal drug trafficking are a substantive problem for those in the business of drug trafficking. You see, it is a lot harder than it seems when trying to be a successful drug lord, though we ought not to shed a tear over such woes.

What can be done about those dastardly unreliable and double-crossing drivers? Replace them with AI.

Envision it this way. A self-driving car is stocked with forbidden drugs. The overseer then instructs the AI driving system to proceed on a journey to a particular destination. The AI dutifully starts driving the car and will indubitably arrive at the stipulated location. No human driver is involved.

Your initial reaction might be that the AI should be “smart enough” to realize that it is being used in a drug hauling operation, acting as a veritable drug mule.

Keep in mind that today’s AI is not sentient. We aren’t even close to achieving sentience. Furthermore, AI currently lacks any semblance of common-sense reasoning. All in all, if you are wishing that the AI will know right from wrong, that is a rather farfetched dream that regrettably does not yet exist and will be a long time coming before turning into reality (perhaps someday).

In this use case, the AI for a self-driving car is going to do as instructed, which would simply be the act of driving from point A to point B.

Plus, the contents housed within the vehicle is not something of significance for the AI.

It doesn’t especially matter whether people are inside the self-driving car, other than the need to try and maintain a smooth ride such as not taking tight turns if there are passengers on-board.

I’ve exhorted extensively that we are going to witness seemingly empty self-driving cars much of the time since there is likely to be a hotly competitive landscape of roaming self-driving cars. Fleet owners are hoping that their self-driving car will be more likely to get chosen for ride-sharing than some competing ones and thus will need to keep their autonomous vehicles wandering in anticipation of being in the right place at the right time for a paid lift request.

Hence, the point is that you might have been thinking that surely an otherwise empty self-driving car (having hidden drugs) will be readily spotted, and someone would detain the vehicle. Nope. The cruising around of empty self-driving cars will become a customary practice. Today, we all would certainly stare in awe and utter amazement but after thousands upon thousands of self-driving cars on our streets and highways, seeing them zipping back-and-forth with no one inside will be considered routine and ostensibly boring.

Admittedly, hard to imagine.

In any case, a self-driving car that is “empty” does not necessarily need to be indeed bereft of something inside it. Though there might not be people inside a self-driving car, there can be any kind of cargo or other elements (for example, you might send your beloved pet dog over to the vet, alone inside a self-driving car).

A self-driving car could be filled with birthday gifts that you got for your best friend that lives across town, and you want the AI to drive those over to your friend’s house.

In theory, the distance doesn’t make a difference either.

Suppose you have some precious items such as jewels, and you think it unsafe to send them via regular mail or overnight courier from Los Angeles to your aunt in New York. Presumably, you could place them into a self-driving car and merely give the AI an address in New York City as the destination. Away the vehicle goes, on a cross-country trek, and the only stops needed would be to get fuel. Note that there are no stops for getting food, no need for rest breaks, and so on.

Okay, so we’ve established that self-driving cars are undeniably handy as a drug mule since there is no human driver needed and the AI will not necessarily suspect that anything is afoot. The AI will, without any presumed hesitation, drive to the desired destination. It would seem that we’ve removed any qualms about double-crossing and other issues of the driver doing something stupid while driving.

The expectation is that self-driving cars will drive strictly by the book, as it were, and never step out of line in terms of driving illegally. This, then, shoots down the chances of the self-driving car getting pulled over for running a stop sign or rushing through a red light.

You could hide the drugs somewhere inside the self-driving car. Maybe place the drugs into boxes and put those inside the normal interior of the vehicle, though perhaps that is rather chancy and might attract undue attention. Instead, the drugs could be placed into the trunk or within the structure of the vehicle. Remember the famous scene of (spoiler alert) Gene Hackman searching for drugs in the movie French Connection?

The drugs could be completely out-of-sight while somewhere embedded within the self-driving car. One might view this as a handy precaution, in case somebody somehow decides to stop the self-driving car and take a quick look inside it.

That is a bit of a teaser.

Up until this point in the discussion, it sure seems like a self-driving car is a perfect way to transport illicit drugs. We need to consider how this might not be quite so perfect for a crime.

For more details about ODDs, see my indication at this link here: https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/

On the topic of off-road self-driving cars, here’s my details elicitation: https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/

I’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop: https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/

Expect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here: https://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/

Mitigating Factors Of The Mule Crime

Life is never easy, not even for lawbreakers. We’ll start with the perhaps biggest hurdle for someone using a self-driving car in this evil way. Do you trust the self-driving car?

I know that seems like an odd question. From a trusted perspective, I am not referring to whether the AI will drive the car safely. We are all pretty much making a base assumption that the only way that we’ll all be allowing self-driving cars on our streets is due to a presumption that they will drive as safely or even more so than human drivers. That’s a given. Those are the table stakes for self-driving cars.

Here’s why a drug lord might be queasy about using a self-driving car.

AI is running the vehicle. This is essentially unseen mechanization. For a human driver, the overseer presumably knows who the person is that is doing the driving. There can be pressure brought to bear on that person to make sure they drive the drugs to the right locale. Furthermore, to some degree, one can assume that the driver will try to protect the shipment (assuming they are following the letter of their orders).

In the case of the AI driving system, it does not have any such equivalent motives nor similar pressure points. With a human driver, someone sworn to loyalty might veer off the path and do something the overseer dislikes. This is not quite the same for AI.

Whoever is the fleet operator of the AI driving system can generally opt to divert the self-driving car or in some manner seek to take over the driving aspects (kind of). This can be done via electronic communications to the self-driving car, such as via the use of OTA (Over-The-Air) messaging between the cloud of the fleet owner (or an automaker or self-driving tech firm) and the AI driving system.

Suppose that a self-driving car is nearing an area that has become flooded by a hurricane. The fleet owner (or similar) might send a message to all their self-driving cars to avoid getting mired in the flooded streets, as such, this might include not being able to then proceed to a designated destination in that vicinity. Perhaps the self-driving cars will be diverted to a waiting area, or come to a halt on a side street and wait for the flood to recede, or maybe the sender will be asked where the vehicle should go as an alternative destination.

The point is that this kind of overarching control is something the overseer might be leery of.

Unless they somehow can get access to the master control point, there is always a chance that the self-driving car will not end-up going where they presume the AI is going to take the vehicle. That’s a loose end, for sure. The same could be said of a human driver, but as mentioned earlier, the human driver can be more readily bullied.

Another concern is the traceability aspects.

Presumably, the AI driving system is keeping track of where it is and where it is going. This is likely to be shared with the fleet owner cloud system (or similar).

Also, the sensors of the self-driving car include video cameras, radar, LIDAR, ultrasonic, thermal, and other such devices, all of which are likely recording whatever they detect around the vehicle. In prior columns, I’ve touted that this provides a handy roving eye for some useful purposes, though it also raises serious privacy intrusion concerns.

It is conceivable that this tracing capability could be used to capture the crooks involved in undertaking the illegal drug transport effort. That being said, they might try various ways to avoid getting snagged in that manner.

Yet another issue for those bad actors is that without a human driver there is essentially no chance of high-speed pursuit and escape. If the police ascertain that the self-driving car might have drugs, it will be a relatively easy matter of having the self-driving car brought to a stop, and no need for those wild and dangerous cars to chase.

Suppose too that someone else who is also a crook finds out about the self-driving car being a mule, and they want to steal the drugs. They could potentially wait along the route of the self-driving car, and when it comes to a stop sign or red light, attempt to break into the vehicle, perhaps switching off the engine and hauling away the deadened car or jumping inside to search for and then leap out with the drugs in-hand.

You might contend that the drug overseer could cope with those issues by putting their own crew inside the self-driving car to protect the hidden drugs within. Aha, if you did that, once again there are humans in the middle of the caper. As such, there is a tradeoff as to whether having humans present versus no humans present rears its head again.

Splitting hairs, you could argue that by having a non-driving crew that the situation is assuredly different from having a crew member that is doing the driving. Also, there is the complicating factor of whether the passengers are aware of what the vehicle is carrying, or they might be unwitting accomplices.

Yet another angle is the chance of someone hacking the AI to subvert what the AI is supposed to do. This might enable a cyber crook to merely alter the destination to their own preferred address. In theory, this is hopefully going to be very improbable since it also opens a can of worms for those of us that will be using self-driving cars on an ordinary everyday basis (see my column postings for details about the cybersecurity facets of self-driving cars, including robo-jacking).

Lots of permutations and combinations ensue.

Conclusion

Sometimes, people upon hearing about this drug mule scenario involving self-driving cars are quick to say that we ought to have the police or other authorities monitoring the AI driving systems. This would apparently somehow enable the authorities to attempt to identify these illicit traffic activities.

Now that’s a whale of a Pandora’s box.

There is a slew of AI ethics considerations that come along with that kind of permitted routine monitoring. In an era that has all of us regularly using AI-based true self-driving cars, will we feel comfortable that our every trip and move is potentially being seen and tracked by our government?

I’ve previously emphasized that there are nation-states that are going to welcome the advent of self-driving cars for that very reason. It remains to be seen whether a free nation would find it justifiable as a potential means to detect or curtail the adverse uses of self-driving cars.

Time will tell.

Copyright 2021 Dr. Lance Eliot. This content is originally posted on AI Trends.

[Ed. Note: For reader’s interested in Dr. Eliot’s ongoing business analyses about the advent of self-driving cars, see his online Forbes column: https://forbes.com/sites/lanceeliot/]

http://ai-selfdriving-cars.libsyn.com/website","['autonomous', 'drugs', 'prescription', 'narco', 'driver', 'vehicle', 'bad', 'ai', 'drug', 'driving', 'cars', 'selfdriving', 'car', 'human', 'mules', 'dealers']","I’ve exhorted extensively that we are going to witness seemingly empty self-driving cars much of the time since there is likely to be a hotly competitive landscape of roaming self-driving cars.
In the case of the AI driving system, it does not have any such equivalent motives nor similar pressure points.
The same could be said of a human driver, but as mentioned earlier, the human driver can be more readily bullied.
I’ve previously emphasized that there are nation-states that are going to welcome the advent of self-driving cars for that very reason.
Note: For reader’s interested in Dr. Eliot’s ongoing business analyses about the advent of self-driving cars, see his online Forbes column: https://forbes.com/sites/lanceeliot/]http://ai-selfdriving-cars.libsyn.com/website"
132,https://www.theguardian.com/science/2021/jan/26/us-has-moral-imperative-to-develop-ai-weapons-says-panel,"US has 'moral imperative' to develop AI weapons, says panel",2021-01-26 00:00:00,"The US should not agree to ban the use or development of autonomous weapons powered by artificial intelligence (AI) software, a government-appointed panel has said in a draft report for Congress.

The panel, led by former Google chief executive Eric Schmidt, on Tuesday concluded two days of public discussion about how the world’s biggest military power should consider AI for national security and technological advancement.

Its vice-chairman, Robert Work, a former deputy secretary of defense, said autonomous weapons are expected to make fewer mistakes than humans do in battle, leading to reduced casualties or skirmishes caused by target misidentification.

“It is a moral imperative to at least pursue this hypothesis,” he said.

The discussion waded into a controversial frontier of human rights and warfare. For about eight years, a coalition of non-governmental organisations has pushed for a treaty banning “killer robots”, saying human control is necessary to judge attacks’ proportionality and assign blame for war crimes. Thirty countries including Brazil and Pakistan want a ban, according to the coalition’s website, and a UN body has held meetings on the systems since at least 2014.

While autonomous weapon capabilities are decades old, concern has mounted with the development of AI to power such systems, along with research finding biases in AI and examples of the software’s abuse.

The US panel, called the National Security Commission on Artificial Intelligence, in meetings this week acknowledged the risks of autonomous weapons. A member from Microsoft for instance warned of pressure to build machines that react quickly, which could escalate conflicts.

The panel only wants humans to make decisions on launching nuclear warheads.

Still, the panel prefers anti-proliferation work to a treaty banning the systems, which it said would be against US interests and difficult to enforce.

Mary Wareham, coordinator of the eight-year Campaign to Stop Killer Robots, said the commission’s “focus on the need to compete with similar investments made by China and Russia … only serves to encourage arms races.”

Beyond AI-powered weapons, the panel’s lengthy report recommended use of AI by intelligence agencies to streamline data gathering and review; $32bn (£23.3bn) in annual federal funding for AI research; and new bodies including a digital corps modelled after the army’s Medical Corps and a technology competitiveness council chaired by the US vice-president.

The commission is due to submit its final report to Congress in March, but the recommendations are not binding.","['weapons', 'autonomous', 'develop', 'panel', 'imperative', 'ai', 'moral', 'treaty', 'security', 'work', 'systems', 'report', 'intelligence']","The US should not agree to ban the use or development of autonomous weapons powered by artificial intelligence (AI) software, a government-appointed panel has said in a draft report for Congress.
Its vice-chairman, Robert Work, a former deputy secretary of defense, said autonomous weapons are expected to make fewer mistakes than humans do in battle, leading to reduced casualties or skirmishes caused by target misidentification.
“It is a moral imperative to at least pursue this hypothesis,” he said.
The US panel, called the National Security Commission on Artificial Intelligence, in meetings this week acknowledged the risks of autonomous weapons.
Still, the panel prefers anti-proliferation work to a treaty banning the systems, which it said would be against US interests and difficult to enforce."
133,https://research.aimultiple.com/chatbot-best-practices/,Top Chatbot Development Best Practices in 2021,2021-01-24 16:03:10+00:00,"We have examined 30 successful chatbot examples and a list of chatbot failures before. Now, we want to explain what we learned from studying those chatbot case studies. Here are best practices of the chatbot development process:

Identify your target audience and understand their needs

Improving customer experience is the first reason why most companies are deploying a chatbot. You should understand who your customers are. This will enable you to create relevant conversation flows including FAQs and special offers.

Restrain your ambition and set realistic goals

The scope of your chatbot is crucial. Save for 2 bots, all bots in the list are laser-focused. For example, Wordsmith takes in structured data to prepare reports, visabot prepares immigration forms.

Even Facebook M’s general-purpose answering machine got shut down. So if even Facebook does not want to handle chatbot queries in any context, then maybe you shouldn’t, too. At least for now.

Let’s look into the more generalist bots. Microsoft’s XiaoIce is impressive, but you need a Microsoft-caliber research team to build a bot with such a strong understanding of the world. As for Mitsuku, she has been under development since 2005, and while she is engaging, she is not as engaging as XiaoIce, which boasts that the average person who adds XiaoIce talks to her more than 60 times per month.

Our articles on chatbot applications and industry/business function-specific chatbot use cases can help you identify a specific focus area for your bot.

Find a valuable area for your bot

It is difficult to market your bot given the intense competition in the space. Every problem has multiple bots trying to solve it! Even only on Facebook, there are already 30K+ bots. The successful bots really solve a specific problem like scheduling meetings or generating reports.

Select the right platform for your needs

There are numerous Natural Language Platform vendors that provide APIs and each has strengths and weaknesses. We have compared top platforms to build a chatbot. If you want to see the extensive list of chatbot platforms and voice bot platforms, you can also check our transparent lists where sorting vendors by AIMultiple score shows the best solutions based on maturity, popularity, and satisfaction.

Focus on improving usability of the chatbot

Make it easy to discover

Embed it in your website or application.

I have an urgent need as a customer and need to access customer service. What do I do? There’s a myriad of options which may or may not result in a customer service interaction: go to the company website hoping that they have a chatbot there, download the app which may not even exist, check out whether they have a chatbot on a platform like Messenger. Most Fortune 500 companies do not have a chatbot on any of these places. So the current situation is bleak in the US. Enter WeChat, dominant chat platform in China where virtually every major corporation offers a text interface for customer service. A dominant platform can make all the difference.

Even a system like Amazon’s Alexa with continuously expanding skills (currently it has >100K skills) has discoverability issues. How do I know which useful skill Alexa gained among the thousands of skills?

Make sure your chatbot has a voice and a character It helps to configure your chatbot as if it mirrors its audience’s personality by writing in the style they speak. Studies show that 46% of consumers would choose a live person over a chatbot even if engaging with a chatbot saved them 10 minutes. They believe chatbots need to work on their accuracy in understanding what customers are looking for. To increase the user trust in your chatbot, creating a consistent voice and putting a profile picture of a face can help consumers feel like they are chatting with a human. For instance, according to a PwC survey, 27% of consumers weren’t sure if the last customer support interaction they had was with a real person or a chatbot.

Use buttons that eases navigation and improve UX Language is ambigious and takes longer to read which is why people increasingly rely on emojis. Similarly, it makes sense to reduce friction in interacting with bots. If users have just 2 options, it may be easier to press a button than say yes. Users should be able to navigate easily while interacting with the bot. Using buttons and quick replies are the easiest way to enable navigation and flow. This improves your bot’s overall user experience along with reducing fallback rates. There are some best practices to use buttons effectively: Don’t provide too many options : Specifically, if you are interested in sales chatbots, multiple offers get 266% fewer leads than single offers.

: Specifically, if you are interested in sales chatbots, multiple offers get 266% fewer leads than single offers. Use fewer words in quick replies : Keeping it simple prevents users from reading and writing too much during the conversation so that they don’t lose interest.

: Keeping it simple prevents users from reading and writing too much during the conversation so that they don’t lose interest. Create context-based relevant buttons: Users should not face a scenario where they see a button without any idea of what it leads to. Source: Anton Yefimenko Source: Dribble

Provide users an easy exit

One of the chatbot failure cases is the one where the chatbot doesn’t accept “no” as an answer. For instance, WSJ bot fails in the image below because it wasn’t understanding the unsubscribe command.

To solve this problem, you can add a visible exit button that says something like “Unsubscribe” or ” End the call” and perform a usability test to see whether the button works or not. Usability testing is critical for bot improvement. Otherwise, your bot may get into an endless cycle where users are re-subscribed as soon as they unsubscribed as Washington Street Journal bot below:

Use creative fallback responses When your chatbot fails, it’s better to respond tactfully rather than messages that look like a 404 error. You should focus on and spend time deciding how the bot will interact with the user when it cannot respond to the user query. Source: Medium

Always have a button that directs user to a human 86% of users expect chatbots should always have an option to transfer to a live agent. When it comes to chatbots, it is always best to give what users want. Though businesses are deploying chatbots rapidly, chatbots are still considerably new to users, and trusting a bot (specifically if the user had a bad experience with a bot before) may deteriorate a customer’s experience. Therefore, at least for now, you should expect your chatbot to free up customer service time rather than completely replace them.

Enable users reengage with bot when needed

After the conversation ends and the chatbot provided the information the user was looking for, provide the option to restart the bot to re-engage with the chatbot if needed.

Ask users about their experience

Collecting feedback from users is one of the conversion rate optimization best practices. Since you want to maximize the user experience so that your bot converts better, asking users to rate their experience and provide additional feedback will help you understand problems and how to improve the bot.

Work with experts and test, test, test

Not every chatbot is worth working with outside experts. However, if this is a strategic initiative for your company, bringing in experience about the topic can be beneficial. And invest in testing to avoid embarrassing mistakes. Our guide is a great starting point for chatbot testing. The KPIs of the chatbot is an important part of testing and bot management. Close attention to user behavior can help identify unintended behavior of the chatbot and improve its functionality.

If you still have questions on chatbots, feel free to contact us:","['experience', 'user', 'chatbots', 'chatbot', 'users', 'customer', 'development', 'practices', 'offers', '2021', 'bots', 'platform', 'best', 'bot']","We have examined 30 successful chatbot examples and a list of chatbot failures before.
Here are best practices of the chatbot development process:Identify your target audience and understand their needsImproving customer experience is the first reason why most companies are deploying a chatbot.
Our articles on chatbot applications and industry/business function-specific chatbot use cases can help you identify a specific focus area for your bot.
Studies show that 46% of consumers would choose a live person over a chatbot even if engaging with a chatbot saved them 10 minutes.
Ask users about their experienceCollecting feedback from users is one of the conversion rate optimization best practices."
134,https://www.sciencedaily.com/releases/2021/01/210121132127.htm,Using VR training to boost our sense of agency and improve motor control,2021-01-21 00:00:00,"With Japan's society rapidly aging, there has been a sharp increase in patients who experience motor dysfunctions. Rehabilitation is key to overcoming such ailments.

A researcher from Tohoku University has developed a new virtual reality (VR) based method that can benefit rehabilitation and sports training by increasing bodily awareness and?improving motor control.

His research was published in the Journal Scientific Report.

Not only can we see and touch our body, but we can sense it too. Our body is constantly firing off information to our brains that tell us where our limbs are in real-time. This process makes us aware of our body and gives us ownership over it. Meanwhile, our ability to control the movement and actions of our body parts voluntarily affords us agency over our body.

Ownership and agency are highly integrated and are related to our motor control. However, separating our sense of body ownership from our sense of agency has long evaded researchers, making it difficult to ascertain whether both ownership and agency truly affect motor control.

Professor Kazumichi Matsumiya from the Graduate School of Information Sciences at Tohoku University could isolate these two senses by using VR. Participants viewed a computer-generated hand, and Matsumiya independently measured their sense of ownership and agency over the hand.

""I found that motor control is improved when participants experienced a sense of agency over the artificial body, regardless of their sense of body ownership,"" said Matsumiya. ""Our findings suggest that artificial manipulation of agency will enhance the effectiveness of rehabilitation and aid sports training techniques to improve overall motor control.""","['ownership', 'university', 'using', 'agency', 'rehabilitation', 'matsumiya', 'body', 'motor', 'vr', 'boost', 'training', 'improve', 'control', 'sense']","A researcher from Tohoku University has developed a new virtual reality (VR) based method that can benefit rehabilitation and sports training by increasing bodily awareness and?improving motor control.
Ownership and agency are highly integrated and are related to our motor control.
However, separating our sense of body ownership from our sense of agency has long evaded researchers, making it difficult to ascertain whether both ownership and agency truly affect motor control.
""I found that motor control is improved when participants experienced a sense of agency over the artificial body, regardless of their sense of body ownership,"" said Matsumiya.
""Our findings suggest that artificial manipulation of agency will enhance the effectiveness of rehabilitation and aid sports training techniques to improve overall motor control."""
135,https://www.theguardian.com/environment/2021/jan/15/how-ai-helped-find-millions-of-trees-in-the-sahara-aoe,"One, two, tree: how AI helped find millions of trees in the Sahara",2021-01-15 00:00:00,"Efforts to map the Earth’s trees are growing – and could change our understanding of the planet’s health

When a team of international scientists set out to count every tree in a large swathe of west Africa using AI, satellite images and one of the world’s most powerful supercomputers, their expectations were modest. Previously, the area had registered as having little or no tree cover.

The biggest surprise, says Martin Brandt, assistant professor of geography at the University of Copenhagen, is that the part of the Sahara that the study covered, roughly 10%, “where no one would expect to find many trees”, actually had “quite a few hundred million”.

Trees are crucial to our long-term survival, as they absorb and store the carbon dioxide emissions that cause global heating. But we still do not know how many there are. Much of the Earth is inaccessible either because of war, ownership or geography. Now scientists, researchers and campaigners have a raft of more sophisticated resources to monitor the number of trees on the planet.

Satellite imagery has become the biggest tool for counting the world’s trees, but while forested areas are relatively easy to spot from space, the trees that aren’t neatly gathered in thick green clumps are overlooked. Which is why assessments so far have been, says Brandt, “extremely far away from the real numbers. They were based on interpolations, estimations and projections.”

Facebook Twitter Pinterest Dryland trees grow in isolation without forming forests (marked in green, top), making them invisible to conventional satellite systems. Scientists used new sensors and AI to map individual trees within the rectangle over west Africa, showing that millions of trees grow in desert and grassland areas.

The most recent attempt at a global tally of trees was in 2015, when researchers, using a combination of satellite data and ground measurements, estimated there were just over 3tn. This was a dramatic increase from the previous estimate of 400bn in 2009, which was based on satellite imagery alone.

The research by Brandt and his colleagues in west Africa promises a more accurate picture in the future. In a collaboration with Nasa’s Goddard Space Flight Center, they were able to use satellite images from DigitalGlobe, previously available only to commercial entities, which were high enough resolution to make out individual trees and measure their crown size.

Using AI deep learning, and one of the world’s most powerful supercomputers – Blue Waters at the University of Illinois – the team was able to count individual trees from space for the first time. They manually marked nearly 90,000 across a variety of terrain, so the computer could “learn” which shapes and shadows indicated the presence of trees. This enabled them to count every tree with a crown size of at least 3 sq metres in a 1.3m sq km area comprising mostly the Sahara but also the semi-arid Sahel area along the southern edge of the desert and a sliver of the sub-humid zone beneath that. Overall, they detected more than 1.8bn trees.

Q&A Why is deforestation a problem? Show Hide Trees play a significant role in producing the oxygen we breathe. But twice as many existed before the start of human civilisation. The continued destruction of forests and trees is a significant contributor to the carbon dioxide emissions that are driving the climate crisis. Trees draw carbon dioxide back out of the atmosphere as they grow, and planting trees will need to play an important part in ending the climate emergency. Forests are also a vital and rich habitat for wildlife. Earth is at the start of a sixth mass extinction event of species and the razing of forests and other ecosystems is the biggest contributor to the losses. Trees are also important in controlling regional rainfall, as they evaporate water from their leaves. In urban areas, the shade from trees has been shown to cool city streets and reduce levels of air pollution. Trees can also boost people’s wellbeing as part of green spaces, with research showing a two-hour “dose” of nature a week significantly improves health.

Those in the Sahara tended to be clustered around human settlements. Arid areas had on average 9.9 trees per hectare, rising to 30.1 in semi-arid zones and 47 in the southernmost sub-humid rim of the patch being studied. There were just 0.7 trees per hectare in areas classified as “super-arid”.

“Most maps show these areas as basically empty,” says Brandt. “But they’re not empty. Our assessment suggests a way to monitor trees outside of forests globally, and to explore their role in mitigating degradation, climate change and poverty.”

Keeping the planet’s arboreal accounts is key to understanding the impact trees are having on our planet’s health. If the number of trees can be mapped, so can the amount of carbon they store.

The most high-profile existing world tree map is released annually by Global Forest Watch. Launched by the World Resources Institute (WRI) in 2014, it uses data from Nasa Landsat satellites (which don’t have as high resolution as commercial equivalents) to keep tabs on what it diplomatically calls “tree cover loss”.

Previously, information on the changing shapes of forests was collated every five years or so by the UN Food and Agriculture Organization, which had to take the numbers on trust from individual countries. The WRI’s aim was to make assessing deforestation data transparent.

Weekly alerts are generated for reductions in forest size in the tropics. “Cambodia basically said they had no deforestation,” says Fred Stolle, deputy director of WRI’s forest programme, “but there’s been so much. The car industry is getting bigger and bigger and we need tyres. Rubber grows well in the tropics and so Cambodia has an enormous amount of deforestation to plant new rubber trees.”

Alerts have also appeared for Ghana, where destruction of primary forests jumped by 60% between 2017 and 2018 – the biggest rise anywhere in the tropics.

However, there is one aspect of the WRI map that Stolle concedes means the picture is incomplete. While the satellites easily show where trees have been cut down, “new tree growth is much more difficult to see. So while Global Forest Watch sees a lot of the deforestation, it doesn’t see much of the reforestation.”

Facebook Twitter Pinterest A rubber plantation in Kampong Cham, Cambodia. Large swathes of the country’s forests have been cleared and replaced by plantations. Photograph: Sean Gallagher

Brandt expects that the higher resolution technology offered by the commercial satellites will become widely available in the coming years, helping to bridge this gap.

Another organisation tracking deforestation is environmental non-profit Canopy, founded in 1999 by its now executive director, Nicole Rycroft. It traces back supply chains for companies because, Rycroft says, “there’s no need to cut down 100-year-old trees to make pizza boxes or T-shirts, or for the trees to come from land inhabited by indigenous communities”.

Using information from a range of scientific sources, Canopy has packaged the raw data and satellite imagery into an interactive tool called ForestMapper, to help companies switch to sustainable supply chains. They can scan the map, which includes information on forest carbon density, endangered species, tree loss so far and projected deforestation over the coming decade. “We’re the applied science side,” says Rycroft, “making the data user-friendly.”

There’s no need to cut down 100-year-old trees to make pizza boxes or T-shirts Nicole Rycroft, Canopy

As well as highlighting risky supply chains, Canopy helps manufacturers find more sustainable sources, including recycled fibres, “so we don’t just shift the problem from one backyard to somebody else’s”.

“We work with 320-odd fashion brands,” Rycroft continues. “Including guys like H&M, Zara and Uniqlo, right down to luxury designers like Stella McCartney. And as you can imagine, there’s a wide range of motivations within those companies, but they’re all committed.”

Seven years ago, she recalls, few in the industry even knew that “200m trees were disappearing into rayon and viscose every year, and some of it from orangutan and grizzly bear habitats, really high carbon forest ecosystems”. Now, she says, “52% of global viscose production is verified by ourselves as being at low risk of originating from high carbon or high biodiversity forests. There’s still 48% of the supply chain to go, but in a relatively short space of time we’ve seen the global supply chain fundamentally starting to transform how they source.”

Facebook Twitter Pinterest Destruction of Ghana’s primary forests jumped by 60% between 2017 and 2018 – the biggest rise anywhere in the tropics. Photograph: Nicolas De Corte/Alamy

Hotspots on the map now include south-east Asia, Indonesia, Vietnam, Laos and Brazil. Working with local NGOs and campaigners, Canopy drills down into the regional detail. Some eucalyptus plantations in Indonesia, for example, are growing on high-carbon peatlands that need restoring. “And we’ve recently discovered that koala habitat in Australia has been logged for fabric production,” says Rycroft.

Shoots and leaves: the shotgun scientist who hunts moving trees Read more

The same scrutiny applies to other supply chains, such as cardboard. “Which are the forests that provide the 3bn trees that disappear into food wrappings, pizza boxes or the packaging that lands on our doorsteps from e-retailers? Is it coming from a sustainably managed plantation? Is there recycled content? Or is it coming from a high-carbon value forest?”

What is hardest to monitor is the illegal clearing of the vast, as yet unknown numbers of trees that exist outside forests. Brandt’s team is close to submitting another research paper for which they have scanned 10 times the area covered by their initial study. As well as forests, says Brandt, individual trees are “valuable in mitigating climate change, providing a variety of ecosystems and services to people, and until now, it was impossible to map them”.

Find more age of extinction coverage here, and follow biodiversity reporters Phoebe Weston and Patrick Greenfield on Twitter for all the latest news and features","['carbon', 'satellite', 'helped', 'ai', 'forests', 'global', 'forest', 'sahara', 'millions', 'tree', 'map', 'supply', 'deforestation', 'trees']","Trees are crucial to our long-term survival, as they absorb and store the carbon dioxide emissions that cause global heating.
Scientists used new sensors and AI to map individual trees within the rectangle over west Africa, showing that millions of trees grow in desert and grassland areas.
Trees draw carbon dioxide back out of the atmosphere as they grow, and planting trees will need to play an important part in ending the climate emergency.
The most high-profile existing world tree map is released annually by Global Forest Watch.
While the satellites easily show where trees have been cut down, “new tree growth is much more difficult to see."
136,https://techcrunch.com/2021/02/03/deep-science-ais-with-high-class-and-higher-altitudes/,TechCrunch is now a part of Verizon Media,2021-02-03 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
137,https://www.sciencedaily.com/releases/2021/02/210204192543.htm,'Audeo' teaches artificial intelligence to play the piano,2021-02-21 00:00:00,"Anyone who's been to a concert knows that something magical happens between the performers and their instruments. It transforms music from being just ""notes on a page"" to a satisfying experience.

A University of Washington team wondered if artificial intelligence could recreate that delight using only visual cues -- a silent, top-down video of someone playing the piano. The researchers used machine learning to create a system, called Audeo, that creates audio from silent piano performances. When the group tested the music Audeo created with music-recognition apps, such as SoundHound, the apps correctly identified the piece Audeo played about 86% of the time. For comparison, these apps identified the piece in the audio tracks from the source videos 93% of the time.

The researchers presented Audeo Dec. 8 at the NeurIPS 2020 conference.

""To create music that sounds like it could be played in a musical performance was previously believed to be impossible,"" said senior author Eli Shlizerman, an assistant professor in both the applied mathematics and the electrical and computer engineering departments. ""An algorithm needs to figure out the cues, or 'features,' in the video frames that are related to generating music, and it needs to 'imagine' the sound that's happening in between the video frames. It requires a system that is both precise and imaginative. The fact that we achieved music that sounded pretty good was a surprise.""

Audeo uses a series of steps to decode what's happening in the video and then translate it into music. First, it has to detect which keys are pressed in each video frame to create a diagram over time. Then it needs to translate that diagram into something that a music synthesizer would actually recognize as a sound a piano would make. This second step cleans up the data and adds in more information, such as how strongly each key is pressed and for how long.

""If we attempt to synthesize music from the first step alone, we would find the quality of the music to be unsatisfactory,"" Shlizerman said. ""The second step is like how a teacher goes over a student composer's music and helps enhance it.""

The researchers trained and tested the system using YouTube videos of the pianist Paul Barton. The training consisted of about 172,000 video frames of Barton playing music from well-known classical composers, such as Bach and Mozart. Then they tested Audeo with almost 19,000 frames of Barton playing different music from these composers and others, such as Scott Joplin.

Once Audeo has generated a transcript of the music, it's time to give it to a synthesizer that can translate it into sound. Every synthesizer will make the music sound a little different -- this is similar to changing the ""instrument"" setting on an electric keyboard. For this study, the researchers used two different synthesizers.

""Fluidsynth makes synthesizer piano sounds that we are familiar with. These are somewhat mechanical-sounding but pretty accurate,"" Shlizerman said. ""We also used PerfNet, a new AI synthesizer that generates richer and more expressive music. But it also generates more noise.""

Audeo was trained and tested only on Paul Barton's piano videos. Future research is needed to see how well it could transcribe music for any musician or piano, Shlizerman said.

""The goal of this study was to see if artificial intelligence could generate music that was played by a pianist in a video recording -- though we were not aiming to replicate Paul Barton because he is such a virtuoso,"" Shlizerman said. ""We hope that our study enables novel ways to interact with music. For example, one future application is that Audeo can be extended to a virtual piano with a camera recording just a person's hands. Also, by placing a camera on top of a real piano, Audeo could potentially assist in new ways of teaching students how to play.""

Kun Su and Xiulong Liu, both doctoral students in electrical and computer engineering, are co-authors on this paper. This research was funded by the Washington Research Foundation Innovation Fund as well as the applied mathematics and electrical and computer engineering departments.","['artificial', 'music', 'audeo', 'synthesizer', 'researchers', 'shlizerman', 'piano', 'sound', 'frames', 'video', 'play', 'tested', 'intelligence', 'teaches']","The researchers used machine learning to create a system, called Audeo, that creates audio from silent piano performances.
When the group tested the music Audeo created with music-recognition apps, such as SoundHound, the apps correctly identified the piece Audeo played about 86% of the time.
""Fluidsynth makes synthesizer piano sounds that we are familiar with.
Future research is needed to see how well it could transcribe music for any musician or piano, Shlizerman said.
Also, by placing a camera on top of a real piano, Audeo could potentially assist in new ways of teaching students how to play."""
138,https://www.sciencedaily.com/releases/2021/02/210208125357.htm,AI researchers ask: What's going on inside the black box?,2021-02-21 00:00:00,"Cold Spring Harbor Laboratory (CSHL) Assistant Professor Peter Koo and collaborator Matt Ploenzke reported a way to train machines to predict the function of DNA sequences. They used ""neural nets,"" a type of artificial intelligence (AI) typically used to classify images. Teaching the neural net to predict the function of short stretches of DNA allowed it to work up to deciphering larger patterns. The researchers hope to analyze more complex DNA sequences that regulate gene activity critical to development and disease.

Machine-learning researchers can train a brain-like ""neural net"" computer to recognize objects, such as cats or airplanes, by showing it many images of each. Testing the success of training requires showing the machine a new picture of a cat or an airplane and seeing if it classifies it correctly. But, when researchers apply this technology to analyzing DNA patterns, they have a problem. Humans can't recognize the patterns, so they may not be able to tell if the computer identifies the right thing. Neural nets learn and make decisions independently of their human programmers. Researchers refer to this hidden process as a ""black box."" It is hard to trust the machine's outputs if we don't know what is happening in the box.

Koo and his team fed DNA (genomic) sequences into a specific kind of neural network called a convolutional neural network (CNN), which resembles how animal brains process images. Koo says:

""It can be quite easy to interpret these neural networks because they'll just point to, let's say, whiskers of a cat. And so that's why it's a cat versus an airplane. In genomics, it's not so straightforward because genomic sequences aren't in a form where humans really understand any of the patterns that these neural networks point to.""

Koo's research, reported in the journal Nature Machine Intelligence, introduced a new method to teach important DNA patterns to one layer of his CNN. This allowed his neural network to build on the data to identify more complex patterns. Koo's discovery makes it possible to peek inside the black box and identify some key features that lead to the computer's decision-making process.

But Koo has a larger purpose in mind for the field of artificial intelligence. There are two ways to improve a neural net: interpretability and robustness. Interpretability refers to the ability of humans to decipher why machines give a certain prediction. The ability to produce an answer even with mistakes in the data is called robustness. Usually, researchers focus on one or the other. Koo says:

""What my research is trying to do is bridge these two together because I don't think they're separate entities. I think that we get better interpretability if our models are more robust.""

Koo hopes that if a machine can find robust and interpretable DNA patterns related to gene regulation, it will help geneticists understand how mutations affect cancer and other diseases.","['box', 'whats', 'ask', 'patterns', 'going', 'network', 'machines', 'researchers', 'neural', 'ai', 'dna', 'inside', 'koo', 'net', 'machine', 'black', 'sequences']","Teaching the neural net to predict the function of short stretches of DNA allowed it to work up to deciphering larger patterns.
The researchers hope to analyze more complex DNA sequences that regulate gene activity critical to development and disease.
Machine-learning researchers can train a brain-like ""neural net"" computer to recognize objects, such as cats or airplanes, by showing it many images of each.
But, when researchers apply this technology to analyzing DNA patterns, they have a problem.
There are two ways to improve a neural net: interpretability and robustness."
139,https://techcrunch.com/2021/02/09/nextmv-raises-8m-series-a-to-increase-accessibility-to-its-automation-optimization-tech/,TechCrunch is now a part of Verizon Media,2021-02-09 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
140,https://techcrunch.com/2021/02/09/with-ai-translation-service-that-rivals-professionals-lengoo-attracts-new-20m-round/,TechCrunch is now a part of Verizon Media,2021-02-09 00:00:00,"TechCrunch is part of Verizon Media. We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.

Your personal data that may be used

Information about your device and internet connection, including your IP address

Browsing and search activity while using Verizon Media websites and apps

Precise location

Find out more about how we use your information in our Privacy Policy and Cookie Policy.

To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices. You can change your choices at any time by visiting Your Privacy Controls.","['information', 'manage', 'privacy', 'data', 'verizon', 'device', 'media', 'partners', 'select', 'personal', 'techcrunch']","TechCrunch is part of Verizon Media.
We and our partners will store and/or access information on your device through the use of cookies and similar technologies, to display personalised ads and content, for ad and content measurement, audience insights and product development.
Your personal data that may be usedInformation about your device and internet connection, including your IP addressBrowsing and search activity while using Verizon Media websites and appsPrecise locationFind out more about how we use your information in our Privacy Policy and Cookie Policy.
To enable Verizon Media and our partners to process your personal data select 'I agree', or select 'Manage settings' for more information and to manage your choices.
You can change your choices at any time by visiting Your Privacy Controls."
141,https://news.mit.edu/2021/language-learning-efficiency-0210,A language learning system that pays attention — more efficiently than ever before,,"Human language can be inefficient. Some words are vital. Others, expendable.

Reread the first sentence of this story. Just two words, “language” and “inefficient,” convey almost the entire meaning of the sentence. The importance of key words underlies a popular new tool for natural language processing (NLP) by computers: the attention mechanism. When coded into a broader NLP algorithm, the attention mechanism homes in on key words rather than treating every word with equal importance. That yields better results in NLP tasks like detecting positive or negative sentiment or predicting which words should come next in a sentence.

The attention mechanism’s accuracy often comes at the expense of speed and computing power, however. It runs slowly on general-purpose processors like you might find in consumer-grade computers. So, MIT researchers have designed a combined software-hardware system, dubbed SpAtten, specialized to run the attention mechanism. SpAtten enables more streamlined NLP with less computing power.

“Our system is similar to how the human brain processes language,” says Hanrui Wang. “We read very fast and just focus on key words. That’s the idea with SpAtten.”

The research will be presented this month at the IEEE International Symposium on High-Performance Computer Architecture. Wang is the paper’s lead author and a PhD student in the Department of Electrical Engineering and Computer Science. Co-authors include Zhekai Zhang and their advisor, Assistant Professor Song Han.

Since its introduction in 2015, the attention mechanism has been a boon for NLP. It’s built into state-of-the-art NLP models like Google’s BERT and OpenAI’s GPT-3. The attention mechanism’s key innovation is selectivity — it can infer which words or phrases in a sentence are most important, based on comparisons with word patterns the algorithm has previously encountered in a training phase. Despite the attention mechanism’s rapid adoption into NLP models, it’s not without cost.

NLP models require a hefty load of computer power, thanks in part to the high memory demands of the attention mechanism. “This part is actually the bottleneck for NLP models,” says Wang. One challenge he points to is the lack of specialized hardware to run NLP models with the attention mechanism. General-purpose processors, like CPUs and GPUs, have trouble with the attention mechanism’s complicated sequence of data movement and arithmetic. And the problem will get worse as NLP models grow more complex, especially for long sentences. “We need algorithmic optimizations and dedicated hardware to process the ever-increasing computational demand,” says Wang.

The researchers developed a system called SpAtten to run the attention mechanism more efficiently. Their design encompasses both specialized software and hardware. One key software advance is SpAtten’s use of “cascade pruning,” or eliminating unnecessary data from the calculations. Once the attention mechanism helps pick a sentence’s key words (called tokens), SpAtten prunes away unimportant tokens and eliminates the corresponding computations and data movements. The attention mechanism also includes multiple computation branches (called heads). Similar to tokens, the unimportant heads are identified and pruned away. Once dispatched, the extraneous tokens and heads don’t factor into the algorithm’s downstream calculations, reducing both computational load and memory access.

To further trim memory use, the researchers also developed a technique called “progressive quantization.” The method allows the algorithm to wield data in smaller bitwidth chunks and fetch as few as possible from memory. Lower data precision, corresponding to smaller bitwidth, is used for simple sentences, and higher precision is used for complicated ones. Intuitively it’s like fetching the phrase “cmptr progm” as the low-precision version of “computer program.”

Alongside these software advances, the researchers also developed a hardware architecture specialized to run SpAtten and the attention mechanism while minimizing memory access. Their architecture design employs a high degree of “parallelism,” meaning multiple operations are processed simultaneously on multiple processing elements, which is useful because the attention mechanism analyzes every word of a sentence at once. The design enables SpAtten to rank the importance of tokens and heads (for potential pruning) in a small number of computer clock cycles. Overall, the software and hardware components of SpAtten combine to eliminate unnecessary or inefficient data manipulation, focusing only on the tasks needed to complete the user’s goal.

The philosophy behind the system is captured in its name. SpAtten is a portmanteau of “sparse attention,” and the researchers note in the paper that SpAtten is “homophonic with ‘spartan,’ meaning simple and frugal.” Wang says, “that’s just like our technique here: making the sentence more concise.” That concision was borne out in testing.

The researchers coded a simulation of SpAtten’s hardware design — they haven’t fabricated a physical chip yet — and tested it against competing general-purposes processors. SpAtten ran more than 100 times faster than the next best competitor (a TITAN Xp GPU). Further, SpAtten was more than 1,000 times more energy efficient than competitors, indicating that SpAtten could help trim NLP’s substantial electricity demands.

The researchers also integrated SpAtten into their previous work, to help validate their philosophy that hardware and software are best designed in tandem. They built a specialized NLP model architecture for SpAtten, using their Hardware-Aware Transformer (HAT) framework, and achieved a roughly two times speedup over a more general model.

The researchers think SpAtten could be useful to companies that employ NLP models for the majority of their artificial intelligence workloads. “Our vision for the future is that new algorithms and hardware that remove the redundancy in languages will reduce cost and save on the power budget for data center NLP workloads” says Wang.

On the opposite end of the spectrum, SpAtten could bring NLP to smaller, personal devices. “We can improve the battery life for mobile phone or IoT devices,” says Wang, referring to internet-connected “things” — televisions, smart speakers, and the like. “That’s especially important because in the future, numerous IoT devices will interact with humans by voice and natural language, so NLP will be the first application we want to employ.”

Han says SpAtten’s focus on efficiency and redundancy removal is the way forward in NLP research. “Human brains are sparsely activated [by key words]. NLP models that are sparsely activated will be promising in the future,” he says. “Not all words are equal — pay attention only to the important ones.”","['models', 'hardware', 'data', 'spatten', 'researchers', 'learning', 'attention', 'key', 'system', 'language', 'nlp', 'pays', 'words', 'mechanism', 'efficiently']","The importance of key words underlies a popular new tool for natural language processing (NLP) by computers: the attention mechanism.
When coded into a broader NLP algorithm, the attention mechanism homes in on key words rather than treating every word with equal importance.
So, MIT researchers have designed a combined software-hardware system, dubbed SpAtten, specialized to run the attention mechanism.
NLP models require a hefty load of computer power, thanks in part to the high memory demands of the attention mechanism.
The researchers developed a system called SpAtten to run the attention mechanism more efficiently."
142,https://thenextweb.com/neural/2021/02/10/anti-trashbug-ai-scours-sky-snaps-to-spot-sea-plastic/,,,,,
143,https://www.analyticsinsight.net/are-psychologists-the-next-target-for-ai-machine-learning/,,,,,
144,https://analyticsindiamag.com/from-being-broke-to-creating-a-leading-chatbot-how-these-two-ex-fractalites-created-ori/,From Being Broke To Creating A Leading Chatbot,2021-02-10 11:30:00+00:00,"Chatbots have come a long way, from the elementary text-based chats to much advanced conversational platforms mimicking near-human interactions. Chatbots are changing how businesses interact with their customers, replacing humans in query management for various fields like banking, e-commerce and retail. As it stands, artificial intelligence, machine learning, and natural language processes are setting the trends for the chatbot industry in 2021.

Leading the way into the future of human-machine conversations is Ori — an end-to-end provider of conversational AI-powered chatbots. The bootstrapped startup has hit the ground running clocking a positive cash-flow in the first year of operations. Ori’s celebrity client list includes Vodafone, Idea, Tata Motor, DishTV, and Supreme Golf.

Microsoft has named Ori as the ‘Best Customer Engagement Chatbot’ in the Asia Pacific. Analytics India Magazine spoke to the co-founders, Maaz Ansari and Anurag Jain of Ori, to know how the company uses AI to automate the customer journey and bridge the gap in customer-client interactions.

The Journey

The founders of Ori — Maaz Ansari and Anurag Jain — met in 2012 while working as analysts at Fractal Analytics — a multinational AI company, focusing on text analytics and NLP research. “This was a time when analytics was in its nascent stages and about to come to the mainstream. Terms like AI and ML were heard far and in-between,” said Ansari. The duo found the idea of deriving meaning from unstructured data and finding patterns from chaos too good to pass.

Engineers by training (from Bombay University and IIT-Kharagpur), the pair saw potential in driving effective conversations between enterprises and customers. “Conversations that form the crux of the relationship, conversations which, when done well and lead to extraordinary results,” clarifies Jain. They realised the existing market solutions did nothing to sustain the hype cycle. Bots weren’t up to the mark, and Ori was conceived to bridge the gap between expectations and delivery.

“With a hand on heart, 90% of folks using bots even today will tell you that they neither match their expectations nor do they fulfil the roadmap of expectations that they had. In other words, there is a sizable gap that is required to be filled,” Jain said.

Sharing the story of the company, Ansari said — “We had just shut down a startup, and were broke. Armed with two slides, we managed to convince the President of Strategy at DishTV to give us a break. He is a big believer in product innovation and jumped onto the chance of leading the way with breakthroughs. Consequently, the DishTV bot became the first AI-driven bot built for customer redressal by any Pay-TV company globally. It went on to win the ‘MSFT AI for All’ Award as the best bot for customer engagement.” Everything changed afterwards. With business steadily flowing in, Ori continued its innovation-driven approach and developed solutions for various use cases. The company features benchmark-defining NPS/CSAT on each conversation. “All of our bots are trained specifically for an organisation and imbibe the brand’s character, jargon, and customer communication norms with our patent-pending algorithm,” he added.

AI @Ori

According to the founders, Ori’s platform truly mimics human conversations. Not only they are authentic, relevant, and empathetic, but are also being layered with multiple languages to create a solution that’s transformational in absolute terms. “When people talk about AI, they refer to natural language processing; in other words, the ability to understand humans. That’s where the buck stops,” said Jain.

Ori’s multilayered AI stack is customisable. The flagship product is a digital sales assistant that completely emulates one-to-one conversations between brand personnel and customers, and has been deployed for multiple clients across diverse segments, including automobile and consumer durables.

Being in the business of automated digital sales representatives using cognitive conversations, Ori has developed many AI and analytics modules. These models have been designed to synchronise with each other to create a world-class experience for users and maximise value for its clients. Natural language processing techniques are at the core of AI and analytics used at Ori.

Some of the AI techniques leveraged in different stages at Ori to enhance its automated sales representatives are:

AI to learn the basics of conversation: The aim here is to learn more about the brand and its domain using deep learning techniques. Ori’s automated sales reps start gauging the brand’s lingo and industry-specific ontology from tons of data gathered from brand vaults. Alongside, industry knowledge is gathered by web scrapers.

AI to gain business insight: Under the supervised contextual business logic training, the team taps supervised deep learning techniques to train the engine. It helps the engine to predict the next action based on business logic and context.

Learning from successes and failures in a live environment: Reinforcement learning is used in the products to reinforce best scenarios based on set goals for one-on-one conversations and readjust the model in real-time.

The full suite of analytics at play to optimise Ori’s automated sales reps behaviour includes:

Funnel Analysis: Ori created a funnel view of all user interactions, which helps the engine analyse progression down the funnel and effectiveness of different campaigns run by the company’s automated sales reps.

Session Flow Analysis: The team at Ori analyses the best conversion paths and gets user behavioural insights.

Anomaly Detection: Proprietary anomaly detection technique has been developed to identify and surface fracture points from thousands of conversations processed by Ori’s automated sales reps.

Free Text Clustering: Ori has also developed a novel clustering technique to visualise top unhandled queries in the form of clusters.

Talking about the core technology stack, Ansari said, “At Ori, we are following a microservices-based architecture and leverage the MERN stack for powering the platform. It enables us to handle volumes at scale.” The machine learning, AI, and NLP libraries are built on Python, TensorFlow, Keras, PyTorch, and Transformers.

Hiring Process At Ori

Ori focuses a lot on internal promotions for specific roles within the company, and has also created a talent community on social sites alongside strong employer branding. “We further ensure that we maintain complete transparency with applicants throughout the hiring process, reducing time to hire,” he added.

Ori even has a strong employee referral program that helps the company address its hiring needs. Passion and enthusiasm to work in a growing startup are the key traits Ori looks for in every candidate. It also expects the candidates to have skills like team compatibility, a proactive and can-do attitude, strong leadership qualities and good communication skills.

Wrapping Up

With its AI-powered chatbots, Ori can take care of the most complex queries with automated customer service response systems. According to the founders, it empowered the clients to service their customers at only 50% of customer service centre capacity, reducing costs by 60%.

In the future, the company aims to develop feature-intensive, AI-powered assistants that help brands get higher ROI and return on ad spend from their sales and marketing initiatives. “We are also increasingly focusing on voice-based chats and search technologies for our conversational platform,” concluded Ansari.

Subscribe to our Newsletter

You can write for us and be one of the 500+ experts who have contributed stories at AIM. Share your nominations here.

Get the latest updates and relevant offers by sharing your email.","['conversations', 'oris', 'chatbot', 'creating', 'sales', 'ori', 'customer', 'leading', 'ai', 'automated', 'broke', 'learning', 'analytics', 'company']","Leading the way into the future of human-machine conversations is Ori — an end-to-end provider of conversational AI-powered chatbots.
Being in the business of automated digital sales representatives using cognitive conversations, Ori has developed many AI and analytics modules.
Ori’s automated sales reps start gauging the brand’s lingo and industry-specific ontology from tons of data gathered from brand vaults.
It enables us to handle volumes at scale.” The machine learning, AI, and NLP libraries are built on Python, TensorFlow, Keras, PyTorch, and Transformers.
Wrapping UpWith its AI-powered chatbots, Ori can take care of the most complex queries with automated customer service response systems."
145,https://www.eetimes.com/israeli-ai-chip-startup-raises-seed-funding/,Israeli AI Chip Startup Raises Seed Funding,2021-02-10 13:00:08+00:00,"Israeli AI chip startup NeuReality has raised seed funding of $8 million to work on its data center AI inference chip and system solution. The company plans to offer an AI platform which enables data centers to easily scale their compute to meet growing AI workloads, while cutting costs, energy bills and footprint.

Details are scarce on what exactly the company is working on, but a company spokesperson told EE Times that NeuReality is “re-architecting the system solution, hence innovating on all three layers: the chip level, system hardware level and software level.” Targeting hyperscalers, solution providers and OEMs means the company must provide solutions at different levels, including the chip level.

The spokesperson did confirm that the company is working on an AI accelerator chip, but declined to say anything further other than the accelerator would be “nothing close to a deep learning accelerator as we know it today in terms of the system architecture and its place in the system.”

Scalability challenges are distributed both in the cloud and closer to the edge, so their software may also reside on the client side for a further boost in efficiency, they said.

NeuReality also announced it has appointed Naveen Rao to its board of directors. Rao was formerly general manager of Intel’s AI products group and CEO of data center AI chip startup Nervana, which was acquired by Intel in 2016.

“After seeing so many AI companies focusing on deep learning accelerators at the device level, NeuReality intrigued me with their refreshing view of overall AI deployment barriers and their unique system-level approach that will eclipse today’s outdated data center architecture,” Rao said, in a statement. “NeuReality’s innovative design solves AI scalability and cost challenges.”

NeuReality was founded in 2019 by CEO Moshe Tanach, vice president of operations Tzvika Shmueli and vice president of VLSI Yossi Kasus. Tanach was previously with Marvell and Intel, while Shmueli was with Mellanox and before that, vice president of engineering at Israeli data center AI chip company Habana Labs, which was acquired by Intel in 2019. Kasus was previously with Mellanox and EZChip.

The company spokesperson said NeuReality had made a lot of progress prior to the seed round by “boot strapping,” or building the company with personal resources. The company intends to launch its first AI platform for ultra-scale AI inference “early this year.”","['israeli', 'seed', 'company', 'data', 'spokesperson', 'level', 'center', 'raises', 'chip', 'ai', 'system', 'neureality', 'funding', 'vice', 'startup']","Israeli AI chip startup NeuReality has raised seed funding of $8 million to work on its data center AI inference chip and system solution.
Rao was formerly general manager of Intel’s AI products group and CEO of data center AI chip startup Nervana, which was acquired by Intel in 2016.
Tanach was previously with Marvell and Intel, while Shmueli was with Mellanox and before that, vice president of engineering at Israeli data center AI chip company Habana Labs, which was acquired by Intel in 2019.
The company spokesperson said NeuReality had made a lot of progress prior to the seed round by “boot strapping,” or building the company with personal resources.
The company intends to launch its first AI platform for ultra-scale AI inference “early this year.”"
146,https://www.prnewswire.com/news-releases/oracle-brings-new-level-of-intelligence-to-construction-projects-301225453.html,Oracle Brings New Level of Intelligence to Construction Projects,,"AUSTIN, Texas, Feb. 10, 2021 /PRNewswire/ -- Oracle today announced Oracle Construction Intelligence Cloud Service, a new suite of AI and analytics applications for the engineering and construction industry. Construction projects often run into problems that impact productivity, safety, and profitability. The new suite uses machine learning to continually analyze project data managed in Oracle Construction and Engineering solutions to identify these potential risks and inefficiencies early, helping organizations make better decisions.

The first application in the suite, Oracle Construction Intelligence Cloud Advisor, is generally available today, with other applications to follow.

""When you see some of the predictive modeling being done, such as Oracle's Construction Intelligence Cloud Service, you see an endless opportunity for us to be more proactively responsive as opposed to reactive,"" said Patty Sullivan, project manager, Strategic Initiatives Group at Burns & McDonnell. ""Additionally, I believe there is an opportunity to manage or mitigate project risk with this technology. It is certainly something we will be looking at this year and we look forward to working with Oracle in utilizing this technology to improve and transform our industry.""

Oracle Construction Intelligence Cloud Advisor

Oracle Construction Intelligence Cloud Advisor provides predictive intelligence to improve decision-making at all levels of an organization. Unlike software that provides only a view into what has happened in a project, the application also anticipates what may happen next. Its re-trainable machine learning models improve in accuracy over time as they learn from an organization's experiences.

Today, the application uses data from Oracle's Primavera scheduling solution to predict project delays, which often lead to cost overruns, and help organizations determine appropriate corrective actions. This includes identifying what project activities might be delayed and why. Oracle Construction Intelligence Cloud Advisor can also help create better estimates, identify the impact of predicted delays on downstream activities, and improve the scheduling process to increase productivity.

For example, imagine that while assessing a construction firm's vast portfolio of projects, Oracle Construction Intelligence Cloud Advisor predicts that the structural work on one of the projects could be delayed by 14 days due to scheduling errors, resourcing constraints, and the historical setbacks in these types of activities. This intelligence enables the project team to work with their in-house scheduling teams and supply chain partners to manage the risk and ensure the factors causing the delay can be mitigated. They can also determine whether similar delays have happened on past projects to identify the root causes and fix the issue systematically.

""It requires a substantial amount of time for HART to review and provide feedback on monthly schedule submittals from our contractors,"" said Nate Meddings, director of project controls for Honolulu Authority for Rapid Transportation (HART). ""We look forward to having Oracle Construction Intelligence Cloud Advisor as a solution that streamlines the analysis by highlighting specific areas of concern before we even begin our review process. We welcome the fact-based, visual indicators we can share with our contractors to help drive improvements in their work product.""

There are plans to expand Oracle Construction Intelligence Cloud Advisor to include data from across the Oracle Construction and Engineering portfolio over the next year. These enhancements may help identify potential risks related to litigation, safety, rework, supply chain performance and cash flow. Additionally, new Oracle Construction Intelligence Cloud Service analytics and data service offerings will be added to the product line.

""Engineering and construction organizations are struggling to mine their data for useful insights into the performance of their projects and operations,"" said Mark Webster, senior vice president and general manager, Oracle Construction and Engineering. ""Oracle Construction Intelligence Cloud Service was shaped by our customers' need for intuitive tools to make their project outcomes more predictable and their businesses more competitive and profitable.""

About Oracle Construction and Engineering

Asset owners and project delivery teams rely on Oracle Construction and Engineering solutions for the visibility and control, connected supply chain, and data security needed to drive performance and mitigate risk across their processes, projects, and organization. Our scalable cloud construction management software solutions enable digital transformation for teams that plan, build, and operate critical assets, improving efficiency, collaboration, and change control across the project lifecycle. www.oracle.com/construction-and-engineering.

About Oracle

Oracle offers suites of integrated applications plus secure, autonomous infrastructure in the Oracle Cloud. For more information about Oracle (NYSE: ORCL), please visit us at www.oracle.com

Trademarks

Oracle and Java are registered trademarks of Oracle Corporation.

SOURCE Oracle

Related Links

www.oracle.com

","['project', 'construction', 'data', 'level', 'advisor', 'brings', 'engineering', 'cloud', 'service', 'oracle', 'projects', 'intelligence']","AUSTIN, Texas, Feb. 10, 2021 /PRNewswire/ -- Oracle today announced Oracle Construction Intelligence Cloud Service, a new suite of AI and analytics applications for the engineering and construction industry.
The first application in the suite, Oracle Construction Intelligence Cloud Advisor, is generally available today, with other applications to follow.
Oracle Construction Intelligence Cloud AdvisorOracle Construction Intelligence Cloud Advisor provides predictive intelligence to improve decision-making at all levels of an organization.
There are plans to expand Oracle Construction Intelligence Cloud Advisor to include data from across the Oracle Construction and Engineering portfolio over the next year.
Additionally, new Oracle Construction Intelligence Cloud Service analytics and data service offerings will be added to the product line."
147,https://venturebeat.com/2020/08/23/the-term-ethical-ai-is-finally-starting-to-mean-something/,The term ‘ethical AI’ is finally starting to mean something,2020-08-23 00:00:00,"Earlier this year, the independent research organisation of which I am the Director, London-based Ada Lovelace Institute, hosted a panel at the world’s largest AI conference, CogX, called The Ethics Panel to End All Ethics Panels. The title referenced both a tongue-in-cheek effort at self-promotion, and a very real need to put to bed the seemingly endless offering of panels, think-pieces, and government reports preoccupied with ruminating on the abstract ethical questions posed by AI and new data-driven technologies. We had grown impatient with conceptual debates and high-level principles.

And we were not alone. 2020 has seen the emergence of a new wave of ethical AI – one focused on the tough questions of power, equity, and justice that underpin emerging technologies, and directed at bringing about actionable change. It supersedes the two waves that came before it: the first wave, defined by principles and dominated by philosophers, and the second wave, led by computer scientists and geared towards technical fixes. Third-wave ethical AI has seen a Dutch Court shut down an algorithmic fraud detection system, students in the UK take to the streets to protest against algorithmically-decided exam results, and US companies voluntarily restrict their sales of facial recognition technology. It is taking us beyond the principled and the technical, to practical mechanisms for rectifying power imbalances and achieving individual and societal justice.

From philosophers to techies

Between 2016 and 2019, 74 sets of ethical principles or guidelines for AI were published. This was the first wave of ethical AI, in which we had just begun to understand the potential risks and threats of rapidly advancing machine learning and AI capabilities and were casting around for ways to contain them. In 2016, AlphaGo had just beaten Lee Sedol, promoting serious consideration of the likelihood that general AI was within reach. And algorithmically-curated chaos on the world’s duopolistic platforms, Google and Facebook, had surrounded the two major political earthquakes of the year – Brexit, and Trump’s election.

In a panic for how to understand and prevent the harm that was so clearly to follow, policymakers and tech developers turned to philosophers and ethicists to develop codes and standards. These often recycled a subset of the same concepts and rarely moved beyond high-level guidance or contained the specificity of the kind needed to speak to individual use cases and applications.

This first wave of the movement focused on ethics over law, neglected questions related to systemic injustice and control of infrastructures, and was unwilling to deal with what Michael Veale, Lecturer in Digital Rights and Regulation at University College London, calls “the question of problem framing” – early ethical AI debates usually took as a given that AI will be helpful in solving problems. These shortcomings left the movement open to critique that it had been co-opted by the big tech companies as a means of evading greater regulatory intervention. And those who believed big tech companies were controlling the discourse around ethical AI saw the movement as “ethics washing.” The flow of money from big tech into codification initiatives, civil society, and academia advocating for an ethics-based approach only underscored the legitimacy of these critiques.

At the same time, a second wave of ethical AI was emerging. It sought to promote the use of technical interventions to address ethical harms, particularly those related to fairness, bias and non-discrimination. The domain of “fair-ML” was born out of an admirable objective on the part of computer scientists to bake fairness metrics or hard constraints into AI models to moderate their outputs.

This focus on technical mechanisms for addressing questions of fairness, bias, and discrimination addressed the clear concerns about how AI and algorithmic systems were inaccurately and unfairly treating people of color or ethnic minorities. Two specific cases contributed important evidence to this argument. The first was the Gender Shades study, which established that facial recognition software deployed by Microsoft and IBM returned higher rates of false positives and false negatives for the faces of women and people of color. The second was the 2016 ProPublica investigation into the COMPAS sentencing algorithmic tool, which found that Black defendants were far more likely than White defendants to be incorrectly judged to be at a higher risk of recidivism, while White defendants were more likely than Black defendants to be incorrectly flagged as low risk.

Second-wave ethical AI narrowed in on these questions of bias and fairness, and explored technical interventions to solve them. In doing so, however, it may have skewed and narrowed the discourse, moving it away from the root causes of bias and even exacerbating the position of people of color and ethnic minorities. As Julia Powles, Director of the Minderoo Tech and Policy Lab at the University of Western Australia, argued, alleviating the problems with dataset representativeness “merely co-opts designers in perfecting vast instruments of surveillance and classification. When underlying systemic issues remain fundamentally untouched, the bias fighters simply render humans more machine readable, exposing minorities in particular to additional harms.”

Some also saw the fair-ML discourse as a form of co-option of socially conscious computer scientists by big tech companies. By framing ethical problems as narrow issues of fairness and accuracy, companies could equate expanded data collection with investing in “ethical AI.”

The efforts of tech companies to champion fairness-related codes illustrate this point: In January 2018, Microsoft published its “ethical principles” for AI, starting with “fairness;” in May 2018, Facebook announced a tool to “search for bias” called “Fairness Flow;” and in September 2018, IBM announced a tool called “AI Fairness 360,” designed to “check for unwanted bias in datasets and machine learning models.”

What was missing from second-wave ethical AI was an acknowledgement that technical systems are, in fact, sociotechnical systems — they cannot be understood outside of the social context in which they are deployed, and they cannot be optimised for societally beneficial and acceptable outcomes through technical tweaks alone. As Ruha Benjamin, Associate Professor of African American Studies at Princeton University, argued in her seminal text, Race After Technology: Abolitionist Tools for the New Jim Code, “the road to inequity is paved with technical fixes.” The narrow focus on technical fairness is insufficient to help us grapple with all of the complex tradeoffs, opportunities, and risks of an AI-driven future; it confines us to thinking only about whether something works, but doesn’t permit us to ask whether it should work. That is, it supports an approach that asks, “What can we do?” rather than “What should we do?”

Ethical AI for a new decade

On the eve of the new decade, MIT Technology Review’s Karen Hao published an article entitled “In 2020, let’s stop AI ethics-washing and actually do something.” Weeks later, the AI ethics community ushered in 2020 clustered in conference rooms at Barcelona, for the annual ACM Fairness, Accountability and Transparency conference. Among the many papers that had tongues wagging was written by Elettra Bietti, Kennedy Sinclair Scholar Affiliate at the Berkman Klein Center for Internet and Society. It called for a move beyond the “ethics-washing” and “ethics-bashing” that had come to dominate the discipline. Those two pieces heralded a cascade of interventions that saw the community reorienting around a new way of talking about ethical AI, one defined by justice — social justice, racial justice, economic justice, and environmental justice. It has seen some eschew the term “ethical AI” in favor of “just AI.”

As the wild and unpredicted events of 2020 have unfurled, alongside them third-wave ethical AI has begun to take hold, strengthened by the immense reckoning that the Black Lives Matter movement has catalysed. Third-wave ethical AI is less conceptual than first-wave ethical AI, and is interested in understanding applications and use cases. It is much more concerned with power, alive to vested interests, and preoccupied with structural issues, including the importance of decolonising AI. An article published by Pratyusha Kalluri, founder of the Radical AI Network, in Nature in July 2020, has epitomized the approach, arguing that “When the field of AI believes it is neutral, it both fails to notice biased data and builds systems that sanctify the status quo and advance the interests of the powerful. What is needed is a field that exposes and critiques systems that concentrate power, while co-creating new systems with impacted communities: AI by and for the people.”

What has this meant in practice? We have seen courts begin to grapple with, and political and private sector players admit to, the real power and potential of algorithmic systems. In the UK alone, the Court of Appeal found the use by police of facial recognition systems unlawful and called for a new legal framework; a government department ceased its use of AI for visa application sorting; the West Midlands police ethics advisory committee argued for the discontinuation of a violence-prediction tool; and high school students across the country protested after tens of thousands of school leavers had their marks downgraded by an algorithmic system used by the education regulator, Ofqual. New Zealand published an Algorithm Charter and France’s Etalab – a government task force for open data, data policy, and open government – has been working to map the algorithmic systems in use across public sector entities and to provide guidance.

The shift in gaze of ethical AI studies away from the technical towards the socio-technical has brought more issues into view, such as the anti-competitive practices of big tech companies, platform labor practices, parity in negotiating power in public sector procurement of predictive analytics, and the climate impact of training AI models. It has seen the Overton window contract in terms of what is reputationally acceptable from tech companies; after years of campaigning by researchers like Joy Buolamwini and Timnit Gebru, companies such as Amazon and IBM have finally adopted voluntary moratoria on their sales of facial recognition technology.

The COVID crisis has been instrumental, surfacing technical advancements that have helped to fix the power imbalances that exacerbate the risks of AI and algorithmic systems. The availability of the Google/Apple decentralised protocol for enabling exposure notification prevented dozens of governments from launching invasive digital contact tracing apps. At the same time, governments’ response to the pandemic has inevitably catalysed new risks, as public health surveillance has segued into population surveillance, facial recognition systems have been enhanced to work around masks, and the threat of future pandemics is leveraged to justify social media analysis. The UK’s attempt to operationalize a weak Ethics Advisory Board to oversee its failed attempt at launching a centralized contact-tracing app was the death knell for toothless ethical figureheads.

Research institutes, activists, and campaigners united by the third-wave approach to ethical AI continue to work to address these risks, with a focus on practical tools for accountability (we at the Ada Lovelace Institute, and others such as AI Now, are working on developing audit and assessment tools for AI; and the Omidyar Network has published its Ethical Explorer toolkit for developers and product managers), litigation, protest and campaigning for moratoria, and bans.

Researchers are interrogating what justice means in data-driven societies, and institutes such as Data & Society, the Data Justice Lab at Cardiff University, JUST DATA Lab at Princeton, and the Global Data Justice project at the Tilberg Institute for Law, Technology and Society in the Netherlands are churning out some of the most novel thinking. The Minderoo Foundation has just launched its new “future says” initiative with a $3.5 million grant, with aims to tackle lawlessness, empower workers, and reimagine the tech sector. [Update: The Minderoo Foundation says the grant is actually $15 million.] The initiative will build on the critical contribution of tech workers themselves to the third wave of ethical AI, from AI Now co-founder Meredith Whittaker’s organizing work at Google before her departure last year, to walk outs and strikes performed by Amazon logistic workers and Uber and Lyft drivers.

But the approach of third-wave ethical AI is by no means accepted across the tech sector yet, as evidenced by the recent acrimonious exchange between AI researchers Yann LeCun and Timnit Gebru about whether the harms of AI should be reduced to a focus on bias. Gebru not only reasserted well established arguments against a narrow focus on dataset bias but also made the case for a more inclusive community of AI scholarship.

Mobilized by social pressure, the boundaries of acceptability are shifting fast, and not a moment too soon. But even those of us within the ethical AI community have a long way to go. A case in point: Although we’d programmed diverse speakers across the event, the Ethics Panel to End All Ethics Panels we hosted earlier this year failed to include a person of color, an omission for which we were rightly criticized and hugely regretful. It was a reminder that as long as the domain of AI ethics continues to platform certain types of research approaches, practitioners, and ethical perspectives to the exclusion of others, real change will elude us. “Ethical AI” can not only be defined from the position of European and North American actors; we need to work concertedly to surface other perspectives, other ways of thinking about these issues, if we truly want to find a way to make data and AI work for people and societies across the world.

Updated on August 25, 2020 to correct spelling of the Minderoo Foundation and the amount of its initial grant.

Carly Kind is a human rights lawyer, a privacy and data protection expert, and Director of the Ada Lovelace Institute.","['ethics', 'justice', 'data', 'term', 'technical', 'ai', 'mean', 'companies', 'ethical', 'tech', 'fairness', 'systems', 'starting', 'finally']","At the same time, a second wave of ethical AI was emerging.
Second-wave ethical AI narrowed in on these questions of bias and fairness, and explored technical interventions to solve them.
It has seen some eschew the term “ethical AI” in favor of “just AI.”As the wild and unpredicted events of 2020 have unfurled, alongside them third-wave ethical AI has begun to take hold, strengthened by the immense reckoning that the Black Lives Matter movement has catalysed.
Third-wave ethical AI is less conceptual than first-wave ethical AI, and is interested in understanding applications and use cases.
But even those of us within the ethical AI community have a long way to go."
148,https://venturebeat.com/2020/05/04/plotmachines-ai-long-form-stories/,PlotMachines AI system writes long-form stories from outlines,2020-05-04 00:00:00,"From TikTok to Instagram, Facebook to YouTube, and more, learn how data is key to ensuring ad creative will actually perform on every platform.

In a preprint paper published this week on Arxiv.org, scientists at Microsoft, the Allen Institute for Artificial Intelligence, and the University of Washington describe PlotMachines, an AI system that learns to transform outlines into stories by tracking plot threads. PlotMachines — whose code is available on GitHub — could bolster the development of systems capable of writing case studies, news articles, and scripts from nothing but phrases describing characters and events, saving companies time and capital.

While story-, article-, and even lyric-generating AI systems exist, they’re mostly tailored to specific domains and adapt poorly to new tasks. Moreover, they’re not particularly skilled at long-form writing; even the most sophisticated models forget plot elements and repeat themselves.

Composing a story requires keeping track of a plot that weaves through characters and events in a coherent narrative, as the researchers explain. This isn’t easy for machines. Because the input provides only rough elements of the plot, it’s incumbent on a model to flesh out how the elements intertwine across different parts of the story.

In the course of developing PlotMachines, the team created several data sets and built on existing story data sets for target narratives, which they paired with automatically constructed input outlines:

Wikiplots, a corpus consisting of movie, TV, and book plots scraped from Wikipedia.

WritingPrompts, a story generation data set collected from the Reddit subreddit /r/WritingPrompts.

NYTimes, a data set containing news articles.

Outline Extraction, a list of plot points from Wikiplots, WritingPrompts, and NYTimes extracted using an algorithm.

The researchers next designed PlotMachines, which they describe as a Transformer built on top of OpenAI’s GPT model. Like all neural networks, Transformers contain functions (neurons) arranged in layers that transmit signals from data and adjust the connections’ strength (weights). But Transformers also have attention, which means that every output element is connected to every input element, and the weightings between them are calculated dynamically.

Given an outline as input, PlotMachines writes five paragraphs — an introduction, three body paragraphs, and a conclusion — and updates a memory matrix that keeps track of plot elements from the outline. Per-paragraph discourse information helps maintain stylistic differences at the beginning, middle, and end of stories (as does memory that observes what’s been written so far), while context representation ensures previous elements are used in the creation of new paragraphs.

Qualitatively, the researchers say PlotMachines learned after training to start stories by setting the scene (e.g. “In the early 1950s, a nuclear weapons testing continues …. “) and end with a definitive closing action (e.g. ” … the film ends with humperdinck and buttercup riding off into the sunset”). In point of fact, they found a news-generating PlotMachines model trained on the NYTimes corpus so capable that they plan to share it only selectively with the research community, so as to prevent malicious actors from creating and spreading misleading stories.

In experiments, a variation of the PlotMachines model built atop OpenAI’s GPT-2 architecture, which contained 460 million parameters (variables) in total, achieved better Recall-Oriented Understudy for Gisting Evaluation (ROUGE) and BLEU scores than several baselines, indicating it had superior summarization and machine translation capabilities. In two separate evaluations involving human teams tasked with reading and reviewing PlotMachine-generated stories, it outranked the baselines in categories like “narrative flow” and “outline usage.”

“We propose the task of outline-conditioned story generation: Given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline … This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story,” wrote the coauthors. “Analysis shows that PlotMachines is effective in composing tighter narratives based on outlines.”","['outline', 'data', 'researchers', 'longform', 'model', 'set', 'ai', 'track', 'system', 'input', 'outlines', 'writes', 'elements', 'plot', 'plotmachines']","Moreover, they’re not particularly skilled at long-form writing; even the most sophisticated models forget plot elements and repeat themselves.
WritingPrompts, a story generation data set collected from the Reddit subreddit /r/WritingPrompts.
NYTimes, a data set containing news articles.
Given an outline as input, PlotMachines writes five paragraphs — an introduction, three body paragraphs, and a conclusion — and updates a memory matrix that keeps track of plot elements from the outline.
Qualitatively, the researchers say PlotMachines learned after training to start stories by setting the scene (e.g."
149,https://www.aimagazine.com/data-and-analytics/data-poisoning-new-front-ai-cyber-war,Data poisoning: a new front in the AI cyber war,,"Machine learning is big business. It’s a core element in the design of ‘automagical’ tools, which intelligently parse data to give humans a critical edge in anything from strategy planning for business to identifying the plants in their flower beds. It’s also frequently (and somewhat mistakenly) conflated with artificial intelligence (AI). It’s so effective that few large enterprises are not at least considering its implications for improving data analysis and automating parts of their operational machinery; it’s a core pillar of digital transformation projects.

An attack on the fidelity of machine learning’s ability to correctly identify types of data could be catastrophic now, and has the potential to be apocalyptic in a digitally-transformed future. Which is why data poisoning – the deliberate corruption of machine learning algorithms – is such a critical threat.

What is data poisoning?

Machine learning algorithms are impressive at dealing with large volumes of data, but they must be trained properly using well-labelled, accurate training data. Corrupting the training data leads to algorithmic missteps that are amplified by ongoing data crunching using poor parametric specifications. Data poisoning exploits this weakness by deliberately polluting the training data to mislead the machine learning algorithm and render the output either obfuscatory or harmful.

Data poisoning isn’t new exactly. Early examples, where spam filters where targeted by cybercriminals, were seen as long ago as 2004.

How does data poisoning work?

Data poisoning relies on the inherent weaknesses of machine learning. While human brains are adept at recognising what is important in a pattern and rejecting what is not, software can only work with the basics and is (currently) unable to tune out interference that may be incidental, rather than indicative. To theorise an example: a machine learning program is shown 500 pictures of black dogs labelled as ‘dog’ and 500 pictures of white cats labelled as ‘cat’. Now the algorithm is shown a picture of a white dog. Output: cat.

The training data in the example is woefully inadequate for a real-world scenario. Yet machine learning software has been tricked by simple visual elements such as logos and watermarks precisely because it cannot – as a human would – identify this visual information as being incidental to the pertinent image information.

Similar tricks can be played with numerical and text data sources.

Who are the bad actors in data poisoning?

Just as machine learning can create competitive advantage, it can be used by unscrupulous competitors to frustrate business operations. Think of data poisoning as a new type of corporate espionage, yet instead of finding out your competitor’s secrets, you hide their own information from them, or deliberately lead them to poor interpretations of their own data.

A bad actor could also use data poisoning to obfuscate transactional data at a bank, preventing AI-led identification of money laundering operations, for example. Or it could be used as ransomware, or a tool for activists who want to frustrate a business operation. Financial markets could also be used to profit from data-led swings orchestrated by feeding poisoned data to quantitative analysis software. A data poisoning cyberattack at government or military level might also be possible. A terrorist faction could, theoretically, use data poisoning to subvert AI-led air traffic control at a major airport.

Data poisoning can also be used in software certification, allowing cybercriminals to circumvent cybersecurity by ‘teaching’ the algorithm to treat malicious code tagged in the correct way as clear for deployment.

How does data become poisoned?

Although machine learning is capable of tripping itself up without guidance, to achieve a specific result a human bad actor needs access to the training data. In the case of an organisation using its own data, this requires infiltration. However, a major concern is that ‘pre-packed’ training data could be an easier target, and such data is already in common usage by companies who are managing project costs. It’s also the case that training data could be poisoned on a platform level, where a company opts to use third-party services to manage its AI requirements.

How to eliminate the data poisoning threat?

The best defence against a data poisoning attack is to use your own training data and be vigilant about who labels it and how. But a better holistic defence might be to look at training a secondary tier of AI to spot mistakes in your primary data analysis. Technology companies such as IBM are already white-hatting data poisoning attacks to find solutions.

In the interim of truly effective oversight or solutions it’s worth bearing in mind that, despite all its advances, machine learning is in its infancy. Companies should retain human oversight on data analysis to check for anomalies in algorithmic learning.

One of the best known real-world data poisoning hacks was orchestrated by data scientists at New York University, who were able to train autonomous vehicle software to recognise a stop sign as a speed limit sign. The lesson, for drivers of semi-autonomous cars and the business intelligence community is: keep your eyes on the road and your hands on the wheel.","['poisoning', 'war', 'data', 'used', 'cyber', 'learning', 'ai', 'business', 'machine', 'human', 'software', 'training', 'analysis']","Which is why data poisoning – the deliberate corruption of machine learning algorithms – is such a critical threat.
Corrupting the training data leads to algorithmic missteps that are amplified by ongoing data crunching using poor parametric specifications.
Data poisoning exploits this weakness by deliberately polluting the training data to mislead the machine learning algorithm and render the output either obfuscatory or harmful.
A bad actor could also use data poisoning to obfuscate transactional data at a bank, preventing AI-led identification of money laundering operations, for example.
The best defence against a data poisoning attack is to use your own training data and be vigilant about who labels it and how."
150,https://www.aimagazine.com/ai-strategy/saudi-arabia-launches-dollar20bn-ai-strategy-bid,Saudi Arabia launches $20bn AI strategy bid,,"Saudi Arabia has launched a national artificial intelligence strategy aiming to attract $20 billion in investments over the next 10 years.

With data often cited as ‘the new oil’, it’s perhaps unsurprising that the oil-rich Gulf state should be eyeing something akin to domination of this rapidly emerging field. Saudi Arabia has already invested heavily in ecological progress as it looks beyond oil for its economic future.

National Strategy for Data and Artificial Intelligence

The launch of the National Strategy for Data and Artificial Intelligence (NSDAI) is the focal point of a bid to attract $20 billion of investment, both locally and from abroad, to Saudi Arabia’s tech industry by 2030.

The Saudi Data and Artificial Intelligence Authority (SDAIA), which will lead the NSDAI strategy, said in a statement: “Saudi Arabia will implement a multi-phase, multi-faceted plan that includes skills, policy and regulation, investment, research and innovation, and ecosystem development.”

The launch of NSDAI is part of Saudi Arabia’s Vision 2030 plan to diversify its economy, energise the private sector and create more jobs for Saudi nationals.","['artificial', 'oil', 'bid', 'data', 'national', 'arabia', 'plan', 'ai', 'nsdai', 'launches', 'strategy', 'intelligence', '20bn', 'saudi']","Saudi Arabia has launched a national artificial intelligence strategy aiming to attract $20 billion in investments over the next 10 years.
With data often cited as ‘the new oil’, it’s perhaps unsurprising that the oil-rich Gulf state should be eyeing something akin to domination of this rapidly emerging field.
Saudi Arabia has already invested heavily in ecological progress as it looks beyond oil for its economic future.
National Strategy for Data and Artificial IntelligenceThe launch of the National Strategy for Data and Artificial Intelligence (NSDAI) is the focal point of a bid to attract $20 billion of investment, both locally and from abroad, to Saudi Arabia’s tech industry by 2030.
The Saudi Data and Artificial Intelligence Authority (SDAIA), which will lead the NSDAI strategy, said in a statement: “Saudi Arabia will implement a multi-phase, multi-faceted plan that includes skills, policy and regulation, investment, research and innovation, and ecosystem development.”The launch of NSDAI is part of Saudi Arabia’s Vision 2030 plan to diversify its economy, energise the private sector and create more jobs for Saudi nationals."
151,https://www.aimagazine.com/ai-applications-1/darpa-seeks-improve-ai-standards-dollar1m-ditto-project,DARPA seeks to improve AI standards with $1m Ditto project,,"The US Defense Advanced Research Projects Agency (DARPA) has issued a solicitation for industry partners in its Ditto project, aimed at improving the abilities of machine learning and artificial intelligence.

The agency’s researchers are hoping to make machine learning faster and more accurate by introducing hierarchical ‘thinking’ into AI and machine learning to overcome current faults in AI/ML architecture, which bypass real-world functionality and are typically trained in isolation.

DARPA researchers said, “Modern machine learning algorithms have proven to be excellent mathematical stand-ins for real-world functions, yet suffer from two key drawbacks: lack of meta-cognition, and lack of composability.”

The Ditto: Intelligent Auto-Generation and Composition of Surrogate Models project (Ditto, for short) plans to develop an automated software framework that can recognise microelectronics system design and train machine learning tools that take account of subsystem components, organising them hierarchically, to allow engineers to spot faults in AI earlier and make decisions earlier. The core purpose is to mitigate risk in critical military applications.

A statement from DARPA said: “Today's system modeling technology can be slow, cumbersome, and not always accurate. The DARPA Ditto program seeks to develop an AI framework that can learn to generate surrogate models for different components of a complex system intelligently, aggregate these models while maintaining and communicating surrogate accuracy and coverage, and then integrate these models into one design.”

In inviting industry proposals, DARPA outlined three system design categories: integrated circuits (ICs), mixed signal circuit boards or networked distributed systems. The agency aims to enable “rapid full-scale simulation” by applying a range of third-wave AI techniques to its framework.

It said: “Then the project will develop a proof-of-concept framework with meaningful performance gains in a full-system simulation. The entire Ditto project should be worth about $1million.”

Companies gave until 6 November 2020 to submit proposals.","['project', 'models', 'surrogate', 'seeks', 'darpa', 'learning', 'ai', 'system', 'standards', 'machine', 'framework', 'ditto', 'improve', '1m']","The US Defense Advanced Research Projects Agency (DARPA) has issued a solicitation for industry partners in its Ditto project, aimed at improving the abilities of machine learning and artificial intelligence.
The agency’s researchers are hoping to make machine learning faster and more accurate by introducing hierarchical ‘thinking’ into AI and machine learning to overcome current faults in AI/ML architecture, which bypass real-world functionality and are typically trained in isolation.
A statement from DARPA said: “Today's system modeling technology can be slow, cumbersome, and not always accurate.
The agency aims to enable “rapid full-scale simulation” by applying a range of third-wave AI techniques to its framework.
The entire Ditto project should be worth about $1million.”Companies gave until 6 November 2020 to submit proposals."
152,https://www.aimagazine.com/ai-strategy/wef-ai-automation-cost-17m-jobs-year,WEF: AI automation to cost 17m jobs per year,,"An average of 17 million people, equivalent to the population of the Netherlands, will lose their jobs to AI each year.

That’s according to the World Economic Forum’s Future of Jobs 2020 report, which estimates 85 million roles will be lost to the AI revolution in the next five years.

The paper, released as part of the WEF’s Jobs Reset Summit, says automation has been unexpectedly accelerated by Covid-19, which has forced many businesses to take a more proactive approach to digital transformation.

AI jobs boost

But the AI revolution will create 97 million new jobs, with roles in data, artificial intelligence, content creation and cloud computing, according to the report. Machines will focus on information and data processing, while human skills will be prized. By 2025, employers will have a 50:50 human-to-machine ratio among their employees.

Saadia Zahidi, managing director of the WEF said, “Covid-19 has accelerated the arrival of the future of work. Accelerating automation and the fallout from the Covid-19 recession has deepened existing inequalities across labour markets and reversed gains in employment made since the global financial crisis in 2007-2008.

“It’s a double disruption scenario that presents another hurdle for workers in this difficult time. The window of opportunity for proactive management of this change is closing fast. Businesses, governments and workers must plan to urgently work together to implement a new vision for the global workforce.

“In the future, we will see the most competitive businesses are the ones that have invested heavily in their human capital – the skills and competencies of their employees.”

Reskilling in AI and data

Nearly half of the workers who keep their jobs will need to retrain, the report suggests. And two-thirds of employers surveyed said they would expect a return on investment in upskilling and reskilling staff.

Jeff Maggioncalda, chief executive officer of Coursera, an online training platform and partner organisation on the report, said the pandemic had disproportionately affected low-skilled workers. “The recovery must include a coordinated reskilling effort by institutions to provide accessible and job-relevant learning that individuals can take from anywhere in order to return to the workforce,” he said.

Karin Kimbrough, chief economist at LinkedIn, said, “As we think about ways to upskill or transition large populations of the workforce who are out of work as a result of COVID-19 into new, more future-proofed jobs, these new insights into career transitions and the skills required to make them have huge potential for leaders in the public and the private sector alike.

“Our research reveals the majority of transitions into jobs of tomorrow come from non-emerging jobs, proving that many of these jobs are more accessible than workers might think. If we can help individuals, and the leaders who are directing workforce funding and investment, identify the small clusters of skills that would have an outsized impact on opening up more sustainable career paths, we can make a real difference in addressing the unprecedented levels of unemployment that we're seeing globally.”

AI leads career shift

The report also suggests a shift towards multiple careers, what Hamoon Ekhtiari, CEO of FutureFit AI, calls a “higher frequency of career transitions”.

""The pandemic has accelerated many of the trends around the future of work, dramatically shrinking the window of opportunity to reskill and transition workers into future-fit jobs,” he said. “No matter what prediction you believe about jobs and skills, what is bound to be true is heightened intensity and higher frequency of career transitions especially for those already most vulnerable and marginalised.”

The Future of Jobs report is in its third edition.","['automation', 'future', 'cost', 'skills', 'career', 'ai', 'covid19', 'wef', 'workforce', 'work', 'jobs', 'workers', '17m', 'report']","An average of 17 million people, equivalent to the population of the Netherlands, will lose their jobs to AI each year.
That’s according to the World Economic Forum’s Future of Jobs 2020 report, which estimates 85 million roles will be lost to the AI revolution in the next five years.
AI jobs boostBut the AI revolution will create 97 million new jobs, with roles in data, artificial intelligence, content creation and cloud computing, according to the report.
Saadia Zahidi, managing director of the WEF said, “Covid-19 has accelerated the arrival of the future of work.
“Our research reveals the majority of transitions into jobs of tomorrow come from non-emerging jobs, proving that many of these jobs are more accessible than workers might think."
153,https://www.aimagazine.com/ai-strategy/ai-transformation-global-trade-ecosystem-underway,AI transformation of the global trade ecosystem is underway,,"Ask the public about Artificial Intelligence (AI) and its application in our lives and some people immediately jump to dystopian, doomsday scenarios where robots have taken over the world. While this has made a good plot for Hollywood, the reality is rather different. What many don't realise is that AI and its subset, machine learning, already forms a central part of our day-to-day lives.

Those new products that Amazon suggested you add to your shopping cart? AI. That gripping TV series you watched on Netflix via an automated recommendation? AI. That self-driving Tesla car you crave to take for a spin (or rather, takes you for a spin)? Yes, you guessed it – its AI!

Today, there’s not a single industry that is not being re-shaped by technology in one form or another. Until recently, however, there was one noteworthy exception to this: global trade. Fortunately, that too is slowly changing.

The financial mechanism that underpins global trade – trade finance – is a centuries-old industry that remains largely paper-based and reliant on manual processes. This USD15 trillion a year industry is now being influenced by a new wave of technological innovation, including AI.

The role of AI in trade finance

AI generally refers to the use of computers and computer-aided systems to help people make decisions or make decisions for them. It usually relies on large volumes of data or sophisticated models to help understand the best ways to make sense of all the information and draw intelligence.

In trade finance, AI is particularly helpful in analysing quantitative data, as there are usually many repetitive small transactions. The nature of trade finance means that there is a lot of non-traditional data at our disposal. This means that when banks and other trade finance providers need to assess the risks of funding a transaction between a business and its counterparty, AI-driven models can be a very efficient tool for data analysis and reveal intelligence and risks.

Crucially, this goes far beyond the traditional credit scoring process, which is often outdated and remains reliant on a small number of historical accounting entries – a major barrier and prevents many small companies from accessing trade finance. In fact, the current short

fall between what banks can lend and what businesses need was around USD1.5 trillion even before the COVID-19 pandemic, 10% of global trading activity!

Transforming the credit scoring process for SMEs

AI can help to tackle this shortfall by creating more accurate credit scoring models that offer deeper levels of intelligence to inform a trade finance provider’s decision. This can include analysing a company’s payment history, measuring the risks of funding a specific transaction when dealing with different counterparties, identifying supply chain risks and benchmarking them against their peer group.

Trade finance providers can use this information to communicate more effectively with their SME clients. This creates more trust between them and establishes better business relationships. For SMEs, this opens up trade finance access for companies that would otherwise not have that access and helps to reduce the trade finance gap.

Tech will continue to shape the future of trade

The adoption of AI is just one of a series of technological advancements that will transform the global trade ecosystem over the next decade. From blockchain-based systems to real-time anti-money laundering and fraud alerts, this industry is in the early stages of a radical transformation.

The timing is not coincidental; these advances are largely driven by a new generation of fintechs that have emerged in recent years. For example, we have seen the industry work together to create a new infrastructure to help banks distribute trade finance assets to other investors in a transparent and standardised format.

The creation of the infrastructure is only possible due to improvements in modern technology and integration across the trade ecosystem in co-operation with banks, insurers and other long-standing industry participants.

That is industry-wide collaboration at its best. Together, they are re-shaping global trade as we know it.

By Michael Boguslavsky, Head of AI at Tradeteq","['scoring', 'underway', 'transformation', 'finance', 'data', 'industry', 'ai', 'global', 'help', 'banks', 'ecosystem', 'trade', 'small']","Until recently, however, there was one noteworthy exception to this: global trade.
The financial mechanism that underpins global trade – trade finance – is a centuries-old industry that remains largely paper-based and reliant on manual processes.
In trade finance, AI is particularly helpful in analysing quantitative data, as there are usually many repetitive small transactions.
For SMEs, this opens up trade finance access for companies that would otherwise not have that access and helps to reduce the trade finance gap.
Together, they are re-shaping global trade as we know it."
154,https://www.aimagazine.com/machine-learning/baidu-debuts-latest-version-ai-platform-baidu-brain,Baidu debuts latest version of AI platform Baidu Brain,,"Chinese technology company Baidu has revealed a raft of new features in its Baidu Brain AI technology platform, which has been updated to version 6.0.

The company says its Baidu Brain, which has over 270 AI capabilities, is used internally as well as being open to third-party developers, having been involved in 310,000 developer models.

The platform takes a layered approach to AI, with a foundation layer supported by additional layers of algorithms supporting features such as VR, voice technology and computer vision, as well as a security layer.

In a blog post , the company said: “In computer vision, state-of-the-art efficiency is becoming a new trend to meet an increasing demand of edge-based computer vision applications. Baidu Brain 6.0 open sourced a series of lightweight vision models including the model that topped the ‘Real-time Image Classification Using Pixel 4 CPU’ competition at CVPR 2020 and a 3.1M OCR model.”

The foundation layer involves the company’s deep learning (the subset of machine learning that takes inspiration from the brain) platform PaddlePaddle, which has been updated with 140 new APIs for better connection with third-party feature sets.

The company’s offering also includes Kunlun, a “cloud-to-edge AI chip” specifically built for AI scenarios, that the company produces in house and claims outperforms Nvidia’s T4 GPU/

“Artificial intelligence is the core technology of the fourth-generation industrial revolution,” said Baidu CTO Haifeng Wang. “Baidu Brain can enable all industries to apply AI technology more efficiently, while accelerating the process of industrial intelligence.”

The demonstration took place at Baidu World 2020 , an annual technology conference run by the company.

Other announcements made at the event included its autonomous driving platform Apollo, which has completed over six million kilometres of on-road testing, and now is capable of using 5G communications technology to replace in-vehicle safety drivers.","['using', 'debuts', 'company', 'baidu', 'updated', 'brain', 'ai', 'platform', 'vision', 'computer', 'technology', 'version', 'latest']","Chinese technology company Baidu has revealed a raft of new features in its Baidu Brain AI technology platform, which has been updated to version 6.0.
The company says its Baidu Brain, which has over 270 AI capabilities, is used internally as well as being open to third-party developers, having been involved in 310,000 developer models.
The platform takes a layered approach to AI, with a foundation layer supported by additional layers of algorithms supporting features such as VR, voice technology and computer vision, as well as a security layer.
In a blog post , the company said: “In computer vision, state-of-the-art efficiency is becoming a new trend to meet an increasing demand of edge-based computer vision applications.
“Baidu Brain can enable all industries to apply AI technology more efficiently, while accelerating the process of industrial intelligence.”The demonstration took place at Baidu World 2020 , an annual technology conference run by the company."
155,https://www.aimagazine.com/machine-learning/ai-and-ml-ace-healthcare-industrys-sleeve,AI and ML: The ace up the healthcare industry’s sleeve,,"The COVID-19 pandemic has forced the healthcare sector to become more agile than ever. Mobile apps like myGP have been crucial to providing remote consultations and to limit the number of patients attending doctors’ surgeries and clinics in person. But technologies like these aren’t always accessible to vulnerable, elderly patients, who may not even own mobile devices. Dreams of healthcare democratisation that we often hear about are still a long way off.

Despite this, there are plenty of under-recognised ways in which AI and automation are being used in the healthcare sector. What people see day-to-day as “AI in healthcare” isn’t even the tip of the iceberg – it’s a whiff of smoke coming off it. In reality, the use of AI and automation is much more widespread and accessible, and is being used by doctors and nurses on a daily basis. From diagnostics to automating the back office, AI is transforming healthcare for good.

The advent of intelligent automation

The first way in which new technologies are transforming healthcare is through Intelligent Process Automation (IPA), which combines Robotic Process Automation (RPA) and machine learning. Through data science applications, simple tasks that were once very sequential in nature are now much more intelligent.

RPA is largely used for invoice scanning on enterprise resource planning (ERP) systems, but IPA goes one step further, enabling the use of optical character recognition (OCR) technology to scan anything. This means entire patient record-keeping systems can be automated and, through image recognition, basic reports can be populated.

This is the advent of intelligent automation in healthcare: freeing up doctors and nurses from manually entering details to instead spend their time working with vulnerable patients on the ground, and saving precious time and resources in a time when the NHS is more stretched than ever.

Standardisation for a clearer view

Right now, every clinician is allocating different evidence in different categories – listing the same types of diagnoses, diseases or predicted outcomes in different ways. As a result, the data available to analyse is ‘corrupt’ – since the evidence isn’t classified, when you think you’re looking at the full picture, you’re actually only looking at one third of the picture. That’s why we must standardise the way things are classified, calling the same thing with the same name, a task that machine learning makes possible.

There are huge benefits to this, highlighted by often tedious clinical trials. Standardisation through machine learning can reduce the time taken for drugs to come to market by 10% to 30%, saving hundreds of millions of pounds for pharma companies – which they can reinvest in making new drugs. We’re already seeing quicker results in terms of drug manufacturing, with important drugs going from ideation to human trials to being stocked on pharmacy shelves much more quickly.

Machine learning also leads to a much better chance of identifying positive side effects and finding uses of medication for other diseases, meaning patient outcomes are much improved.

Artificial intelligence, human delivery

Machine learning analyses data in a completely different way, with machines undertaking repetitive tasks requiring high skillsets. Take a scan carried out on a patient, for example an MRI or X-ray. Thanks to the wealth of data at its disposal, the machine can apply filters to analyse what might be going wrong – prompting you to move the patient slightly, or know if there will be a bad output – meaning the machine stands a much stronger chance of spotting health issues.

Unsurprisingly, this area is highly contested. Will machines and robots be used to diagnose moving forward? Absolutely not. The machine is a tool used to identify potential problems and make more accurate predictions on what might be needed, but an experienced clinician still needs to diagnose and decide what must happen in every case.

These use cases apply to practically every part of a patient’s journey, inside or outside a hospital. It’s not limited to static images or scans, but involves a total ability to collect data on an ongoing basis in a dynamic fashion. The results are significant – recently, San Francisco University installed an AI-enabled sepsis detection system in an Intensive Care ward. There, the death rate fell by more than 12%, and patients using the system were 58% less likely to die.

Machine learning allows clinicians to see what type of behaviour is causing better outcomes, leading to quicker and more accurate diagnosis and treatment. More patients stand to benefit given the accessibility of this, even when they’re at home.

This is not the case for AI interfacing – a remote session where AI software diagnoses patients. There is widespread criticism of this technology, not least because it’s inaccessible for more vulnerable patients who may not have mobile phones or tablets.

As in hospitals themselves, it will always be crucial for a clinician to define what must happen in every case. Interfacing should be used at a service level, such as in logistics or hotels, rather than at the hardcore diagnosis and treatment level. This form of telematics is a way of getting in touch with a patient – but it isn’t artificial intelligence.

Harnessing the power of data

Every single use case for AI and automation in healthcare relies on one thing: data. From capturing data and the availability of data, to data structuring or data cleansing, we must be able to understand and share this information with other databases, that can communicate with each other. If you put bad information in, you get bad information out – which nobody wants in healthcare.

The second most important element is having a clear and cleansed data stream from different patients who are diagnosed and treated. The system can then find similarities to different diseases, suggest treatments and identify other types of treatments to eliminate a disease in its infancy. This means healthcare professionals could more easily find underlying treatments to diseases, and get these drugs to patients more quickly. In the case of the Covid-19 pandemic, using a clear data stream to create a multi-language FAQ avatar would not only provide consistent information, but it would also enable it to map questions, learn, and potentially predict outbreak hotspots.

Evolving with caution

The trajectory of AI adoption isn’t straightforward – there’s still widespread hesitation to try new things. When we think of AI in the enterprise, the risk-takers are often ambitious startups, companies undergoing major restructures, or companies in distress desperate for solutions. In other words, innovation is usually seen more as a dire need rather than a long-term plan. In healthcare, then, it’s understandable why adoption has been slow – the sensitive nature of saving lives is all about long-term planning, not last-minute needs.

However, supposed “legacy” institutions like the NHS are far more advanced in adoption of AI compared to private pharmaceutical companies. The NHS wants to reduce costs where possible, especially after the economic pressure it has faced in 2020, and this necessitates some experimentation. As for pharmas, they enjoy bigger margins and are more likely to want to maintain the status quo.

While AI and machine learning hasn’t caused a dramatic end-to-end transformation of the patient journey, this doesn’t mean it’s been ineffective. Things are changing, and we’re edging towards real innovation. The public may not notice the strides taken in diagnostics and back office activites, and these changes may even fly under the radar of doctors and nurses. Nevertheless, all involved are benefitting. These technologies have significant potential, and we are yet to witness their full, wide-spread impact. But increased buy-in from the clinicians who use the tech on a daily basis, and heightened willingness from the sector itself to embrace new technologies, are promising signs of the enhanced, intelligent healthcare industry to come.

By John Gikopoulos, Global Head for Artificial Intelligence & Automation, Infosys Consulting","['automation', 'industrys', 'ml', 'ace', 'used', 'data', 'patients', 'learning', 'ai', 'sleeve', 'different', 'machine', 'patient', 'healthcare']","Despite this, there are plenty of under-recognised ways in which AI and automation are being used in the healthcare sector.
What people see day-to-day as “AI in healthcare” isn’t even the tip of the iceberg – it’s a whiff of smoke coming off it.
Machine learning allows clinicians to see what type of behaviour is causing better outcomes, leading to quicker and more accurate diagnosis and treatment.
Harnessing the power of dataEvery single use case for AI and automation in healthcare relies on one thing: data.
While AI and machine learning hasn’t caused a dramatic end-to-end transformation of the patient journey, this doesn’t mean it’s been ineffective."
156,https://www.aimagazine.com/machine-learning/eagle-eye-ai-transforming-expense-management-heres-how,Eagle-eye AI is transforming expense management. Here’s how,,"AI has been helping the finance sector to streamline and optimise processes for some time now, but the technology has never been more relevant or useful to finance chiefs than now. Remote work is here to stay, and finance teams are increasingly looking at alternative ways to make their expense management processes more structured, accurate and efficient. The current pandemic has only highlighted the challenges that organisations face during the spend auditing process. These issues range from the slow pace of manual expense processing, to the need for updated company policies.

AI can catch what the human eye misses

Expense audit has historically been plagued by out-of-date and ineffective technologies, on top of a laborious manual process. In contrast, AI-powered spend auditing enables finance leaders to take a more proactive, precise approach to expense reimbursement. AI is capable of auditing 100 per cent of expense claims and is far more nimble at spotting errors and duplicate payments, as well as flagging questionable spend.

Below are some notable instances of when AI has flagged discrepancies in expense reports submitted by employees that would otherwise have gone undetected.

Airbnb – on a mate’s sofa

An employee stayed at his friend’s home whenever he was in town on business. Not a problem, except that he and his friend hatched a plan to run the expense through as an Airbnb stay. The friend even went so far as to post the home on Airbnb at a ridiculously high price, out of line with similar accommodation in the area.

The employee then submitted the inflated-price Airbnb stay in their expenses. Once reimbursed by his employer for the expense, the friends then split the proceeds between them. However, AI immediately detected the higher-than-average room rate for the style of accommodation and location. This was then flagged to the auditor for further review, stopping the schemers in their tracks.

Just a free lunch – or a backhander? When it comes to employee expenses, if an employee commits bribery, it’s not a defence for a business to claim that it didn’t know about it. Under the UK’s Bribery Act, lunches, gifts, or “anything of value” can be considered bribes, and this can result in astronomical fines and untold damage to an organisation’s reputation.

Manual expense reviews are unlikely to spot the names of foreign officials or heads of state associated with meal receipts, but AI can easily detect them. This ensures that if employees are dining with high-risk individuals, the expense can be further reviewed to see if it’s unreasonably extravagant – and keeping a company out of regulatory hot water.

Gentlemen’s clubs

Much like the euphemistic moniker ‘gentlemen’s club’ strip clubs like the well-known Spearmint Rhino often have a ‘doing business as’ (DBA) name that differs from the establishment’s name. This DBA, or ‘trading as’, name appears on receipts so the line item can avoid attracting the attention of expense report approvers.

AI is trained to recognise that “K-Kel, Inc” is not in fact the name of a restaurant, or other approved venue, but is instead the ‘trading as’ name for Spearmint Rhino. It really is surprising how often this scenario comes up, but AI is trained to identify these and other instances where DBA names are masking potentially inappropriate expenses.

Manual vs AI

Although these expense red flags may seem outrageous, or even obvious in retrospect, given the volume and depth of many organisations’ expense claims, manual review simply can’t compete with the power of AI. AI ingests data from a seemingly limitless amount of online resources, in real time, and continuously builds intelligence that enables it to identify problematic spend – something that human auditors simply don’t have the time or resources to do.

To mitigate these spend risks, businesses can streamline the spend audit process with the help of AI. Having technology in place that automatically approves low and medium-risk expense reports allows auditors to focus their precious time on the high-risk items that matter the most – preventing fraud, reducing spend, and ensuring regulatory compliance. AI is crucial to the modern expense auditing process, and is a technology investment well worth investing in to save time and money now, and in the long run.

Andrew Foster is VP consulting at AppZen","['employee', 'finance', 'management', 'process', 'ai', 'auditing', 'expense', 'heres', 'organisations', 'technology', 'eagleeye', 'spend', 'transforming', 'stay']","Remote work is here to stay, and finance teams are increasingly looking at alternative ways to make their expense management processes more structured, accurate and efficient.
The current pandemic has only highlighted the challenges that organisations face during the spend auditing process.
In contrast, AI-powered spend auditing enables finance leaders to take a more proactive, precise approach to expense reimbursement.
Manual vs AIAlthough these expense red flags may seem outrageous, or even obvious in retrospect, given the volume and depth of many organisations’ expense claims, manual review simply can’t compete with the power of AI.
AI is crucial to the modern expense auditing process, and is a technology investment well worth investing in to save time and money now, and in the long run."
157,https://www.aimagazine.com/data-and-analytics/datarobots-enterprise-ai-and-ml-cloud-platform,DataRobot’s enterprise AI and ML cloud platform,,"Boston, Massachusetts-based tech unicorn DataRobot operates an enterprise AI platform to provide businesses access to machine learning.

The platform features automation capabilities across the building, deployment and management stages of creating machine learning models. The company also offers AI implementation, training and support services to businesses.

DataRobot says that more than 2 billion machine learning models have been built using its platform, with customers including the likes of Accenture, Deloitte, Panasonic and Manchester City Football Club.

Since its foundation in 2012, the company has raised over $700mn . Its latest Series F round of funding, announced yesterday, saw the company receive $270mn from lead investor Altimeter Capital, alongside T. Rowe Price, BlackRock, Tiger Global, Silver Lake Waterman, B Capital Group, Glynn Capital, ClearBridge, NEA, and Sapphire Ventures.

In a press release , Dan Wright, President and COO of DataRobot, said: “We’re seeing overwhelming demand for our platform. This latest round of financing further validates this momentum, and we intend to use it to accelerate our rapid growth globally.

“Modern enterprises are generating and capturing more data than ever before, and this data is also changing more rapidly than ever. Traditional business intelligence and manual machine learning methods are completely inadequate to keep pace. Businesses need AI-driven insights into what will happen, rather than simply what has happened, in order to win in today’s increasingly competitive market.”

The round leaves the company valued at $2.7bn, more than double that of its last round.

Brad Gerstner, Founder and CEO, Altimeter Capital, said: “For too long, AI-driven insights were a distant promise, but we have hit an inflection point and DataRobot is at the front of the pack in terms of democratizing AI-driven business intelligence. This capability will be a must-have for all businesses and is poised to catalyze a new era of economic productivity.”","['businesses', 'models', 'ml', 'capital', 'learning', 'ai', 'platform', 'cloud', 'datarobots', 'machine', 'aidriven', 'datarobot', 'round', 'enterprise', 'company']","Boston, Massachusetts-based tech unicorn DataRobot operates an enterprise AI platform to provide businesses access to machine learning.
The platform features automation capabilities across the building, deployment and management stages of creating machine learning models.
DataRobot says that more than 2 billion machine learning models have been built using its platform, with customers including the likes of Accenture, Deloitte, Panasonic and Manchester City Football Club.
In a press release , Dan Wright, President and COO of DataRobot, said: “We’re seeing overwhelming demand for our platform.
Traditional business intelligence and manual machine learning methods are completely inadequate to keep pace."
158,https://www.aimagazine.com/technology-9/graphcores-intelligent-processing-unit-ai-and-ml,Graphcore’s Intelligent Processing Unit for AI and ML,,"Bristol, United Kingdom-based startup Graphcore offers a microprocessor specifically designed for AI and machine learning tasks.

Known as the Intelligent Processing Unit (IPU), the microprocessor has applications in areas such as AI computing in data centres. Traditionally, graphical processing unit-based systems take on the workload of AI training and other tasks thanks to their highly parallel nature, but Graphcore says its offering is capable of offering improved performance.

Since its foundation in 2016, the company has raised around $700mn across six funding rounds. Its latest Series E was its largest to date, raising $222mn alone and launching its valuation to around $2.8bn. The round was led by Ontario Teachers' Pension Plan alongside a raft of existing investors such as Robert Bosch Venture Capital and Dell Technologies Capital, and new investors Fidelity International and Schroders.

In a press release , Graphcore CEO and co-founder Nigel Toon said: ""Having the backing of such respected institutional investors says something very powerful about how the markets now view Graphcore. The confidence that they have in us comes from the competence we have demonstrated building our products and our business. We have created a technology that dramatically outperforms legacy processors such as GPUs, a powerful set of software tools that are tailored to the needs of AI developers, and a global sales operation that is bringing our products to market.""

The company said it would use the funds to drive its expansion globally and further develop its IPU product.

Olivia Steedman, Senior Managing Director, Teachers' Innovation Platform (TIP) at Ontario Teachers Pension Plan, said: ""The market for purpose-built AI processors is expected to be significant in the coming years because of computing megatrends like cloud technology and 5G and increased AI adoption, and we believe Graphcore is poised to be a leader in this space.""","['ml', 'graphcores', 'intelligent', 'investors', 'ai', 'processing', 'unit', 'processors', 'powerful', 'teachers', 'technology', 'plan', 'products', 'graphcore']","Bristol, United Kingdom-based startup Graphcore offers a microprocessor specifically designed for AI and machine learning tasks.
Known as the Intelligent Processing Unit (IPU), the microprocessor has applications in areas such as AI computing in data centres.
Traditionally, graphical processing unit-based systems take on the workload of AI training and other tasks thanks to their highly parallel nature, but Graphcore says its offering is capable of offering improved performance.
The confidence that they have in us comes from the competence we have demonstrated building our products and our business.
The company said it would use the funds to drive its expansion globally and further develop its IPU product."
159,https://www.aimagazine.com/ai-applications-1/intel-and-esa-launch-experimental-ai-satellite-phisat-1,Intel and ESA launch experimental AI satellite PhiSat-1,,"American semiconductor giant Intel and the European Space Agency have launched what Intel calls the first “AI satellite” into space.

The launch, which took place on 2 September, saw the experimental PhiSat-1 ejected from a rocket alongside 45 other small satellites.

PhiSat-1 is one of a pair of satellites intended to monitor polar ice and soil moisture, as well as test communication between satellites for a planned future network.

The satellite contains a thermal camera and, crucially, onboard AI processing capabilities thanks to an Intel Movidius Myriad 2 Vision Processing Unit. The processor will use AI techniques to improve imagery and reduce bandwidth by discarding “cloudy” images, as Gianluca Furano, data systems and onboard computing lead at the European Space Agency, explained in a press release .

“The capability that sensors have to produce data increases by a factor of 100 every generation, while our capabilities to download data are increasing, but only by a factor of three, four, five per generation. And artificial intelligence at the edge came to rescue us, the cavalry in the Western movie.”

Satellites in space are blasted with radiation outside of the protection of the Earth’s atmosphere, but Intel’s chip passed testing in its unaltered form.

Future uses for AI-enhanced satellites might include swapping out networks in a “satellite-as-a-service” model, as Jonathan Byrne, head of the Intel Movidius technology office explained: ““Rather than having dedicated hardware in a satellite that does one thing, it’s possible to switch networks in and out.”

Such possibilities will be tested with the PhiSat-2 satellite which is currently being developed to be capable of running different AI apps

Small satellites are very much in vogue thanks to the decrease in costs to orbit achieved by companies such as SpaceX. SpaceX itself is building a constellation known as Starlink in batches of 60, with the end-goal of 30,000 satellites providing satellite internet access globally.

(Image: Intel)","['satellite', 'data', 'thanks', 'launch', 'intel', 'satellites', 'ai', 'processing', 'onboard', 'esa', 'experimental', 'phisat1', 'space', 'spacex']","American semiconductor giant Intel and the European Space Agency have launched what Intel calls the first “AI satellite” into space.
The launch, which took place on 2 September, saw the experimental PhiSat-1 ejected from a rocket alongside 45 other small satellites.
PhiSat-1 is one of a pair of satellites intended to monitor polar ice and soil moisture, as well as test communication between satellites for a planned future network.
The satellite contains a thermal camera and, crucially, onboard AI processing capabilities thanks to an Intel Movidius Myriad 2 Vision Processing Unit.
SpaceX itself is building a constellation known as Starlink in batches of 60, with the end-goal of 30,000 satellites providing satellite internet access globally."
160,https://www.aimagazine.com/ai-applications-1/beyond-limits-partners-nvidia-ai-energy-sector,Beyond Limits partners with NVIDIA on AI for energy sector,,"Los Angeles, California-based Beyond Limits creates AI solutions for industries such as energy, utilities and healthcare.

The company says its Cognitive AI technology involves the use of conventional techniques such as machine learning, neural networks and deep learning, as well as knowledge-based reasoning to make autonomous decisions that nevertheless leave behind audit trails that can explain the reasoning. Such an approach is crucial in industries such as healthcare, where accountability is an ethical necessity.

The company has announced it is collaborating with US tech firm NVIDIA, a specialist in the graphical processing units (GPUs) useful for machine learning applications thanks to their highly parallel nature. The collaboration will include AI software optimised for GPUs to improve performance and efficiency in the software development cycle.

In a press release , AJ Abdallat, CEO of Beyond Limits, said: “AI has the potential to make a major impact on problems facing the heart of the global energy business, but the technology requires high levels of computing power to operate on the level and scale required by many of today’s global producers. That’s why we’re so excited to collaborate with NVIDIA, a leading provider of AI computing platforms. With NVIDIA technology support and expertise, Beyond Limits is better positioned to offer faster, more intelligent and efficient AI-based solutions for maximizing energy production and profitability.”

One example of AI’s benefit in the energy industry has been demonstrated by Beyond Limit’s use of a deep learning framework on NVIDIA A100 GPUs to predict and recommend the placement of wells in the oil and gas sector.

“The NVIDIA A100 offers the performance and reliability required to meet the demands of the modern day energy sector,” said Marc Spieler, Global Energy Director at NVIDIA. “The ability to process hundreds of thousands of AI simulations in real-time provides the insight required for Beyond Limits to develop scalable applications that advance energy technologies.”","['limits', 'nvidia', 'learning', 'ai', 'gpus', 'global', 'partners', 'solutions', 'technology', 'energy', 'sector', 'required']","Los Angeles, California-based Beyond Limits creates AI solutions for industries such as energy, utilities and healthcare.
The collaboration will include AI software optimised for GPUs to improve performance and efficiency in the software development cycle.
That’s why we’re so excited to collaborate with NVIDIA, a leading provider of AI computing platforms.
“The NVIDIA A100 offers the performance and reliability required to meet the demands of the modern day energy sector,” said Marc Spieler, Global Energy Director at NVIDIA.
“The ability to process hundreds of thousands of AI simulations in real-time provides the insight required for Beyond Limits to develop scalable applications that advance energy technologies.”"
161,https://www.aimagazine.com/machine-learning/sabre-corporation-develops-travel-ai-platform-google,Sabre Corporation develops Travel AI platform with Google,,"Travel tech company Sabre Corporation has announced a partnership with Google focused on developing an AI-driven travel platform.

Sabre is best known for its global distribution systems which has long served as the underlying framework for air travel bookings.

The Sabre Travel AI platform makes use of Google Cloud ’s AI and machine learning capabilities to analyse and predict consumer behaviour in real time. Possibilities enabled by the technology include allowing airlines and other travel businesses to deliver relevant offers to customers.

The work builds on an earlier partnership with Google Cloud signed in January 2020, with Sabre also expecting its customers to be able to integrate their systems with Travel AI, for instance by plugging-in third party databases and the like.

In a press release , Sundar Narasimhan, president of Sabre Labs, said: ""With the creation of Sabre Travel AI, we are rebuilding our platform on cloud-native, data-driven technology that can be integrated into the existing and future products that Sabre offers. We are combining Google Cloud's infrastructure, AI and machine-learning capabilities with Sabre's deep travel domain knowledge to create, not next, but third-generation solutions that we believe are smarter, faster and more cost-effective – a first-of-its kind in travel.""

Sabre said it hoped the partnership would allow it to further utilise machine learning models to solve the dynamic availability and pricing problems which are part and parcel of the travel booking industry.

""Since the inception of our strategic relationship with Sabre, one of our goals has been to bring together the talent and technology of both our companies to create the future of travel,"" said Rob Enslin, president of Google Cloud. ""Sabre Travel AI perfectly represents Google Cloud's strategic vision to partner deeply with thought leaders in industry verticals to utilize Google Cloud's innovative technologies to transform and create industry firsts.”","['create', 'sabre', 'travel', 'corporation', 'ai', 'partnership', 'platform', 'systems', 'google', 'cloud', 'technology', 'clouds', 'develops']","Travel tech company Sabre Corporation has announced a partnership with Google focused on developing an AI-driven travel platform.
Sabre is best known for its global distribution systems which has long served as the underlying framework for air travel bookings.
The Sabre Travel AI platform makes use of Google Cloud ’s AI and machine learning capabilities to analyse and predict consumer behaviour in real time.
In a press release , Sundar Narasimhan, president of Sabre Labs, said: ""With the creation of Sabre Travel AI, we are rebuilding our platform on cloud-native, data-driven technology that can be integrated into the existing and future products that Sabre offers.
""Sabre Travel AI perfectly represents Google Cloud's strategic vision to partner deeply with thought leaders in industry verticals to utilize Google Cloud's innovative technologies to transform and create industry firsts.”"
162,https://www.aimagazine.com/interviews/michael-kanaan-usafmit-ai-accelerator,Michael Kanaan on the USAF/MIT AI Accelerator,,"Michael Kanaan is Director of Operations, U.S. Air Force and MIT Artificial Intelligence Accelerator, having previously been at the Pentagon for four years as co-chair of AI for the Air Force.

The MIT AI Accelerator began in January of 2020. “It’s pursuant to a cooperative agreement with MIT, MIT Lincoln laboratory and the Department of the Air Force,” explains Kanaan. “Our efforts stretch across three main lines. The first is to execute a number of flagship AI projects and the related work to bring that into existence. The second is developing scalable AI education for the workforce - all demographics, all ages and all ranks. And the last is to lead the dialogue in AI, ethics and safety. It’s all about making AI real for our workforce.”

The initiatives include such things as natural language processing for communication with machine and foreign language training, swarming unmanned aerial vehicles for deployment on humanitarian aid missions and using big data to illuminate weather circumstances in areas without a ground station. The projects are linked by a shared focus, as Kanaan explains. “The most important thing is to ensure that we all have a common and shared dialogue and understanding of what AI is, what it isn't, how it works and how to walk along that journey.”

The MIT and Air Force collaboration is of a lineage with some of the most illustrious projects in the history of the United States. “There's a triangular relationship between industry, academia and government in the United States, that's very special and very storied throughout our past.” Kanaan emphasizes that it stems from a common language between government, industry and academia which must be nurtured. “We have to reinvigorate the relationship that, for instance, brought the internet into our homes. Artificial intelligence is something that's going to be viewed as equivalent to electricity in our lives, because of the way it affects us every single day. What could be more important than something like electricity being shared by the greatest minds, by those who build the best technologies and by the government as representative of its people?”

The fruits of the labor being put into these projects are not only for the Air Force’s benefit, with wider society also standing to gain. Kanaan cites humanitarian disasters, such as the wildfires and hurricanes which have had a devastating impact on the US this year, as examples of situations that could benefit from its work. “Humanitarian aid is a huge mission of the United States Air Force, as it is of the Army, the Navy, Coast Guard and so on. AI has a role to play, and that can stretch across swarming drones to using computer vision, to predicting fire lines, to detecting people in flooded areas and delivering telemedical health.”

Kanaan views the collaboration that has enabled the accelerator as key to its success. “I can’t emphasize enough how grateful we are to MIT, to academia, to industry for being a part of this conversation and to our airmen and workforce for wanting to have the dialogue. What makes us special is that, while we are certain to make mistakes along the way, we hold a dialogue afterwards. It's all about diving in.”","['united', 'force', 'usafmit', 'accelerator', 'michael', 'kanaan', 'dialogue', 'ai', 'states', 'shared', 'projects', 'air', 'mit']","Michael Kanaan is Director of Operations, U.S. Air Force and MIT Artificial Intelligence Accelerator, having previously been at the Pentagon for four years as co-chair of AI for the Air Force.
The MIT AI Accelerator began in January of 2020.
“It’s pursuant to a cooperative agreement with MIT, MIT Lincoln laboratory and the Department of the Air Force,” explains Kanaan.
The first is to execute a number of flagship AI projects and the related work to bring that into existence.
“Humanitarian aid is a huge mission of the United States Air Force, as it is of the Army, the Navy, Coast Guard and so on."
163,https://www.aimagazine.com/ai-strategy/interview-pascal-bornet-intelligent-automation,Interview: Pascal Bornet on intelligent automation,,"It would appear that Intelligent Automation (IA) can’t come soon enough for Pascal Bornet. The recognised leading global authority on the subject is passionate about the capacity of artificial intelligence and automation to make our society more human. He’s a busy man, conducting his interview with AI from the car as he abandons Paris for the south of France for a family break.

Making more time could be one of the welcome shared outcomes of IA, as outlined in Intelligent Automation – Learn How to Harness Artificial Intelligence to Boost Business and Make Our World More Human.

The book – co-authored by Bornet, Ian Barkin and Jochen Wirtz – is the first reference guidebook published on the topic of IA, and draws on more than 500 use cases to illustrate a wide range of bold predictions.

Bold? You be the judge. How about:

Saving 10 million+ lives a year

Saving US$10 trillion per year

Eliminating world hunger

Saving the environment

Now we’ve got your attention, read on for Bornet’s vision of the next renaissance, and how IA can make the world more human.

Where does IA come from?

The industrial revolutions started over 200 years ago, automating blue-collar work in the agricultural and manufacturing industries. They provided massive and structural benefits to our society, such as the reduction of famine and an increase in standards of living. They also relieved people from laborious manual work.

Officially coined in 2017 by IEEE, intelligent automation (IA) ushers in a new revolution: that of office work, automating white-collar work. Today, office work accounts for more than 80 per cent of the job roles in our global economy – such as lawyers, financial controllers, or call centre operators. Like the previous automation revolutions, I believe IA will have a significant impact not only on employment but, more broadly, on our society.

What is IA?

IA, also called hyperautomation or cognitive automation, is one of the most recent trends in the field of artificial intelligence. It is a combination of methods and technologies, involving people, organisations, machine learning, low-code platforms, robotic process automation (RPA), and more. It is aimed at automating end-to-end business processes in a computerised environment. It delivers business outcomes on behalf of the employees.

For example, IA can help to automate most work activities in ‘procure to pay’ – from the selection of vendors, sending of orders, reception and processing of their invoices, up to the payment of these vendors.

IA effectively creates a software-based digital workforce that enables synergies by working hand-in-hand with the human workforce. As a result, IA increases process speed, reduces costs, eliminates errors, and enhances compliance. Ultimately, it improves employee and customer satisfaction and boosts revenues.

How does IA boost business efficiency?

Even though IA has only been coined recently, its applications have spread incredibly quickly in the business world, validating its promise. It has already been adopted by more than half of the world's largest companies, including ADP, JP Morgan, ANZ Bank, Netflix, and Unilever. The expected impact on business efficiency is in the range of 20 to 60 per cent. These benefits are available to all organisations, across industries, and regardless of function.

While impactful, implementing IA is certainly not a silver bullet. While it is easy to succeed in implementing a pilot on a limited scope, many organisations have been struggling to scale their transformations.

According to my experience and research, companies which were successful had all implemented five components:

Always put people in the centre of an IA transformation: IA is built by people, for people. Without people, there is no IA. Without IA, there are still people.

Start with a strong and healthy foundation: management support, capability building and change management.

Combine the IA capabilities to automate end to end processes and create synergies.

Democratise IA with the use of low code platforms, for example.

Leverage technology to accelerate IA implementation (eg. process discovery and mining, data discovery, AutoML, automated maintenance).

How does IA improve employee experience?

According to Gallup research, 85 per cent of employees worldwide are not fulfilled by their work, because it is too manual, repetitive, and tedious. IA solves a large part of this issue by freeing up employees from repetitive and transactional tasks, such as keying in invoices in an accounting software. And it refocuses them on more value-added and exciting tasks (ones involving insights, creativity).

It also augments them, transforming them into superhumans able to generate insights from millions of data in a few seconds (such as identifying a tumor on an X-ray).

What impact has Covid-19 had on IA?

In my view, this crisis evidenced the need for more resilient systems to support our society. Our health and economic systems, mainly managed by a human workforce, have been put under extreme stress. Hospitals were desperately in need, while economies were falling into downturns. IA has a large role to play in solving this and make our world ready for the next crisis.

Ilan Oshri, professor at the University of Auckland's Graduate School of Management, said: ""Covid-19 achieved in six to eight weeks what the evangelists of automation have not managed... for more than five years.""

Despite the bad aspects of Covid-19, it has helped the world understand the importance of digitalising processes, enabling remote performance, and automating them to rely less on the human workforce.

Companies that already leveraged IA have been able to continue their operations, collect their cash, manage their operations, and motivate their employees remotely.

How does IA boost the customer experience?

Building trust, while satisfying and retaining customers, is critical for businesses. 96 per cent of unhappy customers don't bother complaining, and 91 per cent of them will simply leave and never return. IA helps to create innovative and customised products, and highly responsive, omnichannel customer services available 24/7. Based on my experience with IA, companies can increase the level of their customer satisfaction by over 50 per cent, while reducing the contact centre workload by over 50 per cent.

How does IA save lives?

IA has the potential to save millions of lives every year by supporting clinical trials and disease diagnosis, and avoiding medical errors. In developing countries, it can help reduce deaths from preventable causes (1.6 million people died from diseases related to diarrhea in 2017) and compensate for the shortage of 4.3 million physicians globally, by enabling remote diagnosis. For example, IA application Tissue Analytics instantly diagnoses chronic wounds, burns, or skin conditions just by taking a photo from a smartphone.

How can IA save money, and reallocate it to better uses?

IA could have the potential to realise a US$10 trillion of annual cost savings, by reducing frauds, errors, and accidents. Indeed, IA not only makes transaction processes more efficient and reliable, but it also generates log files for every action, creating transparency and ease of compliance. Such a vast amount of money would allow us to double our global budget for education, help restore our planet from pollution, or even eliminate hunger.

How can IA help to reinvent our world?

Generalising the use of IA in our world is not without risks. To prepare our world to effectively translate the key benefits of IA, our societies' roadmap should include some imperatives.

First, education needs to be adapted to fit the skills expected in the future. It has to focus on people's competitive edge; that is, on tasks that machines will never be able to do (well). In my view, the most crucial of these skills are creativity, adaptability, and ‘learning how to learn’.

Second, according to economists, the use of digital technologies over the last decades has resulted in increasing wealth inequalities amongst people. To remedy this situation, it seems necessary to consider implementing wealth-sharing mechanisms such as universal basic income.

On top of this, IA's impact on employment needs to be monitored very closely. Similarly to the previous industrial revolutions, an optimistic scenario holds that IA will enable the creation of more jobs than we have today. Conversely, a pessimistic scenario suggests that, due to the fast pace and other specifics, IA will massively reduce the number of jobs available. While the actual scenario will most likely be hybrid, to mitigate risks, we need to be prepared to deal with both scenarios. We owe this to our children and future generations.

Enabled by the above initiatives, IA could help us consider building a new, more human society. One that involves a new, more engaging definition of ‘work’, and that would give us the time to refocus on what matters the most in our lives – family, love, taking care of others, and our planet.

Intelligent Automation – Learn How to Harness Artificial Intelligence to Boost Business and Make Our World More Human, is out now, written by Pascal Bornet, Ian Barkin and Jochen Wirtz.

ABOUT THE AUTHOR

Pascal Bornet is a global expert in the field of intelligent automation (IA). He is a senior executive with 20-plus years of experience leading digital business transformations and creations. Over the past 10 years, he has founded and led intelligent automation practices, first for Ernst & Young and then McKinsey & Company. These lines of business delivered high-impact results to corporate clients across industries through innovation, research, strategic investments, and cutting-edge technology developments.

Bornet is a recognised author, thought leader, lecturer, and speaker on artificial intelligence, automation, and the future of work. He is also an influencer, elected Top Voice in Technology 2019, and has more than 300,000 followers on LinkedIn and Twitter.

Bornet holds an MBA from the University of California Los Angeles, an MBA from the National University of Singapore, a Master of Science in Management from EM-Lyon/Saint-Etienne, as well as several certifications in Data Science and Finance (US CPA).","['automation', 'world', 'intelligent', 'ia', 'pascal', 'bornet', 'cent', 'help', 'business', 'interview', 'human', 'work', 'intelligence']","It would appear that Intelligent Automation (IA) can’t come soon enough for Pascal Bornet.
Officially coined in 2017 by IEEE, intelligent automation (IA) ushers in a new revolution: that of office work, automating white-collar work.
ABOUT THE AUTHORPascal Bornet is a global expert in the field of intelligent automation (IA).
Over the past 10 years, he has founded and led intelligent automation practices, first for Ernst & Young and then McKinsey & Company.
Bornet is a recognised author, thought leader, lecturer, and speaker on artificial intelligence, automation, and the future of work."
164,https://www.aimagazine.com/machine-learning/machine-learning-music-googles-tone-transfer,Machine learning for music: Google’s Tone Transfer,,"Google made a low-key launch this week, when it made Tone Transfer public. Built by two teams at Mountain View – Magenta and AI UX – the tool takes a tonal input (a voice or a line of melody) and can then re-render it with instrument modelling.

The year-long collaboration between AI researchers, UX engineers and designers is built on Magenta’s Differential Digital Signal Processing engine (DDSP). It was created as an exercise in learning about how people perceive music, machine learning and their own practice. Running on an early version of DDSP, it’s an in-browser deployment (on tensorflow.js) which extracts pitch data using another Google Research project, SPICE .

At the moment, the tool has been opened up for experiment by musicians and non-musicians who want to explore music creation. But Magenta made DDSP open source earlier this year and, while it hasn’t been expressly stated, there may be implications for business data collection.

As new routes to mining data are explored, and voice collection via VOIP services becomes more commonplace, there is scope to explore tonal approaches to voice data. Applying machine learning to voice data could help to parse language that otherwise could be misinterpreted by data analysts. Sarcasm or humour are both common idioms that would create a false positive if voice data is mined in like-for-like fashion with text.

Magenta says: “We are excited with upcoming releases enabling you to easily train your own DDSP models and deploy them everywhere: a phone, an audio plugin or a website using the larger tensorflow lite and tensorflow.js ecosystem.”

Data scientists, always on the lookout for a new frontier, should prick their ears.","['music', 'voice', 'using', 'data', 'tensorflowjs', 'googles', 'learning', 'transfer', 'ddsp', 'machine', 'tone', 'tool', 'ux', 'tonal']","It was created as an exercise in learning about how people perceive music, machine learning and their own practice.
Running on an early version of DDSP, it’s an in-browser deployment (on tensorflow.js) which extracts pitch data using another Google Research project, SPICE .
As new routes to mining data are explored, and voice collection via VOIP services becomes more commonplace, there is scope to explore tonal approaches to voice data.
Applying machine learning to voice data could help to parse language that otherwise could be misinterpreted by data analysts.
Sarcasm or humour are both common idioms that would create a false positive if voice data is mined in like-for-like fashion with text."
165,https://www.aimagazine.com/top10/top-10-cars-ai-features,Top 10 cars with AI features,,"The autonomous cars of the future will not be possible without AI. While we wait for those magnificent self-driving machines, here are 10 cars employing AI to assist us mortal charioteers.","['machines', 'future', 'employing', 'possible', 'ai', 'selfdriving', 'cars', 'features', 'wait', 'mortal', 'magnificent']","The autonomous cars of the future will not be possible without AI.
While we wait for those magnificent self-driving machines, here are 10 cars employing AI to assist us mortal charioteers."
166,https://www.aimagazine.com/technology-9/demystifying-role-rpa-among-automation-solutions,Demystifying the role of RPA among automation solutions,,"RPA, or robotic process automation, is increasingly big business. Gartner predicts that total RPA software revenue will reach almost $2bn in 2021, and continue to grow at a double-digit rate up to 2024, even with the pressures of COVID-19.

First, it’s worth demystifying the notion of RPA, which is a term often wrongly ascribed to other automation technologies. Francis Carden, VP, Robotics and Transformation at Pegasystems tells us that “too many people mistake RPA for machine intelligence. Its powerful benefits stem from how quickly it can automate how different software processes talk to each other. In itself, an RPA isn’t very intelligent in how it automates simple tasks.”

Exactly what is RPA capable of, then? According to Carden, RPA is ideally suited to “provide rapid automation of rote tasks and processes where humans are being paid to simply be keyboard and mouse ‘warriors’.” The ideal use case for RPA is in areas with a large volume of highly repetitive work. “The largest scale and faster ROI is in larger contact centers and front and middle offices,” says Carden.

The reasons why so many companies are deploying RPA technology and as a result driving growth in the sector are various, such as the capacity for a robotic worker to operate constantly, without mistake. But Xena Lappin, EVP Transformation & Innovation at Teleperformance, is clear that RPA is no full-scale replacement for human workers. “RPA is not about automating roles out and reducing headcount, but making tasks within jobs more efficient. Before implementing RPA, businesses must start by reviewing processes for improvement opportunities. Then it’s important to target very simple, repetitive tasks first.”

RPA also comes to the fore in serving as a go-between between the disparate enterprise systems that typically make up an organisation’s IT stack. “The prime benefit of RPA is how it unblocks specific bottlenecks that have developed within and between the enterprise applications used to progress a business process like authorising a mortgage or managing an insurance claim,” says Carden. “It is too difficult to quickly transform these processes so RPA can step in to automate how each of them can be seamlessly linked up. The outcome should be a saving of considerable time and the generation of a quick ROI.”

Another of its strengths lies in playing nice with other technologies which can be layered on top, as Andrew Pellegrino, Director of Intelligent Automation at DataRobot, explains: “When AI or machine learning is combined with RPA, organisations can add even greater value by enabling RPA to take action on data-driven decisions or predictions. With Intelligent Automation - the resulting solution - AI or machine learning automates the decision-making and RPA automates the manual next steps within the process.”

The ongoing COVID-19 pandemic presents an unexpected opportunity for RPA to thrive, owing to the restrictions it places on human workers. “By automating manual processes and offering a contactless solution, businesses can free up time and improve operational performance and productivity,” says Lappin. “In a recent Forrester survey, nearly half of respondents said they expect to increase their RPA spend by at least 5 percent in the next year.”

Pegasystems’ Carden concurs, saying: “The pandemic injected real urgency and pace into the take-up of smart automation, including RPA. We surveyed global businesses about their preparedness for future lockdowns. 76% of respondents said the pandemic will cause them to increase their intelligent automation investment. Meanwhile, 74% of survey participants agree that further external shocks that temporarily remove people from the workplace will result in more intelligent automation and artificial intelligence investment, while 76% also say that unpredictable mass illness and/or self-isolation will drive increased business demand for intelligent automation.”

It would be wrong to suggest that it’s plain sailing for the implementation of RPA from here on in, however, with significant challenges remaining. One of the problems that might bring down an RPA deployment is simple coordination, as Carden explains: “The problem is organisations must make sure that the work performed by robots is coordinated between the right bots, people and systems. When this isn’t orchestrated well enough, the benefits of RPA evaporate. Orchestration should be the first point of call regardless of who’s doing the work: a robot, human, system or even an AI.”

Bots are also not the idealised, perfect beings you might imagine, with malfunctions a distinct possibility. Indeed, 87% of IT decision-makers have experienced bot malfunctions, according to a Pegasystems study. That can be mitigated by continual evaluation of the technology. “Once implemented, RPA should not be forgotten about – it requires consistent appraisals, to keep up with the ever-changing world of regulation and advancements in technology,” says Lappin.

The future of the technology will be dictated by its evolution and the addition of new features, with DataRobot’s Pellegrino seeing something of an automation saturation. “It is difficult to find a highly transactional, repetitive task that is not already automated internally,” he says. “This is why the demand for AI/ML in conjunction with RPA has spiked recently as new use cases for automation require adding AI/ML to automate the intelligent decision making RPA alone cannot do.”","['automation', 'carden', 'simple', 'role', 'intelligent', 'rpa', 'processes', 'tasks', 'demystifying', 'solutions', 'work', 'technology', 'repetitive']","First, it’s worth demystifying the notion of RPA, which is a term often wrongly ascribed to other automation technologies.
In itself, an RPA isn’t very intelligent in how it automates simple tasks.”Exactly what is RPA capable of, then?
The reasons why so many companies are deploying RPA technology and as a result driving growth in the sector are various, such as the capacity for a robotic worker to operate constantly, without mistake.
76% of respondents said the pandemic will cause them to increase their intelligent automation investment.
“This is why the demand for AI/ML in conjunction with RPA has spiked recently as new use cases for automation require adding AI/ML to automate the intelligent decision making RPA alone cannot do.”"
167,https://www.aimagazine.com/ai-applications/startup-extend-robotics-launches-vr-controlled-robot-arm,Startup Extend Robotics launches VR-controlled robot arm,,"Reading, United Kingdom-based robotics startup Extend Robotics has announced the launch of a teleoperated robotic arm .

The company, which produces robotic hardware for industries including healthcare, services, utilities and electronics, says the Robot Toolkit can be controlled in real-time via virtual reality, allowing for a high level of dexterity and reachability.

Extend Robotics’ partners include Nvidia, BT, Queen Mary University London, University Hospital Birmingham, Britbots and Innovate UK, with the company hoping to make dextrous teleoperated robotics more accessible by lowering prices of both hardware and control systems.

In a press release, Dr. Chang Liu, the company’s founder and CEO, said: “At Extend Robotics, our vision is to extend human capability beyond physical presence. Our mission is to democratise dexterous teleoperation at scale over the next three years, designing cost-effective robotic arms capable of remote operation from anywhere in the world, using cloud-based teleoperation software.

“Our latest cybernetic bartender robot demo is a great example of an ‘out of the box’ teleoperated robot solution for the service and catering industry. We also plan to develop VR-controlled teleoperated robots featuring highly accurate, smooth and consistent, human-like movements to improve safety conditions and boost efficiencies across a number of other sectors: from agriculture and healthcare through to the utilities and energy industry.”

The company is developing the technology for wider commercial use, with its demonstration highlighting the applicability of the technology to the hospitality industry.

“Right now, as we approach the end of the COVID-19 crisis, we expect to see remote working as ‘the new norm’ across many industries, for numerous health, safety and environmental reasons,” said Liu. “2020 has been an incredibly challenging year for humanity, yet our hope at Extend Robotics is that the recent acceleration in R&D of remote teleoperated working robots will soon result in a wide range of safe, secure and affordable dexterity robot solutions across a number of industries worldwide.”

(Image: Extend Robotics)","['company', 'arm', 'robot', 'vrcontrolled', 'teleoperated', 'industries', 'robotics', 'launches', 'extend', 'remote', 'working', 'robotic', 'utilities', 'startup']","Reading, United Kingdom-based robotics startup Extend Robotics has announced the launch of a teleoperated robotic arm .
Extend Robotics’ partners include Nvidia, BT, Queen Mary University London, University Hospital Birmingham, Britbots and Innovate UK, with the company hoping to make dextrous teleoperated robotics more accessible by lowering prices of both hardware and control systems.
In a press release, Dr. Chang Liu, the company’s founder and CEO, said: “At Extend Robotics, our vision is to extend human capability beyond physical presence.
“Our latest cybernetic bartender robot demo is a great example of an ‘out of the box’ teleoperated robot solution for the service and catering industry.
“2020 has been an incredibly challenging year for humanity, yet our hope at Extend Robotics is that the recent acceleration in R&D of remote teleoperated working robots will soon result in a wide range of safe, secure and affordable dexterity robot solutions across a number of industries worldwide.”(Image: Extend Robotics)"
168,https://www.aimagazine.com/ai-applications-1/uber-sells-atg-self-driving-unit-aurora,Uber sells ATG self-driving unit to Aurora,,"Uber is selling its self-driving unit, Advanced Technologies Group (Uber ATG), to US-based self-driving car company Aurora.

Aurora plans to use the expertise it is importing from Uber ATG to build a self-driving truck, while continuing to work on cars. Logistics and taxis are two areas where self-driving vehicles are expected to make a major impact, in advance of more general use.

In a statement, Aurora said the deal to acquire Uber ATG would “strengthen and accelerate the first Aurora Driver applications for heavy-duty trucks while allowing Aurora to continue and accelerate work on light-vehicle products”.

'Shifting the landscape'

Chris Urmson, co-founder and CEO of Aurora, said, “By adding the people and technology of Uber’s Advanced Technologies Group to the incredible group we’ve already assembled at Aurora, we’re shifting the landscape of the automated vehicle space.

“With the addition of ATG, Aurora will have an incredibly strong team and technology, a clear path to several markets, and the resources to deliver. Simply put, Aurora will be the company best positioned to deliver the self-driving products necessary to make transportation and logistics safer, more accessible, and less expensive.”

The deal includes a strategic partnership with Uber, suggesting that while autonomous trucking would be a priority, “passenger mobility” would not be far behind. Uber is investing $400 million in Aurora as part of the deal and Uber CEO Dara Khosrowshahi will join its executive board.

'Pole position'

Khosrowshahi said, “Few technologies hold as much promise to improve people’s lives with safe, accessible, and environmentally friendly transportation as self-driving vehicles.

“For the last five years, our phenomenal team at ATG has been at the forefront of this effort – and in joining forces with Aurora, they are now in pole position to deliver on that promise even faster.

“I’m looking forward to working with Chris, and to bringing the Aurora Driver to the Uber network in the years ahead.”

Subject to regulatory approval, the deal is expected to close in Q1 2021.","['atg', 'group', 'transportation', 'aurora', 'unit', 'selfdriving', 'sells', 'technologies', 'deal', 'work', 'uber', 'deliver']","Uber is selling its self-driving unit, Advanced Technologies Group (Uber ATG), to US-based self-driving car company Aurora.
Aurora plans to use the expertise it is importing from Uber ATG to build a self-driving truck, while continuing to work on cars.
In a statement, Aurora said the deal to acquire Uber ATG would “strengthen and accelerate the first Aurora Driver applications for heavy-duty trucks while allowing Aurora to continue and accelerate work on light-vehicle products”.
“With the addition of ATG, Aurora will have an incredibly strong team and technology, a clear path to several markets, and the resources to deliver.
Uber is investing $400 million in Aurora as part of the deal and Uber CEO Dara Khosrowshahi will join its executive board."
169,https://www.aimagazine.com/machine-learning/cambridge-university-build-ai-chemical-research-lab,Cambridge University to build AI chemical research lab,,"The University of Cambridge is to build a multimillion-pound research facility which will see AI accelerate chemical engineering progress.

While work has yet to begin on the build, projects have already started at the Innovation Centre in Digital Molecular Technologies (iDMT), part of the university’s department of chemistry.

It is hoped the facility will speed up access to pharmaceuticals, agrochemicals, functional molecules and molecular materias through machine learning and robotics-based synthesis.

The project was part funded by the European Regional Development Fund, and in partnership with pharmaceutical companies AstroZeneca and Shionogi.

'Major bottleneck'

Professor Alexei Lapkin from the University of Cambridge department of chemical engineering and biotechnology is iDMT’s director. He said, “Access to new functional molecules and materials continues to be a major bottleneck in many chemistry-using industries, such as medicine, food, electronics and energy.

“It is very difficult to predict how chemical processes would behave at an industrial scale. For this reason, development and optimisation of chemical processes usually takes quite a long time. AI tools can help solve complex problems of chemical process design speeding up the transition from a working chemical reaction in the lab, to a scaled-up industrial process.

“Combining cross-disciplinary expertise from several departments at the University, with state-of-the-art facilities and support from two of the leading companies in this area has the potential to enable the development of many new solutions for the nascent industry of digital molecular technology.

“Facilitating knowledge exchange to SMEs so that they can develop the right product offer that would serve the needs of the large end-user companies in the pharma, agritech and wider chemical manufacturing sectors will enable an industry-wide shift in how synthesis, process design and manufacture are carried out.”

'AI will free up scientists' time'

iDMT co-director and director of the EPSRC SynTech centre for doctoral training Matthew Guant added, “Despite tremendous advances in chemistry, we still cannot always make all of the molecules we need on demand, especially when set against increasingly competitive business-driven timelines, and this means that we often miss out on many potential opportunities to, for example, develop new medicines.

“The transformational change that we believe is required in the way chemical synthesis is approached is based on a radical increase in the throughput of chemical discovery and process development. This can be achieved through the automation of largely routine procedures, and the adoption of artificial intelligence to guide synthetic chemists towards successful solutions in a more efficient manner. This frees up time of a scientist to develop new ideas.”

Dr Ryuichi Kiyama, senior executive officer, Pharmaceutical Research Division, Shionogi, said: “We are proud to be part of this new innovative chemistry research consortium with the leading research institutes in the United Kingdom. As a pharmaceutical company with strengths in chemistry-driven small molecule drug discovery, we are committed to contribute to the discovery chemistry innovation in collaboration with researchers from the partner institutes and companies.”

The iDMT will support collaborative research projects with small and medium enterprises (SMEs) from across the UK, aiming to develop a technology base to support the emerging digital economy in the third largest manufacturing sector in the UK.

Three ways the iDMT will support research using AI

• Acceleration of synthesis through AI and automation

• Equipment for robotic experiments

• Algorithms and tools for digital process development","['lab', 'research', 'university', 'develop', 'process', 'development', 'chemical', 'ai', 'support', 'cambridge', 'build', 'synthesis', 'digital']","The University of Cambridge is to build a multimillion-pound research facility which will see AI accelerate chemical engineering progress.
'Major bottleneck'Professor Alexei Lapkin from the University of Cambridge department of chemical engineering and biotechnology is iDMT’s director.
AI tools can help solve complex problems of chemical process design speeding up the transition from a working chemical reaction in the lab, to a scaled-up industrial process.
“The transformational change that we believe is required in the way chemical synthesis is approached is based on a radical increase in the throughput of chemical discovery and process development.
Three ways the iDMT will support research using AI• Acceleration of synthesis through AI and automation• Equipment for robotic experiments• Algorithms and tools for digital process development"
170,https://www.aimagazine.com/data-and-analytics/starburst-becomes-latest-enterprise-data-unicorn,Starburst becomes latest enterprise data unicorn,,"Boston, Massachusetts-based Starburst is a data analytics company specialising in accessing data anywhere it resides.

The company is attempting to overthrow the well worn notion of a single source of truth as unachievable in the modern age, thanks to the array of sources and data management systems present in the modern world. Instead, it offers a “single point of access”, powered by technology known as Trino that allows for the quick querying and analysis of data no matter what database it resides in, thus simplifying data infrastructure. Its customers include the likes of Tableau, Comcast and Condé Nast.

Since its 2017 foundation, the company has raised $164mn across three funding rounds. Its latest Series C round, announced yesterday, saw the company raise $100mn, propelling it to a tech unicorn valuation of $1.2bn. The round was led by Andreessen Horowitz, alongside Coatue, Index Ventures and Salesforce Ventures.

In a press release , Justin Borgman, CEO and co-founder of Starburst, said: “Today, the only constant is change, and organizations need to make faster and better decisions in order to adapt. Starburst is changing the game by allowing you to query the data wherever it lives. We provide high-performance data warehousing analytics across data you never had access to before.”

The company said it would use the funds to fuel growth as organisations change their approach to data analytics.

“To succeed in today’s digital economy, organizations need to break through the limits previously experienced when accessing their most critical asset – their data,” said David George, General Partner at Andreessen Horowitz. “With its disciplined approach, long-term vision, and workhorse mentality, Starburst is building the technology required to unlock the value of all data. By enabling organizations to access data wherever it resides, Starburst is providing the foundation for the future of the data-driven enterprise.”","['resides', 'access', 'company', 'data', 'starburst', 'need', 'organizations', 'unicorn', 'round', 'technology', 'single', 'enterprise', 'latest']","Boston, Massachusetts-based Starburst is a data analytics company specialising in accessing data anywhere it resides.
Its latest Series C round, announced yesterday, saw the company raise $100mn, propelling it to a tech unicorn valuation of $1.2bn.
We provide high-performance data warehousing analytics across data you never had access to before.”The company said it would use the funds to fuel growth as organisations change their approach to data analytics.
“With its disciplined approach, long-term vision, and workhorse mentality, Starburst is building the technology required to unlock the value of all data.
By enabling organizations to access data wherever it resides, Starburst is providing the foundation for the future of the data-driven enterprise.”"
171,https://www.aimagazine.com/technology-9/how-unilever-driving-digital-transformation-manufactur-intelligent-powder-towers-are-keeping-its-manufacturing-powder-dry-during-pandemic,Unilever’s Intelligent Powder Towers drive digital change,,"Maintaining product quality is a defining pillar of brand building, and automated production to date has needed a helping hand from attentive humans. While the layman might see a manufacturing plant as a simple I/O with raw materials going in and finished products coming out, the reality is awash with skilled monitoring and adjustments to account for environmental factors, machine wear and other inconsistencies.

For companies such as Unilever, the Covid-19 pandemic has thrown it a challenge. It needs to maintain quality control in its products lines while keeping its workforce protected from exposure to the virus. That’s why the household goods giant has created ‘Intelligent Powder Towers’

What are Unilever’s Intelligent Powder Towers?

Powder towers are an essential piece of the laundry detergent manufacturing puzzle. The ingredients for Unilever’s laundry detergent powders, such as OMO, Comfort and Surf, are sprayed into the towers, which are blasted with hot air to dry it into a powder.

The moisture content of the powder is pivotal to product consistency and crucial to maintaining brand standards. Manual operators have historically controlled this balance in the towers, but as part of Unilever’s digital transformation, the powder towers can now be controlled algorithmically.

The algorithms use artificial intelligence and machine learning to predict optimal operating conditions and simulate different scenarios.

Digital transformation driven by Covid-19

The Intelligent Powder Towers are part of Unilever’s supply chain digital transformation, accelerated by the conditions created by Covid-19. The two powder towers that are part of its digital factory environment are in Indaiatuba, Brazil and Guayas, Ecuador. These can now be operated remotely, with staff able to oversee manufacturing while working from home.

And Unilever’s Intelligent Powder Towers have gone beyond allowing flexible working models, enhancing the speed and agility of data transfer through the process, streamlining the manufacturing workflow.

“Now the technicians in the control room can focus on more strategic decisions regarding production and use data intelligence to improve the process, instead of having to constantly monitor each process parameter,” says Daniel Correia, digital manufacturing manager LATAM, Unilever. “The insights are all in one place, accessible through a Power BI dashboard and the Unilever Digital Factory app, which enables faster decision making based on real-time data to optimise the performance of the tower.”

The digital transformation, using technology such as Intelligent Powder Towers, will also allow Unilever to monitor manufacturing machinery from overseas, and to combine data from global operations to increase efficiency.","['drive', 'transformation', 'unilevers', 'intelligent', 'data', 'change', 'towers', 'process', 'unilever', 'manufacturing', 'digital', 'powder']","That’s why the household goods giant has created ‘Intelligent Powder Towers’What are Unilever’s Intelligent Powder Towers?
Manual operators have historically controlled this balance in the towers, but as part of Unilever’s digital transformation, the powder towers can now be controlled algorithmically.
Digital transformation driven by Covid-19The Intelligent Powder Towers are part of Unilever’s supply chain digital transformation, accelerated by the conditions created by Covid-19.
The two powder towers that are part of its digital factory environment are in Indaiatuba, Brazil and Guayas, Ecuador.
And Unilever’s Intelligent Powder Towers have gone beyond allowing flexible working models, enhancing the speed and agility of data transfer through the process, streamlining the manufacturing workflow."
172,https://www.aimagazine.com/interviews/satyan-abraham-digital-transformation-journey-dubai,Satyan Abraham on a digital transformation journey in Dubai,,"Satyan Abraham, ICT Director at DXB Entertainments, has been with the organisation since 2016. Born in India, Abraham worked in Mumbai before relocating to the Gulf country. Prior to his role at DXB Entertainments, Abraham spent over a decade at Emirates Airlines in a variety of different roles and also headed his own consultancy firm called Josh Consulting. “I was truly enjoying my consulting career, when I was approached by a senior executive at DXB Entertainments, who said that they were building a world-class leisure and entertainment destination, the first of its kind in the UAE and in the region, and that they wanted to bring someone onboard to oversee their major ICT infrastructure project. From the onset, Dubai Parks and Resorts’ vision was to establish itself as a key component of Dubai’s world class leisure and entertainment experience. “The challenge and excitement associated with working on a project of this magnitude, where I could help contribute to the growth of Dubai’s leisure sector, encouraged me to give up my consulting career and join the company full-time.”

Over the past 25 years, Abraham has led transformational programmes across several portfolios, achieving significant cost savings and creating a rich capability of people, processes and technologies. With extensive experience in the travel & transport and leisure & entertainment industries, Abraham has successfully implemented a number of large-scale IT projects, which positively impact passenger and customer experience. Abraham has a strong track record, leading organisations from vision through to execution. He is passionate about digital innovation, with his main interests enhancing customer experience through the launch of innovative technologies and solutions. In addition, Abraham sits on the advisory board of startup organisations, providing free mentoring and advisory services to early stage entrepreneurs.

Abraham holds a Master’s in Business Administration from London Business School, as well as a Bachelor’s in Computer Engineering from Bombay University, India.","['abraham', 'experience', 'consulting', 'dxb', 'transformation', 'project', 'leisure', 'journey', 'vision', 'technologies', 'entertainments', 'digital', 'entertainment', 'dubai', 'satyan']","Satyan Abraham, ICT Director at DXB Entertainments, has been with the organisation since 2016.
Prior to his role at DXB Entertainments, Abraham spent over a decade at Emirates Airlines in a variety of different roles and also headed his own consultancy firm called Josh Consulting.
From the onset, Dubai Parks and Resorts’ vision was to establish itself as a key component of Dubai’s world class leisure and entertainment experience.
With extensive experience in the travel & transport and leisure & entertainment industries, Abraham has successfully implemented a number of large-scale IT projects, which positively impact passenger and customer experience.
In addition, Abraham sits on the advisory board of startup organisations, providing free mentoring and advisory services to early stage entrepreneurs."
173,https://www.aimagazine.com/interviews/panasonic-using-technology-address-customer-challenges,Panasonic: using technology to address customer challenges,,"Combining over 50 years of experience between them, John Harris, GM for European Tech Strategy at Panasonic Toughbook, Ian Woozley, Head of Supply Chain Solutions UK&I and France at Panasonic Business, and Jonathan Tucker, GM for Solutions Development and Implementation at Panasonic Toughbook are experts in understanding, anticipating and resolving the challenges faced by customers with a clearly defined approach for delivering quality outcomes.

Harris summarises it succinctly, “It's always been about solving problems or solving a need with innovative products.” Indeed, the company’s daily ethos is to contribute value to customer’s lives through what it creates. “We’re very much an engineering company,” adds Tucker. “Our position as a historically well-known brand has been solidified by our consumer goods items, TVs, and DVD players.” However, not satisfied with merely enjoying the rich legacy that it has accumulated, Panasonic continues to keep its sights fixed firmly on the future: 7% of its global turnover is reinvested in R&D (research and development), an approach which has netted the company substantial acclaim from consumers and a robust portfolio of over 15,000 products.

Tucker clarifies that the company, first and foremost, values its employees and clientele above all. “What I love about the organisation is the degree of autonomy we’re given to go and try something new. It's very much a people before products attitude.” It is because of this that Panasonic positions itself as a problem-solver, which subsequently means that it keeps abreast of the latest industry challenges and stays relevant to customers. Furthermore, rather than simply find surface-level workarounds, the company develops holistic solutions that incorporate hardware, software and aftercare to ensure long-term value. It develops these solutions shoulder-to-shoulder with the client themselves, building strong, durable relationships which recognise the quality of Panasonic’s work. “Our focus now is to replicate the success that we've built in Japan and bring it to the Western world,” says Woozley.

Trends such as automation can similarly unlock pathways to business optimisation, but Woozley makes it clear that the most effective automation shouldn’t sacrifice the human touch. “A lot of what Panasonic is exploring is based on partial automation and the retention of workers. Full automation isn’t necessary, although it can be done; we consider partial automation more than adequate to improve workspaces and create a more efficient business in many circumstances.” It is this ability to see past the mere novelty of new technology and instead focus on its customer-centric application which distinguishes Panasonic from competitors in the field and will continue to shape its approach going forward. “We want to understand what the challenges customers are experiencing and use our insights to solve them. It's about finding those tension points and then actioning them before they become pain points,” Harris concludes.","['automation', 'customers', 'woozley', 'using', 'address', 'customer', 'approach', 'value', 'challenges', 'business', 'solutions', 'technology', 'panasonic', 'company']","“What I love about the organisation is the degree of autonomy we’re given to go and try something new.
Furthermore, rather than simply find surface-level workarounds, the company develops holistic solutions that incorporate hardware, software and aftercare to ensure long-term value.
It develops these solutions shoulder-to-shoulder with the client themselves, building strong, durable relationships which recognise the quality of Panasonic’s work.
“A lot of what Panasonic is exploring is based on partial automation and the retention of workers.
“We want to understand what the challenges customers are experiencing and use our insights to solve them."
174,https://www.aimagazine.com/company/cellcard-delivering-cambodias-5g-digital-transformation,Cellcard: delivering Cambodia’s 5G digital transformation,,"“We pride ourselves on our Cambodian heritage; 99% of our employees are Cambodian,” Watson continues. “But you can’t be digital on the outside unless you’re digital on the inside. As a result, and as part of this journey to the 5G digitialsing of Cambodia, we’re having to re-engineer and reprocess virtually the entire company from the top down. And it’s not just about processes, it’s about people, having good digital change managers and a concerted effort to bring the whole company into the digital world.”

It is also, says Watson, about building and developing technology that is embedded in that Cambodian heritage. “For example, we’ve created a Cambodian Chat app called Mith Laor, which means ‘good friends’. The app includes a TV platform to host video and other content, a payment platform that sees us explore more lifestyle-type services, and more. It’s all part of this broader end-to-end digital platform that we will deliver through the 5G rollout, and it all works through a smartphone app. To ensure this is best-in-class we’re already looking to integrate other innovations, such as AI, integrated voice assistants, chatbots and robotics”.

As to the implementation, Watson explains that Cellcard’s network is pre-5G enabled, with all 5G testing already being concluded some two years ago. The company is currently working with the government ministries on a test and releasing spectrum that, once completed, will see it “having built one of the best, most dynamic digital 5G platforms not only in Cambodia, but in the world”, says Watson.

Cellcard has several sites that it has been running for some time as part of that process. The company recently used its 5G infrastructure to provide an important service as part of Cambodia’s efforts to combat the COVID-19 pandemic. In March it announced the Kingdom’s first use of 5G for a telemedicine service at four locations across Phnom Penh to help with critically ill patients. The service allowed doctors to use video conferencing technology linked to mobile phones and devices across Cambodia to assess patients in real-time. This extends to the provision of real-time clinical care, counselling and therapy, as well as education for consultants and support for medical teams.

“It’s part of what we are trying to do as we move into a full 5G environment,” Watson explains. “We’re looking at using the technology for remote diagnostics in all provinces, to support medical staff and for other diagnosis and treatment services such as recognising cataracts and more. It’s all been helped and underpinned by our existing, superfast 4G network too, which remains the best performing in Cambodia.”

Naturally, any significant transformation journey brings challenges and a degree of change management. For example, says Watson, a company can’t digitalise if it doesn’t have the technology. Cellcard has invested significantly in its network to facilitate the rollout of 5G, but also in building a team of people with the correct mindset and ambition to drive the Kingdom’s digitalisation.

On the technologies that will enable this, Watson explains that “to have a dynamic 5G network it has to be accessible. It must be fast and deliver everything that we promise. But it’s not just about speed, it’s about understanding and incorporating all of the new opportunities that can be gained from a 5G world, so things like network splicing, enhanced mobile capabilities and the Internet of Things. Second to that, is ensuring you have the necessary platforms to deliver all of the products and services. This includes a robust cloud computing platform and watertight cybersecurity practices.”

The latter, he explains, is crucial as networks and data grow. “In a true digital world, it won’t just be your handset that you should be concerned about, it’ll be every device in your home,” he states. “For organisations like us, it means the need to authenticate a large number of devices latching to the network, ensuring robust perimeter defences so that people can’t get into the network in the first instance and more. We invest huge amounts into our security protocols - we simply have to.”

For this, and other areas of technology, Watson states that partnerships prove important. A good technology partner, he says, must fit Cellcard’s digital vision and be prepared to work towards the long-term ambitions of the company and the Kingdom. He cites a long-term partnership with Microsoft as a good example, through which Cellcard has access to the company’s video conferencing and other technologies.

Looking ahead to a 5G-enabled Cambodia, Watson elaborates on the benefits that the technology will bring, not just to consumers but also the Kingdom’s enterprise economy. “The B2B market is, in my opinion, one of the biggest selling points of 5G,” he states. “We’re already setting up a dedicated 5G digital enterprise department to focus solely on the B2B and SME sectors and we are investing heavily in data centres so that enterprises can manage and have access to the vast amounts of data that they will need.

“For Cambodia, the future is very exciting. It’s a young, dynamic county and it has huge potential that can be realised by digitalisation. 5G will underpin the true digital transformation of the Kingdom and we’re plugged in and ready to deliver on that vision,” he concludes.","['network', 'transformation', 'delivering', 'cambodias', 'good', 'platform', 'cambodia', 'cellcard', 'technology', 'watson', 'digital', '5g', 'explains', 'company']","It’s all part of this broader end-to-end digital platform that we will deliver through the 5G rollout, and it all works through a smartphone app.
As to the implementation, Watson explains that Cellcard’s network is pre-5G enabled, with all 5G testing already being concluded some two years ago.
“It’s part of what we are trying to do as we move into a full 5G environment,” Watson explains.
On the technologies that will enable this, Watson explains that “to have a dynamic 5G network it has to be accessible.
5G will underpin the true digital transformation of the Kingdom and we’re plugged in and ready to deliver on that vision,” he concludes."
175,https://www.aimagazine.com/data-and-analytics/get-data-science-out-it-departments-and-boardrooms,Get data science out of IT departments and into boardrooms,,"Where do your data scientists sit? Perhaps they occupy a typically gloomy, computer-filled basement, or maybe they have a glassy building all to themselves. Either way, you’ll not always see business decision-makers walking the same corridors. After all, analytics is best left to the experts, isn’t it?

Yet, back in the boardroom, the statistics and insights that the managers have ordered from the data science department make little sense. How can these be turned into profit? The truth is that where data scientists sit in your organisation is vital – strategically, as well as physically. When they’re separated from decision-makers, data analytics won’t line up very easily with business goals.

I recently had the chance to speak with SAS Collaborators Jen Stirrup and Neil Cattermull about optimising the role of the data scientist. A catalyst for positive change, the influence of data professionals should be felt throughout the entire organisation.

One analytics strategy for all to participate in

Data is nothing without strategy. While many business decision-makers stress the importance of data, Jen Stirrup pointed out that often this appears to be a case of “data FOMO”. These business leaders have a fear of missing out on the potential insights their data has to offer… but in reality, they have no idea what insights they’re looking for.

For all analytics use cases there needs to be a clear understanding first of exactly what the business problem is that you’re trying to tackle, and how analytics can help. Only then can the relevant data be used and the right models created, refined and refreshed that will lead to key business insights to help resolve the problem.

Analytics is for all, as we become more data-driven and analytical in our thinking and our work. But, it’s not up to each department to ‘shake the data hard enough and get results’. Companies must strategically democratise analytics , turning the data into insights which are valuable to all who could benefit from them. This means starting with the right high-quality datasets, selected in collaboration with decision-makers. Then, model design and deployment should prioritise the achievement of the agreed business goals. Finally, the findings should be presented comprehensibly, using visual analytics or simple interfaces tailored for those who need to understand and then act upon them.

Speed is of the essence in analytics, and yet so many projects become a “poisoned chalice”, as Neil Cattermull stated: passed down through teams and taking years to complete. This can be sped up through collaboration at every stage. Take the furlough scheme. Desperately needed to preserve jobs and businesses through COVID-19, the efficient collaboration of hundreds of policy-makers, data scientists and employers helped reverse the tax system within three to four weeks, saving countless jobs.

We are all data scientists

As data becomes more and more pivotal in business, it shouldn’t be left up to the tech experts to advocate its value. Asking data scientists to take on the role of a business analyst is only half the solution; the other route to truly data-driven success is to nurture the evolution of new analyst roles outside of the data science department.

Citizen data scientists . While working in their own field – say, supply chain, or customer service – citizen data scientists might create models that leverage analytics specifically for their own department. They can rely on the strength of their contextual knowledge and wider understanding of business priorities to tailor the analytics process to produce insights that their team can act on immediately.

. While working in their own field – say, supply chain, or customer service – citizen data scientists might create models that leverage analytics specifically for their own department. They can rely on the strength of their contextual knowledge and wider understanding of business priorities to tailor the analytics process to produce insights that their team can act on immediately. Data translators. With a breadth of knowledge spanning technical data science and top-level business management, a data translator can turn insights into a more “business-friendly framework”, as Jen noted. As intermediaries between data scientists and decision-makers, they guarantee a process that has impact at scale in the organisation. At the same time, data translators are by default teaching and nurturing each group to think more collaboratively.

Data literate employees across the business. A sensitivity for the raw data generated from customers or operations isn’t just for business analysts. It has the potential to revolutionise business thinking in all departments and among all staff members. Making this organisational shift to data literacy is arguably one of the greatest drivers of reliable growth that businesses can commit to today.

What place does data science have in the future?

Companies aiming to be competitive in the modern day must have a greater focus on data, analytics and insights than ever. But the role of the data scientist is beginning to change.

Ultimately, coding could become a smaller part of what data scientists do, as models become more interface-based. This means data scientists will soon be working to fit machine scale data into existing models, and interpreting the results. They will become responsible for the wider process of managing the insights, and they’ll be constantly reviewing what they mean for the end-user.

In the boardroom, data scientists are invaluable. As Neil pointed out, their knowledge of reference cases can provide the evidence to convince the C-suite of key decisions, informed by data insights. But data science can also have a huge impact on public perception. Organisational ethics is fast becoming a non-negotiable for customers. Sensitivity for how data science capacity can be leveraged worldwide helps organisations to use data for good , to make a difference on issues from deforestation to reducing injustice.

Data-driven businesses are leading the way in the business landscape, and data science is at the helm. When data scientists are stuck in the engine room, their influence on navigation is wasted. By encouraging collaboration, skill-sharing and a broadening of roles among data scientists and others, organisations can be confident of their bearings as they chart a way towards their goals.","['boardrooms', 'models', 'insights', 'data', 'knowledge', 'process', 'departments', 'science', 'business', 'scientists', 'decisionmakers', 'analytics']","The truth is that where data scientists sit in your organisation is vital – strategically, as well as physically.
This means data scientists will soon be working to fit machine scale data into existing models, and interpreting the results.
But data science can also have a huge impact on public perception.
Sensitivity for how data science capacity can be leveraged worldwide helps organisations to use data for good , to make a difference on issues from deforestation to reducing injustice.
Data-driven businesses are leading the way in the business landscape, and data science is at the helm."
176,http://aiweekly.co/issues/186,Issue #186: AI detects asymptomatic Covid-19 infections through cellphone-recorded coughs,,"Applied use cases

Who am I to decide when algorithms should make important decisions? In doing so, it reduced the scores of poor, Black, and brown students, while giving higher marks to students from elite schools and wealthy areas. In the United States, this means that a handful of firms in Silicon Valley are at the heart of AI development, including building algorithmic models they...

How a fake persona laid the groundwork for a Hunter Biden conspiracy deluge One month before a purported leak of files from Hunter Biden's laptop, a fake ""intelligence"" document about him went viral on the right-wing internet, asserting an elaborate conspiracy theory involving former Vice President Joe Biden's son and business in China.","['detects', 'wealthy', 'bidens', 'conspiracy', '186', 'went', 'ai', 'coughs', 'cellphonerecorded', 'students', 'viral', 'asymptomatic', 'covid19', 'valley', 'fake', 'issue', 'hunter', 'vice', 'infections']","Applied use casesWho am I to decide when algorithms should make important decisions?
In doing so, it reduced the scores of poor, Black, and brown students, while giving higher marks to students from elite schools and wealthy areas.
In the United States, this means that a handful of firms in Silicon Valley are at the heart of AI development, including building algorithmic models they...How a fake persona laid the groundwork for a Hunter Biden conspiracy deluge One month before a purported leak of files from Hunter Biden's laptop, a fake ""intelligence"" document about him went viral on the right-wing internet, asserting an elaborate conspiracy theory involving former Vice President Joe Biden's son and business in China."
177,http://aiweekly.co/issues/185,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"In the digital arms race with China, the only thing worse than fearing AI itself is the fear of not having it at all. Consequently, service members lead personal lives digitally connected to almost everything and military lives connected to almost nothing.","['artificial', 'lives', 'lead', 'connected', 'learning', 'military', 'ai', 'leading', 'deep', 'weekly', 'worse', 'thing', 'members', 'personal', 'service', 'race', 'intelligence', 'newsletter']","In the digital arms race with China, the only thing worse than fearing AI itself is the fear of not having it at all.
Consequently, service members lead personal lives digitally connected to almost everything and military lives connected to almost nothing."
178,http://aiweekly.co/issues/183,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"In The News

AI Is Throwing Battery Development Into Overdrive In 2019, a team of researchers from Stanford, MIT, and the Toyota Research Institute used AI trained on data generated from these machines to predict the performance of lithium-ion batteries over the lifetime of the cells before their performance had started to slip.

Cory Doctorow: ‘Technologists have failed to listen to non-technologists’ He has worked for the Electronic Frontier Foundation and helped found the Open Rights Group – he is an advocate of liberalising copyright law. If they ever do bury me, as opposed to scattering my ashes in the Haunted Mansion at Disneyland, I want my tombstone to read: “This will all be so great if...","['artificial', 'technologists', 'used', 'team', 'learning', 'ai', 'leading', 'deep', 'performance', 'weekly', 'worked', 'tombstone', 'trained', 'throwing', 'toyota', 'intelligence', 'newsletter', 'started']","In The NewsAI Is Throwing Battery Development Into Overdrive In 2019, a team of researchers from Stanford, MIT, and the Toyota Research Institute used AI trained on data generated from these machines to predict the performance of lithium-ion batteries over the lifetime of the cells before their performance had started to slip.
Cory Doctorow: ‘Technologists have failed to listen to non-technologists’ He has worked for the Electronic Frontier Foundation and helped found the Open Rights Group – he is an advocate of liberalising copyright law.
If they ever do bury me, as opposed to scattering my ashes in the Haunted Mansion at Disneyland, I want my tombstone to read: “This will all be so great if..."
179,http://aiweekly.co/issues/182,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"We argue that this Whiteness both illuminates particularities of what (Anglophone Western) society hopes for and fears from these machines, and situates these affects within long-standing ideological structures that relate race and technology.

More about us on: https://montrealethics.ai/about/ If someone has forwarded this to you and you want to get one delivered to you every week, you can subscribe to receive this newsletter by clicking below: Summary of the content this week: In research summaries this week, the whiteness of AI, a...","['artificial', 'western', 'structures', 'technologymore', 'learning', 'ai', 'leading', 'deep', 'whiteness', 'summary', 'situates', 'weekly', 'summaries', 'society', 'subscribe', 'intelligence', 'newsletter', 'week']","We argue that this Whiteness both illuminates particularities of what (Anglophone Western) society hopes for and fears from these machines, and situates these affects within long-standing ideological structures that relate race and technology.
More about us on: https://montrealethics.ai/about/ If someone has forwarded this to you and you want to get one delivered to you every week, you can subscribe to receive this newsletter by clicking below: Summary of the content this week: In research summaries this week, the whiteness of AI, a..."
180,http://aiweekly.co/issues/180,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Applied use cases

The Supply of Disinformation Will Soon Be Infinite CounterPunch published a January 2018 postmortem detailing what its investigation had found: articles plagiarized from The New Yorker, the Saudi-based Arab News, and other sources; prolific “journalists” who filed as many as three or four stories a day, but whose bylines disappeared after inquiries...

The Cruel New Era of Data-Driven Deportation The agency had long tapped into driver address records through law enforcement networks. Eyeing the breadth of DMV databases, agents began to ask state officials to run face recognition searches on driver photos against the photos of undocumented people.","['artificial', 'soon', 'searches', 'yorker', 'driver', 'sources', 'undocumented', 'learning', 'state', 'ai', 'leading', 'deep', 'weekly', 'supply', 'photos', 'tapped', 'intelligence', 'newsletter']","Applied use casesThe Supply of Disinformation Will Soon Be Infinite CounterPunch published a January 2018 postmortem detailing what its investigation had found: articles plagiarized from The New Yorker, the Saudi-based Arab News, and other sources; prolific “journalists” who filed as many as three or four stories a day, but whose bylines disappeared after inquiries...
The Cruel New Era of Data-Driven Deportation The agency had long tapped into driver address records through law enforcement networks.
Eyeing the breadth of DMV databases, agents began to ask state officials to run face recognition searches on driver photos against the photos of undocumented people."
181,http://aiweekly.co/issues/179,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"“We thought it should be possible for us to use a robot to remove the health care worker from the risk of directly exposing themselves to the patient.” Using four cameras mounted on a dog-like robot developed by Boston Dynamics, the researchers have shown that they can measure skin temperature,...","['artificial', 'shown', 'using', 'robot', 'researchers', 'learning', 'ai', 'leading', 'deep', 'thought', 'temperature', 'weekly', 'worker', 'skin', 'remove', 'intelligence', 'newsletter', 'risk']","“We thought it should be possible for us to use a robot to remove the health care worker from the risk of directly exposing themselves to the patient.” Using four cameras mounted on a dog-like robot developed by Boston Dynamics, the researchers have shown that they can measure skin temperature,..."
182,http://aiweekly.co/issues/177,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Robotics

Flexibility and Manufacturing Productivity: Part 2 – Human Capabilities Overcome Technological Limitations Relying on a few examples, we will show how the inefficiencies and inflexibilities of high levels of automation show up in suboptimal labor and capital allocations and higher-than-warranted manufacturing costs.

This Week’s Awesome Tech Stories From Around the Web (Through August 29) How Special Relativity Can Help AI Predict the Future Will Douglas Heaven | MIT Technology Review “The AI can make guesses about the future without having to learn anything about the progression of time, says Vlontzos.","['artificial', 'future', 'suboptimal', 'weeks', 'web', 'learning', 'ai', 'leading', 'deep', 'tech', 'technological', 'weekly', 'technology', 'manufacturing', 'vlontzos', 'intelligence', 'newsletter']","RoboticsFlexibility and Manufacturing Productivity: Part 2 – Human Capabilities Overcome Technological Limitations Relying on a few examples, we will show how the inefficiencies and inflexibilities of high levels of automation show up in suboptimal labor and capital allocations and higher-than-warranted manufacturing costs.
This Week’s Awesome Tech Stories From Around the Web (Through August 29) How Special Relativity Can Help AI Predict the Future Will Douglas Heaven | MIT Technology Review “The AI can make guesses about the future without having to learn anything about the progression of time, says Vlontzos."
183,http://aiweekly.co/issues/175,Issue #175: Facebook’s AI for detecting hate speech is facing its biggest challenge yet,,"Machine learning will impact some industries more than others. Manufacturing, retailing, transportation, and food services have many tasks suitable for ML.

Any researcher who’s focused on applying machine learning to real-world problems has likely received a response like this one: “The authors present a solution for an original and highly motivating problem, but it is an application and the significance seems limited for the machine-learning community.”","['facing', 'speech', 'tasks', 'solution', 'biggest', 'suitable', 'ai', 'services', 'detecting', '175', 'transportation', 'machine', 'facebooks', 'whos', 'significance', 'issue', 'hate', 'learning', 'challenge', 'retailing']","Machine learning will impact some industries more than others.
Manufacturing, retailing, transportation, and food services have many tasks suitable for ML.
Any researcher who’s focused on applying machine learning to real-world problems has likely received a response like this one: “The authors present a solution for an original and highly motivating problem, but it is an application and the significance seems limited for the machine-learning community.”"
184,http://aiweekly.co/issues/174,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"In The News

Baby-mounted cameras teach AI to ‘see’ through a child’s eyes Ievgen Chabanov/Alamy The way that babies learn about and navigate the world could prove to be a good model for training artificial intelligence. AIs don’t learn as efficiently or flexibly as children.

Amazon's Machine Learning University is making its online courses available to the public To help meet that demand, Amazon founded its in-house Machine Learning University (MLU) in 2016. The first three online courses cover natural language processing (the machine understanding of human language), computer vision (the machine understanding of images and video), and tabular data (machine...","['artificial', 'university', 'world', 'understanding', 'learning', 'ai', 'leading', 'deep', 'language', 'online', 'courses', 'weekly', 'machine', 'way', 'intelligence', 'newsletter', 'learn']","In The NewsBaby-mounted cameras teach AI to ‘see’ through a child’s eyes Ievgen Chabanov/Alamy The way that babies learn about and navigate the world could prove to be a good model for training artificial intelligence.
AIs don’t learn as efficiently or flexibly as children.
Amazon's Machine Learning University is making its online courses available to the public To help meet that demand, Amazon founded its in-house Machine Learning University (MLU) in 2016.
The first three online courses cover natural language processing (the machine understanding of human language), computer vision (the machine understanding of images and video), and tabular data (machine..."
185,http://aiweekly.co/issues/172,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Applied use cases

Google Ad Portal Equated “Black Girls” with Porn Google’s Keywords Planner, which helps advertisers choose which search terms to associate with their ads, offered hundreds of keyword suggestions related to “Black girls,” “Latina girls,” and “Asian Girls”—the majority of them pornographic, The Markup found in its research.

Rite Aid deployed facial recognition systems in hundreds of U.S. stores While Rite Aid declined to disclose which locations used the technology, Reuters found facial recognition cameras at 33 of the 75 Rite Aid shops in Manhattan and the central Los Angeles metropolitan area during one or more visits from October through July.","['artificial', 'aid', 'rite', 'visits', 'terms', 'used', 'learning', 'ai', 'leading', 'deep', 'hundreds', 'recognition', 'facial', 'weekly', 'black', 'intelligence', 'newsletter', 'girls']","Applied use casesGoogle Ad Portal Equated “Black Girls” with Porn Google’s Keywords Planner, which helps advertisers choose which search terms to associate with their ads, offered hundreds of keyword suggestions related to “Black girls,” “Latina girls,” and “Asian Girls”—the majority of them pornographic, The Markup found in its research.
Rite Aid deployed facial recognition systems in hundreds of U.S. stores While Rite Aid declined to disclose which locations used the technology, Reuters found facial recognition cameras at 33 of the 75 Rite Aid shops in Manhattan and the central Los Angeles metropolitan area during one or more visits from October through July."
186,http://aiweekly.co/issues/170,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Robotics

Roomba and the role of future robots Today, the house-cleaning Roomba seems almost ubiquitous, but in a recent essay, its inventor, Joe Jones, recalls his wrong prediction in the 1980s that “in three to five years, robots will be everywhere doing all sorts of jobs.”

Letting robots manipulate cables For humans, it can be challenging to manipulate thin flexible objects like ropes, wires, or cables. As a cable slides between the fingers, its shape is constantly changing, and the robot’s fingers must be constantly sensing and adjusting the cable’s position and motion.","['artificial', 'ubiquitous', 'today', 'learning', 'cables', 'ai', 'leading', 'deep', 'manipulate', 'wrong', 'wires', 'weekly', 'sorts', 'robots', 'constantly', 'fingers', 'intelligence', 'newsletter']","RoboticsRoomba and the role of future robots Today, the house-cleaning Roomba seems almost ubiquitous, but in a recent essay, its inventor, Joe Jones, recalls his wrong prediction in the 1980s that “in three to five years, robots will be everywhere doing all sorts of jobs.”Letting robots manipulate cables For humans, it can be challenging to manipulate thin flexible objects like ropes, wires, or cables.
As a cable slides between the fingers, its shape is constantly changing, and the robot’s fingers must be constantly sensing and adjusting the cable’s position and motion."
187,http://aiweekly.co/issues/168,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"In The News

MIT apologizes, permanently pulls offline huge dataset that taught AI systems to use racist, misogynistic slurs Thanks to MIT's cavalier approach when assembling its training set, though, these systems may also label women as whores or bitches, and Black and Asian people with derogatory language.

A deep learning pioneer’s teachable moment on AI bias In response to a colleague calling the Obama photo an example of the dangers of AI bias, LeCun asserted that “ML systems are biased when data is biased.” Analysis of a portion of the data set found far more White women and men than Black women, but people quickly took issue with the characterization...","['artificial', 'biased', 'whores', 'data', 'women', 'learning', 'set', 'ai', 'leading', 'deep', 'white', 'bias', 'weekly', 'black', 'systems', 'intelligence', 'newsletter']","In The NewsMIT apologizes, permanently pulls offline huge dataset that taught AI systems to use racist, misogynistic slurs Thanks to MIT's cavalier approach when assembling its training set, though, these systems may also label women as whores or bitches, and Black and Asian people with derogatory language.
A deep learning pioneer’s teachable moment on AI bias In response to a colleague calling the Obama photo an example of the dangers of AI bias, LeCun asserted that “ML systems are biased when data is biased.” Analysis of a portion of the data set found far more White women and men than Black women, but people quickly took issue with the characterization..."
188,http://aiweekly.co/issues/167,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Robotics

A robot sloth will (very slowly) survey endangered species Most animal-inspired robots are designed to move quickly, but Georgia Tech’s latest is just the opposite. Their newly developed SlothBot is built to study animals, plants and the overall environment below them by moving as little as possible.

Why robotics startups fail! Fresh Consulting analyzed significant industry case studies from Rethink Robotics to iRobot for their whitepaper “Why Robotics Companies Fail,” and launched it on June 11 at a panel discussion moderated by James Dietrich, from Fresh Consulting, with guest speakers Aaron Prather, Senior Advisor for...","['artificial', 'fresh', 'consulting', 'startups', 'study', 'techs', 'studies', 'learning', 'ai', 'leading', 'deep', 'survey', 'robotics', 'weekly', 'whitepaper', 'fail', 'intelligence', 'newsletter']","RoboticsA robot sloth will (very slowly) survey endangered species Most animal-inspired robots are designed to move quickly, but Georgia Tech’s latest is just the opposite.
Their newly developed SlothBot is built to study animals, plants and the overall environment below them by moving as little as possible.
Why robotics startups fail!
Fresh Consulting analyzed significant industry case studies from Rethink Robotics to iRobot for their whitepaper “Why Robotics Companies Fail,” and launched it on June 11 at a panel discussion moderated by James Dietrich, from Fresh Consulting, with guest speakers Aaron Prather, Senior Advisor for..."
189,http://aiweekly.co/issues/166,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Such conversations tend to revolve around AI fairness, intended use, potential misuse, privacy pushback, worker displacement and product accuracy. The rise of AI also creates accuracy, privacy and worker displacement concerns.

In a newly published paper on the preprint server Arxiv.org, researchers at the Montreal AI Ethics Institute, McGill University, Carnegie Mellon, and Microsoft propose a four-pillar framework called SECure designed to quantify the environmental and social impact of AI.

The spark that sent Inioluwa Deborah Raji down a path of artificial-intelligence research came from a firsthand realization that she remembers as “horrible.” Raji was interning at the machine--learning startup Clarifai after her third year of college, working on a computer vision model that would...","['artificial', 'university', 'privacy', 'accuracy', 'learning', 'displacement', 'ai', 'leading', 'deep', 'working', 'weekly', 'worker', 'vision', 'raji', 'intelligence', 'newsletter', 'tend']","Such conversations tend to revolve around AI fairness, intended use, potential misuse, privacy pushback, worker displacement and product accuracy.
The rise of AI also creates accuracy, privacy and worker displacement concerns.
In a newly published paper on the preprint server Arxiv.org, researchers at the Montreal AI Ethics Institute, McGill University, Carnegie Mellon, and Microsoft propose a four-pillar framework called SECure designed to quantify the environmental and social impact of AI.
The spark that sent Inioluwa Deborah Raji down a path of artificial-intelligence research came from a firsthand realization that she remembers as “horrible.” Raji was interning at the machine--learning startup Clarifai after her third year of college, working on a computer vision model that would..."
190,http://aiweekly.co/issues/164,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Applied use cases

AI Isn’t Magical and Won’t Help You Reopen Your Business What do you do when a sudden break from past trends profoundly reorders the way the world works? If you’re a business, one thing you probably can’t do is turn to existing artificial intelligence.

A computer scientist gets candid about her quest to bring empathy to artificial intelligence In Girl Decoded, el Kaliouby and coauthor Carol Colman have created a riveting memoir of a “nice Egyptian girl” who, despite cultural conditioning that encouraged her to put her duties as a wife and mother first, went on to pursue her professional dreams.","['artificial', 'youre', 'world', 'went', 'learning', 'ai', 'leading', 'deep', 'business', 'way', 'weekly', 'wont', 'girl', 'wife', 'works', 'intelligence', 'newsletter']","Applied use casesAI Isn’t Magical and Won’t Help You Reopen Your Business What do you do when a sudden break from past trends profoundly reorders the way the world works?
If you’re a business, one thing you probably can’t do is turn to existing artificial intelligence.
A computer scientist gets candid about her quest to bring empathy to artificial intelligence In Girl Decoded, el Kaliouby and coauthor Carol Colman have created a riveting memoir of a “nice Egyptian girl” who, despite cultural conditioning that encouraged her to put her duties as a wife and mother first, went on to pursue her professional dreams."
191,http://aiweekly.co/issues/163,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Ethics

Forbes Insights: How Europe Is Leading The Way With Responsible AI As businesses rush to adopt artificial intelligence (AI) for products and services—from driverless cars to algorithms that can spot cancer—technologists, philosophers and governments are starting to tap the brakes. How can we be sure that AI will be trustworthy?

How Britain’s oldest universities are trying to protect humanity from risky A.I. Computer scientists at the the likes of Google and Facebook are aiming to make AI more “general” in the years ahead, and that’s got some big thinkers deeply concerned.

Ada Lovelace Institute curates ‘Ethics & Society’ at CogX 2020 This year’s CogX festival theme frames the question: ‘How do we get the next 10 years right?’ On Monday 8 June from 10am to 6.45pm BST, join us and 26 thought-leaders from policy, academia and industry to tackle the knotty, real-life trade-offs of benefits and harms emerging technologies bring to...","['artificial', 'trying', 'trustworthyhow', 'cogx', 'learning', 'tradeoffs', 'ai', 'leading', 'deep', 'universities', 'thinkers', 'way', 'weekly', 'thoughtleaders', 'theme', 'intelligence', 'newsletter']","EthicsForbes Insights: How Europe Is Leading The Way With Responsible AI As businesses rush to adopt artificial intelligence (AI) for products and services—from driverless cars to algorithms that can spot cancer—technologists, philosophers and governments are starting to tap the brakes.
How can we be sure that AI will be trustworthy?
How Britain’s oldest universities are trying to protect humanity from risky A.I.
Computer scientists at the the likes of Google and Facebook are aiming to make AI more “general” in the years ahead, and that’s got some big thinkers deeply concerned.
Ada Lovelace Institute curates ‘Ethics & Society’ at CogX 2020 This year’s CogX festival theme frames the question: ‘How do we get the next 10 years right?’ On Monday 8 June from 10am to 6.45pm BST, join us and 26 thought-leaders from policy, academia and industry to tackle the knotty, real-life trade-offs of benefits and harms emerging technologies bring to..."
192,http://aiweekly.co/issues/162,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Ethics

Inside AI (May 15th, 2020) Earlier this week, several AI researchers told CNBC that Musk has been spreading false information about AI, including the idea that machines will be as smart as humans in less than ten years.

Ethically Aligned Design The implementation of ethical principles must be validated by dependable applications of A/IS in practice, and here are three ways you can start that process in your life and work: The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems (The IEEE Global Initiative) has launched...","['artificial', 'initiative', 'learning', 'ai', 'leading', 'deep', 'global', 'ieee', 'yearsethically', 'weekly', 'ways', 'work', 'told', 'validated', 'intelligence', 'newsletter', 'week']","EthicsInside AI (May 15th, 2020) Earlier this week, several AI researchers told CNBC that Musk has been spreading false information about AI, including the idea that machines will be as smart as humans in less than ten years.
Ethically Aligned Design The implementation of ethical principles must be validated by dependable applications of A/IS in practice, and here are three ways you can start that process in your life and work: The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems (The IEEE Global Initiative) has launched..."
193,http://aiweekly.co/issues/160,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Almost as soon as iRobot released the Roomba into the world, a community of autonomous vacuum enthusiasts started giving their Roombas names, backstories, and custom wardrobes.

Mr. Schmidt is pressing forward with a Silicon Valley worldview where advances in software and A.I. are the keys to figuring out almost any issue.

You need to have a good appearance model to get the good lighting. Park says his team’s system can calculate the character of that light to “give you a very realistic estimate of the appearance of any viewpoint of the scene.” This process is called view reconstruction, or novel view synthesis.","['artificial', 'world', 'teams', 'appearance', 'worldview', 'view', 'learning', 'ai', 'leading', 'deep', 'wardrobesmr', 'weekly', 'vacuum', 'good', 'valley', 'viewpoint', 'intelligence', 'newsletter']","Almost as soon as iRobot released the Roomba into the world, a community of autonomous vacuum enthusiasts started giving their Roombas names, backstories, and custom wardrobes.
Mr. Schmidt is pressing forward with a Silicon Valley worldview where advances in software and A.I.
are the keys to figuring out almost any issue.
You need to have a good appearance model to get the good lighting.
Park says his team’s system can calculate the character of that light to “give you a very realistic estimate of the appearance of any viewpoint of the scene.” This process is called view reconstruction, or novel view synthesis."
194,http://aiweekly.co/issues/158,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Throughout the pandemic, great emphasis has been placed on the sharing (or lack of it) of critical information across countries — in particular from China — about the spread of the disease.","['artificial', 'information', 'placed', 'great', 'lack', 'learning', 'sharing', 'ai', 'leading', 'deep', 'disease', 'spread', 'particular', 'weekly', 'pandemic', 'emphasis', 'intelligence', 'newsletter']","Throughout the pandemic, great emphasis has been placed on the sharing (or lack of it) of critical information across countries — in particular from China — about the spread of the disease."
195,http://aiweekly.co/issues/156,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"In his new book, “Reprogramming the American Dream,” Kevin Scott, Microsoft’s chief technology officer, looks at how he went from a childhood in rural Virginia to being a leader in the field of AI – and why he thinks there is ample opportunity for people from all walks of life to take advantage of...","['artificial', 'scott', 'opportunity', 'went', 'learning', 'ai', 'leading', 'deep', 'officer', 'weekly', 'virginia', 'rural', 'thinks', 'technology', 'reprogramming', 'walks', 'intelligence', 'newsletter']","In his new book, “Reprogramming the American Dream,” Kevin Scott, Microsoft’s chief technology officer, looks at how he went from a childhood in rural Virginia to being a leader in the field of AI – and why he thinks there is ample opportunity for people from all walks of life to take advantage of..."
196,http://aiweekly.co/issues/149,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Ethics

Pentagon to Adopt Detailed Principles for Using AI “While it’s great that the DIB principles affirm the import of international law, there are a number of areas where it is still unclear what international law requires for AI systems or weapon systems with increasingly autonomous capabilities,” she said.

The future of minds and machines: how artificial intelligence can enhance collective intelligence In The Future of Minds and Machines we introduce an emerging framework for thinking about how groups of people interface with AI and map out the different ways that AI can add value to collective human intelligence and vice versa.","['artificial', 'machines', 'future', 'international', 'collective', 'learning', 'ai', 'leading', 'deep', 'weekly', 'law', 'systems', 'minds', 'intelligence', 'newsletter', 'principles']","EthicsPentagon to Adopt Detailed Principles for Using AI “While it’s great that the DIB principles affirm the import of international law, there are a number of areas where it is still unclear what international law requires for AI systems or weapon systems with increasingly autonomous capabilities,” she said.
The future of minds and machines: how artificial intelligence can enhance collective intelligence In The Future of Minds and Machines we introduce an emerging framework for thinking about how groups of people interface with AI and map out the different ways that AI can add value to collective human intelligence and vice versa."
197,http://aiweekly.co/issues/146,AI Weekly — AI News & Leading Newsletter on Deep Learning & Artificial Intelligence,,"Applied use cases

Going deep on deep learning with Dr. Jianfeng Gao Today, Dr. Gao gives us an overview of the deep learning landscape and talks about his latest work on Multi-task Deep Neural Networks, Unified Language Modeling and vision-language pre-training.

Announcing AI Now’s Data Genesis Program Machine learning systems are profoundly influenced by the methods of data collections and labelling that are used in their creation.

Artificial intelligence: Does another huge language model prove anything? This article is part of our reviews of AI research papers, a series of posts that explore the latest findings in artificial intelligence.","['artificial', 'data', 'gao', 'learning', 'ai', 'leading', 'deep', 'language', 'weekly', 'work', 'dr', 'intelligence', 'newsletter', 'latest']","Applied use casesGoing deep on deep learning with Dr. Jianfeng Gao Today, Dr. Gao gives us an overview of the deep learning landscape and talks about his latest work on Multi-task Deep Neural Networks, Unified Language Modeling and vision-language pre-training.
Announcing AI Now’s Data Genesis Program Machine learning systems are profoundly influenced by the methods of data collections and labelling that are used in their creation.
Artificial intelligence: Does another huge language model prove anything?
This article is part of our reviews of AI research papers, a series of posts that explore the latest findings in artificial intelligence."
198,https://www.aitrends.com/healthcare/google-move-into-healthcare-leveraging-its-ai-getting-more-pronounced/,Google Move Into Healthcare Leveraging its AI Getting More Pronounced,2020-12-29 17:02:58+00:00,"By John P. Desmond, AI Trends Editor

Google is making a more pronounced move into healthcare, leveraging its power to acquire companies and to use AI technology to disrupt the industry.

From an investment point of view, the company’s move has attracted attention. Google now has 57 digital health startups in its portfolio, according to a recent account in The Motley Fool. While the November 2019 acquisition of Fitbit generated headlines, “its investments and partnerships with healthcare service providers are more likely to be the gateway to the next big thing,” the authors stated.

The company’s investments have been focused on improving electronic health records (EHRs), diagnostic capabilities, bundling healthcare services in the cloud, and leveraging its AI expertise to advance scientific research.

Alphabet, the parent company of Google, formed a partnership with EHR provider Meditech in 2019, making its EHRs available through Google Cloud as a subscription service. This was aimed at alleviating manual tasks for clinicians, such as charting, billing, and data entry. However, the EHR market share leader Epic, abandoned a project with Google last January after a privacy concern. Epic then chose to work with Microsoft’s Azure and Amazon Web Services. “For Google to be successful in this area, it will have to shake off the data privacy stigma that seems to follow it from project to project,” the Motley Fool authors stated.

Alphabet is aiming to become a platform for developers building healthcare solutions, as well. The company’s Cloud Healthcare API allows developers to build products in the cloud that can transfer data securely across multiple sources and leverage Alphabet’s machine learning and analytics.

In research, Alphabet can demonstrate its power in AI technology. The company’s life sciences arm, Verily, initiated a project in 2017 to describe a “baseline” for human health. By creating a platform and tools—including wearables—to collect health data, the company hopes to speed up future clinical trials. In 2019, four major pharmaceutical companies signed on to utilize the work. “This makes Alphabet’s pending $2.1 billion acquisition of Fitbit much clearer,” the authors stated. Fitbit users generate health care the company can use to create a more robust model for testing and developing new drugs.

“While it may not be competing directly with legacy healthcare companies, Alphabet is slowly pulling the industry into its own circle of competence,” the authors stated. “Hosting EHRs in the cloud, offering enhanced search and predictive capabilities, and automating diagnostic capabilities creates a value proposition that cash-strapped health systems and cost-conscious insurers will find hard to ignore.”

New Google Tools Help Extract Patient Info from Text Notes

Among Google’s recent announcements in health was a suite of fully-managed AI tools including the Healthcare Natural Language API and AutoML Entity Extraction for Healthcare. These are aimed at assisting healthcare professionals with reviewing and analyzing large volumes of medication documents.

Google Cloud solutions provider, SADA, said in an account in HIT Infrastructure that the new tools will help healthcare customers implement medical analysis projects in days.

“The richest information about the health of a patient is typically not found within the structured fields of a medical record system. Instead, it is contained within the lengthy free-text notes that a clinician either types or dictates into the medical record in the course of care,” stated Michael Ames, Sr., director healthcare and life sciences at SADA. “I’m very excited for the opportunities this suite of Healthcare Natural Language AI tools from Google Cloud will create.”

CB Insights Report Outlines Google’s Plan to Reinvent the Healthcare Industry

A recent report from CBInsights entitled, “How Google Plans to Use AI to Reinvent the $3 Trillion US Healthcare Industry,” states that as Google enters healthcare, it is leaning heavily on its expertise in AI. “Health data is getting digitized and structured, from a new electronic record standard to imaging to DNA sequencing. Google is both helping speed up this process by creating new means of ingesting health data and betting that it can use AI to make sense

of the data quickly and potentially more accurately than current methods,” the report states. “Among the big 5 tech giants (Facebook, Apple, Microsoft, Google, Amazon), Google emphasizes its progress on machine learning much more than the rest.”

The three Google subsidiaries most focused on healthcare are Verily, DeepMind, and Calico, the authors state. Verily is focused on using data to improve healthcare with analytics tools, interventions and research. The unit is run by Andrew Conrad, a geneticist, who co-founded the National Genetics Institute. He has recruited a multidisciplinary team of chemists, doctors, engineers, behavioral scientists and data scientists to research health and disease.

The Study Watch, announced by Verily in April 2017, is working through FDA approvals as a medical device. A collaboration with iRhythm, aims at helping to detect irregular heartbeats, called atrial fibrillation (AFib), estimated to be a condition of 10 million Americans. “Our objective is clear: work together to develop best-in-class solutions for improved screening, diagnosis and management of patients with AFib,” stated Dr. William Marks, Head of Clinical Science for Verily in a recent press release.

Diabetes detection and management is another major healthcare focus for Google, with the condition affecting 30 million in the US presenting an opportunity. In a joint venture with Sanofi, Verily has recently launched Onduo, its virtual diabetes management program. The two companies invested $500 million into the solution in 2016, with the goal of using sensors and coaching,to help type 2 diabetics manage their condition. However, the current status of the partnership is unclear with new Sanofi CEO Paul Hudson stating in December 2019 that the company has “over-invested” in the joint venture.

Trust is an issue that will hound Google as it pursues its healthcare initiatives, the CB Insight report suggested. “Consumers’ trust in Google’s ability to keep their personal data private is vital,” the report states. The fact that Google generates most of its revenue from advertising enhanced by its collection of personal information, poses a challenge. In a survey, CB Insights found that 53 percent of respondents trusted Google “Not at all/Not very much” to keep their health data private.

Read the source articles in The Motley Fool, in HIT Infrastructure, the Verily press release on AFib, and see the report from CBInsights on Google’s strategy to disrupt healthcare with help from AI.","['getting', 'authors', 'data', 'pronounced', 'health', 'ai', 'verily', 'google', 'cloud', 'report', 'leveraging', 'tools', 'healthcare']","By John P. Desmond, AI Trends EditorGoogle is making a more pronounced move into healthcare, leveraging its power to acquire companies and to use AI technology to disrupt the industry.
Alphabet, the parent company of Google, formed a partnership with EHR provider Meditech in 2019, making its EHRs available through Google Cloud as a subscription service.
By creating a platform and tools—including wearables—to collect health data, the company hopes to speed up future clinical trials.
“I’m very excited for the opportunities this suite of Healthcare Natural Language AI tools from Google Cloud will create.”CB Insights Report Outlines Google’s Plan to Reinvent the Healthcare IndustryA recent report from CBInsights entitled, “How Google Plans to Use AI to Reinvent the $3 Trillion US Healthcare Industry,” states that as Google enters healthcare, it is leaning heavily on its expertise in AI.
“Health data is getting digitized and structured, from a new electronic record standard to imaging to DNA sequencing."
199,https://www.aitrends.com/2021-ai-predictions/2021-predictions-rise-of-glocalization-model-monitoring-focus-on-supply-chain/,"2021 Predictions: Rise of ‘Glocalization,’ Model Monitoring, Focus on Supply Chain",2020-12-29 15:11:50+00:00,"By AI Trends Staff

[Ed. Note: We have heard from a range of AI practitioners for their predictions on AI Trends in 2021. Here are predictions from a selection of those writing.]

From Dr. Lance Eliot, author of AI Trends Insider on Autonomy column in AI Trends, and global expert on AI:

The Machine Learning (ML) juggernaut continues to grow. In 2021, interest in all things entailing ML will get even hotter, which is hard to imagine since this area of AI is already red hot. Turns out that the pandemic of 2020 actually dampened many Machine Learning projects that had not yet gotten off-the-ground. Once the post-pandemic era of 2021 takes hold, expect all-hands-on-deck gangbusters for ML and Deep Learning efforts. This includes endeavors that had gone into sleep mode for 2020 and also will encompass enthralling new projects that leap into the AI unstoppable stream.

AI Ethics begins to make its mark on the AI sphere. There has been a lot of talk about AI Ethics and the importance of ensuring that AI systems are unbiased, transparent, explainable, and so on. Sadly, it is mostly up until now just talk. There are enough AI Ethics sets of principles that you can wallpaper your house with them, but very few of those vital guidelines seem to have received genuine traction. In 2021, there will be an increasing hue and cry about the continued unearthing of AI For Bad, and a realization that the only path to AI For Good involves turning the “talk” about AI Ethics into the “walk” for AI Ethics (actual implementation and adherence).

AI-based autonomous cars emerge onto public roadways, cautiously and without their babysitter. Yes, 2021 will witness AI-based autonomous cars on our roadways, which you might at first exhort that they have been there for the last several years, but it has ostensibly been with a human handler or so-called safety driver sitting at the ready to take over the vehicle. Do not conflate this toe-in-the-water with any overnight tsunami of self-driving cars on all highways and byways. You can bet your bottom dollar that these experiments, which is what they are rightfully called, will hit some bad patches and get themselves into trouble. If handled well, things will continue. If poorly dealt with, expect a cold freeze to descend upon those driverless car dreams.

From Guy Kirkwood, the Chief Evangelist at UiPath:

Digital transformation will—at last—start to become transformational. At this point, “digital transformation” has become a buzzword that all enterprises have learned to recognize, yet the vast majority (80% according to IDC) of these efforts are still too tactical in nature. Robotic process automation (RPA), for example, may be considered a transformational tool, but on its own it’s not. In order for organizations to see true transformation in 2021, they’ll need to leverage more advanced platforms that combine core automation and AI features—such as text analytics, document understanding, and process mining. It’s also critical that these platforms have low-code capabilities that enable citizen developers to build and deploy enterprise grade automations that drive value back to their organizations. Without that, it will continue to be challenging for companies to deliver enterprise-wide digital transformation—which is fueled by the ability to easily deploy automation, even to the most complex processes.

From Adzmel Adznan, Partner & Operating Manager, Piva Capital:

The Rise of Industrial Robots. The public health crisis of COVID-19 has created a dire urgency for worker safety in the industrial and manufacturing sectors and on many operations across the assembly line. In order to reduce human exposure, ensure factory workers remain healthy and COVID-free across the globe, and continue to increase productivity, the appetite to find viable AI and robotics solutions will soar to new heights. In 2021, more companies will be open to adopt these types of technologies to optimize their assembly lines, protect their employees, and operate in a leaner, cleaner and greener manner.

Industry 4.0 and COVID-19 Leads to More ‘Glocalization.’ With COVID-19 shutting down international borders and business travel, expect to see a continued rise in “glocalization” as more countries understand the implications of the Fourth Industrial Revolution and take proactive steps in ensuring they keep an advantage over their neighbors. Due to this, there will be a renewed focus on discovering the technology solutions that can improve the resiliency and efficiency of critical supply chains in localized regions. Optimizing and strengthening these supply chains will present viable opportunities for emerging countries to advance their economies and obtain a competitive edge.

From Nick Elprin, CEO, Domino Data Lab:

IT empowered to end “Shadow IT” Rapidly evolving, data science has until now lacked both appropriate governance and a centralized platform within enterprises—leading to “shadow IT” practices that include data scientists downloading unapproved tools and data science packages, while using unofficial infrastructure for storage and specialized compute. Use of these rogue systems create significant risk from a security and IT perspective. Now, as we see data science becoming increasingly pervasive and critical to every business function, in 2021 we will see more organizations treat models as digital assets, motivating IT to take the reins and provide the infrastructure to support data science at scale.

Increased pressure to ensure explainability and transparency around the use of algorithms and predictive models. While increased AI engineering capabilities provide greater structure and sophistication about how we bring models into production, rapidly evolving privacy standards first seen with GDPR and now California’s CCPA, will require in 2021 that attention equally be paid to making AI models more transparent and secure. This will require a very heavy lift involving ModelOps, DevOps, MRM, xAI and ethical AI requiring both an evolution of technology and processes.

COVID-19 to accelerate model monitoring solutions COVID-19 has had an enormous impact nearly every facet of business operations and, coincident with this, massive data drift, a change in model input leading to performance degradation is occurring. Model monitoring will be critical in 2021.

Jon Hirschtick, President of the SaaS Division at PTC:

Supply Chain to get more attention. Smart companies must revamp their supply chains in case of a severe second wave and to be more nimble in light of new market demands and conditions. We see changes affecting both procurement (supply) and the production (manufacturing) of products. Firms are pulling back from overseas (China) due to Covid and shifting to domestic production due to the need for more predictable, shorter delivery times. What this means: Instead of a Just-in-Time supply chain that delivers materials just when the customer needs it, companies will look to implement a Just-in-Case supply chain to serve as a backup in case of a future disruption. A Just-in-Case approach can reduce variability in timelines and production. This could include the possibility of firms buying and storing more raw materials.

Talent acquisition and retention/training will be important—and challenging. Remote work gives employees many more options to jump to new opportunities. How are firms attracting and retaining the best talent? What are the expectations that these folks have (Cloud, intuitive/latest tech) to allow them to do the job they were hired to do versus admin tasks? What this means: Companies will need to find new ways to identify potential employees, develop new onboarding processes to get them to feel part of the team and its mission, and support and nurture them remotely.","['models', 'data', 'trends', 'model', 'monitoring', 'predictions', 'rise', 'glocalization', 'ai', 'companies', 'supply', 'chain', 'science', 'focus', 'production', '2021', 'ethics']","By AI Trends Staff[Ed.
Note: We have heard from a range of AI practitioners for their predictions on AI Trends in 2021.
From Dr. Lance Eliot, author of AI Trends Insider on Autonomy column in AI Trends, and global expert on AI:The Machine Learning (ML) juggernaut continues to grow.
AI Ethics begins to make its mark on the AI sphere.
There has been a lot of talk about AI Ethics and the importance of ensuring that AI systems are unbiased, transparent, explainable, and so on."
200,https://www.aitrends.com/startups/startup-truera-raising-money-to-get-ai-explainability-solution-to-market/,Startup: Truera Raising Money to Get AI Explainability Solution to Market,2020-12-29 16:00:14+00:00,"By AI Trends Staff

In the black box problem in machine learning, data goes in, suggested decisions come out, and how the model arrived at its suggestions may or may not be explainable. This problem intrigued Prof. Anupam Datta of Carnegie Mellon University, who in 2014 with his PhD student Shayak Sen began researching explainable AI. At the same time, Will Uppington was among the founders at Bloomreach, which was trying to make black box machine learning models into a commercial product, and was running into similar issues around visibility into how models produce their answers.

Meeting each other in 2018, they decided to form the startup Truera in early 2019 to address the challenge. The company offers a Model Intelligence Platform, using AI explainability to power it. CEO Uppington recently responded via email to queries from AI Trends about the startup.

AI Trends: What business problem are you trying to solve?

Will Uppington: Machine Learning (ML) is exploding, but ML has a big flaw: it’s a black box. That means even when models work, data scientists don’t necessarily know why. This hinders data scientists from building high quality ML applications quickly and efficiently. It also becomes a problem when non-data scientists, such as business operators, regulators, or consumers ask questions about a result or when models break once they go live.

The science behind what drives outputs of ML models is called AI Explainability. AI explainability is the breakthrough technology that can address these challenges but it’s not enough on its own. You need a new class of software that analysts are calling Model Intelligence Platforms that leverage AI explainability, to address these problems throughout the model lifecycle: development, evaluation/testing, and monitoring.

AI Trends: How does your solution address the problem?

Uppington: Truera’s model intelligence software removes the “black box” surrounding Machine Learning (ML) and provides intelligence and actionable insights throughout the ML model lifecycle—enabling companies to improve the quality and accuracy of their models, boost stakeholder collaboration, and address responsible AI concerns including explainability and bias. Truera’s technology builds on six years of AI Explainability research performed at Carnegie Mellon University (CMU), which pioneered many of the methods that are becoming the standard for explaining popular ML models such as Tree models and Neural Networks.

AI Trends: How are you getting to the market?

Uppington: Truera is helping enterprise customers across all industry verticals including financial services, insurance, healthcare, pharmaceutical, manufacturing, and retail. Headquartered in the US, Truera sells through a direct sales force and has recently expanded its sales team globally to enter markets in Europe and Asia.

AI Trends: Do you have any users or customers?

Uppington: The Truera platform is already deployed at and delivering value to a number of early Fortune 100 customers.

AI Trends: Any anecdotes/stories?

Uppington: Here’s a good example from a recent case study: Standard Chartered is a leading international banking group, with a presence in 60 of the world’s most dynamic markets and serving clients in a further 85.

To see how Truera could help Standard Chartered ensure responsible AI, the teams worked with a number of credit decisioning models (and associated training data) built on multiple development platforms, including open source. The Truera Model Intelligence software helped data scientists and other stakeholders build trust in the quality of the model by explaining the model, assessing it for unfair bias, examining its stability over time, and surfacing other relevant intelligence about its inner workings.

This allowed Standard Chartered’s data scientists and business stakeholders to gain greater confidence in the quality of the model. Data scientists used the Truera platform to identify the most important features that drove individual credit decisions; e.g. identifying that Jane may have been denied credit because of her low income and high debt-to-income ratio. They could also understand how individual features contributed to the model’s assessment of risk; e.g. how increasing income affects the model’s risk scores. Crucially, they could assess that the model remained stable over time and that it was possible to identify and mitigate unjust bias.

Collectively, these capabilities provided all of the relevant Standard Chartered teams, including first line development, second line validation, third line audit/compliance and other stakeholders with the level of visibility into the inner workings of the model that they needed to build trust in the quality of the model and move it to production.

AI Trends: How is the company funded?

Uppington: Truera is a venture-funded startup. The company has raised over $17.3M over two rounds of funding led by Greylock and Wing VC with participation from other investors including Conversion Capital, Data Community Fund, B Capital Group via the firm’s Ascent Fund, and Harpoon Ventures.

Truera has some competition, according to an account in VentureBeat. Fiddler is a Mountain View, California-based startup developing an “explainable” engine that is designed to analyze, validate, and manage AI solutions. Also, Kyndi last June raised $20 million for an explainable AI product that analyzes documents.

Vishu Ramachandran, Group Head, Retail Banking at Standard Chartered, based in London stated that the effective use of data and analytics at the bank is a “key enabler” of its business strategy to better serve its customers. Ramachandran added that as the bank continues to scale its operations, it will continue to leverage AI and algorithmic decision-making, but plans to use these technologies in a “fair, transparent and responsible way.”

He stated, “We see Truera as an essential partner in how we do this and in how we build and operationalize higher quality, trusted AI models faster and more efficiently.”

Jerry Chen, a Partner at Greylock, stated, “Truera’s Model Intelligence platform and its AI.Q technology are fundamental breakthroughs in AI. Removing the black box problem of machine learning is essential to build effective and responsible ML applications.”

Learn more at Truera.","['models', 'ml', 'raising', 'data', 'standard', 'trends', 'model', 'truera', 'ai', 'scientists', 'solution', 'market', 'explainability', 'money', 'intelligence', 'startup']","The company offers a Model Intelligence Platform, using AI explainability to power it.
CEO Uppington recently responded via email to queries from AI Trends about the startup.
AI Trends: What business problem are you trying to solve?
The science behind what drives outputs of ML models is called AI Explainability.
AI Trends: How does your solution address the problem?"
201,https://www.aitrends.com/ai-insider/ai-autonomous-cars-might-not-know-they-were-in-a-car-crash/,AI Autonomous Cars Might Not Know They Were In A Car Crash,2020-12-29 14:19:18+00:00,"By Lance Eliot, the AI Trends Insider

I’d bet that most of us would agree that if you have ever been in a car crash, you know that you were in a car crash.

This seems perhaps absurdly obvious, but do not let your gut instincts lead you astray. In theory, you could have a fender bender and perhaps be oblivious to it, though this seems to raise eyebrows as to how distracted a driver you must be to not have felt the impact.

All in all, it is relatively sensible to assert that people would usually know when they’ve been in a car crash. I’ll clarify in a moment why that’s an important point, so hang onto that thought.

When referring to car crashes, for convenience’s sake herein, let’s agree that a car crash is widely defined and can encompass a variety of car-related adverse incidents, including outright head-on collisions, minor fender-benders, sideswipes, frightening rollovers, being rear-ended, destructive pile-ups, and even demonstrative dents and pings by striking objects or getting stricken by flying debris, etc.

I hesitate to admit that I’ve logged several of my own sad tales of car crash woes.

In one especially scary instance, a car rammed into me from behind, shoving my car into the vehicle ahead of me. I suppose this is an example of a twofer, namely two car crashes for the price of one. On another occasion, a car sideswiped me, while underway in my prized automobile, tearing off completely the side view mirror, along with creating lengthy scratches and dents along the entire side. I’ve had a golf ball skyrocket from out of nowhere (actually, while driving past a golf course), busting up my front windshield.

Perhaps the most unusual circumstance involved a pickup truck ahead of me on the freeway that opted to absentmindedly let a half-dozen cans full of paint flop off the back of the truck. In this situation and with no time to react and no viable options to veer away, I drove directly over those scattering cans of paint. They bounced up and down at 65 miles per hour under my car, damaging some of the underbody areas of the vehicle. Also, they cracked open and spilled white paint all over the underside and somewhat onto the bottom exteriors of my car.

Later on, some pals chided me for being upset about the incident and said there was no reason to cry over spilled (paint) milk. Of course, it is easier to laugh about it now, though at the time it decidedly did not seem to be a laughing matter.

Anyway, we all inevitably seem to have our tales of woe about car crashes.

Sometimes referred to formally as Motor Vehicle Accidents (MVA) or Motor Vehicle Collisions (MVC), we get into them quite a bit. According to statistics by the National Highway Traffic Safety Administration (NHTSA), there are approximately six million car crash incidents each year in the United States. Think of this as being about 16,000 such incidents per day, taking place somewhere perhaps near to you as you are driving around for your daily commute or heading to a grocery store.

Some detest calling these “accidents” since the explicit wording of considering something as an accident seems to imply that the collision or crash just somehow magically and mysteriously happened, accidentally so. It is seemingly a roll of the dice or bad karma. This phrasing is criticized as letting people off the hook for their responsibility while driving. An alternative is to refer to these as car incidents, though the problem with that wording is that an incident seems somewhat inconsequential, while at least the word “accident” has a semblance of something more harrowing.

Whether they are called car accidents or car incidents, automobiles have gradually been improved over the years to make them more formidable for handling various kinds of such clashes or collisions. There are impact-absorbing exterior panels on today’s cars. Special deformable front ends are pretty much standard these days. Bumpers are made to take a beating. And so on.

How do you know when you’ve been in a car crash or collision?

Generally, it is fair to say that anyone inside a car that encompasses a substantive car crash is going to likely feel it, hear it, see it, and possibly even smell it.

Yes, the sense of smell is included in the list, which might at first seem an odd inclusion. In the case of my getting bumped and shoved during my car crash, the blow from behind was so severe that it ruptured the gas tank of my car, instantly filling the vehicle with an overpowering and terrifying odor of gasoline. That still stands out in my mind about the incident, thus, the smell associated with a car crash can certainly count too.

It would seem though that “feeling” the incident is probably the more frequent sensation. We usually feel the car getting smashed or pushed, including that our bodies and limbs go flying back-and-forth upon the physics forces tugging at us. It could be a rocking sensation or a flinging motion that happens without our being able to directly control or stop it. Fortunately, seat belts are usually able to prevent our bodies from becoming unguided missiles within the vehicle. Sadly, some riders do not use their seatbelts and get flung fully out of the car and into added danger.

You probably hear crash-related sounds too. There is the crunch of metal upon metal. Perhaps the glass of the car windows crackles as it shatters. And so on. Things tend to happen so fast that the sounds do not necessarily register at the moment of impact, instead, you tend to remember and make sense of the sounds after the impact has occurred.

And then there is the eyeing of what happens. You can see yourself or others as they get shoved, and watch as the car crumples or breaks in various spots. Again, this oftentimes occurs so quickly that we seemingly do not register what we see as the incident plays out, and instead have a kind of mind’s eye afterward that recalls the incident.

As a driver, you likely saw that a car incident was imminent, sometimes tensing up and freezing at the wheel, while other times making rapid driving maneuvers to try and avert the crash. Passengers in your car might not have any forewarning as they were watching a video on their smartphone or idly looking out a side window and not watching the roadway traffic, getting caught utterly off-guard.

A car crash can end-up wrecking a car entirely or can be much less brutal and just bash the car but not destroy it. When my car windshield got hit by an errant golf ball, the vehicle itself was generally untouched, other than the windshield splintering and making things harder to see as a driver. In the incident of getting the rear-end hit and the consequent ram into the car ahead of me, the car was essentially totaled.

The resulting status of the car involved in a car crash can be one of these three major types:

Inoperable

Operable, but not safely so

Operable

You could somewhat classify the car driver in the same manner. Someone driving a car that gets into a car crash could become “inoperable” due to being harmed in the car incident, or they might be operable but are no longer considered a safe driver per se (perhaps due to the initial shock of the crash), or might be just fine and able to proceed to drive normally.

Now that we’ve laid out the core facets covering the onset of a car crash, let’s ponder a noteworthy viewpoint about the future of cars.

Here’s today’s intriguing question: Will an AI-based true self-driving car be able to discern that it has been in a car crash, and what does this foretell about the advent of self-driving cars?

Let’s unpack the matter and see.

For my framework about AI autonomous cars, see the link here: https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/

Why this is a moonshot effort, see my explanation here: https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/

For more about the levels as a type of Richter scale, see my discussion here: https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/

For the argument about bifurcating the levels, see my explanation here: https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/

Understanding The Levels Of Self-Driving Cars

As a clarification, true self-driving cars are ones that the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.

These driverless vehicles are considered a Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at a Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).

There is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.

Meanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).

Since semi-autonomous cars require a human driver, the adoption of those types of cars won’t be markedly different from driving conventional vehicles, so there’s not much new per se to cover about them on this topic (though, as you’ll see in a moment, the points next made are generally applicable).

For semi-autonomous cars, it is important that the public needs to be forewarned about a disturbing aspect that’s been arising lately, namely that despite those human drivers that keep posting videos of themselves falling asleep at the wheel of a Level 2 or Level 3 car, we all need to avoid being misled into believing that the driver can take away their attention from the driving task while driving a semi-autonomous car.

You are the responsible party for the driving actions of the vehicle, regardless of how much automation might be tossed into a Level 2 or Level 3.

For why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here: https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/

To be wary of fake news about self-driving cars, see my tips here: https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/

The ethical implications of AI driving systems are significant, see my indication here: https://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/

Be aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms: https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/

Self-Driving Cars And Car Crashes

For Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task. All occupants will be passengers. The AI is doing the driving.

Realize that the AI is not a robot sitting in the driver’s seat of the car. Though there are various research efforts underway to create a walking-talking and driving robot, though that is generally not how we will witness the emergence of self-driving cars. The AI system is under-the-hood, as it were, running on computer processors that might be hidden inside the body of the vehicle or placed in the trunk, etc.

In short, you will not see the driver of the self-driving car.

The reason this is worthwhile to point out is that unlike a human driver that can “feel” a car crash, there isn’t a robotic body that sits in the vehicle and equally ascertains the sensations that arise when a crash occurs (though, via the IMU and the potential addition of other special tactile related sensors, this might be possible to detect).

Nor smells of the car crash (well, just a heads-up, some are adding e-nose features into their self-driving cars, see my coverage on the topic).

We’ll get in a moment to seeing or hearing the crash.

In theory, a self-driving car could get sideswiped by another car, for example, and the AI might be oblivious that this kind of car collision has even occurred.

Upping the ante, it is conceivable that a self-driving car might strike a small animal head-on, and not have detected the collision. There is a chance, too, of rolling over something or someone (heaven forbid, say a child laying on the street), and not detecting this has happened.

To be clear about this, there are ways for the AI to detect a car crash, but of course, that requires that the AI developers crafting the self-driving car have made sure to include the needed features and functions to do so. Depending upon what sensors are used on the self-driving car, and how the AI has been crafted, there can inexorably be loopholes or blind spots related to being able to discern a car crash incident that involved the self-driving car.

As they say, your mileage may vary, meaning that the nature of the self-driving car, the AI developed for it, the sensors used, and other such factors all determine how well or how poorly a particular brand or make of a self-driving car can do at ascertaining that a car crash has taken place.

Some self-driving cars are being devised to extensively figure this out, while others are less rigorously pursuing this angle, at this time, and the capability of gauging all possible car crashes is labeled as an edge or corner case. Generally, the automakers and self-driving car tech firms are having to prioritize what they are building, such that the foundational aspects like getting safely from point A to point B are needed sooner than other “extraordinary” or long-tail elements (which are classified as at the edges or far corners of what is needed right away).

Next, let’s take a closer look at the detection facets.

An obvious way for the AI to realize that a car crash has occurred would be via the deployment of airbags. Typically, there are sensors at the exterior of the car that are activated when a crash happens. Those sensors then notify the airbags to deploy. Since these are capabilities usually already built into a conventional car, the AI-based self-driving mechanisms can tap into those same systems and utilize those sensors as an indicator of a car crash.

There is a small chance that an airbag will deploy when it is not supposed to do so, such as when smacking against the curb or perhaps when driving across an onerous pothole. Even if the airbag deployment and the sensors are somewhat incorrect, it nonetheless is sensible for the AI to assume that something has gone awry. All told, as a human passenger, imagine your chagrin if the airbag deployed, and meanwhile, the AI kept driving along without trying to safely come to a stop. It would be quite troubling, for sure.

Recall that earlier I had mentioned that a human driver will frequently spot an impending car crash.

Presumably, the AI can do likewise. The video cameras that are streaming the street scene are being examined by the AI driving system to ascertain what is taking place outside the vehicle. Based on the interpretation of detected objects, a virtual world model is usually being kept up-to-date in real-time. Overall, the AI would be calculating the chances of a car crash. Not only is visual data being used, but by-and-large self-driving cars also use additional sensors such as radar, LIDAR, ultrasonic, thermal imaging, etc.

Some pundits insist a self-driving car will never get into a car crash.

This is hogwash.

Undeniably, self-driving cars will get into various kinds of car crashes and collisions. It will happen. The good news is that hopefully the odds of those occurrences will decrease, significantly, and we’ll end-up reducing substantively the 40,000 car crash fatalities nationally each year and the 2.5 million related injuries. Despite outstretched claims of reaching zero fatalities, there is a zero chance of that happening and instead, the number of incidents will assuredly be non-zero. See my columns for more background about why this will be a non-zero matter.

Via the array of sensors and the programming of the AI, encompassing Machine Learning (ML) and Deep Learning (DL), many car crashes might indeed be detected. Furthermore, if the vehicle has been damaged such that it can no longer steer properly, for example, this is something that the AI is programmed to try and discern.

One somewhat vexing question is whether the AI should try to warn passengers when a suspected car crash is about to occur. Via the Natural Language Processing (NLP) capability, the AI could tell the occupants to brace themselves. This might be useful and prepare the passengers for a bruising, or it might unduly get the passengers agitated and cause them to take adverse actions that will cause even greater injury or harm to themselves than if they did not realize a crash was looming.

Here’s another twist related to the role of passengers.

Suppose a passenger is watching traffic and believes that a car crash involving their self-driving car is going to occur. It seems likely that a passenger in that setting might scream at the AI and urge the driving system to take evasive action.

Should the AI take into account what a passenger reports about a potential car crash?

We would expect a human driver to be listening. At the same time, we would also expect that a human driver would make a judgment about whether the passenger was right or wrong in their assessment. If a toddler in the vehicle makes such a claim, an adult driver might give the remark a lesser weighting than if it was a fellow adult.

For some AI driving systems, it makes no difference what a passenger says about an impending car crash and there is no provision for receiving or analyzing any such advice offered by a passenger (this is, again, considered an edge or corner case). The viewpoint is that right now, the AI is the driver, the sole driver, and no kind of so-called backseat driving is being sought, encouraged, or included.

In terms of hearing a car crash, the odds are that self-driving cars will have microphones inside the vehicle, used to aid in undertaking the NLP aspects. Thus, yes, it is feasible that the AI could “hear” during a car crash. Some self-driving cars are adding microphones on the exterior of the car too, allowing for hearing street sounds, perhaps the sound of a police car or ambulance siren, and therefore could play a role in detecting a pending car accident (potentially picking up the sound of screeching tires, that kind of thing).

Would a self-driving car realize that a golf ball has landed on the windshield and made a large crack?

Maybe yes, maybe no.

It is hard to say whether the sensors would have detected the ball while in the air. Once the golf ball struck the windshield, there might not be any sensors that would alarm at this, since the cameras used by the AI are not particularly affected by the visibility associated with a windshield. The vehicle would still be fully drivable and thus no indication would arise from how the vehicle controls were responding to the AI.

Would a self-driving car realize that the vehicle was sideswiped by another car?

Assuming that none of the sensors mounted in the side of the car were damaged, this again is a questionable incident in terms of being detected.

What about the cans of paint that fell off the truck?

Well, this one brings up some important points. Yes, the odds are that the sensors would have detected the paint cans. It seems a lot less likely that the AI would “sense” the paint cans bouncing off the underbody of the car. The rattling and ping-ponging are maybe detected via the interior microphones, but that’s a stretch for most of today’s AI self-driving systems. And, let’s assume that the car driving controls are not damaged, ergo that’s not a telltale clue either.

The interesting point here is that if the AI determines that a car crash or collision of some kind was likely imminent, and yet if there does not seem to be any demonstrative result or outcome, what should the AI do?

You could argue that the AI can just continue driving along.

If the passenger speaks up and complains or raises an issue, perhaps the AI would then come to a safe stop, or the passenger might use an in-car OnStar-like feature to make contact with the fleet operator to seek assistance.

On the other hand, you could also assert that if a car crash or incident was expected, and yet no apparent damage detected, nonetheless the AI ought to come to a safe stop, doing so out of an abundance of caution.

For more details about ODDs, see my indication at this link here: https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/

On the topic of off-road self-driving cars, here’s my details elicitation: https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/

I’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop: https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/

Expect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here: https://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/

Conclusion

A human driver would seemingly know when their car has been involved in a car crash or collision. Also, they can stop the car, get out, walk around the vehicle, and upon inspection ascertain what if any damage might have been encountered. In some cases, you don’t even need to get out of the car and can lean over to take a look or otherwise do the inspection while still inside the vehicle.

Currently, few self-driving cars are being outfitted with cameras that look at the self-driving car itself. In other words, the cameras are aimed outward, rightfully so, seeking to detect what is around or coming at the vehicle.

Do we need cameras that can look at the self-driving car, doing so to aid in ascertaining and assessing the status of the vehicle?

One approach being considered is to use a drone that is associated with a self-driving car. The drone might be used for a variety of purposes, such as bringing an item to the car, while the vehicle is in transit, or taking something to someone else. This drone capability could also be used to get a bird’s eye view of the self-driving car.

All told, I am abundantly hoping that I will not end-up with any additional stories about being in a car crash, regardless of whether I’m doing the driving, or one day while riding routinely inside ubiquitous self-driving cars.

Copyright 2020 Dr. Lance Eliot. This content is originally posted on AI Trends.

[Ed. Note: For reader’s interested in Dr. Eliot’s ongoing business analyses about the advent of self-driving cars, see his online Forbes column: https://forbes.com/sites/lanceeliot/]

http://ai-selfdriving-cars.libsyn.com/website","['autonomous', 'driver', 'vehicle', 'level', 'ai', 'driving', 'cars', 'car', 'selfdriving', 'sensors', 'crash', 'know']","By Lance Eliot, the AI Trends InsiderI’d bet that most of us would agree that if you have ever been in a car crash, you know that you were in a car crash.
All in all, it is relatively sensible to assert that people would usually know when they’ve been in a car crash.
A car crash can end-up wrecking a car entirely or can be much less brutal and just bash the car but not destroy it.
Some pundits insist a self-driving car will never get into a car crash.
Suppose a passenger is watching traffic and believes that a car crash involving their self-driving car is going to occur."
202,https://www.aitrends.com/healthcare/how-ai-is-helping-with-covid-19-vaccine-rollout-and-tracking/,How AI is Helping with COVID-19 Vaccine Rollout and Tracking,2020-12-17 20:26:04+00:00,"By John P. Desmond, AI Trends Editor

AI has been employed since the early days of the COVID-19 pandemic to track the spread of positive cases, to crunch through thousands of scientific papers to search for treatment options and to help develop a vaccine. Now AI and other digital tools are being deployed to manage complex supply chains for the vaccine.

With the third highest number of coronavirus cases in Europe after France and Italy, according to data from Johns Hopkins University, the UK is the first country to distribute the Pfizer vaccine. The UK has 1.7 million confirmed cases in a population of close to 68 million people, Tracking side effects from the vaccine rollout is a huge task, UK health officials have said, according to an account from Nasdaq. To help meet the challenge, the UK Medicines & Healthcare products Regulatory Agency (MHRA) recently partnered with the UK unit of Genpact, the global professional services firm specializing in digital transformation. The company is integrating components of its AI software suite with the government’s website where adverse effects are reported.

“When a vaccine gets distributed at scale and speed, a technology solution needs to track the batch and lot numbers to know exactly where each dose is and who received it,” stated Eric Sandor, drug safety AI lead at Genpact. “There’s a lot of information, in a number of different formats, and it’s very manually intensive to try to codify it in a way that makes sense. AI will help with processing all that data faster than humans can. It’s quite complicated at scale, but is a critical element to overall public health.”

With the Genpact AI, the UK government will be able to track events by batch, lot, and location so that any adverse effects can be reported back to the drug manufacturers. The technology will also track issues or trends related to ethnicity, age, gender, or other demographic factors that can come into play with the vaccine. “Humans are good at balancing about seven different dimensions of data before we sort of run out of road,” stated Sandor. “AI can handle thousands of dimensions of data and find patterns and signals in the data very rapidly, something that would take humans much longer to find.”

Pharmaceutical companies are also just beginning to explore the role of AI in supply chain management, another challenge of the vaccine rollout. The procurement, delivery logistics, tracing and storage all affect the availability of the vaccine and are potential risk factors for private companies. Genpact has been preparing. “We’re running AI applications for clients that model the distribution of the vaccine and how that’s going to work not only for pharmaceuticals, but for other companies in the life sciences space,” stated Katie Stein, chief strategy officer for Genpact.

RPA Bot Well-Positioned to Now Help Track Flu Vaccinations in the UK

An AI tool that had been deployed to help track flu vaccinations in the UK, is now well-positioned to also track the COVID-19 vaccine rollout.

Newcastle Upon Tyne Hospitals NHS Foundation Trust is offering the flu vaccine to its more than 14,000 employees, but previous reporting processes have been time-intensive and done manually. The Trust entered into a collaboration with Automation Anywhere, supplier of Robotics Process Automation (RPA) tools, to launch a flu-reporting bot.

The need for reporting on the number of hospital staff being vaccinated and to track flu-related absences has intensified over the winter months. The flu-reporting bot has captured updates for more than 10,000 staff vaccinations across multiple clinical sites so far, saving an estimated 2,000 hours of admin time, according to a press release from Automation Anywhere.

The NHS is now reviewing plans to extend the technology to report on COVID-19 vaccinations, recently approved to be administered in the UK.

“Right from the start, intelligent automation has helped us enhance staff engagement, free up valuable resources and has had a positive impact on the work we do for our staff and patients,” stated Neil Picton, Head of Workforce Engagement and Information, Newcastle Upon Tyne Hospitals NHS Foundation Trust. “The flu-reporting bot has helped to streamline a heavy administrative load for our Occupational Health team. It has also helped improve data accuracy and ensured valuable time is not diverted from supporting staff and patient care.”

Prince Kohli, CTO of Automation Anywhere, stated, “Healthcare facilities have been under tremendous pressure this year, and intelligent automation is helping to refocus precious time back to where it’s most needed—on the front line, centered on patient care.”

The use of AI to confront the COVID-19 challenge is likely to have a wide-reaching impact in many areas of science, suggested a recent account in The Hill.

“Necessity is the mother of invention, and Artificial Intelligence (AI) has been a tremendous enabling force in the fight against COVID,” stated author Evan Sparks, founder and CEO of Determined AI. “Like never before, private companies, non-profits, and government agencies have come together to point GPUs at speeding scientific research and work toward a cure. Early in the pandemic, we saw tech giants including Microsoft and Google partner with government agencies and nonprofits such as the White House, NIH, The CDC, and the Allen Institute for AI. Through collaboration, the best minds and best technology worked together to begin to solve the biological puzzle of COVID-19.”

This collaboration is likely to have downstream effects that boost the role of AI in the economy, Evans suggested. This can include increased AI investments in the government, into smart cities research and into healthcare research.

AI Seen as Having Potential to Help Lower Income Countries Obtain Vaccine

AI also has the potential to help the COVID-19 vaccine reach the 80% of the world’s population that currently live in low- and middle-income countries (LMICs), some two billion people, suggested a recent account in devex.

“The advent of artificial intelligence as an adaptive and predictive technology offers the possibility for radical optimization of business practices, reshaping market opportunities for pharmaceutical companies and ultimately challenging the status quo on access to medicine worldwide,” stated authors Jayasree Iyer, executive director of the Access to Medicine Foundation, and Thomas Collin-Lefebvre, researcher at the foundation.

Pharmaceutical companies are only starting to explore AI fields of application. More than 50% of AI professionals work for the tech sector, while only three percent currently work for health organizations and one percent for nonprofit organizations, according to data released by Microsoft. This disproportion hampers the speed and extent of the technology’s adoption within the sector, the authors state.

“COVID-19 is a litmus test for whether AI can help in the crisis response,” the authors state. Ways AI can help include in demand forecasting, reporting substandard medicines and ensuring a continuing supply.

Read the source articles from Nasdaq, a press release from Automation Anywhere, in The Hill and in devex.","['automation', 'helping', 'data', 'vaccine', 'stated', 'ai', 'covid19', 'track', 'help', 'companies', 'uk', 'rollout', 'work', 'tracking']","Now AI and other digital tools are being deployed to manage complex supply chains for the vaccine.
The company is integrating components of its AI software suite with the government’s website where adverse effects are reported.
RPA Bot Well-Positioned to Now Help Track Flu Vaccinations in the UKAn AI tool that had been deployed to help track flu vaccinations in the UK, is now well-positioned to also track the COVID-19 vaccine rollout.
“COVID-19 is a litmus test for whether AI can help in the crisis response,” the authors state.
Ways AI can help include in demand forecasting, reporting substandard medicines and ensuring a continuing supply."
203,https://www.aitrends.com/ai-in-marine-industry/first-harvest-selects-sea-machines-to-launch-autonomous-hybrid-cargo-vessel/,First Harvest Selects Sea Machines to Launch Autonomous Hybrid Cargo Vessel,2020-12-17 19:42:48+00:00,"By AI Trends Staff

First Harvest Navigation, a marine transportation company that connects family farms to urban and suburban neighborhoods, has selected technology from Sea Machines of Boston to launch a hybrid cargo vessel, believed to be the first in the US.

The Sea Machines autonomous command and remote helm control system, called the SM300, will be installed on the Captain Ben Moore, which delivers food and other cargo between Norwalk, Conn. and Huntington, NY. By truck, the trip to deliver this food to the Harbor Harvest food market takes nine hours. The Captain Ben Moore can complete the terminal-to-terminal voyage across Long Island Sound in 35 to 45 minutes, reducing highway congestion.

“Part of our transportation goals are to develop autonomous, hybrid catamarans to move farm products across Long Island Sound. The Sea Machines SM300 autonomous navigation system will help us achieve many of our goals because it enables shipping movements to be completed very reliably and efficiently in a seamless and sustainable delivery system,” stated Bob Kunkel, president, First Harvest Navigation, in a press release from Sea Machines. “Shifting cargo from streets and highways also alleviates the growing congestion, lower emissions, and reestablishes our waterways as a viable and cost-efficient alternative to land-based transport.”

Michael Johnson, founder and CEO of Sea Machines, stated, “Sea Machines and First Harvest Navigation are aligned in our commitments to innovation to bolster the U.S. marine highway system and in our support of family farms. It often takes determined entrepreneurial leaders like First Harvest Navigation to move an industry into new waters.”

The hybrid vessel can carry approximately 28 pallets, 10 of which are positioned in a fully refrigerated and protected walk-in space. The remaining cargo spaces are open and covered according to customer requirements. It is powered by a pair of Cummins QSB 6.7 diesels, generating 104 kW each at 2,400 kW, and lithium batteries connected to a pair of BAE Systems HybriDrive electric motors.

First Harvest Navigation and Harbor Harvest are providing new regional transportation, warehousing and retail marketing for family farms, local business and artisan products within New York and New England areas.

Sea Machines SM300 Currently at Level 3-4 Autonomy

In a webcast on the announcement, the current autonomy level of the Sea Machines SM300 is between a level three and four on a six-point scale, with six being unsupervised operation, said Phil Bourque, director of business development for Sea Machines.

“Actions are performed with human intervention. It is a human in the loop system. It’s a lot of work to get this to the later stages,” he said, adding, “Sea Machines is essentially a software company. We have developed hardware to get that software into the marine environment on boats.”

The SM300, a 20inx20inx8in box, gets installed on the boat and is integrated with the steering, communication, compass, radar, camera and propulsion systems. For the project with First Harvest, the goal is to complement the crew. “We are not taking people off the boat. We are adding a layer of technology that enhances the overall ability to conduct the operation safely,” Bourque said.

Sea Machines was founded in Texas, then moved to Boston “largely because of the great autonomy talent pool in Boston,” Bourque said. The company has a test fleet in Boston Harbor that operates “every day, rain, shine or snow,” he said.

The Captain Ben Moore is a hybrid electric vessel with lithium-ion batteries that operates on many of the same protocols Sea Machines is using, said Micah Tucker, technician working with First Harvest. The diesel generators on board are used as “range extenders,” to recharge the batteries as needed. “We have lots of commercial traffic in the sound at all times, so the SM300 will be an alternative second or third system for controlling the vessels. It allows us to operate from our command stations and also semi-autonomously from the boat itself.”

Kunkel said the term “autonomy” took a “wrong turn” when first applied to the marine industry, “because people talked about removing the crew. We are looking at enhancing the crew’s capability.” He noted the original mission was to reduce truck traffic.

The crew is now getting operating experience, loading and discharging cargo at the two terminals. They hope to gain experience with the collision avoidance systems when traffic picks up in late spring and early summer. Sea Machines is currently testing the capability. First Navigation is working with the Coast Guard to develop needed regulations, none of which exist at present. Most of First Navigation’s crew are former military or shipyard workers, Kunkel noted.

Asked if the SM300 is capable of controlling multiple boats in a swarm, Bourque said the capability is in the software development plan.

Sea Machines Testing Higher Autonomy System on Ship in Denmark

Asked after the session what needs to happen for the SM300 to get from a level 3 and 4 autonomy, to levels 5 and 6, Bourque replied in an email: “It is essential we develop operator equivalency of ML/AI systems and the data collected and processed, before we get to level 5/6.”

“As a first step, Sea Machines built the SM400 AI-powered perception and situational awareness system, which is currently in development and being trialed aboard an A.P. Moller-Maersk’s vessel, VISTULA MAERSK, an ice-class container ship in Denmark.

The project has been significant not only to Sea Machines and Maersk, but also to the larger maritime industry as the installation marked the first time that computer vision, LiDAR and perception software have been utilized aboard a container vessel to augment and upgrade transit operations.”

The system is expected to become commercially available to maritime operators and naval architecture and marine engineering firms next year. The system will fall into level 5, which is a “rarely supervised” operation mode called “full automation.”

The system will provide mariners aboard with a full picture of the ship’s surrounding domain, traffic and obstacles, using data from conventional marine sensors (like radar and AIS) fused with new technologies, such as real-time image recognition for vessel detection and tracking and Light Detection and Ranging (LiDAR).

“The main advantages of advanced perception and situational awareness technologies is the reduced risk of uncontrolled incidents, accidents and delays that impact cargo schedules and reduce operators’ bottom lines,” Bourque said. “These incidents are traditionally caused by limitations in conventional shipboard instruments and the perception limitations of human operators.”

Read the press release from Sea Machines.","['machines', 'autonomous', 'sm300', 'sea', 'hybrid', 'launch', 'harvest', 'level', 'selects', 'cargo', 'vessel', 'system', 'navigation', 'marine']","First Harvest Navigation and Harbor Harvest are providing new regional transportation, warehousing and retail marketing for family farms, local business and artisan products within New York and New England areas.
Sea Machines SM300 Currently at Level 3-4 AutonomyIn a webcast on the announcement, the current autonomy level of the Sea Machines SM300 is between a level three and four on a six-point scale, with six being unsupervised operation, said Phil Bourque, director of business development for Sea Machines.
It’s a lot of work to get this to the later stages,” he said, adding, “Sea Machines is essentially a software company.
Sea Machines was founded in Texas, then moved to Boston “largely because of the great autonomy talent pool in Boston,” Bourque said.
“These incidents are traditionally caused by limitations in conventional shipboard instruments and the perception limitations of human operators.”Read the press release from Sea Machines."
204,https://www.aitrends.com/workforce/itserve-alliance-wins-court-ruling-on-h-1b-visa-prevailing-wages/,ITServe Alliance Wins Court Ruling on H-1B Visa Prevailing Wages,2020-12-17 19:34:27+00:00,"By AI Trends Staff

In a dispute with the US Department of Labor over how much to pay H-1B visa holders, the ITServe Alliance prevailed in a Dec. 3 court ruling that had the effect of resetting the rules to what they had been before the new Trump Administration policy went into effect.

The ITServe Alliance is an association of IT solutions and services organizations, representing over 1,400 member companies. Founded in 2010, the Alliance has 16 regional chapters in the US. The H-1B is a temporary visa category that allows employers to petition for highly educated foreign professionals to work in specialty occupations that require at least a bachelor’s degree or equivalent.

The Department of Labor had issued a new rule on Oct. 6, resetting the prevailing wage required to hire H-1B employees, making them much higher. Before the new rule, for example, a software engineer in Los Angeles who had a prevailing wage of $85,550, would have to be paid $116,418, a 36% increase, according to an account in Forbes. This came with two days notice.

The Department of Homeland Security at the same time issued new rules around H-1B visa renewals. This was four weeks before the US presidential election on Nov. 3.

Immediately following the win by ITServe in court, the DOL rescinded the new rules as of Dec. 4, 2020.

“Everybody won,” said ITServe Alliance President Amar Varada, in comments to AI Trends. The effect of the new rules would have been to drive the jobs offshore, where the tech talent could be hired for less. While the workers in the US would have made more money, they were uncomfortable that DHS was reviewing the terms of the visas. “They were nervous about the DHS regulations; they knew it would not be good for anyone,” Varada said. “They were stressed about whether they would have to leave the country.”

IT Sector Has More Jobs Open than Qualified Workers to Fill Them

IT unemployment before the onset of the pandemic was two to three percent, according to Kishore Khandavalli, advisory director for ITServe, noting that the pretense for the rule changes was to protect American jobs. “More than a million jobs are open in the IT sector,” he said. “We always have more jobs than people available to fill them.”

With the new rules in place, the options for the service providers were to not comply or go elsewhere to hire the needed tech talent, namely from India, China, or Eastern Europe. “That’s why we had to take action,” he said, by filing for the injunction to halt the new rules from going into effect.

Skills in the IT sector need to be frequently renewed. “Unless you update your skills every three or four years, your skills get outdated,” Khandavalli said. Cobol programmers are not in much demand in the US, for example. “But a software engineer with AI skills with a couple of years’ experience, can command $150,000,” he said. In response to questions he gets from college students as to what technical field to pursue, he said, “Look up what is going to be in demand for the next three to five years: data, AI, and the cloud. That’s the future. Everything is going to the cloud; AI is driving it; and AI is based on data.”

H-1B visas are currently capped at 65,000, and 20,000 in the master’s cap category. During the Clinton presidency, no limits were set on H-1B visas, then it dropped to 185,000.

“We don’t have any opinions about it,” Khandavalli said. “As business owners, we just don’t want to be surprised. We comply with the rules. Just tell us what the rules are and don’t change them overnight.”

Policy Aimed at Protecting US Jobs has Unintended Consequences

The move by the Department of Labor to ostensibly protect US jobs, had the effect of pricing many H-1B professionals out of the market in the US, driving them overseas. “Policies that are motivated by concerns about the loss of native jobs should consider that policies aimed at reducing immigration have the unintended consequence of encouraging firms to offshore jobs abroad,” stated Britta Glennon, an assistant professor at the Wharton School of Business who has researched the market, to Forbes.

An H-1B visa is typically the only practical way for a high-skilled foreign national, including an international student, to work long-term in the United States. The denial rate for H-1B visas stood at six percent halfway through President Obama’s second term, according to the National Foundation for American Policy. Since President Trump took office, the rejection rate increased steadily, reaching 24% in FY 2018, and 30% by FY 2020.

Companies that provide business services, such as ITServe Alliance members, had far higher H-1B denial rates under Trump administration policies.

Read the source articles and information from the ITServe Alliance blog, in Forbes and at the National Foundation for American Policy.","['itserve', 'ruling', 'alliance', 'visas', 'prevailing', 'jobs', 'court', 'ai', 'department', 'visa', 'wages', 'wins', 'h1b', 'rules', 'trump']","The ITServe Alliance is an association of IT solutions and services organizations, representing over 1,400 member companies.
An H-1B visa is typically the only practical way for a high-skilled foreign national, including an international student, to work long-term in the United States.
The denial rate for H-1B visas stood at six percent halfway through President Obama’s second term, according to the National Foundation for American Policy.
Companies that provide business services, such as ITServe Alliance members, had far higher H-1B denial rates under Trump administration policies.
Read the source articles and information from the ITServe Alliance blog, in Forbes and at the National Foundation for American Policy."
205,https://www.aitrends.com/ai-insider/ai-autonomous-cars-to-ultimately-bolster-the-christmas-holidays/,AI Autonomous Cars To Ultimately Bolster The Christmas Holidays,2020-12-17 19:23:46+00:00,"By Lance Eliot, the AI Trends Insider

This year seems to have been rather bleak and had an untold number of adverse tidings, but if you don’t mind, I’d like to end the year on something a bit more upbeat.

I dare say that the Christmas season is upon us and there is a moment of joy in the air. Homes are festooned with ornaments and sparkling lights. Songs and the music of the season can be heard on our radios and online. One way or another, we strive to persevere and seek to ensure that there is something heartwarming to cling to this holiday.

Okay, with that rather spirited opening, I don’t want to spoil the mood, yet there is an aspect that still seems to be annoying and exasperating, namely the abundance of unruly traffic on our highways and byways.

It seems nearly unimaginable to discover that car drivers are just as rude as always. How can that be? Wouldn’t it seem logical and sensible for drivers this year to be especially civil and polite to each other? Apparently, not so.

Frustrations abound. Driving to do simple errands can be a chore. I’ve witnessed horn-honking traffic jams. There have been drivers that cut off other drivers. Much of the time, you find yourself shaking your head in sadness and disgust that the holiday spirit seems to not enter people’s minds when they are driving their cars, and instead they appear to be overcome by a selfish get-out-of-my-way take-no-prisoners attitude.

Last year, according to national statistics, approximately 100 million Americans traveled 50 miles or more from home during the Christmas holiday period, and nearly 90% of that travel was done on the roadways (versus flying or say taking a train). Presumably, the amount of seasonal travel will drop significantly this year, though we’ll need to see the final tally once the year is over.

All told, there is way too much frustration and angst involved in the roadway traveling during the holidays, and it’s a darned shame that there’s seemingly nothing that can be done to avert the anguish.

Wait a minute, maybe Santa has something for us that can help.

Here’s a question to ponder: Will the advent of AI-based autonomous self-driving cars provide some relief from the holiday angst and aid in making the season as wonderful as it ought to be?

I say yes. I exclaim to the rooftops, yes! Let’s unpack the matter (and make sure to put a bow on it too).

For my framework about AI autonomous cars, see the link here: https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/

Why this is a moonshot effort, see my explanation here: https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/

For more about the levels as a type of Richter scale, see my discussion here: https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/

For the argument about bifurcating the levels, see my explanation here: https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/

Understanding The Levels Of Self-Driving Cars

It is important to clarify what I mean when referring to true self-driving cars. True self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.

These driverless vehicles are considered a Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at a Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).

There is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.

Meanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our roads, some point out).

Since semi-autonomous cars require a human driver, the adoption of those types of cars won’t be markedly different from driving conventional vehicles, so I’m not going to include them in this discussion about the holidays.

For semi-autonomous cars, it is equally important that I mention a disturbing aspect that’s been arising, namely that despite those human drivers that keep posting videos of themselves falling asleep at the wheel of a Level 2 or Level 3 car, we all need to avoid being misled into believing that the driver can take away their attention from the driving task while driving a semi-autonomous car.

You are the responsible party for the driving actions of the vehicle, regardless of how much automation might be tossed into a Level 2 or Level 3.

For why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here: https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/

To be wary of fake news about self-driving cars, see my tips here: https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/

The ethical implications of AI driving systems are significant, see my indication here: https://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/

Be aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms: https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/

Self-Driving Cars And The Holidays

For Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task. All occupants will be passengers. When you go to a mall, you won’t be driving, and instead, the AI will do the driving for you.

Guess what? This means that you no longer need to be the one that bears the frustration and angst of being at the wheel.

There you are, riding along in a true self-driving car, and letting your mind wander to dreams of sugarplums dancing and not needing to be aware that some idiot driver ahead of you is cutting off your car or going as slow as a snail.

Let the AI worry about it.

Furthermore, you can be watching videos or live-streaming video while going over to the store, doing so by the likely addition of LED displays mounted inside the driverless car. The odds are that self-driving cars will have high-def displays and be connected to the Internet at top speeds such as 5G.

You can do a live connection with a loved one and via a Facetime-like interaction be able to discuss what gifts to get for friends and family.

Maybe have some eggnog during the drive, perhaps spiked (which you would never do as a driver), though please don’t let things get out of hand (it would be unseemly to pour out of a driverless car and be as drunk as a skunk).

In terms of gift getting, we’ve already begun to see a large shift from going to brick-and-mortar stores to instead ordering online and having your packages delivered to your home. With true self-driving cars, most pundits predict that we will dramatically increase the amount of home-delivered items since driverless cars will be able to drive those purchased packages to your house.

Family vacations will be easier to undertake too. You and the family can enjoy the time together during a cross-country road trip. Rather than the adults having to constantly trade-off doing the driving task, the AI will be doing the driving. This allows the adults to have fun with the kids while inside the driverless car, playing games and otherwise devoting attention that would have been going toward the driving chore.

Speaking of kids, another facet of driverless cars will be that children can get around to places without requiring an adult driver to be present and being there to drive the car. Suppose you want the kids to get over to grandma’s house and you aren’t yet home to drive them. By using a self-driving car, the kids can pile into the vehicle and be driven by the AI, allowing you to get over to grandma’s once your Scrooge boss lets you out of the office.

As an aside, there is some controversy about letting kids ride in driverless cars without having any adult supervision, and it is hard to imagine such a future, but there is a bona fide case to be made that we culturally might eventually change our views on this matter and find this to be a valid form of transport for non-adults.

Think about the other possibilities of how driverless cars can alleviate the stress and strain of the holidays. A self-driving car can drop you at your destination and thus there’s no need for you to deal with parking the car. Toss out those crazy parking lot fisticuff moments since you won’t ever need to be in a parking lot to begin with, and toss out the annoying act of driving round and round to find a parking spot.

Some pundits are saying that we’ll no longer have traffic congestion once we have self-driving cars, but that’s a rather Utopian viewpoint. For many years to come, likely for decades, we are going to have a mixture of both conventional cars and self-driving cars on our roadways (there are about 250 million conventional cars today in the United States).

All in all, we are going to continue to have traffic congestion for a long time to come.

Yet, despite the traffic congestion, when you are inside a driverless car you might not especially notice that the traffic is backed-up. If you are watching a classic movie while inside a driverless car, such as It’s A Wonderful Life, you probably won’t care that a morass of cars is all backed-up and crawling along.

Another nifty aspect will be that people today that are mobility marginalized or disadvantaged will likely have greater mobility access due to the emergence of self-driving cars. Maybe your elderly father is not able to drive a car and lives far away from the rest of the family. It might be logistically difficult for you to go pick him up and drive him to a family holiday get together.

On the other hand, he could use a driverless car and show-up ready to enjoy some cherished time with you all. That’s truly a wonderful life moment.

For more details about ODDs, see my indication at this link here: https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/

On the topic of off-road self-driving cars, here’s my details elicitation: https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/

I’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop: https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/

Expect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here: https://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/

More Reasons For Holiday Cheer

I’m not saying that AI-based autonomous cars will erase or eradicate all the stress of the holidays.

No doubt, there will still be lots of holiday stress to be had. At least you can have some contemplative meditation time while inside a driverless car. Or, better still, use the time inside the self-driving car to catch some sleep. It is expected that most driverless cars will have reclining seats so that you can take a nap or go to sleep while on a driving journey. After a day’s hard work, you can grab a nap on the way home, and feel refreshed when you walk in the door, greeting the rest of the family rather than snarling at them.

Something else is worth considering too about self-driving cars.

Many are hoping and expecting that driverless cars will save lives, meaning that the number of lives lost by car crashes and car injuries will be substantially reduced. Currently, there are about 40,000 annual car-related deaths and approximately 2.7 million car-related injuries in the United States. Take a somber moment to reflect on the fact that 40,000 people each year in the U.S. won’t be celebrating the holidays with their loved ones due to being killed in a car crash.

And if that number doesn’t seem disheartening enough, consider that over a decade or so of such losses amounts to 400,000 or more people killed in car crashes in America, or nearly a half million people consisting of beloved fathers, mothers, and children that won’t be able to see the holidays. Via self-driving cars, presumably those deaths and injuries will be a lot less, since the AI won’t be prone to drinking and driving, and won’t be distracted using a smartphone, etc.

As you can see, self-driving cars bode well for making the holidays a time for family and friends to come together and ease the burden of driving, along with making it feasible for people to avert many of the unfortunate adverse consequences of car driving.

Conclusion

In case you are reading this to your children as a bedtime story akin to ‘Twas the Night Before Christmas, and they are perhaps worried that maybe Santa is going to ditch his reindeer and use a self-driving sleigh, I assure you that Santa has fully committed to keeping those reindeer.

Yes, St. Nick is going to keep on exclaiming to Dasher, Dancer, Prancer, Vixen, Comet, Cupid, Donner, and Blitzen that they need to dash away, dash away all.

For the particularly smarmy kids, they might crack a wee smile and whisper that the reindeer are all robots and AI-based, but don’t let them get away with this, and tell them that the reindeer are real and the prancing and pawing of each little hoof is genuine.

And that’s the merry and rosy truth on the matter!

Copyright 2020 Dr. Lance Eliot This content is originally posted on AI Trends.

[Ed. Note: For reader’s interested in Dr. Eliot’s ongoing business analyses about the advent of self-driving cars, see his online Forbes column: https://forbes.com/sites/lanceeliot/]

http://ai-selfdriving-cars.libsyn.com/website","['autonomous', 'going', 'holidays', 'driver', 'ultimately', 'level', 'bolster', 'ai', 'driving', 'cars', 'car', 'selfdriving', 'wont', 'driverless', 'christmas']","The odds are that self-driving cars will have high-def displays and be connected to the Internet at top speeds such as 5G.
With true self-driving cars, most pundits predict that we will dramatically increase the amount of home-delivered items since driverless cars will be able to drive those purchased packages to your house.
Think about the other possibilities of how driverless cars can alleviate the stress and strain of the holidays.
Some pundits are saying that we’ll no longer have traffic congestion once we have self-driving cars, but that’s a rather Utopian viewpoint.
Note: For reader’s interested in Dr. Eliot’s ongoing business analyses about the advent of self-driving cars, see his online Forbes column: https://forbes.com/sites/lanceeliot/]http://ai-selfdriving-cars.libsyn.com/website"
206,https://www.aitrends.com/ethics-and-social-issues/google-wades-into-controversy-with-dismissal-of-ai-ethicist-timnit-gebru/,Google Wades Into Controversy with Dismissal of AI Ethicist Timnit Gebru,2020-12-10 23:07:24+00:00,"By John P. Desmond, Editor, AI Trends

Google ignited a firestorm around its ethics program last week when it let go a prominent AI ethicist, Timnit Gebru, apparently over contents of an email where she expressed her feelings, following a request by Google that a paper on large language models she had submitted to an industry conference be withdrawn.

Gebru had sent an email saying she felt “constantly dehumanized” at the company, according to an account in The Washington Post. She had been the co-leader of Google’s Ethical AI Team, where she was researching the fairness and risks associated with Google’s technology.

Of Ethiopian descent, Gebru was a rarity in the Silicon Valley culture known for its racial homogeneity. She became known in a senior role at Google for critically examining bias in the technology and its repercussions. She co-founded the Blacks in AI advocacy group that has pushed for more Black roles in AI development and research.

Gebru’s team had been researching large language models, such as OpenAI’s GPT-3 system, which has been used to generate seemingly human-created news reports, poems and computer code. Google may be researching consumer-facing products that would generate convincing passages of text difficult to distinguish from human writing, the Post reported.

In an interview in Bloomberg, Gebru said she was asked to remove the names of other Google employees from the paper, which was to be submitted to the ACM Conference on Fairness, Accountability, and Transparency, a conference Gebru co-founded to be held in March.

Google managers told Gebru the paper had to go through an approval process, and not enough time was allowed for the review. Gebru called it a “fundamental silencing.”

Thousands Sign Petition Supporting Gebru

Google protest group Google Walkout For Real Change, according to the Bloomberg account, posted a petition in support of Gebru on Medium, which had been initially signed by more than 400 employees of the company and more than 500 from academic and industry figures. Later it was signed by 2,278 Google employees and 3,114 industry allies.

The paper, according to Bloomberg, called out the dangers of using large language models to train algorithms that could, for example, write tweets, answer trivia and translate poetry. The models are essentially trained by analyzing language from the internet, which runs the risk that much of the world’s population would not be reflected in the training data.

This week Google CEO Sundar Pichai issued an apology to Gebru for the way the company handled her departure, according to an account in Axios. He said the company would look at all aspects of the situation.

“I’ve heard the reaction to Dr. Gebru’s departure loud and clear: it seeded doubts and led some in our community to question their place at Google,” stated Pichai in the memo he emailed to Google employees. “I want to say how sorry I am for that, and I accept the responsibility of working to restore your trust.”

Responding on Twitter, Gebru stated Pichai’s memo did not address the core issues around her departure. “I see no plans for accountability,” she stated.

Paper Focused on Ethical Implications of GPT-3 Large Language Model

A report from NPR, which had reviewed Gebru’s paper, said the paper explored the potential pitfalls of relying on the GPT-3 tool, which scans massive amounts of information on the Internet and produces text as if written by a human. The paper argued it could end up mimicking hate speech and other types of derogatory and biased language found online. The paper also cautioned against the energy cost of using such large-scale AI models.

Gebru told NPR she was given an insufficient account for why Google had objections to her research paper. “Instead of being like, ‘OK let’s talk,’ they’re like, ‘You know what? Nope, bye,’ ” Gebru told NPR.”I don’t feel like they thought it through. They could have had a much better outcome through dialogue.”

Gebru authored a study in 2018 with Joy Buolamwini, a computer scientist based at the MIT Media Lab, and founder of the Algorithmic Justice League, an organization that tries to challenge bias in decision-making software. The study showed facial recognition software was much more likely to misidentify people of color, particularly women, versus white men. IBM, Amazon, and Microsoft rolled back their face recognition product lines as a result. (See AI Trends.) This was during national protests over the death of George Floyd.

Reached by NPR, Buolamwini was critical of the Google move. “Ousting Timnit for having the audacity to demand intellectual integrity severely undermines Google’s credibility for supporting rigorous research on AI ethics and algorithmic auditing,” Buolamwini stated. “She deserves more than Google knew how to give, and now she is an all-star free agent who will continue to transform the tech industry.”

Gebru’s 12-page paper was “uncontroversial,” according to an account from writers who had reviewed the paper at Wired. “This article is a very solid and well-researched piece of work,” stated Julien Cornebise, an honorary associate professor at University College London who had seen a draft of the paper. “It is hard to see what could trigger an uproar in any lab, let alone lead to someone losing their job over it.”

In their open letter, the Google employees ask that senior leadership meet with the artificial intelligence team Gebru helped lead to explain how and why the paper Gebru co-authored was “unilaterally rejected” by management at the company.

Efforts to get a statement from Google beyond CEO Pichai’s email message were not successful by our deadline.

Read the source articles in The Washington Post, Bloomberg, Axios, NPR and Wired.","['gebru', 'ethicist', 'according', 'controversy', 'dismissal', 'stated', 'employees', 'ai', 'language', 'timnit', 'account', 'google', 'paper', 'wades', 'npr']","She co-founded the Blacks in AI advocacy group that has pushed for more Black roles in AI development and research.
Google managers told Gebru the paper had to go through an approval process, and not enough time was allowed for the review.
Later it was signed by 2,278 Google employees and 3,114 industry allies.
“I’ve heard the reaction to Dr. Gebru’s departure loud and clear: it seeded doubts and led some in our community to question their place at Google,” stated Pichai in the memo he emailed to Google employees.
Gebru told NPR she was given an insufficient account for why Google had objections to her research paper."
207,https://www.aitrends.com/ai-in-business/new-ai-chips-managed-services-among-flood-from-aws-at-reinvent-2020/,"New AI Chips, Managed Services Among Flood from AWS at re:Invent 2020",2020-12-10 22:55:41+00:00,"By AI Trends Staff

Amazon Web Services CEO Andy Jassy delivered a three-hour keynote at a virtual event on Dec. 1, the AWS re:Invent 2020 event. Jassy, who has been with Amazon for over 23 years, and who is now seen as the most likely successor to Amazon founder Jeff Bezos, made many, many announcements.

“There is no way to unpack Andy’s entire keynote as there were so many announcements across computer, storage, networking, AI/ML, developer tools, software and more,” according to an account from Futurum Research written by Daniel Newman, book author and principal analyst with the firm.

Highlights for enterprise applications included new R5 instances for EC2, the Amazon Elastic Compute Cloud, for memory-intensive applications such as high-performance databases, distributed web scale in-memory caches, in-memory databases and real time big data analytics.

“AWS continues to develop a comprehensive portfolio of Elastic Compute Cloud (EC2) instances to address the varying needs of customers,” Newman stated. “The diverse platforms give the company a wide breadth, and with the continued development of their Arm variants (Graviton2), the company continues to be more of a juggernaut in silicon.” The Graviton2 processor is a 64-core server chip, first announced several years ago.

Amazon also announced multiple options for deploying containers on-premises with AWS. ECS Anywhere enables customers to run Amazon Elastic Container Services in their own data centers. Amazon EKS Anywhere provides the ability to run Amazon Elastic Kubernetes Services in their own data centers.

“Amazon ECS has gained popularity due to the fact that customers see it as simple to use and deploy,” Newman stated. “However, a setback for AWS has been that often user requirements may require deployments beyond AWS-owned infrastructure. Up to this point, AWS hasn’t had an answer for this. Fans of ECS have sought access to a single experience that allows them to achieve the flexibility that they need.”

Amazon also announced an easier path to migrate from SQL Server databases to Amazon Aurora, a relational database service developed by AWS and first offered in 2014. The announcements included Babelfish for Aurora PostgreSQL, designed to simplify migrations.

“The reason this announcement is so powerful is in its simplicity and its implications,” Newman stated. “Babelfish enables PostgreSQL to understand both the command and protocol database requests from applications designed for Microsoft SQL Server without material impact to libraries, database schema, or SQL statements.” The developers focused on “correctness,” so that applications designed to use SQL Server will behave the same way on PostgreSQL, increasing the competitiveness of AWS against other SQL databases. Amazon announced that an open source version of Aurora is expected to be available in 2021.

In other chip news, Amazon announced that new Habana Gaudi-based Amazon EC2 instances for machine learning will be offered in the first half of 2021, through a partnership between AWS and Intel. The Gaudi AI accelerators promise 40% better price-performance than the best performing GPU instances today, according to AWS.

“It will work with all the main machine learning frameworks, PyTorch as well as TensorFlow,” and will help the company keep pushing the price-performance envelope and machine learning training advancements, stated Jassy, according to an account in EnterpriseAI. The Gaudi accelerators are designed for training deep learning models for workloads that include natural language processing, object detection and machine learning training, classification, recommendation and personalization.

Intel acquired Habana Labs in 2019. Gaudi-based EC2 instances are designed to deliver increased performance and greater cost efficiencies for customers, while allowing developers to build new or port existing training models from graphics processing units to Gaudi accelerators, according to Intel.

AWS Trainium Chip Announced for Machine Learning

Amazon announced a new chip, the AWS Trainium chip, for machine learning. The chip is custom-designed by AWS to deliver the most cost-effective training in the cloud, according to Jassy.

Trainium chips are optimized for deep learning training workloads for applications including image classification, semantic search, translation, voice recognition, natural language processing, and recommendation engines. Trainium should be more cost-effective than the Habana chip, Jassy stated, and will support all the major frameworks including TensorFlow, PyTorch and [Apache] MXnet.

“AWS is expanding its custom chip capabilities for the end-to-end ML lifecycle,” stated Arun Chandrasekaran, an analyst covering cloud native platforms, big data and AI for Gartner, to EnterpriseAI. “Data and analytics is one of the fastest growing use cases in cloud,” and is a computer-intensive workload.

Amazon also announced SageMaker Clarify to help reduce bias in machine learning models, according to an account in TechCrunch. “It allows you to have insight into your data and models throughout your machine learning lifecycle,” stated Bratin Saha, Amazon VP and general manager of machine learning.

The tool aims to analyze the data for bias before data preparation is begun, so bias problems such as varying numbers in different classes, can be identified before the model-building stage. ”We have a set of several metrics that you can use for the statistical analysis so you get real insight into easier data set balance,” Saha stated.

After the model is built, the developer can run SageMaker Clarify again to check for bias that might have entered the model as it was under construction. “So you start off by doing statistical bias analysis on your data, and then post training you can again do analysis on the model,” he stated.

Amazon also announced DevOps Guru, a managed operations service that aims to improve application availability by detecting operational issues and recommending fixes in an automated manner, according to an account in AnalyticsIndiaMag. The service applies machine learning to collect and analyze application metrics.

Cited benefits of the new service included quick alerts to developers and operators, so they can quickly understand the scope of a problem, automated recommendations for how to fix problems, and no specialized hardware required.

Amazon also announced Lookout for Equipment, an API-based machine learning system that aims to detect abnormal equipment behavior. The system is said to automatically test possible combinations and build an optimal machine learning model to learn the model behavior of the equipment.

Customers can bring in historical time series data and past maintenance events data generated from industrial equipment that can have up to 300 data tags from components such as sensors and actuators per model.

Read the source articles from Futurum Research, EnterpriseAI, TechCrunch and AnalyticsIndiaMag","['according', 'data', 'announced', 'stated', '2020', 'learning', 'chip', 'services', 'managed', 'ai', 'amazon', 'machine', 'reinvent', 'training', 'aws', 'flood', 'chips']","By AI Trends StaffAmazon Web Services CEO Andy Jassy delivered a three-hour keynote at a virtual event on Dec. 1, the AWS re:Invent 2020 event.
The Gaudi accelerators are designed for training deep learning models for workloads that include natural language processing, object detection and machine learning training, classification, recommendation and personalization.
AWS Trainium Chip Announced for Machine LearningAmazon announced a new chip, the AWS Trainium chip, for machine learning.
Trainium chips are optimized for deep learning training workloads for applications including image classification, semantic search, translation, voice recognition, natural language processing, and recommendation engines.
The system is said to automatically test possible combinations and build an optimal machine learning model to learn the model behavior of the equipment."
208,https://www.aitrends.com/ai-research/fords-use-of-ai-an-example-of-shaping-of-innovation-in-mit-future-of-work-session/,Ford’s Use of AI an Example of Shaping of Innovation in MIT Future of Work Session,2020-12-10 21:53:48+00:00,"By AI Trends Staff

The Ford Motor Co. has made a substantial investment in AI, from investing $1 billion in Argo AI in 2017 to advance its self-driving car efforts, to developing centers of excellence to focus on machine learning and AI, where engineers determine the AI tools and methods that can be dispersed throughout the company.

The use of AI for predictive maintenance, anticipating when a part may fail before it does, is proving productive for manufacturing at Ford, according to Jeanne Magoulick, Advanced Manufacturing Manager, Ford Motor Co.. She spoke as a member of a panel on Shaping Technology Innovation at MIT’s recent AI and the Work of the Future Congress 2020 held virtually.

“We are excited about predictive maintenance,” Magoulick said. “It will make us more efficient. We can identify when a machine is trending out of control and may need maintenance, so we can schedule at the next available window. It’s the next level of predictive maintenance from what we do today.”

It also helps in the ordering of needed replacement parts. “If we know the part is going bad, rather than holding the cash in our inventory, we can order it on demand,” she said.

AI is also being applied to vision systems, making for more powerful abilities to conduct inspections during manufacturing. ”We can find defects anywhere, including seeing paint scratches,” Magoulick said.

In addition, AI is being applied to further automate the auto manufacturing process, with research into where to apply the innovation ongoing. “We are using machine learning to try to reduce our cycle times,” she said. “We recently reviewed a use case for transmission assembly, which reduced the cycle time slightly the first time through.”

In addition, Ford is experimenting with the use of natural language for voice commands to communicate with machines on the shop floor. “It’s Siri for manufacturing,” she said.

Additional areas of research include studying audio to detect quality defects, “using AI to assess what is a good and what is a bad digital audio signature,” she said. Also, Ford is experimenting with collaborative robots on the shop floor, she said.

Domain Expertise Comes from Those Doing the Work Today

Asked by session moderator David Mindell, Co-chair, MIT Task Force on the Work of the Future, and MIT Professor of Aeronautics, where the domain expertise comes from, Julie Shah, MIT Associate Professor, Department of Aeronautics and Astronautics, said it is primarily from the people doing the work today. “The domain expertise is with people on the shop floor doing the job today, learned through years of apprenticeship in some cases,” said Shah. “It might look easy in some cases on first look but it can be challenging to program.”

She added, “Being able to learn from observation and demonstration is best done directly from someone doing the task on the shop floor, to see the key factors in doing the job successfully.”

Panelist Daron Acemoglu, MIT Professor of Economics, in response to a question from Dr. Mindell on whether AI will make better engineers, stressed the need for AI engineers to have a “concrete understanding” of the social implications of decisions they will make. He also stressed the importance of government policy.

“Government priorities are signals,” he said. “If the government gives up on the agenda of creating better technology, it’s natural for researchers to do that too.”

He is concerned that AI researchers maintain autonomy from the corporate world, and that big tech companies fund much of AI research in their own AI labs. “They have their own agendas,” he said. “If those companies set the tone for leading AI labs, how can we expect the AI research to do anything but parrot the priorities of those companies. It’s a difficult lesson. We are not really establishing our autonomy. We are saying good research means we are more integrated with Google, Amazon and IBM. Autonomy is critical in this area.”

Rus Sees “Problem-Driven” Research With Industry as Productive

He was challenged on this point a bit by panelist Daniela Rus, Director MIT CSAIL, and MIT Professor of Electrical Engineering and Computer Science, who said she has had some good experiences collaborating with researchers in private industry.

“I think there is a fair bit of autonomy and a number of programs that support problem-driven research,” she said. “Maybe there is not enough funding, and in some sense, where the government is lacking, the companies are stepping in.”

She added, “I find working with companies can be enriching and empowering,” mentioning a collaboration with Toyota Research Institute about five years ago to advance the science and AI and robotics research. Outlining her thoughts, she said, “When I think about how the university and industry research labs connect together, I have a mental model where the industry development lab works on products for today, the industry research lab works on problems of tomorrow, and the university research role is to think about the day after tomorrow, connecting to how those advances matter. The applications allow us to root our ideas into things the world cares about.”

Mindel asked if we should worry about AI taking over too many functions of humans. Prof. Acemoglu said, “There is a choice. There is no iron-clad rule on what humans can do and what technology can do. They are both fluid. It depends on what we value.”

Prof. Shah agreed with the sentiment. “The machines are still performing very narrowly-defined tasks,” she said. “Deep learning is a functional approximator, like algebra and calculus. It’s how we take those tools and use them for a purpose, and how we define success for those systems that matters. We might be trying to replace some aspect of what a human is doing today, but none of these systems operate truly independently. So asking what is the way we can have these technologies achieve our larger goals is the critical question.”

Rus ended the session on an optimistic note. “In the scientific community, we advance the science and engineering or intelligence and in doing so, we accomplish many things. We get a better handle on life, and we develop a broader range of machine capabilities. I am excited about using the latest advances in AI, machine learning and robotics to make certain jobs easier, to make life easier,” she said.

Technology has allowed people to come closer together during the pandemic, she said, “Despite the fact that the world is in the middle of a pandemic. And technology has allowed us to develop a vaccine more quickly, and that is helping us address the disease.”

Read the 2020 report from the MIT Task Force on the Future of Work","['session', 'future', 'research', 'example', 'today', 'fords', 'ai', 'innovation', 'companies', 'machine', 'work', 'manufacturing', 'ford', 'shaping', 'doing', 'mit']","AI is also being applied to vision systems, making for more powerful abilities to conduct inspections during manufacturing.
In addition, AI is being applied to further automate the auto manufacturing process, with research into where to apply the innovation ongoing.
“If those companies set the tone for leading AI labs, how can we expect the AI research to do anything but parrot the priorities of those companies.
We might be trying to replace some aspect of what a human is doing today, but none of these systems operate truly independently.
I am excited about using the latest advances in AI, machine learning and robotics to make certain jobs easier, to make life easier,” she said."
209,https://www.aitrends.com/ai-insider/complexities-when-ai-autonomous-cars-attempt-zipper-merging/,Complexities When AI Autonomous Cars Attempt Zipper Merging,2020-12-10 21:38:00+00:00,"By Lance Eliot, the AI Trends Insider

You undoubtedly know what a zipper merge is, though the name of it might seem unfamiliar.

Here’s how it goes. Imagine you are driving along on the highway, minding your own business, when you spy up ahead an indication that your lane is being narrowed out and you’ll need to get over into the other lane next to you. An electronic board sign is flashing warnings that your existing lane is going to end soon (“Merger Ahead” it veritably screams at you). In addition, a series of weather-worn red cones are set up in your lane that inch you over, step-by-step, and are positioned to gradually shunt all traffic out of the lane you are in.

It is the classic 2-into-1 traffic control squeeze play.

This is also commonly known as the zipper merge because it looks like a zipper as cars are pinned into veering from two lanes into one. Anyone that drives around with any frequency is apt to encounter these 2-into-1 situations on any given day and during any given driving journey. They exist aplenty.

So, what do you do?

One answer is that upon immediately spotting that a merger request is being proffered, you would as quickly and as safely as feasible guide your car into the next lane over and expeditiously get out of the lane that is going to disappear. You would not wait. You would not remain in the vanishing lane. You would act decisively and obey what you believe to be a lawful order to switch lanes. Let’s label that kind of driver as an Early Merge type of person, employing a driving strategy of trying to perform the merging action as soon as possible.

Another answer to the driving scenario is to try and remain in the fading lane as long as you can. The idea is to wait until the last possible moment and then dart over into the remaining available lane. In some ways, this is kind of exciting and maybe provides a bit of a thrill. In any case, the person using this approach is apt to be thinking that there is no particular reason to act like a scaredy-cat and abandon a perfectly good lane, postponing the abandonment until the only option left involves getting out and into the bordering lane. We’ll label this kind of driver as the Late Merge type of person.

Okay, so which camp do you fall into, the Early Merge members of society or the Late Merge members of our world? This is where the heated and acrimonious debates start to unfold.

The Early Merge types are bound to exhort that the Late Merge people are miscreants. Those Late Merge drivers are outright idiots that do not realize they ought to obey the indicated signs and get over in a prudent, timely, and earliest feasible way. Doing so is decidedly safer for all concerned. When those dolts dart over at the endpoint of merging, they are going to create trouble, heaps, and heaps of trouble. The odds are that it will cause the traffic to react in a knee jerk way, unfairly forcing other drivers to avert getting into a collision with these wrongdoers that have no civility, no politeness, and are steeped in pure unadulterated driving greed when behind the wheel of a car.

In contrast, the Late Merge types are likely to exclaim that the Early Merge drivers are skittish dimwits. The Early Merge types are so frightened at being on the highways that they get frazzled at the drop of a hat. If they see a posted roadway sign, they feel compelled to instantly do what it says, regardless of using any semblance of common-sense as to what to do. Those Early Merge nervous nellie drivers are the real troublemakers since they react to the traffic conditions without letting a single thought enter into their noggin. Furthermore, they seem to think that they alone are the arbiters of controlling traffic and will often attempt to force others to abide by their dictums about how to respond to a 2-into-1 merging situation. They are busybodies, acting high-and-mighty that they somehow own the road and want everyone to get over just like them, unwilling to allow freedom of thought or expression to other drivers.

Obviously, these are two diametrically opposed viewpoints.

Indeed, when the Early Merge purist meets with the Late Merger perfectionist, during a roadway 2-into-1 traffic condition, there is a sizable potential for sparking road rage. Each is upset at the other. Each believes they are truly in the right. Each profusely believes the other is absolutely and unquestionably in the wrong.

It is a combustible interaction, that is for darned sure. The conundrum and discourse about which driving strategy is right and which is wrong has been going on since the invention of the revered and at times loathed 2-into-1 contrivance. Using everyday intuitive logic does not seem to help clear-up the matter.

The Early Merge logically allows for a more measured approach that calmly allows for drivers to exit from their existing lane and comfortably merge into the next lane over, giving the drivers in that lane some breathing room to let in the other drivers. This would seem to provide a more seamless meshing of traffic and not cause any disruptive leaps or skirmishes, enabling a smoother and ultimately more effective flow of traffic. Seemingly this should mean fewer car crashes in a zipper merging context and less frustration and angst for all drivers involved.

Indubitably solid logic.

The Late Merge logically allows for better use of the roadway availability. If drivers prematurely get out of an available lane, they are going to crowd into the other remaining lane, thus underutilizing the lane that still has room and time available for usage. When you attempt to cram one queue or line of cars into another lane, it is going to cause an adverse and unwarranted distending of traffic and disturb the overall flow of traffic, therefore you should only do so when there are no other viable means to continue using the second lane.

Well, seems like more absolutely sound logic.

Maybe they are both wrong, or then again, maybe they are both right. But that doesn’t solve any of this matter and leaves us with nothing tangible as to what ought to be strictly done.

Numerous simulations have been undertaken to try and ferret out the proper choice.

A robust simulation will consider a variety of factors such as traffic throughput, queue lengths, travel times, car crashes, near collisions, and so on. These are computer-based models that attempt to simulate or pretend what might occur during actual driving situations. Not all such simulations are the same, and different authors or developers take differing paths to how they structure and program their particular simulation of a zipper merge scenario.

By-and-large, the simulations tend to suggest that waiting to merge is the “better” option, assuming that you are aiming to make maximal use of the roadway. Unfortunately, few of these studies incorporate the foibles of human drivers and assume that a human driver will always do the right thing in terms of how they drive, acting as a kind of driving automata.

In other words, in a simulated setup, the simulated drivers are oftentimes mathematically composed as though they will always anticipate that all other drivers will be doing the Late Merge, and none of the simulated drivers will do the Early Merge and that all drivers will accordingly drive to accommodate the sole driving strategy of the Late Merge.

This is an unheard-of and rarely if ever witnessed homogenization of driving.

Humans do not drive that way, and we would be dreaming to assume that they would act in such a wholly consistent and unerringly way while at the driving controls of their cars. Simulations that incorporate both the day-to-day mindful and at times mindless ways that humans drive are usually more likely to be better aligned with the realities of driving in a messy, confounding world composed of zany and emotionally spurred human drivers.

All told, the real-world question about the zipper merge is a tough one to resolve.

Except that the future might provide a ready resolution. Consider this intriguing point: Will the advent of AI-based true self-driving cars make zipper merges into an easy peasy situation and thus obviate any further qualms about dealing with the infamous 2-into-1 quandary?

Let’s unpack the matter and see.

For my framework about AI autonomous cars, see the link here: https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/

Why this is a moonshot effort, see my explanation here: https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/

For more about the levels as a type of Richter scale, see my discussion here: https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/

For the argument about bifurcating the levels, see my explanation here: https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/

Understanding The Levels Of Self-Driving Cars

As a clarification, true self-driving cars are ones that the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.

These driverless vehicles are considered a Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at a Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).

There is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.

Meanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).

Since semi-autonomous cars require a human driver, the adoption of those types of cars won’t be markedly different than driving conventional vehicles, so there’s not much new per se to cover about them on this topic (though, as you’ll see in a moment, the points next made are generally applicable).

For semi-autonomous cars, it is important that the public needs to be forewarned about a disturbing aspect that’s been arising lately, namely that despite those human drivers that keep posting videos of themselves falling asleep at the wheel of a Level 2 or Level 3 car, we all need to avoid being misled into believing that the driver can take away their attention from the driving task while driving a semi-autonomous car.

You are the responsible party for the driving actions of the vehicle, regardless of how much automation might be tossed into a Level 2 or Level 3.

For why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here: https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/

To be wary of fake news about self-driving cars, see my tips here: https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/

The ethical implications of AI driving systems are significant, see my indication here: https://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/

Be aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms: https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/

Self-Driving Cars And Zipper Merges

For Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task. All occupants will be passengers. The AI is doing the driving.

Not only will the AI be doing the driving, but it will also most likely have a means to electronically communicate with other nearby self-driving cars.

This is aiming to be accomplished via the use of V2V, vehicle-to-vehicle electronic messaging. Suppose a self-driving car notices some debris in the roadway, it can quickly send out a V2V message to other nearby self-driving cars to be wary of the debris. This would give those other AI driving systems a heads-up to perhaps change lanes before coming upon the debris or otherwise take any needed evasive driving actions beforehand.

What does this all portend for the zipper merge?

I’m betting that you’ve already added together the one-plus-one involved and come to a logically apparent conclusion that the zipper merge problem is solved.

Each AI-based self-driving car could coordinate with each other, during a zipper merge, and readily handle the merging activity. By communicating to each other via the V2V, and by allowing each other the courtesy of getting into the remaining lane, this whole matter would be as seamless as watching a flock of birds that weave back-and-forth together effortlessly.

The self-driving cars will do a dance that is undertaken without any visible waving of hands or arms, and nor any dangerous attempts at cutting each other off. They will instead be conveying their requests to each other electronically and silently fall into a beauteous pattern of streaming without hesitation and absence of any disturbances through the zipper. The timing can happen at the fastest speeds that traffic will allow.

Adding to the impressive nature of this effort, there is also going to be V2I, vehicle-to-infrastructure electronic communications. That means that roadway infrastructure such as road closures and the like will be sending out beaconing signals to warn about various traffic conditions. When a zipper merge is put in place, a V2I indicator will start broadcasting to alert any upcoming traffic. The AI driving systems would know then about the zipper due to either the V2I or via the V2V (as tipped from another nearby self-driving car).

Yay, excitedly, the zipper merge is another one of life’s many problems that we can put to rest.

Sorry to say, life is never that easy.

We’ll start with the biggest hurdle and then make our way to smaller ones.

The 500-pound gorilla is the fact that we are still going to have human drivers on our roadways for quite a while to come, despite the emerging advent of true self-driving cars.

Keep in mind that there are about 250 million conventional cars in the United States today, and those aren’t going to be tossed into the junk heap anytime soon merely due to the appearance of self-driving cars. Economically it is just not a feasible notion. As such, there will still be tons upon tons of human-driven cars on our roadways for decades to come.

Furthermore, we do not yet know whether human driving is really going to be given up entirely. One argument is that people must give up their driving to allow for the saving of the 40,000 annual driving fatalities and the 2.3 million estimated car-related injuries that occur each year in the U.S. It is assumed and hoped that the use of self-driving cars is going to diminish dramatically that volume of carnage.

But will people really be willing to stop driving? Many fervently assert that driving is their right (well, it is legally considered a privilege rather than a right), and you cannot force them to give up driving. Some insist you will only do so when you pry their cold dead hands from their steely grip upon the steering wheel.

How does the continued human-driven element make a difference in the zipper merge and self-driving cars?

The point is that the self-driving cars will not merely be able to coordinate with each other, they will also need to contend with human drivers. Recall that human drivers are, well, human, and therefore will continue to drive in their haphazard and perplexing ways. This means that you can toss aside the aforementioned harmonious dance of the self-driving cars.

Human interlopers will trounce the dance.

You can readily bet your bottom dollar that if self-driving cars choose to do the Late Merge, there will be humans that instead choose the Early Merge. If self-driving cars choose the Early Merge, there are undoubted will be Late Merge oriented human drivers. The earlier chaos of each driver tackling the matter in their own proprietary way has come back into the picture.

Once self-driving cars are prevalent, the numbers might end-up inexorably in a gradual shift toward the AI winning the zipper merge game. In essence, the fewer number of human-driven cars still on the roadways will be but a speck and therefore the AI driving systems will be pretty much able to drive around in a relatively smooth and coordinated fashion, only contending from time-to-time with those irritating and irascible human drivers.

In any case, the zipper merge in the near term, even with the arrival of self-driving cars, will continue to be a thorn in the sides of all drivers, both human drivers and AI driving systems.

This does bring up another facet about that state of the world, namely, how should an AI driving system act or react to a human-driven car during a zipper merge?

Right now, most of the automakers and self-driving car tech firms are making the AI rather timid when it comes to driving near humans. Generally, whatever a human driver wants, they will get, in terms of a nearby AI-based self-driving car that will readily give way to the human driver. If you are driving your car in the vanishing lane and suddenly want to move over, a self-driving car in that other lane is going to let you in, assuming that it is physically possible to do so. No questions asked. Unlike a human driver in that lane that might be spiteful at your interruption, the AI is just going to be, shall we say, a pushover.

Plus, human drivers will realize that AI driving systems are pushovers. In that case, the human drivers that already think of other human drivers as sheep will assuredly consider the AI driving systems to be like sheep, or whatever is even more docile and unaggressive in the animal kingdom.

One significant concern overall about the advent of self-driving cars is that some human drivers are going to try and exploit the situation. Those scofflaw pushy human drivers will routinely and gladly cut off the AI self-driving cars. This is going to up the ante on potential car crashes, which at first glance might not seem possible, since the AI will do what it can to avoid a collision, but the physics of the vehicles might not allow for an evasive maneuver and we’ll end-up with human drivers and AI driving systems getting into car crashes.

For human drivers desiring to exploit the AI timidity (as programmed), their opinion is going to be that if those AI driving systems were made to be taken advantage of, they surely ought to be so skewered.

For more details about ODDs, see my indication at this link here: https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/

On the topic of off-road self-driving cars, here’s my details elicitation: https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/

I’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop: https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/

Expect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here: https://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/

Conclusion

I earlier mentioned that some other less apparent considerations can toss a monkey wrench into the zipper merge riddle when it comes to adding self-driving cars into the mix.

Here’s a taste.

Assume that only self-driving cars are about to converge on a zipper merge (no human drivers are nearby).

In what order or sequence should the AI driving systems jointly ascertain which cars get through the squeeze play?

First come, first serve is not necessarily a completely viable option per se, due to the cars arriving at the bottleneck in relative unison. In that case, you might assert it is simply a matter of a random selection, done without preference to any particular AI or self-driving car. On the other hand, suppose we decide as a society that it is permitted to have passengers that can up the priority of their self-driving car, perhaps by being a highly ranked government official or being a celebrity. Or, perhaps by paying an added fee to the fleet owner of the self-driving car.

A final question to ponder, beyond the realm of discussing just the zipper merge, involves whether we will be willing to allow self-driving cars to operate differently depending upon the passenger or the payment by a passenger. Some say this smacks of elitism and undercuts hoped-for democratization of mobility for all that the promise of self-driving cars seems to offer.

Anyway, please be careful of other human drivers when doing those maddening zipper merges, and if you can, take it easy on the AI driving systems too.

Copyright 2020 Dr. Lance Eliot. This content is originally posted on AI Trends.

[Ed. Note: For reader’s interested in Dr. Eliot’s ongoing business analyses about the advent of self-driving cars, see his online Forbes column: https://forbes.com/sites/lanceeliot/]

http://ai-selfdriving-cars.libsyn.com/website","['autonomous', 'complexities', 'drivers', 'ai', 'driving', 'cars', 'attempt', 'selfdriving', 'merging', 'human', 'zipper', 'lane', 'car', 'merge']","This is also commonly known as the zipper merge because it looks like a zipper as cars are pinned into veering from two lanes into one.
Each AI-based self-driving car could coordinate with each other, during a zipper merge, and readily handle the merging activity.
Once self-driving cars are prevalent, the numbers might end-up inexorably in a gradual shift toward the AI winning the zipper merge game.
Those scofflaw pushy human drivers will routinely and gladly cut off the AI self-driving cars.
Assume that only self-driving cars are about to converge on a zipper merge (no human drivers are nearby)."
210,https://www.aitrends.com/ai-in-industry/construction-industry-beginning-to-use-ai-powered-robots-and-drones-on-site/,Construction Industry Beginning to use AI-powered Robots and Drones on Site,2020-12-03 20:58:24+00:00,"By AI Trends Staff

Robots and drones directed and managed with the help of AI are appearing on construction sites, as the construction industry gains experience with uses ranging from improving site security to management of multi-year projects.

Such large scale, multi-year projects require the coordination of many complicated tasks and moving parts including designs and blueprints, permits, and unexpected delays and changes. These can quickly challenge the ability of humans to manage without the assistance of technology, according to a recent account in Forbes.

For scheduling, the use of advanced AI can help to prevent costly delays on sites or among suppliers, vendors, and others involved in the process. AI is also being applied to contingency planning, running through multiple scheduling scenarios for example if a permit is delayed or an incident happens.

Drones are being used for surveying and taking overhead images of construction sites through stages of construction. Robots are being used to help with tasks such as bricklaying, pouring concrete, or installing drywall, augmenting the labor force to help with labor costs and to keep the project on schedule.

Increasingly, construction sites are being equipped with cameras, IoT devices, and sensors that monitor many aspects of construction operations. AI-enabled systems are able to watch what is going on 24/7 without distraction.

Startups Eye Opportunity; Built Robotics Moves Earth

A number of startups are taking advantage of this opportunity. Built Robotics, for example, got its start using autonomous skid-steer loaders to move sand or gravel on construction sites. Today the company has autonomous systems for bulldozers and 40-ton excavators.

“We have a software platform that actuates the equipment that takes all the data being read by the sensors on the machine every second to make decisions and actuate the equipment accordingly,” stated Gaurav Kikani, VP of strategy for Built Robotics, in an account in VentureBeat.

Built has focused on earth moving projects at remote job sites in California, Montana, Colorado, and Missouri that are far removed from human construction workers. Autonomous heavy equipment monitored by a human overseer tills the earth in preparation for later stages of construction, when human crews arrive to do things like build homes or begin wind or solar energy projects. In the future, the startup wants to help with more infrastructure projects. Built raised $33 million last fall in a series B round, making its total funding $48 million, according to Crunchbase.

Built started out focusing on standalone activities at a site, with one machine working alone to complete a job, and later moved into excavators and smaller dozers working together. The dozers would push material away or create space for the excavator to be more productive.

Software Needed to Manage “Morphologies” on Construction Sites

“The fleet coordination element here is going to be critical. Realistically, to get into the heart of construction, I think we’re going to start to coordinate with other types of equipment,” Kikani stated. “The trickiest thing about construction is how dynamic the environment is. Building technology that is pliable or versatile enough to account for the changing conditions, and being able to update in real time to plan to accommodate for that is going to be the key here.”

Computer vision systems are being used to track progress on construction sites. Startup Indus.ai, among a handful of companies in the business, offers cameras to track the flow of trucks entering a site, the number of floors completed in a building and the overall pace of progress. It is capable of following daily work production and helps supervisors determine whether the work of individuals and teams follows best practices.

“We can observe and use a segmentation algorithm to basically know every pixel—what material it is—and therefore we know the pace of your concrete work, your rebar work, your form work and [can] start predicting what’s happening,” stated Indus.ai CEO Matt Man to VentureBeat.

He envisions a mix of working humans and machines collaborating on construction sites. “There could be armies of robot-building things, but then there is an intelligent worker or supervisor who can manage five or 10 robotic arms at the same time,” Man stated.

Software for directing the on-site activity will become more critical as contractors embrace robotics, in his view. “Having all these kinds of logistical things run together really well, it’s something I think AI can do,” Man stated. “But it’s definitely going to take some time for the whole orchestration to be done well.”

Boston Dynamics, known for years as the maker of cutting-edge robots, also entered construction sites last year as part of its transition from an R&D outfit to a commercial company. Boston Dynamics construction technologist Brian Ringley echoed the notion that software platforms will emerge to coordinate multiple machines on construction sites.

“In the same way we use lots of different people and lots of machines on sites now to do things, I believe there will be multiple morphologies on construction sites and it will be necessary to work together,” Ringley stated.

That seems to be happening. For example, the International Union of Operating Engineers, which has over 400,000 members, last spring established a multi-year training partnership with Built Robotics.

Read the source articles in Forbes and VentureBeat.","['construction', 'going', 'stated', 'things', 'site', 'industry', 'ai', 'help', 'beginning', 'robotics', 'robots', 'sites', 'aipowered', 'work', 'drones', 'built']","By AI Trends StaffRobots and drones directed and managed with the help of AI are appearing on construction sites, as the construction industry gains experience with uses ranging from improving site security to management of multi-year projects.
Drones are being used for surveying and taking overhead images of construction sites through stages of construction.
Increasingly, construction sites are being equipped with cameras, IoT devices, and sensors that monitor many aspects of construction operations.
Built Robotics, for example, got its start using autonomous skid-steer loaders to move sand or gravel on construction sites.
Boston Dynamics construction technologist Brian Ringley echoed the notion that software platforms will emerge to coordinate multiple machines on construction sites."
211,https://www.aitrends.com/security/cryptographic-breakthrough-on-io-said-to-be-a-crown-jewel-for-security/,Cryptographic Breakthrough on iO Said to be a ‘Crown Jewel’ for Security,2020-12-03 20:49:20+00:00,"By AI Trends Staff

A team of scientists has achieved what one account referred to as “the crown jewel of cryptography” with a breakthrough in a technique studied for many years.

The technique of “indistinguishability obfuscation”—iO—is a low-level cryptographic algorithm that hides the implementation of a program while still allowing users to run it.

“Our top math geniuses point to iO as a cornerstone needed to unleash the full potential of artificially intelligent (AI) programs running across highly complex and dynamic cloud platforms, soon to be powered by quantum computers,” states a recent account in SecurityBoulevard written by Pulitzer Prize-winning business journalist Byron V. Acohido. “Simply put, iO must be achieved in order to preserve privacy and security while tapping into the next generation of IT infrastructure.”

The future in automation is driverless ground transportation, green cities that optimize energy usage and self-improving medical treatments. But to get there, these next-generation, AI-dependent systems need to run securely and in ways that preserve individual privacy.

While iO is the consensus solution, to date it has been the missing piece. Achohido recently spoke with Dr. Tatsuaki Okamoto, director of NTT Research’s Cryptography and Information Security Lab, and Dr. Amit Sahai, professor of computer science at the UCLA Samueli School of Engineering. NTT Research sponsored research led by Sahai that has recently resulted in an iO milestone.

Drawing an analogy to the human brain, Sahai said to consider what would happen if a mind reader could not only see everything stored in your brain but also could tinker with your synapses and manipulate your critical thinking.

Ability for Hackers to Alter Remote Code is Today’s ‘Core Security Challenge’

The software programs running digital services are the equivalent of human critical thinking. “It’s currently trivial for a proficient hacker to remotely access and alter just about any piece of software coding,” Acohido wrote, adding, “This is the core security challenge companies face defending their business networks.”

As the move to cloud infrastructure and IoT systems marches on, the risks increase. Sahai stated, “Sending your program out to an untrusted cloud to be executed raises the stakes even more.”

iO promises to render software coding unintelligible while preserving its function. The first iO theories date to the 1970s and have been viewed as unsolved problems. Sahai’s team, which included Aayush Jain, a UCLA graduate student, and Huijia Rachel Lin, an associate professor at the University of Washington’s Paul G. Allen School of Computer Science & Engineering—puts us one step closer to a working iO prototype, Acohido wrote.

“We are still at a very early stage here,” Sahai stated. “For the first time, we can prove that reverse engineering the software is as hard as solving certain standing conjectures in mathematics.”

Urgency is high to deliver new tools commercially that can deepen cybersecurity and reinforce privacy. Cloud and mobile computing continue to accelerate; our reliance on IoT systems and 5G networks is rising. The race is on to extend AI-enabled automation services, soon to be further enabled by quantum computers.

In this environment, telecom giant NTT Corp. of Tokyo chose to open NTT Research in Silicon Valley in July 2019 and begin to recruit top scientists and researchers. NTT funded its US research lab with a portion of its $3.6 billion budget.

“Our labs only conduct basic research,” Okamoto told Acohido. “We do not require any contributions to any of NTT’s business. We focus on basic research.”

All attempts to build practical obfuscators have failed to date. “The ones that have come out in real life are ludicrously broken,… typically within hours of release into the wild,” stated Sahai in a recent account in Quantamagazine.

“It really is kind of the crown jewel” of cryptographic protocols, stated Rafael Pass of Cornell University. “Once you achieve this, we can get essentially everything.”

Lin in 2016 began to research a way to overcome the weaknesses of iO. Several years ago, she joined forces with Jain and Sahai to work on a new technique. “We were stuck for a very, very long time,” Lin stated. Eventually, they arrived at a technique—“the pseudo-randomness generator”—that expands a string of random bits into a longer string that can fool computers. This is what is described in the new paper and results in an iO protocol that avoids the security weaknesses of the previous approaches.

“Their work looks absolutely beautiful,” stated Pass.

Schneier Sees iO Breakthrough as ‘Not Remotely Close to Being Practical’

A sobering thought was offered on the blog of Bruce Schneier, a cryptographer who works at the intersection of security, technology and people, Schneier on Security. Author of a number of books, he lectures at Harvard’s Kennedy School of Government and is chief of security architecture at Inrupt.

“This is a pretty amazing theoretical result, and one to be excited about. We can now do obfuscation, and we can do it using assumptions that make real-world sense,” Schneier stated of the breakthrough on iO.

“But—and this is a big one—this result is not even remotely close to being practical. We’re talking multiple days to perform pretty simple calculations, using massively large blocks of computer code,” Schneier stated. “And this is likely to remain true for a very long time. Unless researchers increase performance by many orders of magnitude, nothing in the real world will make use of this work anytime soon.”

Another view of this is that the iO breakthrough from Jain, Lin, and Sahai will inspire more researchers into the field to work on making the scheme and to develop new approaches, suggested the account in Quantamagazine. Researcher Yuval Ishai of the Technion in Haifa, Israel, stated, “Once you know that something is possible in principle, it makes it psychologically much easier to work in the area.”

Read the source articles in SecurityBoulevard, Quantamagazine and Schneier on Security.","['research', 'cryptographic', 'stated', 'crown', 'sahai', 'schneier', 'cloud', 'security', 'io', 'jewel', 'work', 'software', 'ntt', 'breakthrough']","NTT Research sponsored research led by Sahai that has recently resulted in an iO milestone.
“It really is kind of the crown jewel” of cryptographic protocols, stated Rafael Pass of Cornell University.
Schneier Sees iO Breakthrough as ‘Not Remotely Close to Being Practical’A sobering thought was offered on the blog of Bruce Schneier, a cryptographer who works at the intersection of security, technology and people, Schneier on Security.
We can now do obfuscation, and we can do it using assumptions that make real-world sense,” Schneier stated of the breakthrough on iO.
We’re talking multiple days to perform pretty simple calculations, using massively large blocks of computer code,” Schneier stated."
212,https://www.aitrends.com/ai-in-business/kdp-using-ai-to-fuel-expansion-strategy-with-sales-boosted-during-pandemic/,"KDP Using AI to Fuel Expansion Strategy, with Sales Boosted During Pandemic",2020-12-03 20:44:15+00:00,"By AI Trends Staff

Keurig Dr. Pepper (KDP), experiencing dramatic volume increases as people drink more coffee at home during the pandemic, is pursuing AI to help fuel its expansion strategy.

Keurig Green Mountain acquired the Dr Pepper Snapple group in 2018, in an $18.7 million deal. The rationale from Keurig CEO Bob Gamgort was to combine hot and cold beverages to create a platform for higher growth.

“The business and Wall Street appeared in cold and hot as two entirely different sectors,” Gamgort stated in an article in Fortune. KDP is now rated the seventh-largest food and drink business in the US, with $11.1 billion in earnings last year.

KDP in early March formed “related panels” that 10,000 home brewers could connect to digitally, to browse pictures of each K-Cup pod to see changes in titles and mixes. Along with this data being viewed by consumers sheltering in place during the pandemic, “Coffee intake has been through the roof, Gamgort stated. He anticipated that customers would look to stockpile supplies so they would shop where they could buy major quantities, in the megastores. He positioned to meet the demand by increasing production of cans, including from Mexico which had cut back beer production after the government deemed beer unessential.

Its moves have resulted in increased market share. In the 20-week period ended July 26, KDP garnered 34.1% of the $1.4 billion increase in revenue for all U.S. carbonated soft drinks, according to Consumer Edge, quoted in Fortune That boosted its overall market share from 22.7% to 24.0%. “KDP has done the best job of any beverage company in navigating the crisis,” stated Consumer Edge analyst Brett Cooper.

Move to Google Cloud and AI

At the same time, KDP is overhauling its compute infrastructure. In July, the company announced it was making a multi-year commitment to Google Cloud to house its “data footprint,” according to an account in CIODive.

By the end of 2020, KDP plans to shift to virtual machines running on Google Cloud, retiring two data centers with more than 1,000 servers. The migration represents most but not all the company’s data footprint.

The move is critical to KDP’s “merger integration and modernization efforts,” stated John Gigerich, SVP and CIO for Keurig Dr Pepper, in a statement.

Several consumer product good brands are working to “expedite their digital transformations to really understand their data, deepen relationships with consumers and ultimately drive their business forward,” Carrie Tharp, VP of Retail & Consumer, Google Cloud told CIO Dive in an email. Similar to retailers and direct-to-consumer brands, CPG companies want data from direct customer interaction.

Also in a play for more personalization, The Procter & Gamble Company is using Google’s data analytics and AI technology, Google announced earlier this month. The company is working to integrate consumer, brand, and media data for deeper insights.

“We’re always looking to ensure a great consumer experience across all our categories, from healthcare to beauty products and much more,” stated Vittorio Cretella, CIO, Procter & Gamble, in the statement. “As a leader in analytics and AI, Google Cloud is a strategic partner helping us offer our consumers superior products and services that provide value in a secure and transparent way.”

Google Cloud’s Live Migration Services Tapped to Move KDP’s Servers

Meanwhile at KDP, CIO Gigerich stated in an account from aiTechPark, “Google Cloud is a true partner that gives us the stability and flexibility to support critical business applications needed to drive innovation and ensure business continuity.”

He added, “The migration to Google Cloud has been seamless and was a key project in our merger integration and modernization efforts as Keurig Dr Pepper. We look forward to exploring additional partnership opportunities with Google in the future.”

KDP used Google Cloud’s Live Migration service, which enables enterprises to move virtual servers from one physical machine to another with what is hoped to be a minimum of disruption. For applications like SAP, where even a few minutes of downtime can cost tens of thousands of dollars in lost revenue.

“Data is the fuel to help companies transform their businesses, and those that leverage the cloud will thrive in this new era of retail,” stated Carrie Tharp, VP of Retail & Consumer at Google Cloud. “We’re thrilled to partner with Keurig Dr Pepper, and we look forward to helping the company reach the next level in its digital transformation journey.”

The contract with Keurig Dr Pepper will be implemented in conjunction with Google Cloud services partner HCL America.

Elsewhere in AI news at KDP, the company is moving to insert more intelligence into its recycling efforts. The company has committed to making all its K-cup pods recyclable by the end of 2020, starting with changing the material used in the container to polypropylene, a plastic sought for recycling, according to an account in Vox.

Keurig is also working with AMP Robotics to bring AI to the sorting process at recycling facilities. (See AI Trends for account of AI in recycling.)

Read the source articles in Fortune, in CIODive, from aiTechPark and in Vox.","['kdp', 'using', 'boosted', 'data', 'stated', 'sales', 'pepper', 'ai', 'pandemic', 'expansion', 'google', 'cloud', 'consumer', 'fuel', 'strategy', 'dr', 'company']","By AI Trends StaffKeurig Dr. Pepper (KDP), experiencing dramatic volume increases as people drink more coffee at home during the pandemic, is pursuing AI to help fuel its expansion strategy.
Keurig Green Mountain acquired the Dr Pepper Snapple group in 2018, in an $18.7 million deal.
“KDP has done the best job of any beverage company in navigating the crisis,” stated Consumer Edge analyst Brett Cooper.
Move to Google Cloud and AIAt the same time, KDP is overhauling its compute infrastructure.
Elsewhere in AI news at KDP, the company is moving to insert more intelligence into its recycling efforts."
213,https://www.aitrends.com/ai-insider/prospects-of-empty-roaming-ai-autonomous-cars-aplenty/,Prospects Of Empty Roaming AI Autonomous Cars Aplenty,2020-12-03 19:26:50+00:00,"By Lance Eliot, the AI Trends Insider

One of the most challenging aspects of ridesharing involves the empty roaming time.

In short, human drivers are continually underway in hopes of being in the right place at the right time in order to be the car most likely chosen for a ride request. Since the ride-sharing firms oftentimes aren’t on the hook for the effort and cost associated with cars that are roaming, the firms don’t especially care as much about the idle time of those drivers. The primary focus is on making sure that the shortest wait time can be achieved for the riders making requests, or those paying passengers might switch over to a competing ride-sharing network.

Sure, there is a concern too about those drivers due to the aspect that a driver that seems to endlessly roam will not be desirous of listing with the ride-sharing network that leaves them high-and-dry. In that sense, it behooves both the drivers and the ride-sharing operation to try and minimize the empty roaming time and seek to maximize the passenger riding time.

This is a complex problem overall.

There are some number of unknown requests that will arise, occurring from unknown locations, for which the aim is to have a driver and car that is relatively nearby when the request comes in. There is (generally) an unknown number of cars and drivers, along with an unknown dispersal of where those cars and drivers are. A driver might be tempted to simply park their car and wait for a request, but it seems more proactive to keep underway and attempt to be in the midst of the action.

There is a natural kind of tension between the ridesharing firm making suggestions to the drivers as to where they ought to roam versus the drivers wanting to remain independent and use their own wits to figure out where to be. Some drivers seem to be either astute or lucky, happening to keep their plate full and avoid being in the wrong places at the wrong times. Others somehow are driving in a desert and rarely seem to get the pick of the rides.

It is a gigantic logistics issue and one that attracts top-notch data scientists and promises handsome rewards for algorithmically trying to find the best possible approaches.

Let it be said that one of the thorniest and problematic aspects that can stymie ride going services is the percentage of time that the passenger-hauling car is empty of passengers. Presumably, no money is being made at those times. Worse too, there is cost during those times, such as the cost of the fuel being consumed, the cost associated with the wear-and-tear of the vehicle, and obviously the cost associated with the driver labor. If the ridesharing firm is not directly paying the driver, the cost then is a lost opportunity cost for the driver rather than a cost to the networking firm per se.

Generally, in recap, a car being driven by, say, an Uber or Lyft driver that is bereft of any passengers is not a good thing.

You can pretty much assume that when there isn’t a passenger there also isn’t any income being derived by the ridesharing effort. One quasi-exception consists of instances in which the driver is delivering something, perhaps a pizza or a burger or a package, and as such, there is presumably income covering the delivery action.

For purposes of simplicity, let’s assume that delivering something is equivalent to having a passenger.

Thus, when I refer to an empty vehicle, it means that the car doesn’t have a passenger and nor is it in the act of delivery. Also, for clarity’s sake, a passenger could be a human being, though the rider could also be a favored dog or cat that someone is paying for a breezy ride to the vet or grandma’s house.

Another factor to keep in mind about a so-called “empty vehicle” is that there is a human driver sitting in the driver’s seat.

Sometimes it is confusing to refer to an empty car since it could imply that nobody is in the vehicle at all. In the context of ridesharing, an empty vehicle traditionally means that there is a driver and no one else in the vehicle.

I’ll further point out that there might be a passenger in a car, but if they aren’t paying anything to be there, they essentially count as an empty vehicle. Maybe a ridesharing driver decides to give a friend a free ride to the store, or perhaps the other person is a fellow driver that might be in training. If there’s no dough derived from the passengers, it’s considered an empty vehicle.

The formal wonky moniker is often referred to as the utilization component of ridesharing or e-hailing service, typically calculated as the percentage of time that a driver spends with a passenger. We can quibble a bit about whether emptiness and utilization are the same or not, but for sake of discussion let’s assume that they indeed are one and the same.

Industry Average Of Empty Car Time

Can you guess what the industry-wide average of empty vehicle time is?

When I ask this question at conferences, attendees toss out all kinds of wild numbers.

Some think it must be rare that a ridesharing car is empty and so guess what it is maybe 1% or 2% of the time. Others guess that it might be relatively common to have an empty ridesharing car, and they guess that it might be the seemingly astronomical amount of say 20% or one-fifth of the time.

According to industry reported stats, the average time that a ridesharing vehicle is “empty” comes to about 41% of the time.

I hope that wasn’t overly shocking for you (I should have said trigger alert).

Though the stats don’t also say how much of that empty car time involves the car being in-motion, most would assert that the ridesharing cars are nearly always in motion when available to provide a ride.

Generally, when a ridesharing car is parked, it implies that the car is not available for a ride. There are exceptions, of course, but the gist is that the 41% implies that a significant chunk of the time that a ridesharing car is underway, it is empty.

The emptiness factor produces a lot of ugly and undesirable consequences.

A ridesharing car that 41% of the time is not earning money means that the cost of the driver and the cost of the car are having to be covered by the 59% of the time that a passenger exists. Presumably, if you could decrease the emptiness, you could produce more income, and in theory pay, drivers more and more readily cover the costs of the car, along with gaining a profit.

Another aspect that gets people mad about the empty car phenomenon is that the vehicle is adding to traffic snarls, presumably exacerbated by the wasted 41% of the time there isn’t a passenger in the car. If you stand on the street corner of a busy city, you can likely see a parade of ridesharing cars after ridesharing cars that are cruising the streets, absent a passenger, and trolling or waiting for a passenger to hail the vehicle.

Anyone driving their car for their own purposes often gets steamed to see themselves surrounded by empty ridesharing cars. What might have taken ten minutes to drive from your office to a restaurant gets extended multifold by those darned empty ridesharing cars that are choking up traffic?

Cities have qualms too about the pollution that those empty cars are producing. It’s one thing to be driving a passenger and producing pollution but driving an empty vehicle and gushing out pollutants seems especially egregious and foul.

An often-unstated downside of roaming empty cars is that presumably the risk of getting into a car crash or other incident is increased. This risk is not due to the car being empty, but instead due to the length of time on the roadway and the distance traveled. The more any car is on the roadway, the chances of getting into an accident continue to be present.

Some question whether the safety compromise of having roaming ridesharing cars that are empty is warranted to the public at large, though few studies exist to predict how much of the 41% of that overall driving time is leading to car accidents (other than using generic driving stats as an approximation).

Kerfuffle In NYC

A bit of a kerfuffle rose last year in New York City (NYC) on the emptiness conundrum. The NYC Taxi and Limousine Commission (TLC) passed a regulation that required ridesharing firms in NYC to achieve a lowered rate of emptiness, striving for a 31% (or less) goal.

On the surface, aiming for a lessened emptiness factor is a good thing, though drivers are typically worried that it means they might be forced to cut back on their driving efforts and therefore earn less money, and the ridesharing services are concerned about backlash from all quarters.

For example, suppose that the time for you to wait once you’ve requested a ridesharing ride goes up, which could happen if the number of cruising empty cars is reduced to knock down the emptiness time. As a customer, you aren’t likely to be pleased that the emptiness factor went down and yet the delay for you to get a ride went up. All you’ll care about is the wait time. Most ride-seeking people are bound to say that they don’t care about emptiness and only care about getting a prompt ride.

You can argue that maybe their journey in the ridesharing car might be faster, and they might end-up shell out less money for the ride once the emptiness rates drop to 31%, though right now that’s a theoretical proposition and the actual impact could come out quite differently.

Here’s an interesting point to consider: Will the advent of self-driving cars eliminate the emptiness factor, or will it be the same or perhaps even worse than with today’s conventional cars?

Let’s unpack the matter.

For my framework about AI autonomous cars, see the link here: https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/

Why this is a moonshot effort, see my explanation here: https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/

For more about the levels as a type of Richter scale, see my discussion here: https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/

For the argument about bifurcating the levels, see my explanation here: https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/

Self-Driving Cars And Ridesharing

True self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.

These driverless cars are considered a Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at a Level 2 or Level 3. The cars that co-share the driving task are considered semi-autonomous, and typically contain a variety of automated add-ons that are referred to as ADAS (Advanced Driver-Assistance Systems).

There is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there. Meanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some point out).

Since the semi-autonomous cars require a human driver, akin to any of today’s conventional cars, I’m not going to consider the emptiness factor a Level 2 or Level 3 car (it would be the same as the emptiness of conventional cars). Instead, let’s focus on the emptiness or utilization aspects involving true self-driving cars, ones at Level 4, and Level 5.

Well, the first big difference is that when a true self-driving car is considered empty, it really is empty.

Recall that with conventional cars we were willing to say that a car was empty when there weren’t any passengers, even though there was a human driver present. Now, due to the AI driving system, there isn’t a human driver and presumably not a provision allowing for a human driver (this is a controversial point, for which some believe that human driving ought to be still allowed in true self-driving cars).

From the perspective of a ridesharing or ride-hailing firm, removing the human driver is somewhat like a godsend. No more dealing with those darned human drivers that are cantankerous and complaining about the money they are making or being denied. Human drivers oftentimes pick and choose when they want to work and dare to take off days whenever they feel like it.

Overall, the headaches of human drivers are erased.

Furthermore, the AI driving system can drive whenever and wherever the owner of the self-driving car dictates. There are no debates about working late at night or having to drive to Timbuktu for fares. Instead, the owner deploys the self-driving car to any place and at any time.

Many are tempted to also say that the cost of the driver is removed. Yes, certainly the hourly fee paid to human drivers or commissions is no longer pertinent, but you need to impute the cost of the AI driving system as a kind of surrogate for the cost of a human driver. The AI driving system consists of the plethora of sensors on the self-driving car, plus the computer processors, plus the AI software, and so on. The hardware is prone to wear-and-tear. The software will need updates.

All in all, there is still a form of “cost” associated with the driving act. Nobody knows what the cost of these AI driving systems will be.

It will be interesting to see what happens when true self-driving cars start to compete head-to-head with human-driven ridesharing and ride-hailing cars. We will begin to know how the costs compare and whether the AI driving systems will indeed drive down the costs as most assume or hope will happen.

For why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here: https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/

To be wary of fake news about self-driving cars, see my tips here: https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/

The ethical implications of AI driving systems are significant, see my indication here: https://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/

Be aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms: https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/

Self-Driving Cars And Emptiness

Returning to our focus on the topic of emptiness, there is nothing magical about self-driving cars that obviates the emptiness factor.

You can have self-driving cars roaming around and cruising the streets, doing so without passengers, and in the same manner that human-driven “empty” ridesharing cars do so.

If that’s the case, what do you predict the emptiness percentage will be? It is tempting to say that it might be about the same as today’s 41%. Maybe, maybe not. Some worry that it might be a lot higher.

Here’s why that could happen.

Everyone that owns a true self-driving car is going to want to wring every dollar of ridesharing revenue they can out of the driverless vehicle. Might as well put the self-driving car into service nearly 24×7, minus the time needed to refuel or recharge the car, and minus the time needed for maintenance or repairs. Imagine then a myriad, nay a swarm, consisting of hundreds or maybe thousands of self-driving cars roaming around, all waiting for that moment when they will be called into action to carry a passenger.

Furthermore, pundits keep saying that we can get rid of parking lots in downtown areas since the advent of self-driving cars will make parking lots no longer needed. You would be stupid or outright foolhardy to park your self-driving car when it could instead be picking up and transporting passengers.

At least that’s the conventional wisdom about our future.

Some pundits are quick to gush about the fact that downtown areas will be able to reclaim parking lots for more important uses such as additional housing, or businesses, or retail space, or green space and trees. If you insist that parking ultimately is needed for those self-driving cars, the answer glibly stated is that parking in areas outside of downtown could be set up, doing so in places that the land is much cheaper and no humans live or want to live nearby.

Consider once again the emptiness factor.

You’ve got zillions of self-driving cars roaming around the streets of downtown, and for some percentage of the time, those self-driving cars are empty. Also, those self-driving cars are going to be driving empty to an outside area that has a parking lot. According to those pundits, no humans are going to be getting a lift out to those parking lots because the self-driving car parking space will be on the outskirts where no humans are desirous of the land.

When you start to add-up the emptiness time, it could be that self-driving cars will break the sound barrier of emptiness and reach astoundingly high percentages.

For those that own self-driving cars, the emptiness is going to hurt and could make the use of the self-driving car for ridesharing unpalatable.

For more details about ODDs, see my indication at this link here: https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/

On the topic of off-road self-driving cars, here’s my details elicitation: https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/

I’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop: https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/

Expect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here: https://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/

Tackling The Emptiness

Ponder some of the adverse consequences already mentioned about emptiness and recast them into a world that includes true self-driving cars.

Here are some key points:

The traffic snarls caused by self-driving cars vying for fares could be worse than what we already experience, partially due to the urgent need by the owners to seek out revenue to cover their expensive self-driving car purchases, and ongoing maintenance, and therefore sending their vehicles into an infinite search for paying riders.

Human drivers that are also trying to drive, whether for ridesharing or personal purposes, are certainly going to get irked at seeing all those headless self-driving cars jamming up the streets. Might human drivers’ rebel against their AI brethren?

Passengers seeking a ride might have elongated wait times, especially if the predicted induced demand materializes (induced demand means that people today that don’t use a car might be induced to do so, once we have self-driving cars).

Journeys inside a self-driving car might be pleasant due to being able to relax and watch TV, but if the trip across town takes twice as long as it used to take, will riders be happy with the result?

On a safety basis, once again the longer a car is in-motion and consuming miles, the risks of getting into a car accident continue unabated. Those that live in a Utopian world and insist that self-driving cars won’t get into car accidents are making a false assumption that only self-driving cars will be on our roadways. We today have 250 million conventional cars in the United States alone and they aren’t going away anytime soon, so stop pretending that we’ll only have self-driving cars and realistically expect that we’re going to have a mixture of human-driven cars and self-driving cars for a long time ahead.

Pollution might be the one factor that does get impacted in a good way, namely that self-driving cars are likely to be Electrical Vehicles (EV), therefore the amount of pollution being pumped out of the cars will be less than today’s gasoline-powered engines. The flip side to that coin is that there is a pollution footprint produced in the generation of electricity that is needed to recharge EVs.

Some say that we should restrict downtown areas to only self-driving cars and refuse to let human-driven cars into those areas.

In doing so, perhaps the emptiness factor might come down.

The logic being that if you have less of a supply of ridesharing vehicles in the restricted area, the demand is going to fill-up the self-driving cars more so. Of course, it might have the added effect of increasing wait times to get a ridesharing lift.

Will human-drivers be willing to get shoved down on the totem pole and sit on the outside while self-driving cars get the vaunted and exclusive access to making money in the steeped ridesharing locales such as a downtown area?

You can already see the ridesharing human drivers protesting that the small guy is getting the shaft, while the big businesses that own fleets of self-driving cars are getting outrageously wealthy.

Conclusion

I’m not trying to paint a doomsday picture concerning the role of self-driving cars. My overall point is that some believe that the advent of self-driving cars will be a boon for mobility and make car rides readily accessible to all. Though I am hopeful and optimistic about such an outcome, we need to realize that the real world is going to intrude on how things will play out.

Empty cars, whether for human drivers or self-driving cars, do not provide a free lunch. If a ridesharing car is not carrying a passenger or a package, presumably there is no one paying to have the resource meandering along on our streets. Self-driving cars are a resource and have an associated cost.

Trying to figure out how to best manage that resource will be an issue for fleet owners and individual owners, along with being an issue for cities and for people that simply want to get a ride.

It’s going to be a challenge, and hopefully, one that we can avert the woe.

Copyright 2020 Dr. Lance Eliot. This content is originally posted on AI Trends.

[Ed. Note: For reader’s interested in Dr. Eliot’s ongoing business analyses about the advent of self-driving cars, see his online Forbes column: https://forbes.com/sites/lanceeliot/]

http://ai-selfdriving-cars.libsyn.com/website","['autonomous', 'cost', 'emptiness', 'driver', 'drivers', 'roaming', 'ai', 'ridesharing', 'driving', 'cars', 'aplenty', 'car', 'prospects', 'selfdriving', 'human']","You can have self-driving cars roaming around and cruising the streets, doing so without passengers, and in the same manner that human-driven “empty” ridesharing cars do so.
You’ve got zillions of self-driving cars roaming around the streets of downtown, and for some percentage of the time, those self-driving cars are empty.
We today have 250 million conventional cars in the United States alone and they aren’t going away anytime soon, so stop pretending that we’ll only have self-driving cars and realistically expect that we’re going to have a mixture of human-driven cars and self-driving cars for a long time ahead.
Some say that we should restrict downtown areas to only self-driving cars and refuse to let human-driven cars into those areas.
Empty cars, whether for human drivers or self-driving cars, do not provide a free lunch."
214,https://www.aitrends.com/cloud-2/power-of-ai-with-cloud-computing-is-stunning-to-microsofts-nadella/,Power of AI With Cloud Computing is “Stunning” to Microsoft’s Nadella,2020-11-24 23:03:59+00:00,"By AI Trends Staff

Asked what in the march of technology he is most impressed with, Microsoft CEO Satya Nadella said at MIT’s AI and the Work of the Future Congress 2020 held virtually last week that he is struck by the ability of cloud computing to provision massive computing power.

“The computing available to do AI is transformative,” Nadella said to David Autor, the Ford Professor of Economics at MIT, who conducted the Fireside Chat session.

Nadella mentioned the GPT-3 general purpose language model from OpenAI, an AI lab searching for a commercial business model. GPT-3 is an autoregressive language model with 175 billion parameters. OpenAI agreed to license GPT-3 to Microsoft for their own products and services, while continuing to offer OpenAI’s API to the market. Today the API is in a limited beta as OpenAI and academic partners test and assess its capabilities.

The Microsoft license is exclusive however, meaning Microsoft’s cloud computing competitors cannot access it in the same way. The agreement was seen as important to helping OpenAI with the expense of getting GPT-3 up and running and maintaining it, according to an account in TechTalks. These include an estimated $10 million in expenses to research GPT-3 and train the model, tens of thousands of dollars in monthly cloud computing and electricity costs to run the models, an estimated one million dollars annually to retrain the model to prevent decay, and additional costs of customer support, marketing, IT, legal and other requirements to put a software product on the market.

Earlier this year at its Build developers conference, Microsoft announced it worked with OpenAI to assemble what Microsoft said was “one of the top five publicly disclosed supercomputers in the world,” according to an account on the Microsoft AI blog. The infrastructure will be available in Azure, Microsoft’s cloud computing offering, to train “extremely large” AI models.

The partnership between Microsoft and OpenAI aims to “jointly create new supercomputing technologies in Azure,” the blog post stated.

“And it’s not just happening in the cloud, it’s happening on the edge,” Nadella said.

Applications for cloud and edge computing working together—such as natural language generation, image completion, or virtual simulations from wearable sensors that see the work—are very compute-intensive. “It’s stunning to see the capability,” of the GPT-3 model applied to this work, Nadella said. “Something in the model architecture gives me confidence we will have more breakthroughs at an accelerating pace,” he said.

Potential Strategic Advantage in Search, Voice Assistants from GPT-3 Models

Strategically, it could be that the GPT-3 models will give Microsoft a real advantage, the article in TechTalks suggested. For example in the search engine market, Microsoft’s Bing has just over a 6% market share, behind Google’s 87%. Whether GPT-3 will enable Microsoft to roll out new features that redefine how search is used remains to be seen.

Microsoft is also likely to explore potential advantages GPT-3 could bring to the voice assistant market, where Microsoft’s Cortana sees a 22% share, behind Apple’s Siri, which has 35%.

Nadella does have concerns related to the power of AI and automation. “We need a set of design principles, from ethics to actual engineering and design and a process to allow us to be accountable, so the models are fair and not biased. We need to ‘de-bias’ the models and that is hard engineering work,” he said. “Unintended consequences” and “bad use cases” are also challenges, he said, without elaborating. [Ed. Note: A ‘misuse case” or bad use case describes a function the system should not allow, from Wikipedia.]

Moderator Autor asked Nadella how Microsoft makes decisions on what problems to work on using AI. Nadella mentioned “real world small AI” and the company’s Power Platform tools, which enables several products to work well together as part of a business application platform. This foundation is built on what had been called the Common Data Service for apps, and as of this month (November), is called “Dataverse.” Data is stored in tables which can reside on the cloud.

Using the tools, “People can take their domain expertise and turn it into automation using AI capabilities,” Nadella said.

Asked what new job opportunities are being created from the use of AI he anticipates in the future, Nadella compared the transition going on today to the onset of computer spreadsheets and word processors. “The same thing is happening today,” as computing is getting embedded in manufacturing plants, retail settings, hospitals, and farms. “This will shape new jobs and change existing jobs,” he said.

‘Democratization of AI’ Seen as Having Potential to Lower Barriers

The two discussed whether the opportunities from AI extend to those workers without abstract skills like programming. Discussion ensued on “democratization of AI” which lowers barriers for individuals and organizations to gain experience with AI, allowing them, for example, to leverage publicly available data and algorithms to build AI models on a cloud infrastructure.

Relating it to education, Autor wondered if access to education could be “democratized” more. Nadella said, “STEM is important, but we don’t need everyone to get a master’s in computer science. If you can democratize the expertise to help the productivity of the front line worker, that is the problem to solve.”

Autor asked if technology has anything to do with the growing gap between low-wage and high-wage workers, and what could be done about it. Nadella said Microsoft is committed to making education that leads to credentials available. “We need a real-time feedback loop between the jobs of the future and the skills required,” Nadella said. “To credential those skills, we are seeing more companies invest in corporate training as part of their daily workflow. Microsoft is super focused on that.”

A tax credit for corporations that invest in training would be a good idea, Nadella suggested. “We need an incentive mechanism,” he said, adding that a feedback loop would help training programs to be successful.

Will “telepresence” remain after the pandemic is over? Autor asked. Nadella outlined four thoughts: first, the collaboration between front line workers and knowledge workers will continue, since the collaboration has proved to be more productive in some ways; second, meetings will change but collaboration will continue before, during, and after meetings; third, learning and the delivery of training will be better assisted with virtual tools; and “video fatigue” will be recognized as a real thing.

“We need to get people out of their square boxes and into a shared sense of presence, to reduce cognitive load,” Nadella said. “One of my worries is that we are burning the social capital that got built up. We need to learn new techniques for building social capital back.”

Learn more about AI and the Work of the Future Congress 2020, GPT-3 inTechTalks and on the Microsoft AI blog, the Power Platform and Dataverse.","['openai', 'stunning', 'need', 'model', 'gpt3', 'nadella', 'computing', 'ai', 'cloud', 'microsofts', 'work', 'power', 'microsoft']","Nadella mentioned the GPT-3 general purpose language model from OpenAI, an AI lab searching for a commercial business model.
The Microsoft license is exclusive however, meaning Microsoft’s cloud computing competitors cannot access it in the same way.
The infrastructure will be available in Azure, Microsoft’s cloud computing offering, to train “extremely large” AI models.
“It’s stunning to see the capability,” of the GPT-3 model applied to this work, Nadella said.
Using the tools, “People can take their domain expertise and turn it into automation using AI capabilities,” Nadella said."
215,https://www.aitrends.com/ai-and-business-strategy/it-departments-find-timing-is-good-to-modernize-legacy-systems-ai-can-help/,IT Departments Find Timing is Good to Modernize Legacy Systems; AI Can Help,2020-11-24 22:59:42+00:00,"By AI Trends Staff

The pandemic era of increased remote work and powerful available AI is motivating IT departments to examine legacy software systems for renewal. A legacy application, as defined by Gartner, is “an information system that may be based on outdated technologies, but is critical to day-to-day operations.”

This process of renewal can also be called modernization and often involves a move from on-premises hardware to the cloud.

“Getting rid of legacy is a perennial issue, but modernization is a top issue now more than ever,” stated Diane Carco, president and CEO of management consulting company Swingtide and a former CIO, in a recent account in CIO.

According to The State of IT Modernization 2020 report from IDG and tech company Insight, 26% of organizations are only at the beginning stages of IT modernization, and 19% have made moderate progress.

The first step of modernization is to take an inventory of what you have, Carco and other experts suggest. “Without that, you’re just going to spend a lot of money and have very little to show for it at the end of the day,” stated Thomas Klinect, a senior director and analyst with tech research and advisory firm Gartner.

Klinect suggests investing in an enterprise complexity analysis tool, to see how data is flowing through the organization. Then base the modernization strategy on the analysis, to reduce the risk of failure.

Citrix Making Incremental Approach to Modernizing

An incremental approach toward modernizing the application portfolio is well-advised, suggests Meerah Rajavel, CIO of Citrix, the software company with virtualization products.

“Think big, but deliver in increments,” Rajavel stated. “The idea of an ERP modernization can seem very daunting, because they are not pebbles to move, they’re mountains. If you try to move the mountain, you’re not going to know everything you need to know to move it when you start. It’s an impossible goal, and there’s a lot of risks. The execution has to be iterative, and when it is, the business gets value along the way.”

To modernize the company’s sales platform, which needed to continue to run during the process, she worked with sales and marketing leaders to determine their priorities. She delivered those first while determining next steps on the path to a fully modernized platform.

To ensure IT decisions have an impact on business value, experts recommend adopting practices of IT governance. The key stakeholders of the organization take ownership of the IT decisions, and IT management is responsible to these stakeholders. This can especially help when organizations move to a microservice architecture, in which applications are collections of loosely-coupled services. In what is also called a ‘polyglot’ architecture, decisions over which technology stack and programming languages to use are delegated to service providers, such as Amazon with AWS.

Rely on IT Governance Practices for Coherent Microservice Architecture

“But if you have dozens, or hundreds, or thousands, of microservices and each one is using its own blend of technologies, your IT infrastructure can get wildly out of control,” stated tech veteran Patrick Walsh, now senior vice president of training and technology with SkillStorm, an IT workforce development firm.

Walsh recommends the move to a polyglot architecture be made consistent with sound IT governance practice, which would include a process for selecting the technologies required for each service so that the environment does not become unwieldy and difficult to support. “It shouldn’t just be a free-for-all,” he stated.

Verizon considered the overall business strategy into the development of its IT roadmap and modernization plans, stated Shoma Chakravarty, vice president of enterprise architecture at Verizon. Any system that did not fit the business needs for agility, elasticity and reliability was not considered.

Verizon set priorities based on which systems would deliver tangible wins when modernized, leaving until later systems that may be older but are not creating a drag on business. For example, IT focused on upgrades to its UI/UX technologies because the work aligned with the company’s efforts to deliver a strong user experience, even though those technologies were not the oldest in the portfolio.

It’s best to think of modernization initiatives that do not have a start and end date, but are an exercise in continuous improvement. “Modernization is not… a point-in-time effort, it’s ongoing,” Chakravarty stated. “Technology is moving so fast, that even what we built yesterday is going to be due for modernization in the near future.”

Federal Agencies Identifying Systems to Modernize

The federal government is home to many legacy applications in need of modernization, according to the GAO. Some use out of data languages, some have unsupported hardware and software and many have known security vulnerabilities, according to a recent account in fcw.com. Many federal agencies are in the process of identifying systems that need modernization, and working through procurement decisions to ensure components meet regulatory and mission requirements.

Not every application, service, platform, or infrastructure is a good candidate for migration to the cloud. “It’s important to understand what the agency is currently running, where it is running, the associated service-level agreements (SLA), the uptime required for various workloads, and the sensitivity of those workloads,” stated Steve Thamasett, a senior security field solution architect at CDW·G, a company offering IT products and support for the education, government and healthcare markets.

AI and machine learning can be employed to analyze the data agencies need to run their systems, with the goal of saving money and improving services to citizens. AI can especially help to assess cybersecurity, how vulnerable the system is to attack. Advanced AI techniques aim to thwart hackers intent on breaching system defenses.

Agencies are finding many ways to use these technologies. The VA is incorporating AI in several projects to reduce wait time, monitor customer service and predict patient outcomes. The State Department is using data analytics and machine learning to better understand what is happening at posts throughout the world—and to better understand the technology that will be needed in the future.

IBM Offering Targeted System for Modernizing Legacy Systems with AI

IBM is offering a targeted service for modernizing legacy systems with AI, called Accelerator for Application Modernization with AI. It is a suite of tools within the Cloud Modernization Service designed to reduce the overall effort and costs associated with modernizing by tapping advanced AI technology from IBM Research. The accelerator leverages continuous learning and interpretable AI models to adapt to the preferred software engineering practices of the client. The new offering and service, that IBM is calling “AI for IT,” is the culmination of years of research and development at IBM Research into how AI can be used to transform the IT lifecycle.

The IT leaders investigating the move to a hybrid, multicloud architecture see that automation and AI are needed to help reduce IT costs and preserve business continuity, according to IBM’s AI blog. Many businesses have moved simpler workloads with lower complexity to the cloud, but they are struggling with the task of modernizing mission-critical applications because of incompatibility with cloud-native architectural principles. These applications span generations of software technologies—decades—and contain millions of lines of code with intellectual property and expertise locked in them.

IBM’s accelerator is meant to provide a guide. The new Watson AIOps services is said to “reimagine” IT operations with AI by examining patterns in IT operations, removing noise, correlating problems across multiple data sources and making recommendations to fix them. The Watson AIOps service can correlate among diverse data sources to localize the root cause of a problem, create an explainable diagnosis and recommend the best course of action, IBM states.

Read the source articles in CIO, fcw.com and IBM’s AI blog.","['legacy', 'data', 'stated', 'modernization', 'departments', 'good', 'ai', 'help', 'business', 'technologies', 'technology', 'systems', 'service', 'modernize', 'timing']","By AI Trends StaffThe pandemic era of increased remote work and powerful available AI is motivating IT departments to examine legacy software systems for renewal.
The VA is incorporating AI in several projects to reduce wait time, monitor customer service and predict patient outcomes.
IBM Offering Targeted System for Modernizing Legacy Systems with AIIBM is offering a targeted service for modernizing legacy systems with AI, called Accelerator for Application Modernization with AI.
It is a suite of tools within the Cloud Modernization Service designed to reduce the overall effort and costs associated with modernizing by tapping advanced AI technology from IBM Research.
The accelerator leverages continuous learning and interpretable AI models to adapt to the preferred software engineering practices of the client."
216,https://www.aitrends.com/ai-insider/ai-autonomous-cars-contending-with-human-bullying-drivers/,AI Autonomous Cars Contending With Human Bullying Drivers,2020-11-24 22:46:23+00:00,"By Lance Eliot, AI Trends Insider

Human drivers are opting to bully nearby AI-based autonomous self-driving cars, according to increasing reports. Actions being taken by these overbearing drivers include cutting off a self-driving car while in traffic, nudging toward a self-driving car to get it to veer away, and otherwise opting to be devious toward these emerging autonomous driving systems.

Some human bullies are doing this for sporting purposes, relishing the thrill of causing the AI to take evasive action. Other bullies act in this manner because they figure all is fair in love and war, as it were, and believe that if the AI is going to be readily intimidated by other drivers than by-gosh it ought to be pushed around at will.

A few bullies seem to simply hate AI that drives a car and therefore perhaps are making these moves to showcase that humans are divine and superior to AI.

Is it legal for these human bully drivers to get away with this practice?

Time to consult the driving regulations. Here in California, the official Department of Motor Vehicles (DMV) Driver Handbook provides prescribed driving practices that everyone is supposed to comply with while driving on our state roadways.

Topics include safe driving methods when the roads are wet from rain (admittedly, we don’t get much rain, but when we do, California drivers are known to freak-out and drive crazily), and driving when there is a tough curve or when on a steep hill, plus what to do when driving nearby animal-drawn vehicles or coming up to railroad tracks. There are over 130 pages of crucial material in our DMV Driver Handbook, which licensed California drivers get tested on and presumably need to understand and are expected to obey (alright, I acknowledge that many don’t, but without those explicitly “you are on notice” regs, I think we’d agree that there might be chaos or at least even worse driving exploits than we already experience).

Furthermore, the booklet is chock full of stern warnings about how you can get a moving violation ticket or get charged with other kinds of criminal infractions, and potentially have your license revoked, if you don’t drive properly as codified in the laws and regulations stipulated in the California Vehicle Code (CVC).

Here’s a question for you to consider: Should the state handbooks on licensed driving include prescribed practices for human drivers to abide by when they are driving nearby to self-driving driverless autonomous cars?

Some say yes, namely that we need to update our myriad of state driving handbooks, and correspondingly each of the state sets of vehicle codes, to include specifics about expectations for human drivers when traffic-wise encountering autonomous cars. The logic being that since there are already explicitly stated expectations about driving nearby to motorcyclists, nearby to bicyclists, nearby to horse-drawn carriages, there ought to be a similar inclusion about driving nearby to driverless cars.

Those that say no, meaning that they don’t see a need to include driving practices depicting autonomous cars as a special roadway consideration for human drivers, argue that an autonomous car should be treated like any other car being driven on our streets and highways. In essence, they insist that the existing driving regulations are sufficient and that human drivers are to drive in the same manner that they drive when around other human drivers; thus, in this viewpoint, it makes no difference whether a car happens to be driven by a human or an AI system. Treat them all the same, they would contend.

For my framework about AI autonomous cars, see the link here: https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/

Why this is a moonshot effort, see my explanation here: https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/

For more about the levels as a type of Richter scale, see my discussion here: https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/

For the argument about bifurcating the levels, see my explanation here: https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/

What The Fuss Is All About

You might be somewhat perplexed on this subject because there are pundits that keep predicting that we’ll have only autonomous cars on our roadways and the human driving act will become as extinct as the dodo bird.

Imagine a world in which all cars are driverless cars. No need to worry about the wild antics of human drivers. No need to license human drivers. No longer pine away about distracted driving, drunk driving, and the other sad and dangerous driving larks that contribute to our annually disheartening car crash death rates and injuries. Instead, AI “drivers” will be whisking us to-and-fro, and presumably communicating with each other, via V2V (vehicle-to-vehicle electronic communications), ensuring that they don’t step on each other’s toes or get into traffic bogs.

Let’s set the record straight on this aspect.

For the foreseeable future, we’ll have a mixture of human-driven cars and AI-driven cars, and it won’t be some overnight transformation that we suddenly have all and only autonomous cars on our roadways. In the United States alone, there are over 250 million conventional cars, and they aren’t going to disappear simply due to the emergence of self-driving cars. Presumably, driverless cars will gradually be introduced, slowly and gradually increasing in numbers, which will likely lead to a gradual decreasing of human-driven cars, all of this taking place over many decades to come.

Also, there is an open question about whether we’ll ever fully get rid of human driving, which some argue that human driving should always be retained for those that wish to drive (claiming it is a right, but I’ll gently point out it is actually considered a privilege and not a right per se), plus concerns that if we don’t allow human driving then humans will become deskilled at the driving task, apparently alluding to the idea that we will become dependent and subservient to AI systems (a conspiracy theory that driverless cars are part of an elaborate AI-takeover plot).

Anyway, let’s, for now, agree that there will be a period during which human drivers and AI self-driving cars will be on our roads together. Will they play nicely with each other?

For AI developers, they are struggling with getting autonomous cars to gauge what human drivers do. There are AI developers that whine about the pesky human drivers and the sloppy and perilous driving tricks that humans employ while at the steering wheel of a car. It’s a lot easier to set up an AI system to drive a car when there aren’t human drivers in the mix of traffic. If we could somehow ban all human driving, it is presumed that we’d already had autonomous cars galore on our streets (or, at least be getting much closer to doing so).

For why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here: https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/

To be wary of fake news about self-driving cars, see my tips here: https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/

The ethical implications of AI driving systems are significant, see my indication here: https://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/

Be aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms: https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/

The Exploitation By Human Drivers Of Nearby Driverless Cars

There are two facets of the nature of human drivers that need to get considered in this matter:

Human drivers driving in their everyday questionable ways, and then perchance are doing so while nearby autonomous cars.

Human drivers realizing that an autonomous car is nearby, and purposely aiming to maneuver in a manner intended to play with, confound, overrule, or otherwise exploit the driverless car.

Many of the automakers and tech firms that are developing and starting to field public roadway tryouts of their autonomous cars are generally trying to deal with the first facet already. Earlier versions of driverless cars might have been coded in a fashion that the AI assumed there weren’t human drivers around, but the latest versions tend to be aiming to deal with human drivers that are nearby and that are (hopefully) driving in a normal human way, including being a commonplace greedy driver, being an uncaring driver, being a reckless driver, and so on.

The second facet is a bit of a twist on this topic, and one that has not yet gotten much attention.

For the second facet, human drivers are at times changing their driving behavior specifically and explicitly when they encounter an autonomous car. You might liken this to what some human drivers do when they see a novice teenager driving a car that’s marked as a driver instruction vehicle. Believe it or not, some human drivers do rather dastardly things toward such a vehicle.

For example, some “seasoned” drivers try to cut off the novice teenage driver, wanting to see how the neophyte will handle it. I suppose you could say it is a kind of experiment, perhaps driven by curiosity, wanting to gauge the reaction of the novice teenage driver. Other times a weathered driver might zip around the novice teenage driver, doing so because the novice is going no faster than the speed limit, and the zipping past driver has no time to waste and gets exasperated at trailing behind the so-called slowpoke. And so on.

In case you weren’t already aware, human drivers are now starting to do that same kind of “triggered” driving whenever they see that an autonomous car is nearby.

It’s usually pretty easy to spot an autonomous car, either due to the sensors protruding from the car, or sometimes the car has a branding message on the side that tells you it is a driverless car, or it could be that there’s no human in the driver’s seat (there is nearly always so far a human back-up driver in the car, sitting at the driver’s seat, thus, it would appear to be human-driven, though sometimes the back-up driver might be seated in the backseat and be using auxiliary driving controls).

During your first-ever encounter with an autonomous car, doing so when you are driving nearby it, your reaction usually involves a kind of open-mouthed gaping fascination of what you are seeing, but, since these tryouts are typically occurring in the same geofenced areas, over and over again, you eventually get more accustomed to seeing these driverless cars.

Apparently, with familiarity comes a bit of disdain.

Some like to cut off the driverless car and see what happens, akin to the same trickery pulled on a novice teenage driver. Others are annoyed that the self-driving car is abiding by the speed limit and do the zip around maneuver to leave the autonomous car in the dust. Pranks are being played on self-driving cars, such as coming up to a four-way stop, and the human driver seizing the right-of-way by rolling through the stop sign, meanwhile, the driverless car waits patiently because it has been coded to only take its dutiful turn when so permitted (even if it reached the stop sign first).

For more details about ODDs, see my indication at this link here: https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/

On the topic of off-road self-driving cars, here’s my details elicitation: https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/

I’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop: https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/

Expect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here: https://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/

Conclusion

How shall we as a society contend with these human drivers that want to prank the AI systems of driverless cars?

Merely telling people to stop their hunger-games efforts is not likely to do much. I think we all realize that human behavior on a societal scale is arduous to adjust and shift.

Here are some more armed options that have been voiced:

Make it the law. Put into the official driving regulations in each state that any such driving acts by human drivers against or toward an autonomous car are against the law.

By explicitly naming autonomous cars as a kind of protected class, as it were, this might get human drivers to be more accommodating. At least it would allow for clearly having provided a forewarning (otherwise those offending human drivers might say they didn’t know it was wrong to do such acts toward an autonomous car if you buy into that kind of rather a vacuous argument), and make those human drivers legally susceptible to getting a ticket or losing their human driving privileges.

Catch the scofflaws. Since human drivers know they can get away with these dreadful driving antics and not likely get caught (you can’t have traffic police on every corner), consider using the sensory data collected by autonomous cars as a means to legally pursue the untoward human drivers.

Keep in mind that autonomous cars will have cameras capturing whatever is visually happening around the driverless car. Perhaps the recorded video could be uploaded to your local police station that would then examine it and if a human driver did something wrong, a ticket would be issued, and ultimately after enough violations, their driver’s license would be revoked. This approach though has a lot of privacy concerns and other contentious elements to it, so don’t place your bets on this option for happening anytime soon.

Divide up our roads. Perhaps it is best to not have human drivers mixing with autonomous cars, and thus we could divide up our roads, having some streets or highways, or particular lanes, declared as for human-driven cars only and others for autonomous cars only.

The notion of dividing the roads is fraught with all kinds of problematic issues. The odds are that the cost to do this would be enormous. Is that really the best way to spend our limited infrastructure dollars? Also, some worry that this is a short-term fix not worthy of investment and that if indeed human driving is going to winnow down, time will solve this conundrum.

Toughen-up the AI. This viewpoint says that autonomous cars ought to be able to drive in the same way that other humans drive, fighting fire-with-fire, so to speak.

If a human driver is going to be aggressive toward an autonomous car, the driverless car shouldn’t necessarily back down (bowing to the human bully), and instead needs to showcase that it too is willing to play the dog-eat-dog driving game.

This will presumably “teach” human drivers not to mess with autonomous cars, and overtime society will adjust to allowing autonomous cars the same driving “courtesies” as they do among other humans.

Some worry though that if we toughen up the AI, we will have self-driving cars that will start driving in the same untoward manner that humans do. We’ll wake up one morning and find ourselves confronted by “angry” AI systems that dare human drivers, in the same way, that human drivers do, which could indeed be a consequence of Machine Learning, whereby the AI has merely mimicked driving behavior patterns based on the collected driving of how human drivers antagonistically maneuver.

One wonders, would bullying by AI driverless systems get those human drivers that we all already despise to become reborn as civil and polite drivers? If so, maybe it would be worth having slightly aggressive autonomous cars. Or, would we only end-up with lots more car accidents, road rages by human drivers against those in-your-face AI systems, and fail to gain the hoped-for reductions in car crash-related deaths and injuries?

We’re in the midst of placing a new set of “drivers” onto our roadways, and we need to figure out how the existing drivers will deal with these strangers, perhaps welcoming them with open arms or instead exploiting them like they are newbies that deserve a hazing.

Copyright 2020 Dr. Lance Eliot. This content is originally posted on AI Trends.

[Ed. Note: For reader’s interested in Dr. Eliot’s ongoing business analyses about the advent of self-driving cars, see his online Forbes column: https://forbes.com/sites/lanceeliot/]

http://ai-selfdriving-cars.libsyn.com/website","['autonomous', 'driver', 'drivers', 'ai', 'bullying', 'driving', 'cars', 'car', 'selfdriving', 'human', 'driverless', 'contending']","By Lance Eliot, AI Trends InsiderHuman drivers are opting to bully nearby AI-based autonomous self-driving cars, according to increasing reports.
For AI developers, they are struggling with getting autonomous cars to gauge what human drivers do.
Perhaps it is best to not have human drivers mixing with autonomous cars, and thus we could divide up our roads, having some streets or highways, or particular lanes, declared as for human-driven cars only and others for autonomous cars only.
This will presumably “teach” human drivers not to mess with autonomous cars, and overtime society will adjust to allowing autonomous cars the same driving “courtesies” as they do among other humans.
One wonders, would bullying by AI driverless systems get those human drivers that we all already despise to become reborn as civil and polite drivers?"
217,https://www.aitrends.com/ai-and-business-strategy/ai-applied-to-aquaculture-aims-for-improved-efficiency-healthier-fish/,"AI Applied to Aquaculture Aims for Improved Efficiency, Healthier Fish",2020-11-24 22:11:32+00:00,"By AI Trends Staff

Fish farmers in Norway are using AI models designed to cut costs and improve the efficiency of their efforts to raise salmon, one of the country’s major exports, thanks to efforts of the Norwegian Open AI Lab.

The efforts are part of a growing trend to apply AI automation to aquaculture, which is the farming of fish, crustaceans, mollusks, aquatic plants, algae and other organisms.

The AI models are designed to optimize feeding, keep the fish clean and healthy, and help companies make better decisions regarding farm operations, according to an account in WSJ Pro. The Norwegian Open AI Lab is run by Norwegian telecommunications carrier Telenor AS A, which along with other companies, provides technology services such as testing of 5G mobile connectivity, to salmon farms.

Salmon exports in 2019 totaled some $11.3 billion, according to the Norwegian Seafood Council. Representing fisheries and fish farm industries, the trade group reported that fish exports increased about one percent between January and August 2020.

Under pressure to improve environmental standards and reduce waste, the industry has been working with tech companies to start providing AI tools to Norway’s fish farms.

For example, Alphabet Inc.’s Tidal initiative is partnering with seafood company Mowi AS A to use AI to analyze and monitor fish and environmental conditions. Microsoft, Swiss engineering firm ABB Ltd. and fish farm operator Norway Royal Salmon ASA are piloting an AI solution to remotely track fish populations. And IBM has created a machine-learning tool that predicts outbreaks of sea lice, which are parasites that threaten farmed fish.

The Norwegian Open AI Lab has based its AI initiatives on neural networks, which learn based on large sets of training data, and a type of AI known as “tiny machine learning,” which encompasses hardware and software capable of performing on-device sensor data analytics at extremely low power.

One of the neural network applications is designed to help fish farm workers understand salmon feeding behavior. It analyzes data from underwater cameras to determine behavior changes that signal the fish are no longer hungry, according to Bjørn Taale Sandberg, head of Telenor Research. Some 40% of the cost of fish farming is in feed.

The company is also developing small computers that could remain on site at a fish farm and eventually make decisions automatically based on what cameras detect. The computers use “tiny machine learning,” which can be especially useful for remote fish farms where the internet networks might not be strong. The system could automate some decisions without connecting to the shore, reducing manual labor required to monitor the farm.

“In the ocean or in a wild fiord, you want to avoid the number of times you visit the farm to check for problems,” Sandberg stated.

Opportunity for AI in Aquaculture Attracting Startups

The trend of increased application to AI to aquaculture has attracted some startup firms who see an opportunity, as outlined in a recent account from The Fish Site.

For example, Observe Technologies offer to track measurable patterns when stocks are feeding. Their goal is to provide farmers empirical and objective guidance on how much to feed. The system aggregates data from sources including sensors, cameras, and acoustics, then extracts relevant information for its algorithms, and sends alerts to farmers for when to increase or decrease feeding. The software learns as it goes, getting smarter over time, and can be operated remotely.

Another player called eFishery has developed a system which uses sensors to detect hunger levels in shrimp and fish, controlling dispensers which release the right amounts of food; the company claims this can reduce feed costs by up to 21%. Founded in 2013, the company is based in Indonesia.

Elsewhere, Japanese and Singaporean aquaculture technology firm Umitron Cell offers a smart fish feeder which can be controlled remotely. “Farmers are given data-driven decision-making advice to optimize feeding schedules. This reduces waste, improves both profitability and sustainability while offering users a better work-life balance by eliminating the need to be out in the water in dangerous conditions,” stated Umitron product manager Andy Davison.

Among its recent projects, Umitron is leading a project to develop a data platform for shrimp farming in the ASEAN region by using IoT and AI technologies. The project aims to improve shrimp farming productivity and working conditions while conserving the natural environment.

The company also recently announced the Pulse mobile application for Android users, to provide a high-resolution ocean map of critical environmental parameters such as water temperature, chlorophyll, dissolved oxygen, salinity and wave height.

Startup XpertSea focuses on optimizing the economics of harvesting, which most farmers gauge based on educated guesses. The company’s product uses computer vision and AI to calculate the growth of shrimp, helping farmers predict the most profitable harvest periods. Deep learning techniques are used to pinpoint timeframes by continuously using machine learning on historical growth cycle data.

“The company’s Growth Platform provides online management software which uses AI to capture, ingest, store and process field data to give farmers and industry experts actionable, data-driven insights throughout the whole production cycle,” stated Valérie Robitaille, CEO of XpertSea. “This platform is used by farmers but also feed, health, genetics, and certification enterprises to provide data-driven services to farmers.”

Another part of the product, XperCount, collects critical animal data by using cameras and machine learning which is applied to count, size, and weigh animals in seconds.

The company reports having over 600 farmers and other customers, and in the past year has processed over 2.3 billion animal data points and optimized the performance of 6,000 crops.

Strides are being made in the automation of aquaculture to produce more seafood to feed the world population while residing the environmental footprint of operations.

Read the source articles in WSJ Pro and The Fish Site.","['company', 'efficiency', 'farmers', 'data', 'aquaculture', 'feeding', 'fish', 'learning', 'improved', 'ai', 'farm', 'norwegian', 'based', 'aims', 'applied', 'healthier']","Representing fisheries and fish farm industries, the trade group reported that fish exports increased about one percent between January and August 2020.
Microsoft, Swiss engineering firm ABB Ltd. and fish farm operator Norway Royal Salmon ASA are piloting an AI solution to remotely track fish populations.
One of the neural network applications is designed to help fish farm workers understand salmon feeding behavior.
The company is also developing small computers that could remain on site at a fish farm and eventually make decisions automatically based on what cameras detect.
Elsewhere, Japanese and Singaporean aquaculture technology firm Umitron Cell offers a smart fish feeder which can be controlled remotely."
218,https://www.aitrends.com/healthcare/startup-cognoa-seeks-fda-approval-for-device-with-ai-to-help-diagnose-autism/,Startup Cognoa Seeks FDA Approval for Device with AI to Help Diagnose Autism,2020-11-20 00:31:38+00:00,"By John P. Desmond, AI Trends Editor

Dennis Wall is a doctor who started a software company to pursue a noble cause.

He is working to apply machine learning to the diagnosis of autism at an earlier age than is typically the case today.

Dr. Wall is Associate Professor of Pediatrics, Psychiatry and Biomedical Data Sciences at Stanford Medical School and co-founder of Cognoa, supplier of a digital therapeutic product incorporating AI for assessing behavioral health.

Specifically, the first generation product is aimed at helping to diagnose autism at an earlier age. Dr. Wall has pioneered the use of machine learning and artificial intelligence for fast, quantitative and mobile detection of neurodevelopmental disorders in children, as well as the use of machine learning systems on wearable devices, such as Google Glass, for real-time “exclinical” (‘out of’ clinic) therapy.

He is in the top ten of the world’s top 30 autism researchers. He completed his PhD at the University of California, Berkeley and a National Science Foundation postdoctoral fellowship in Computational Genetics at Stanford University before joining the faculty at Harvard Medical School, where he taught from 2004 to 2014.

Challenges to Autism Diagnoses

Autism spectrum disorder (ASD) diagnosis is a field that sees multiple challenges. A 2018 study published in Pediatrics found that the Developmental and Behavioral Pediatrics workforce is struggling to meet the demands for service, with many projected to retire in coming years. A 2016 study on PubMed.gov found that a two-year wait time from initial concerns to diagnosis of autism spectrum disorder was typical. A 2017 study from the National Center for Biotechnology Information found disparities in access to diagnosis for ASD based on race.

The average age of a child diagnosed with autism is 4.5 years, Dr. Wall said in an interview with AI Trends. He founded the company to try to speed up access to the needed diagnostics services, and have them happen at an earlier age. When he was teaching at Harvard, he began looking at a machine learning analysis of data from children admitted to Boston Children’s Hospital. He studied how the healthcare system was diagnosing children with autism. “I trained myself on the standard of care,” he said.

In that process, a behavioral observation by a trained clinician takes 45 to 60 minutes. The child goes through a specific set of activities using a box of toys. After that, the clinician hand scores the child on a range of behaviors; doctors refer to that to help make a diagnosis. “The number of kids who need to be seen far outnumber the number of available clinicians. And the opportunity for subjectivity to enter the equation is enormous. The system is set up for failure,” Dr. Wall said.

He had a better idea. “I thought these things can be fixed through machine-learning based prediction, with quantitative models that produce a distribution with levels of confidence,” he said.

The Cognoa app processes data from three questionnaires: from the pediatrician, the parents, and an autism specialist who watches two short videos of the child filmed by parents in the Cognoa app. The pediatrician receives a result from the AI program, which they use to help make a diagnosis.

“It can be done quickly and costs almost nothing more to get more ratings on the same child,” said Dr. Wall.

The AI and ML models within his tool are now being reviewed for approval by the Food and Drug Administration as a medical device. “It would be the first device ever approved by the FDA as a tool for autism,” Dr. Wall Said. So far, so good, with Cognoa’s app surpassing all the FDA benchmarks.

FDA Approval Likely to Lead to Insurance Company Reimbursement

The approval means the diagnosis would be more likely to be reimbursed by medical insurance and thus more widely available. “That’s important to me,” Dr. Wall said, and it should be helpful to parents with lower income, who tend to have more difficulty accessing healthcare services.

He sees the AI as important to making this tool more accessible. “A key thing about the AI is that it is a democratized way of providing consistent access to healthcare,” he said.

As to when the FDA is expected to make a decision, Dr. Wall said, “I hope it will be soon, hopefully early in 2021.” In a recent account in Forbes, Cognoa CEO Dave Happel said he expected the device to be approved in the second half of 2021.

Cognoa did get Breakthrough Device designation status from the FDA in 2019, a program to speed development if the device is more effective in diagnosing “debilitating human disease or conditions.”

To get to the market, Dr. Wall hopes the tool will be used by pediatricians, who number 64,000 in the US. “It should be used in the primary care setting, because it will reach kids who are younger,” he said. “The goal is to bring down the age of diagnosis from 4.3 to two years of age,”

Cognoa is communicating with healthcare insurance companies to recognize the new diagnostics services.

Getting FDA approval is important to gaining trust in the app, especially from parents. “The field of AI in medicine is extremely new and delicate. There are going to be issues of trust and assurances around accuracy and privacy. The FDA is grappling with this too, how to regulate machine learning and AI in a medical device tool that is making a recommendation.”

Autism Services Market Seeing Infusion of Capital

The market is expected to get more crowded. Cognoa has raised some $60 million since its founding in 2013. The market for autism products and services has received an infusion of capital, fueled by an increase in the number of children diagnosed with autism, and the passage of laws in 50 states that require insurance coverage to cover autism services, according to Ronit Molko, CEO of Empowering Synergy, a consultant on private equity investments in behavioral health companies, quoted in the Forbes article. More than 100 deals involving autism-related companies have recently been reached. “It’s like a feeding frenzy,” Molko stated.

The majority of autism companies are service providers that teach social, language, and behavioral skills, stated Molko, who cofounded one such business, Autism Spectrum Therapies, before selling it in 2014. Companies making diagnostic tools like Cognoa are rare, she said.

The potential of AI to cut down the waiting time for autism diagnoses would help children get services and be enormously helpful to parents. “The impact on life, and the family, and that individual child is huge,” she stated.

Read the source articles and information in Pediatrics, on PubMed.gov, from the National Center for Biotechnology Information and in Forbes.","['diagnose', 'seeks', 'cognoa', 'wall', 'ai', 'device', 'help', 'diagnosis', 'tool', 'approval', 'dr', 'autism', 'child', 'startup', 'fda']","The average age of a child diagnosed with autism is 4.5 years, Dr. Wall said in an interview with AI Trends.
The system is set up for failure,” Dr. Wall said.
“It can be done quickly and costs almost nothing more to get more ratings on the same child,” said Dr. Wall.
“It would be the first device ever approved by the FDA as a tool for autism,” Dr. Wall Said.
“That’s important to me,” Dr. Wall said, and it should be helpful to parents with lower income, who tend to have more difficulty accessing healthcare services."
219,https://www.aitrends.com/robotics/recycling-robots-with-ai-helping-to-improve-financial-viability-of-recycling/,Recycling Robots with AI Helping to Improve Financial Viability of Recycling,2020-11-20 00:21:45+00:00,"By AI Trends Staff

When China closed off the import of recycled waste in 2018, it was bad news for the nation’s 600-plus recycling facilities, processing some 67 million tons of waste. In response, industry in western countries turned to robotic technologies to strengthen processing and try to find a solution.

AMP Robotics of Louisville, Colorado, founded in 2014, has sold or leased 100 of its AI-powered robots to more than 40 recycling plants in North America, Europe and Japan, according to a recent account in Forbes. The robots cost upwards of $300,000 to buy or $6,000/month to lease. Forbes estimated that the company’s revenue will reach $20 million this year, double its 2019 revenue of $10 million. Recycling is estimated to be a $6.2 billion revenue market in the US. Recycling facilities are trying to figure out how to get more out of the waste, most of which still ends up in landfills.

AMP was founded by Matanya Horowitz, who has a PhD in robotics from Caltech, and who bootstrapped the business to start and now has raised $23 million in venture funding, with its latest round in November 2019 led by Sequoia.

At Caltech, Horowitz got interested in deep learning, for its ability to empower computer vision to mimic human sight. Rather than pursue self-driving cars like many of his classmates, he decided to pursue recycling, seeing it as an industry slow to adopt new technology and presenting an opportunity to improve the environment.

Faster computer processing speeds opened up the potential for deep learning. Horowitz focused on single-stream recycling, in which newspaper, cardboard and plastics are all mixed together, as the area to apply deep learning. He could teach the robots how to recognize objects based on colors, shapes, textures and logos.

AMP began licensing its AI software to Canadian firm Machinex, a leader in recycling equipment, in 2018, enabling that company to get to market more quickly.

The COVID-19 pandemic has boosted the market for recycling, with more online ordering resulting in more packages from Amazon in the waste stream, and plant operators having a more difficult time hiring needed help.

Labor costs vary by region, but most recycling workers make $25,000 per year. Robots can pick up 80 pieces of material per minute versus 50 for a human, according to the Forbes account, so that each machine can handle the work of nearly two employees. Adding other employment costs, and two people can cost the facility $70,000 per year; the robot pays off in three or four years as a result.

After China banned recycled material from coming in, much of the US recycled stream was not pure enough. The EPA estimates the rate of recycled materials in the US stands at 35 percent. AMP’s automation helps to produce a cleaner stream with more market value. “AMP is making a lot of headway cleaning up the end bales that are sold and getting them to be sold for a higher value,” stated recycling consultant Juri Freeman of Resource Recycling Systems.

In the year of the pandemic, orders are picking up. “Recycling facilities are looking at placing larger orders now,” Horowitz stated to Forbes. “Instead of one robot, they’re saying, ‘Maybe we need six out of the gate.’”

Since more waste is electronic, consisting of smartphones, tables, digital home appliances and electric vehicles, containing material such as gold, silver, platinum and cobalt, taking products apart will become increasingly important, suggests an account in the Financial Times.

The automotive sector is a prime target for more recycling, with cars increasingly built with more electronics and electric vehicles containing higher value raw materials, in the view of Shahin Rahimifard, professor of sustainable engineering at the UK’s Loughborough University.

“Our current way of recycling cars focuses on extracting three core metals — steel, copper and aluminium — that account for around 60 per cent of the value,” stated Rahimifarde. “In electric cars, most of the value will be the precious metals like gold and platinum which, by weight, could be only four or five per cent. So, we are moving from having 60 percent of the weight giving 80 per cent of the recycled value, to more like 4 per cent.” Prof Rahimifard’s team is developing a robotic dissembler that can deal with this fundamental shift in recycling economics.

Low Cost Recycling Robot Built with Raspberry Pi by UK Researchers

A low-cost Raspberry Pi computer can be set up with AI, high-resolution cameras and robots, to sort through rubbish and reduce waste going to landfills. Engineers from Liverpool Hope University built the system with the Raspberry Pi 3 model, combined with optical sensors and computer vision algorithms, to create a tool that can distinguish between paper, glass, plastic, metal and cardboard, according to an account in ZDNet.

“It is designed to be integrated with any of the robotic systems that are on the market at the moment,” stated Karl Myers of Liverpool Hope University’s department of mathematics and computer science. “The Raspberry Pi sends a signal via serial communication to the robotic arm about the position of the recyclables, and the robot just grabs the object.”

The researchers said that the algorithm achieved up to a 92 percent success rate, suggesting this makes it viable for commercial use.

The UK generated 229.9 million tons of solid waste in 2017, and some 47 percent was recycled. The UK’s Department of the Environment has set a goal of pushing recycling rates to 50 percent for 2020.

The Liverpool Hope University AI team trained the algorithm with a database of 3,500 different

images of rubbish, combining a resource called TrashNet with images from Google. The researchers used transfer learning, an approach in machine learning that enables the AI system to store the knowledge gained solving one problem, and apply it to solve a different but related one.

“It removes the individual learning paradigm,” stated Myers. “In this case, it means that no training whatsoever is required for the system – it will use all the images and past knowledge from other datasets and apply it to the problem it is working on. It’s essentially a plug and play.”

One drawback is that the Pi-based recycling robot is slower than humans. Myers hopes the low cost will help make his system an attractive choice for many recycling plants.

Read the source articles in Forbes, in the Financial Times and in ZDNet.","['helping', 'viability', 'market', 'stated', 'robot', 'recycled', 'learning', 'financial', 'ai', 'waste', 'value', 'robots', 'recycling', 'improve', 'million']","By AI Trends StaffWhen China closed off the import of recycled waste in 2018, it was bad news for the nation’s 600-plus recycling facilities, processing some 67 million tons of waste.
AMP’s automation helps to produce a cleaner stream with more market value.
“AMP is making a lot of headway cleaning up the end bales that are sold and getting them to be sold for a higher value,” stated recycling consultant Juri Freeman of Resource Recycling Systems.
“Our current way of recycling cars focuses on extracting three core metals — steel, copper and aluminium — that account for around 60 per cent of the value,” stated Rahimifarde.
It’s essentially a plug and play.”One drawback is that the Pi-based recycling robot is slower than humans."
220,https://www.aitrends.com/ai-in-business/ai-hybrid-cloud-and-quantum-computing-seen-by-ibms-krishna-as-shaping-it/,"AI, Hybrid Cloud and Quantum Computing Seen by IBM’s Krishna as Shaping IT",2020-11-19 23:51:21+00:00,"By AI Trends Staff

AI, hybrid cloud computing and quantum computing will shape the future of the IT industry, according to IBM CEO Arvind Krishna, speaking at the virtual Nikkei Global Management Forum in Tokyo last week.

Citing an estimate that AI could add up to $16 trillion to global productivity over the next decade or so. “We are only four percent of the journey there,” Krishna said, in an account from Nikkei Asia, adding “Our assertion is that every company will become an AI company.”

Krishna took over the top job at IBM in April, becoming its 10th CEO, succeeding Ginny Rometty, who was named IBM’s president and CEO in January, 2012. IBM made cloud services a bigger priority during Rometty’s tenure; she joined the company in 1981.

Krishna, who joined IBM in 1990, worked on security software and information management, and is the co-author of 15 patents. He has an undergraduate degree from the Indian Institute of Technology, Kanpur, India, and a Ph.D. from the University of Illinois at Urbana-Champaign.

Krishna is known as the “principal architect” of IBM’s biggest acquisition ever, Red Hat, which the company bought for $34 billion in late 2018, according to an account in Reuters.

Speaking at the Nikkei event, Krishna said the hybrid cloud approach has more value for IBM clients, and that the technology will “unlock two and a half times more value than a singular public cloud.”

He is also a believer in quantum computing, stating it “can unlock many benefits for both industry and society that are beyond the reach of today’s computers.” And he sees it coming soon. “We believe within three to five years you can begin to tackle problems that are beyond the reach of normal computers,” Arvind stated. He cited some potential applications. “You can solve problems around molecules like lithium hydride, which is an element in many electric batteries,” he stated.

Today quantum computers generate too many errors; this needs to be addressed before quantum computing can approach its potential, according to Arvind.

IBM Has Defined an AI Governance Framework

IBM is also concerned with AI being perceived as fair, transparent, and accurate. Toward this end, IBM has defined an AI governance framework centered around the IBM internal AI ethics board, co-led by Francesca Rossi, AI Ethics Global Leader for IBM and the next president of the Association for the Advancement of AI, according to an account written by Rossi in Harvard Business Review.

IBM has also defined its policies around AI, releasing in 2018 its Principles for Trust and Transparency, a guide to its approaches. “These principles outline our commitment to using AI to augment human intelligence, our commitment to a data policy that protects clients’ data and insights gleaned from their data, and a commitment to a focus on transparency and explainability to build a system of trust in AI,” stated Rossi.

IBM is working with partners on these efforts, having recently joined the European Commission’s (EC) High-Level Expert Group on AI, designed to deliver ethical guidelines for trustworthy AI in Europe. These are being used to guide possible future regulations and standards for AI.

IBM has also contributed open-source toolkits to “move the needle” on AI Trust. This included a 2018 contribution of AI Fairness 360 (AIF360), that allows developers to share state-of-the-art codes and datasets related to AI bias detection and mitigation. Since AIF360, IBM Research has released additional tools to advance transparency, including AI Explainability 360 (AIX360), which supports understanding and innovation in AI explainability,

“Overall, only a multi-dimensional and multi-stakeholder approach can truly address AI bias by defining a values-driven approach,” stated Rossi.

Emergent Alliance Co-Founded by Rolls Royce Aims at COVID-19 Recovery

In a specific effort in Europe, IBM has signed a letter of intent to join the Emergent Alliance, a group of technology companies and their data science professionals who share a belief that AI can help accelerate economic recovery from COVID-19.

Members of the Alliance state they are committed to collaborate on shared datasets, platforms and tools, with all innovations models and insights from the effort to be released freely to the public via the Emergent Alliance website and GitHub, according to an account on IBM’s Journey to AI blog.

Committed to openness, transparency, and trust, the alliance collaborates on shared datasets, platforms and tools, with all innovations, models and insights released freely to the public via the Emergent Alliance website and GitHub.

The IBM Data Science and AI Elite team is working on solving real-world problems with R²Data Labs, Rolls-Royce’s nucleus for AI innovation. Dr. Klaus Paul, team lead at Rolls-Royce R²Data Labs in Germany, focuses on data and AI innovation for all sectors of the Rolls-Royce Group.

IBM data scientist Erika Agostinelli, a member of the Emergent team, outlined three efforts as part of a practical response: produce a risk index, considering factors including infection rates of COVID-19 in a specific region; a “pulse” of that region focusing on sentiment data analytics, measured by media consumption or tourism activity may prove useful; and a “simulation track,” which creates scenarios to allow government or business to assess potential conscience of lockdown measures are areas and businesses.

“AI needs to be trusted,” stated Agostinelli. “Our goal is to create something transparent and interpretable so that anyone in the community can use the tools.”

Read the source articles and information at Nikkei Asia, Reuters, Harvard Business Review, at AI Fairness 360 and at IBM’s Journey to AI blog.","['according', 'alliance', 'data', 'trust', 'hybrid', 'ibm', 'stated', 'krishna', 'computing', 'ai', 'nikkei', 'ibms', 'seen', 'cloud', 'transparency', 'shaping', 'quantum']","By AI Trends StaffAI, hybrid cloud computing and quantum computing will shape the future of the IT industry, according to IBM CEO Arvind Krishna, speaking at the virtual Nikkei Global Management Forum in Tokyo last week.
Today quantum computers generate too many errors; this needs to be addressed before quantum computing can approach its potential, according to Arvind.
IBM Has Defined an AI Governance FrameworkIBM is also concerned with AI being perceived as fair, transparent, and accurate.
IBM has also contributed open-source toolkits to “move the needle” on AI Trust.
The IBM Data Science and AI Elite team is working on solving real-world problems with R²Data Labs, Rolls-Royce’s nucleus for AI innovation."
221,https://www.aitrends.com/ai-insider/consequences-of-bike-riding-kids-amidst-ai-autonomous-cars/,Consequences Of Bike Riding Kids Amidst AI Autonomous Cars,2020-11-19 23:44:44+00:00,"By Lance Eliot, the AI Trends Insider

What has happened to all those cheery children that used to relish going on carefree bike rides? Doing so provided a fleeting opportunity to experience a revered sense of independence while seeking out new and joyful adventures in a local neighborhood or nearby community.

You might already intuitively know that bike riding by kids has steadily been declining. Recent studies indicate that children in the United States are no longer riding their bikes as much as they used to do so, whereby the annual numbers continue to slope further downward, slipping further and further into less and less bike riding by the younger generations.

Besides this being a bad sign for the bike makers, especially since bike riding tends to be acquired when young and then carried over into adulthood (thus, the pipeline is thinning), the other concern is that the lack of bike riding is not being replaced by some other equal or better physical activity.

It would be one thing if kids opted to say go running or jogging or used the bike riding time to play a sport, but it appears that the bike riding time is giving over to (in essence) motionless sitting.

By motionless sitting, I mean that some claim that the youth of America are using their bike riding time to instead play online video games. Though playing a video game can exert some energy and spirit, most would concede that it is not the physical equivalent of the health benefits from riding a bike. Video game playing is a more sedentary task, and beyond building up perhaps stronger finger muscles to manipulate the game controls, it seems doubtful that your child will have generated impressive ripped abs or tiger muscles after hours of Fortnite playing.

Apparently, the volume of kids regularly riding their bikes has decreased by about a million such children over the last four years according to published statistics. Reported sales of children’s bikes dropped by 7.5% in quantities sold over the last year. Regular biking is considered taking rides around your neighborhood, riding to the park or events, and also encompasses the heads-down competitive bike riding realm too. Per national stats, bike riding tends to be done in urban areas (71%), and slightly more so in daylight (51%) than in darkness.

Health and fitness proponents, along with bike makers and retailers, say that this disturbing trend of less bike riding should be a banner alerting us that something needs to be done to get more kids on bikes. The idea is twofold, more kids riding bikes, plus kids riding their bikes more often.

I’d like to offer a modest proposal about why kids aren’t riding their bikes as much, along with a proposed solution that might be coming down the pike, namely, it has to do with human drivers as the problem and potentially having self-driving cars as the solution.

Let’s unpack the matter and see.

For my framework about AI autonomous cars, see the link here: https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/

Why this is a moonshot effort, see my explanation here: https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/

For more about the levels as a type of Richter scale, see my discussion here: https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/

For the argument about bifurcating the levels, see my explanation here: https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/

Scary To Ride A Bike In Today’s Driver Unfriendly Environment

When my children started riding their bikes, I realized that there was a big difference between riding bikes when I was a child and the current environment of riding on today’s traffic-clogged and traffic endangering streets.

The significant difference that I observed was the emergent wildness of human drivers in their cars, most of whom seem to no longer care much about bike riders, perhaps even more so kids on bikes (children are often lower profile than adults, can be harder to see or predict in terms of movement, and are less aware of being mindful of cars and wayward car drivers).

In an earlier era, it seemed that car drivers were conscientious about watching for and avoiding bicycle riders. Nowadays, just as drivers won’t pull over when they hear a siren or see an oncoming ambulance that is flashing its lights, it seems that the mainstay of drivers is also giving short shrift to bike riders.

In a bit of irony, or perhaps just plain infuriatingly, the growth of bike lanes does not seem to have prodded drivers to be more cautious about bike riders.

On a daily basis, I see drivers that weave into bike lanes. I see cars that decide to park in a bike lane, apparently because it is easy to do so since naturally the bike lane is otherwise free of any obstructions (duh!). All in all, it appears by the informal observation that bike riders aren’t getting sufficient attention from car drivers. It is as though the painted line that marks the bike lane is invisible, or maybe the line is an attractor for some drivers, but in any case, a marking on the street is not enough of a barrier to prevent a multi-ton car from being steered into the path of a bike rider, sadly so.

Going beyond my intuition, consider the statistics reported by the Center for Disease Control (CDC), which tracks and reports on bike-related injuries and deaths.

The CDC’s latest available numbers are that about 1,000 bicyclists are killed each year in the U.S., and around 467,000 bicyclists are injured. Besides the human toll, the CDC also estimates that the dollar cost to our society is approximately $10 billion as a result of medical expenditures and lost productivity in the aftermath of a biking incident.

I realize that you might be saying right now that you are a very safe driver and would never cut-off or barge into a bike rider. In fact, you might even be voicing a complaint that bike riders often are impolite, out-of-control, and do not abide by the rules of the road.

Let me make clear that I am not saying that all drivers are bike seeking monsters, and nor am I suggesting that all bike riders are blameless in terms of getting themselves into hot water. The world of bike riding is a two-way street, metaphorically meaning that the bike riders need to do their part in riding safely, just as the human drivers need to do so too.

In any case, I noticed that my children and their friends all realized the dangers of bike riding in today’s world, and it contributed, I believe, toward their hesitancy to go bike riding. Yes, even young kids can be that sensible.

How much did the push away from bike riding due to the fear of crazed car drivers add to the pull of spending time instead of playing video games? I can’t pin it down per se, though I think it is fair to assert that if bike riding was able to be done with much less looking over the shoulder, I’m pretty sure there would be a lot more biking riding going on.

For why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here: https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/

To be wary of fake news about self-driving cars, see my tips here: https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/

The ethical implications of AI driving systems are significant, see my indication here: https://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/

Be aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms: https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/

Self-Driving Cars Might Be A Spur Toward Bike Riding

Let’s assume for a moment that we will ultimately have self-driving driverless cars.

I’m referring to autonomous cars, ones that have the AI system doing the driving and there is no human driving involved. Most of today’s modern cars require co-sharing of the driving task between the automation and a licensed human driver, considered Level 2 and Level 3 cars, while the hope is to eventually get to Level 4 and Level 5 as fully autonomous cars.

In theory, a properly developed, tested and fielded autonomous car will be as safe or presumably safer than human drivers. When it comes to respecting bike lanes, overall it would seem likely that the AI system would be more adherent to staying out of the bike lanes, and also be more attentive to the actions of bike riders.

As such, the increase in roadway safety might lure bike riders back onto the streets, including kids that could be newly introduced to bike riding that otherwise hadn’t tried it, and also for children that started bike riding but gave up in concern for getting injured or killed.

I know that some view this idea with either skepticism or argue that the point is actually counterintuitive.

If we really do end-up with self-driving cars, and those vehicles are readily prevalent, and they are relatively low in cost to use for ridesharing or ride-hailing purposes, it would lead one to assume that people, including kids, will ride bikes even less so than now, and will be using cars more so than now. Parents won’t need to drive their kids to school anymore since the autonomous car will take on that duty. Thus, the allure and ease of using a car will be so inviting that it will ruin any remaining impetus to go bike riding.

Well, yes, that could happen.

I’m going to use the glass-is-half-full viewpoint and claim that the enhanced roadway safety will spark parents and kids to revisit bike riding. Furthermore, the added convenience of sitting in a car to get someplace will be an instigator for parents to get their kids to do some kind of physical activity, including bike riding.

On top of that, the desire to use online video games might be somewhat satiated (if that’s possible) by being able to play while inside the driverless car, which will readily have fast internet access such as 5G. This could shift the time devoted to video game playing from the periods when kids today are at-home play online games to instead use some of that time for bike riding, regularly.

It could be the saving grace for rejuvenation of getting more kids bike riding and more of the time.

For more details about ODDs, see my indication at this link here: https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/

On the topic of off-road self-driving cars, here’s my details elicitation: https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/

I’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop: https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/

Expect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here: https://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/

Conclusion

Which do you think we’ll see:

A continued downward spiral of kids not going bike riding, and for which this will hit rock bottom once the advent of self-driving cars arises (that’s the pessimistic view), or

Do you think (perhaps optimistically) that the advent of self-driving cars will make our roads safer for bike riders and encourage kids to get outside and ride their bikes?

Though it’s a guess on my part, I’m going to keep my bikes in good shape and ready for a future when bike riding becomes a grand everyday activity, once again, fueled in some ways by the AI-based autonomous cars that will be cruising our roadways.

Copyright 2020 Dr. Lance Eliot This content is originally posted on AI Trends.

[Ed. Note: For reader’s interested in Dr. Eliot’s ongoing business analyses about the advent of self-driving cars, see his online Forbes column: https://forbes.com/sites/lanceeliot/]

http://ai-selfdriving-cars.libsyn.com/website","['autonomous', 'bikes', 'drivers', 'bike', 'kids', 'amidst', 'ai', 'cars', 'selfdriving', 'car', 'human', 'consequences', 'riding', 'riders']","Besides this being a bad sign for the bike makers, especially since bike riding tends to be acquired when young and then carried over into adulthood (thus, the pipeline is thinning), the other concern is that the lack of bike riding is not being replaced by some other equal or better physical activity.
Regular biking is considered taking rides around your neighborhood, riding to the park or events, and also encompasses the heads-down competitive bike riding realm too.
The idea is twofold, more kids riding bikes, plus kids riding their bikes more often.
In any case, I noticed that my children and their friends all realized the dangers of bike riding in today’s world, and it contributed, I believe, toward their hesitancy to go bike riding.
It could be the saving grace for rejuvenation of getting more kids bike riding and more of the time."
222,https://www.aitrends.com/ethics-and-social-issues/compute-power-concentration-creating-a-digital-divide-in-ai-research-study-finds/,"Compute Power Concentration Creating a Digital Divide in AI Research, Study Finds",2020-11-19 22:10:35+00:00,"By AI Trends Staff

In the era of deep learning, cloud compute power is being concentrated in the hands of elite universities, at the expense of efforts to “democratize” access to AI technology.

A team of AI researchers from Virginia Tech and Western University conducted an analysis of 171,394 research papers from 60 prestigious computer science conferences to reach their conclusions, according to an account in VentureBeat.

The effect of the concentration is to crowd out students at mid- to low-tier research organizations, according to the analysis of accepted papers on topics including computer vision, data mining, machine learning and natural language processing.

Noting that the rise in use of GPUs since 2012 has resulted in wider availability of the powerful computing needed for AI research, the papers’ authors state, “We find that AI is increasingly being shaped by a few actors, and these actors are mostly affiliated with either large technology firms or elite universities.” They suggest this divide will need to be bridged with the help of government policy. “To truly ‘democratize’ AI, a concerted effort by policymakers, academic institutions, and firm-level actors is needed to tackle the compute divide,” the authors state.

Study authors Nur Ahmed and Muntasir Wahed summarized their findings and recommendations in the paper entitled, “The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research.” The paper was published recently on arXiv and presented in late October at Strategic Management Society, a business research conference.

The cost to access the computing resources needed for AI training and research can go into millions of dollars. The top six contributors at leading AI research conferences were found to be Google, Stanford University, MIT, Carnegie Mellon University, UC Berkeley, and Microsoft.

Smaller schools often lack the financial resources to consider deep learning applications. This limitation tends to accelerate the brain drain to Big Tech companies of academics with the talent to teach AI, the study found. Some leave the prestigious universities for high-paying industry jobs.

Brain Drain from Academia to Industry Documented from 2004 to 2018

This trend is also found in a paper entitled “Artificial Intelligence, Human Capital and Innovation”. From 2004 to 2018, more than 200 academics versed in AI left for industry positions. Top universities, Ph.D. students, and startups in deep learning were found to have benefited the most from a shortage of talent in AI in the overall job market. Carnegie Mellon University, MIT, and Stanford University ranked highest among colleges whose alumni go on to launch AI startups.

Universities ranked 301-500 by U.S. News and World Report have published on average six fewer papers at AI research conferences since the rise of deep learning, the study found. “To the best of our knowledge, this is the first study that finds evidence that an increased need for specialized equipment can result in ‘haves and have-nots’ in a scientific field,” stated the authors on the study of the Computer Divide in AI Research.

The history of AI can be divided into two eras, authors Ahmed and Wahed suggest. The first stretches from the 1960s to about 2012, when general purpose hardware was used to train AI. In the second era, deep learning models running on specialized hardware such as GPUs have defined the industry.

The findings point to a need for a national research cloud, and shared public datasets that can help train and test AI models, accessible to resource-constrained organizations.

US National Research Cloud Initiative Making Way Through Congress

Legislation to fund a national cloud did move along in the US Congress over the summer. More than 20 major tech companies and universities joined the National AI Research Resource Task Force Act, which aims to spur and democratize AI-centered studies and applications by developing a national asset for scientists and students to use, according to an account in NextGov .

“We must maintain our AI leadership,” stated Sen. Rob Portman, R-Ohio, a member of the Senate Armed Services Committee “I heard from constituents and stakeholders about how vital this is for cutting edge AI research that will benefit the entire country.”

If passed, the bill would require the National Science Foundation and Office of Science and Technology Policy to establish a task force of experts from government, academia, and industry to pursue a “coordinated roadmap and implementation plan” for forming and sustaining the AI-focused research resource.

The policymakers aim to pave the way to lower the barrier for entry to researchers across the nation, especially those outside the major tech companies and elite universities, by opening up compute power, time and datasets.

“For the U.S. to maintain its global leadership in AI, researchers must be enabled to access high-power computing, large datasets, and educational resources that are required for AI research and development,” stated Rep. Anna G. Eshoo, D-Calif., a member of the House Committee on Energy and Commerce, who co-sponsored a House version of the bill. “This effort is critical for our economy and national security.”

UKCloud Initiative Helping Public Sector Deliver on Digital Services

Elsewhere, the UK founded UKCloud in 2011 aimed at helping the UK public sector deliver better digital services. All the UKCloud’s infrastructure technologies and services are hosted in UK-based data centers and supported and managed by staff located in the UK.

Working with partner companies who are independent software vendors, system integrators, and managed service providers, UKCloud offers software capabilities including AI, cybersecurity, big data, disaster recovery and backup, according to an account in CIO.

“UKCloud was founded on core values that include doing what’s right to improve public services for UK citizens and protecting UK data, like the national asset it is, with sovereign cloud services,” stated Leighton James, Chief Technology Officer at UKCloud. “The needs of our public sector differ from the private sector in that there is more emphasis on data governance. From patient data and citizen records all the way to military details, everything must be handled safely and in a secure cloud environment.”

Read the source articles and information in VentureBeat, in a paper entitled “Artificial Intelligence, Human Capital and Innovation,” in NextGov and in CIO.","['research', 'university', 'study', 'national', 'data', 'creating', 'finds', 'learning', 'ai', 'deep', 'services', 'divide', 'compute', 'cloud', 'power', 'digital', 'concentration', 'universities']","By AI Trends StaffIn the era of deep learning, cloud compute power is being concentrated in the hands of elite universities, at the expense of efforts to “democratize” access to AI technology.
The top six contributors at leading AI research conferences were found to be Google, Stanford University, MIT, Carnegie Mellon University, UC Berkeley, and Microsoft.
In the second era, deep learning models running on specialized hardware such as GPUs have defined the industry.
The findings point to a need for a national research cloud, and shared public datasets that can help train and test AI models, accessible to resource-constrained organizations.
US National Research Cloud Initiative Making Way Through CongressLegislation to fund a national cloud did move along in the US Congress over the summer."
223,https://www.aitrends.com/ai-in-business/amazon-beefs-up-ai-in-alexa-and-gets-charged-by-eu-with-unfair-practices/,"Amazon Beefs Up AI in Alexa, and Gets Charged by EU With Unfair Practices",2020-11-13 00:01:57+00:00,"By John P. Desmond, AI Trends Editor

AI took center stage in recently-announced updates to the Alexa virtual voice assistant, and in the charges this week from the European Commission that Amazon is breaking EU competition rules.

During Amazon’s Alexa Live event held in July, the company announced a major update to Alexa’s developer toolkit that brings AI improvements. Since launching in 2014, Amazon’s voice assistant has shipped hundreds of millions of units, which are targeted by a sizable developer community offering voice apps, called Skills, that extend the Alexa default feature set. Just as the Android and iOS large selections of third party applications differentiate those operating systems, so Skill plays an important role in Amazon’s growth strategy for Alexa, according to a recent account in siliconAngle.

Amazon added deep learning models for natural language understanding that the company said will enable Skills to recognize users’ voice commands with 15% higher accuracy on average. Current Skills users can use the new technology without any modifications, according to Amazon.

Amazon also enhanced the voice assistant platform for more specific uses that are emerging as Alexa is added to more devices, including smartphones, wearables and smart displays. A new tool, Apps for Alexa, allows developers of mobile apps to enable customer control in a hands-free way, such as with the Echo Buds wireless earbuds. Another tool enables developers to allow purchases such as food delivery orders on Alexa-powered smart screens, such as the Echo Show smart display.

Developers of Skills for the Echo Bud are getting a new capability called “skill resumption,” which allows Skills to automatically “resume” at opportune times. For example, if a consumer uses Echo Buds to hail an Uber car, Uber’s Alexa skill can automatically notify them when their ride arrives without requiring a manual invocation.

Skills have momentum; Amazon announced that customer engagement with Alexa Skills nearly doubled over the past year.

AZ1 Edge Processor Can Perform On-Device Processing, a Privacy Win

Alexa is also moving to the edge with its own chip in smart home edge devices. The Echo devices are using the company’s AZ1 Neural Edge processor, which consumes 20x less power, 85% less memory and features double the speech processing power as predecessors, according to an account from ZDNet.

The AZ1 in concert with Amazon’s AI advances is aimed at making the Echo more aware of its surroundings. Dave Limp, senior vice president of devices and services at Amazon, stated that the new Echo devices are designed to make “moments count.” The new versions of Alexa will be able to learn from humans by asking follow-up questions when Alexa has a gap in its understanding, according to Rohit Prasad, VP and head scientist for Alexa AI at Amazon, in a presentation on new Alexa features at the virtual event. New versions will also use deep learning space parsers to understand gaps and extract new concepts, will show more natural conversation, and will engage a follow–up mode when interacting with humans.

Alexa can use visual and acoustic cues to determine the best action to take. “This natural turn-taking allows people to interact with Alexa at their own pace,” Prasad stated.

The new AI foundation technology for Alexa’s ability to interpret context and adjust how to speak to you, has been in development for years at Amazon, Prasad said.

The AZ1 edge processor is making Alexa faster. “The processor on the device is key with a fast-paced conversation,” stated Prasad. “The neural accelerator on the device makes decisions much faster.”

Alexa for Business, rolled out over a year ago, has been adding features via AWS. Skill Blueprints were launched in April 2018 as a way to allow anyone to create skills and publish them to the Skills Stores with a 2019 update.

Prasad did not outline the roadmap for Alexa for Business, but did say Echo’s new capabilities would apply to office settings as well as to yet-to-be-determined use cases. “There’s the potential to be able to teach Alexa anything in principle,” Prasad stated.

The AZ1 processor, built with Taiwanese semiconductor company MediaTek, will speed Alexa’s response to queries and commands by hundreds of milliseconds per response, according to an account in The Verge. That allows for on-device neural speech recognition.

Amazon’s preexisting products without the AZ1 send both the audio and its corresponding interaction to the cloud to be processed and back. Only the Echo and Echo Show 10 currently have the on-device memory needed to support Amazon’s new all-neural speech models. Given that the data is stored and deleted locally, the edge computing is seen as a privacy win.

European Commission Charging Amazon with Unfair Competition

All this smart processing is getting Amazon into trouble in Europe, with the European Commission this week charging the company with gaining an illegal advantage in the European marketplace. This was based on the use by Amazon of sales data of independent retailers selling through its site, data not available to other companies in the European market, and which Amazon uses to sell more of its most profitable products.

Margrethe Vestager, the commission’s executive vice-president, stated that the commission’s preliminary conclusion was that Amazon used “big data” to illegally distort competition in France and Germany, the biggest online retail markets in Europe, according to an account in The Guardian. The investigators will examine whether Amazon set rules on its platform to benefit its own offers and those of independent retailers who use Amazon’s logistics and delivery services.

“We do not take issue with the success of Amazon or its size. Our concern is very specific business contacts which appear to distort genuine competition,” Vestager stated. The EU team has since July analyzed a data sample of more than 18 million transactions on more than 100 million products.

The commission determined that real time business data relating to independent retailers on the site was being fed into an algorithm used by Amazon’s own retail business. “It is based on these algorithms that Amazon decides what new products to launch, the price of each individual offer, the management of inventories and the choice of the best supplier for a product,” Vestager stated. “We therefore come to the preliminary conclusion that the use of this data allows Amazon to focus on the sale of the best-selling products, and this marginalizes third party sellers and caps their ability to grow.”

Amazon faces a possible fine of up to 10% of its annual worldwide revenue. That could amount to as much as $28 billion, based on its 2019 earnings.

In a statement Amazon said it disagreed with the findings. “There are more than 150,000 European businesses selling through our stores that generate tens of billions of euros in revenues annually,” the company stated.

Read the source articles in siliconAngle, ZDNet, The Verge and The Guardian.","['according', 'voice', 'echo', 'skills', 'eu', 'data', 'alexa', 'stated', 'practices', 'gets', 'ai', 'charged', 'amazon', 'amazons', 'unfair', 'edge', 'beefs']","During Amazon’s Alexa Live event held in July, the company announced a major update to Alexa’s developer toolkit that brings AI improvements.
Current Skills users can use the new technology without any modifications, according to Amazon.
Amazon also enhanced the voice assistant platform for more specific uses that are emerging as Alexa is added to more devices, including smartphones, wearables and smart displays.
Skills have momentum; Amazon announced that customer engagement with Alexa Skills nearly doubled over the past year.
Only the Echo and Echo Show 10 currently have the on-device memory needed to support Amazon’s new all-neural speech models."
224,https://www.aitrends.com/healthcare/internet-of-medical-things-is-beginning-to-transform-healthcare/,Internet of Medical Things is Beginning to Transform Healthcare,2020-11-12 23:58:01+00:00,"By AI Trends Staff

The Internet of Medical Things (IoMT) market is expanding rapidly, with over 500,000 medical technologies currently available, from blood pressure and glucose monitors to MRI scanners. AI poised to contribute analysis crucial to innovations such as smart hospitals.

Today’s internet-connected devices aim to improve efficiencies, lower care costs and drive better outcomes in healthcare, according to a recent account in HealthTech Magazine. Devices in the IoMT domain extend to wearable external medical devices such as skin patches and insulin pumps; implanted medical devices such as pacemakers and cardioverter defibrillators; and stationary devices such as for home monitoring and connecting imaging machines.

Projections for IoMT market size were aggressive before the COVID-19 pandemic hit, with Deloitte sizing the market at $158.1 billion by 2022, with the connected medical device segment expected to take up to $52.2 billion of that by 2022.

Now the estimates are growing. The global IoMT market was valued at $44.5 billion in 2018 and is expected to grow to $254.2 billion in 2026, according to AllTheResearch. The smart wearable device segment of IoMT, inclusive of smartwatches and sensor-laden smart shirts, made up for the largest share of the global market in 2018, at roughly 27 percent, the report found.

This area of IoMT is poised for even further growth as artificial intelligence is integrated into connected devices and can prove capable of real-time, remote measurement and analysis of patient data.

Fitbit Trackers Found to Help Patients with Heart Disease

Evidence is coming in on the effectiveness of IoMT for health care. A study conducted by researchers from Cedars-Sinai Medical Center and UCLA found that Fitbit activity trackers were able to more accurately evaluate patients with ischemic heart disease by recording their heart rate and accelerometer data simultaneously. Some 88% of healthcare providers were found in a survey last year of 100 health IT leaders by Spyglass Consulting Group, to be investing in remote patient monitoring (RPM) equipment. This is especially true for patients whose conditions are considered unstable and at risk for hospital admission.

Cost avoidance was the primary investment driver for RPM solutions, which are hoping to achieve reduced hospital readmissions, emergency department visits, and overall healthcare utilization, the study stated.

Wearable activity trackers have also proven to be a more reliable measure of physical activity and assessing five-year risk than traditional methods, according to a study by Johns Hopkins Medicine, as reported in mHealthIntelligence.

Adult participants between 50 and 85 years old wore an accelerator device at the hip for seven consecutive days to gather information on their physical activity. Individual data came from responses to demographic, socioeconomic, and health-related survey questions, along with medical records and clinical laboratory test results.

IoMT Devices Seen as Helping to Control Health Care Costs

Medical cost reductions of $300 billion are being estimated by Goldman Sachs, through remote patient monitoring and increased oversight of medication use. Startup activity is picking up. Proteus Discover, for example, has focused its smart pill capabilities on measuring the effectiveness of medication treatment; and HQ’s CorTemp is using its smart pills to monitor patients’ internal health and transmit wireless data such as core temperatures, which can be critical in life or death situations.

AI systems are seen as able to reduce therapeutic and therapeutic errors in human clinical practice, according to an account in IDST. Developing IoMT strategies that match sophisticated sensors with AI-backed analytics will be critical for developing smart hospitals of the future. “Sensors, AI and big data analytics are vital technologies for IoMT as they provide multiple benefits to patients and facilities alike,” stated Varun Babu, senior research analyst with Frost & Sullivan TechVision Research, which studies emerging technology for IT.

The rise of AI and its alliance with IoT is one of the critical aspects of the digital transformation in modern healthcare, according to an account in IoTforAll. The central pairing is likely to result in speeding up the complicated procedures and data functionalities that are otherwise tedious and time-consuming. AI along with sensor technologies from IoT can lead to better decision-making. Advances in connectivity through AI are expected to promote an understanding of therapy and enable preventive care that promises a better future.

The impact of AI on personal healthcare is attracting wide comment. “AI is transforming every industry in which it is implemented, with its impact upon the healthcare sector already saving lives and improving medical diagnoses,” stated Dr. Ian Roberts, Director of Therapeutic Technology at Healx, a biotechnology company based in Cambridge, England, in an account in BBH (Building Better Healthcare). “The transformative effect of AI is set to switch healthcare on its head, as the technology leads to a shift from reactive treatments targeting populations to proactive prevention tailored to the individual patient.”

In the future, AI-generated healthcare recommendations are seen as extending to include personalized treatment plans. “Currently we are in the infancy of AI in healthcare, and each company drives forward another piece of the puzzle and once fully integrated the future of medicine will be forever transformed,” Dr. Roberts stated.

However, the increasingly-connected environment of IoMT is seen as bringing new risks as cyber criminals seek to exploit device and network vulnerabilities to wreak havoc. A recent global survey by Extreme Networks, a network infrastructure provider, found that one in five healthcare IT professionals are unsure if every medical device on their network has all the latest software patches installed — creating a porous security infrastructure that could potentially be bypassed.

“2020 will be the year when healthcare organizations of all sizes will need to realize that they are easy pickings for cyber criminals, and put a robust, reliable and resilient network security infrastructure in place to protect themselves adequately,” stated Bob Zemke, director of healthcare solutions for Extreme.

Data science is seen as leading to more precise analytics. “In 2020, we can expect to see better patient outcomes fueled largely by the growing prevalence of data science and analytics,” stated lan Jacobson, chief data and analytic officer at Alteryx, a software company providing advanced analytics tools. “Much of the data that is required to solve some really-key challenges already exists in the public domain, and in the next year we expect more and more healthcare organizations will implement tools that help to assess this rich information as well as gain actionable insight.” The tools are seen as being effective in monitoring proper use of prescription drugs.

Read the source articles and information in HealthTech Magazine, Deloitte, AllTheResearch, mHealthIntelligence, IDST, IoTforAll and in BBH (Building Better Healthcare).","['devices', 'iomt', 'data', 'things', 'patients', 'ai', 'transform', 'seen', 'beginning', 'internet', 'smart', 'better', 'healthcare', 'medical']","By AI Trends StaffThe Internet of Medical Things (IoMT) market is expanding rapidly, with over 500,000 medical technologies currently available, from blood pressure and glucose monitors to MRI scanners.
Today’s internet-connected devices aim to improve efficiencies, lower care costs and drive better outcomes in healthcare, according to a recent account in HealthTech Magazine.
Devices in the IoMT domain extend to wearable external medical devices such as skin patches and insulin pumps; implanted medical devices such as pacemakers and cardioverter defibrillators; and stationary devices such as for home monitoring and connecting imaging machines.
IoMT Devices Seen as Helping to Control Health Care CostsMedical cost reductions of $300 billion are being estimated by Goldman Sachs, through remote patient monitoring and increased oversight of medication use.
Read the source articles and information in HealthTech Magazine, Deloitte, AllTheResearch, mHealthIntelligence, IDST, IoTforAll and in BBH (Building Better Healthcare)."
225,https://www.aitrends.com/ai-in-science/scientists-employing-chemputers-in-efforts-to-digitize-chemistry/,Scientists Employing ‘Chemputers’ in Efforts to Digitize Chemistry,2020-11-12 23:41:37+00:00,"By AI Trends Staff

A “chemputer” is a robotic method of producing drug molecules that uses downloadable blueprints to synthesize organic chemicals via programming. Originated in the University of Glasgow lab of chemist Lee Cronin, the method has produced several blueprints available on the GitHub software repository, including blueprints for Remdesivir, the FDA-approved drug for antiviral treatment of COVID-19.

Cronin, who designed the “bird’s nest” of tubing, pumps, and flasks that make up the chemputer, spent years thinking of a way researchers could distribute and produce molecules as easily as they email and print PDFs, according to a recent account from CNBC.

“If we have a standard way of discovering molecules, making molecules, and then manufacturing them, suddenly nothing goes out of print,” Cronin stated. “It’s like an ebook reader for chemistry.”

Beyond creating the chemputer, Cronin’s team recently took a second major step towards digitizing chemistry with an accessible way to program the machine. The software enables academic papers to be made into ‘chemputer-executable’ programs that researchers can edit without learning to code, the scientists announced in a recent edition of Science. The University of Glasgow team is one of dozens spread across academia and industry racing to bring chemistry into the digital age, a development that could lead to safer drugs, more efficient solar panels, and a disruptive new industry.

Cronin’s team hopes their work will enable a “Spotify for chemistry” — an online repository of downloadable recipes for molecules that could enable more efficient international scientific collaboration, including helping developing countries more easily access medications.

“The majority of chemistry hasn’t changed from the way we’ve been doing it for the last 200 years. It’s a very manual, artisan–driven process,” stated Nathan Collins, the chief strategy officer of SRI Biosciences, a division of SRI International. “There are billions of dollars of opportunity there.” He added, “This is still a very new science; it’s started to really explode in the last 18 months.”

The Glasgow team’s software includes the SynthReader tool, which scans a chemical recipe in peer-reviewed literature — like the six-step process for cooking up Remdesivir — and uses natural language processing to pick out verbs such as “add,” “stir,” or “heat;” modifiers like “dropwise;” and other details like durations and temperatures. The system translates those instructions into XDL, which directs the chemputer to execute mechanical actions with its heaters and test tubes.

The group reported extracting 12 demonstration recipes from the chemical literature, which the chemputer carried out with an efficiency similar to that of human chemists.

Cronin founded a company called Chemify to sell the chemistry robots and software. In May of 2019, the group installed a prototype at the pharmaceutical company GlaxoSmithKline.

“The chemputer as a concept and the work [Cronin]’s done is really quite transformational,” stated Kim Branson, the global head of artificial intelligence and machine learning at GSK. The company is exploring various automation technologies to help it make a wide array of chemicals more efficiently. Cronin’s work may let GSK “teleport expertise” around the company, he stated.

Researchers at SRI are pursuing their SynFyn synthetic-chemistry system to expedite discovery of selective molecules. Collins recently published related research, Fully Automated Chemical Synthesis: Toward the Universal Synthesizer. AutoSyn, “makes milligram-to-gram-scale amounts of virtually any drug-like small molecule in a matter of hours,” he said in a recent account in The Health Care Blog.

He sees the combination of AI and automation as an opportunity to improve the pharma R&D process. “Progress in AI offers the exciting possibility of pairing it with cutting-edge lab automation, essentially automating the entire R&D process from molecular design to synthesis and testing — greatly expediting the drug development process,” Dr. Collins stated.

SRI is pursuing partnerships to help accelerate the digitized drug discovery. A recent example is a collaboration with Exscientia, a clinical state AI drug discovery company, to work on integration of Exscientia’s Centaur Chemist AI platform to the SynFini synthetic chemistry system, described recently in a press release from SRI.

Exscientia applies AI technologies to design small molecule compounds that have reached the clinic. Molecules generated by Exscientia’s platform are highly optimized to satisfy the multiple pharmacology criteria required to enter a compound into the clinic in record time. Centaur Chemist is said to transform drug discovery into a formalized set of moves while also allowing the system to learn strategy from human experts.

Andrew Hopkins, CEO of Exscientia stated, ”The opportunity to apply AI drug design through our Centaur Chemist system with SynFini automated chemistry offers an exciting opportunity to accelerate drug discovery timelines through scientific innovation and automation.”

SRI also announced a partnership earlier this year with Iktos, a company specializing in using AI for novel drug design, to use Iktos’ generative modeling technology will be combined with SRI’s SynFini platform, according to a press release from Iktos. The goal is to accelerate the identification of drug candidates to treat multiple viruses, including influenza and COVID-19.

The Iktos AI technology is based on deep generative models, which help design virtual novel molecules that have all the desirable characteristics of a novel drug candidate, addressing challenges including simultaneous validation of multiple bioactive attributes and drug-like criteria for clinical testing.

“We hope our collaboration with SRI can make a difference and speed up the identification of promising new therapeutic options for the treatment of COVID-19,” stated Yann Gaston-Mathé, co-founder and CEO of Iktos.

Read the source articles and information in CNBC, Science, The Health Care Blog, a press release from SRI and a press release from Iktos.","['company', 'employing', 'chemputers', 'digitize', 'stated', 'process', 'chemputer', 'ai', 'chemistry', 'drug', 'system', 'scientists', 'sri', 'efforts', 'molecules']","By AI Trends StaffA “chemputer” is a robotic method of producing drug molecules that uses downloadable blueprints to synthesize organic chemicals via programming.
“If we have a standard way of discovering molecules, making molecules, and then manufacturing them, suddenly nothing goes out of print,” Cronin stated.
“The majority of chemistry hasn’t changed from the way we’ve been doing it for the last 200 years.
It’s a very manual, artisan–driven process,” stated Nathan Collins, the chief strategy officer of SRI Biosciences, a division of SRI International.
The system translates those instructions into XDL, which directs the chemputer to execute mechanical actions with its heaters and test tubes."
226,https://www.aitrends.com/ai-trends-insider-on-executive-leadership/ai-holistic-adoption-for-manufacturing-and-operations-data/,AI Holistic Adoption for Manufacturing and Operations: Data,2020-11-12 23:36:45+00:00,"By Dawn Fitzgerald, the AI Executive Leadership Insider

Part Three of Four Part Series: “AI Holistic Adoption for Manufacturing and Operations” is a four-part series which focuses on the executive leadership perspective including key execution topics required for the enterprise digital transformation journey and AI Holistic Adoption for manufacturing and operations organizations. Planned topics include: Value, Program, Data and Ethics. Here we address our third topic: Data.

The Executive Leadership Perspective

For the executive leader who is taking their enterprise on a journey of Digital Transformation and AI Holistic Adoption, we started this series with the foundation of Value and then moved to the framework of the Program. Although these are the fundamental building blocks required for success, the results of any enterprise’s analytics, do, in the end, rely on the Data.

The executive leader has the responsibility to ensure that they and their team are dedicated to mastering data fluency and data excellence in the enterprise. The facets of Data Management are vast with the standard areas of focus including data discovery, collection, preparation, categorization and protection. Strategies for achieving maturity in these areas are well-established in most industries, and yet many industries still struggle. These standard areas of focus in Data Management are indeed necessary but are not sufficient for the needed AI Holistic Adoption.

To incorporate AI Holistic Adoption, a value focus must be employed where we create Value Analytics (VAs) as output from our enterprise Analytics Program. To support this program, we must expand our enterprise Data Management definition to include a Data Optimality metric, a Data Evolution Roadmap and a Data Value Efficiency metric.

The Data Optimality metric tells us how close the Value Analytics (VA) Baseline Dataset is to ‘optimal’. The Data Evolution Roadmap captures the milestones for the evolution of our Baseline Dataset for each Value Analytics release and the corresponding goals for harvesting data. The Data Value Efficiency metric simply measures how much value we achieve from harvested data. The combination of these is a powerful tool set for the executive leader to ensure the data provides the highest value to enterprise analytics at the lowest cost to the organization.

The Data Optimality Metric Definition

The Data Optimality metric tells us how close the Value Analytics (VA) Baseline Dataset is to the Data Scientist-defined ‘optimal’. The Baseline Dataset is a key component to any Value Analytic. The Baseline Dataset captures the data used for the VA as it relates to a specific development release. This link to a release is a critical distinction. By tying the Baseline Dataset to the VA design release, we recognize a snapshot of the training data associated with a specific release. We recognize that it may not be optimal so may change during the lifetime of the VA, and we plan for its change on a Data Evolution Roadmap.

To achieve enterprise AI Holistic Adoption the executive leader must ensure the foundation of Value which anchors the effort. They must also incorporate the nature of a technical development effort. Specifically, they must account for the go-to-market demands that drive risk management decisions regarding minimal viable product (MVP) in Agile or SAFe (Scaled Agile Framework) methodologies. By the very nature of development, the MVP-driven organization will plan early deliverables with incremental improvements over time. This will apply to the Baseline Dataset as well and thus, the Data Optimality Metric is created. It is used for visibility of the state of our Baseline Dataset, used to communicate expectations of its impact on the VA and used to drive the evolution of the data.

Data Optimality Metric Example

To illustrate the power of the Data Optimality metric, consider the Data Scientist who has defined an equipment predictive maintenance algorithm and has a corresponding Baseline Dataset definition. They will have defined the optimal dataset that they want which includes the IoT measurements (for example: temp, pressure and vibration), the duration of time they would like the Data collected over (for example: 6 months), the population size (for example: data collected from 10 Data Centers covering four key climate zone geographies) and a guaranteed data quality level (for example less than 10% data gaps). Since there is a low probability of this optimal Baseline Dataset availability aligning with the market-driven release timeline demands, the Data Scientist may be forced to compromise their initial Baseline Dataset by taking fewer IoT parameters (for example: only temp and pressure but no vibration), having shorter collection duration (for example: 3 months vs 6), having a smaller population size (for example: only 3 Data Centers vs 10) or accepting a lower quality level guarantee. The Data Scientist may also create simulated data for some or all of the data gaps.

The Data Scientist will then assign a Data Optimality metric to the current release Baseline Dataset (for example: current available data achieves 60% of the optimal dataset criteria). They will also state the lower Data Optimality metrics potential impact on the Value Analytic (for example: customers can expect only a 30-day prediction vs 90-day prediction pre-failure).

The executive leader can then make a business decision to go forward with this Data Optimality metric or wait the extra time necessary to harvest improved data to achieve a higher Data Optimality metric and corresponding VA improvement. To conclude this scenario example, input from the marketing team may indicate that a Q2 release of the VA with the current Data Optimality metric is acceptable due to first mover advantage and significant value, compared to competitive offers, delivered to the customer.

They may also specify that the higher Data Optimality metric must be achieved by Q4 in order to remain competitive. The Data Optimality metric enables defined incremental improvements to the Baseline Dataset over time which transcend to the ongoing VA improvement lifecycle.

The visibility provided by the Data Optimality metric is especially valuable with leading edge Value Analytic capabilities where first mover advantage in the market can lead to a substantial market penetration foothold for the business. The metric drives cost saving by bringing the decision point of release impacting information down to the local business, where the knowledge of the business is the highest. This simultaneously gives visibility to future data management actions through the enterprise and should be captured in the Data Evolution Roadmap.

The Data Evolution Roadmap

Driven by Data Optimality metric inputs, the Data Evolution Roadmap captures the milestones for the evolution of our Baseline Dataset for each Value Analytics release and the corresponding goals for harvesting future required data. The Data Evolution Roadmap establishes an enterprise framework that provides visibility, alignment, clarity and flexibility for local business decisions. It also challenges the business to define the Data Optimality metric and track Baseline Dataset improvements.

The power of the Data Evolution Roadmap enables the local businesses’ Agile development methodologies, gives cross-functional visibility of data management actions and delivers Data Management cost saving to the enterprise. Incremental improvements of the Data Optimality metric for a specific Value Analytic can be timed on the Data Evolution Roadmap based on demand. Early market traction data can be incorporated to update the business decision thus generating higher confidence in the data management expenditures and potential cost savings if deemed no longer necessary.

To achieve AI Holistic Adoption, the Data Evolution Roadmap must align directly to the Value Analytics Roadmap. Data management tasks must align and be traceable through both roadmaps to a higher end value. Successful execution of this requires rapid, tightly coupled agile development teams that span the key enterprise stakeholders such as IoT development, Data management, Data Science, platform development and marketing/sales functions. This demand-pull approach to Data Management aligns well with Agile development practices and combats the seemingly overwhelming challenges of exponential data repository growth and corresponding data management costs.

Data Repository Growth

The growth of the data repository should parallel the growth and maturity of the Analytics Program to ensure data excellence and avoid dark data obsolescence. The cost of technical debt must be acknowledged and measured.

Many companies make the mistake of a volume goal of collecting IoT data without a defined data evolution strategy aligned with the Analytics Program grounded in value. This leads to the data swamp, a stalling of the realization of Value from the AI solutions and an overall low Data Value Efficiency score as defined below.

A tighter alignment of the Data Management tasks with the Value Analytics also provides opportunity for more value-based incremental improvements of the enterprises’ tagging strategy. Tagging data with both technical and business metadata is critical but seldom done correctly first pass and certainly not without a Value focus, which requires a cross-functional team of a data architect, data scientist, subject-matter expert and marketing that anchor the value. The mechanism to continuously improve your data tagging methodology must be close to the value goals of the Analytics Program.

The Data Value Efficiency

Once the Data Optimality metric and Data Evolution Roadmap are established, a Digital Value Efficiency (DVE) metric can be measured. The Data Value Efficiency (DVE), a measurement attached to data elements, is simply the measure of how much value we achieve from harvested data. The DVE tracks the use of the data by its inclusion in different VA Baseline Datasets over time.

In most industries using AI, this metric would be considered very low. IDC research defines that currently, “80% of time is spent on data discovery, preparation, and protection, and only 20% of time is spent on actual analytics and getting to insight.” To achieve high DVE, a larger portion of our data harvested must translate into higher value actionable insights.

Since the executive leader’s responsibility is to ensure that the organization is efficient with the data management, they must focus their organization on shifting the percentage of time invested from data discovery, collection and preparation to a higher amount of time used in training models and insight generation. The DVE metric gives visibility to progress toward this goal.

The Data Evolution Roadmap pivots the enterprise focus from one of maximum data collection, and corresponding cost, to one of minimized data collection driven by the Value Analytics roadmap. Over time, this will improve the DVE metric and overall data excellence of the enterprise.

Dawn Fitzgerald is VP of Engineering and Technical Operations at Homesite, an American Family Insurance company, where she is focused on Digital Transformation. Prior to this role, Dawn was a Digital Transformation & Analytics executive at Schneider Electric for 11 years. She is also currently the Chair of the Advisory Board for MIT’s Machine Intelligence for Manufacturing and Operations program. All opinions in this article are solely her own and are not reflective of any organization.","['metric', 'data', 'holistic', 'management', 'ai', 'value', 'optimality', 'dataset', 'adoption', 'evolution', 'manufacturing', 'baseline', 'analytics', 'enterprise', 'operations']","These standard areas of focus in Data Management are indeed necessary but are not sufficient for the needed AI Holistic Adoption.
To support this program, we must expand our enterprise Data Management definition to include a Data Optimality metric, a Data Evolution Roadmap and a Data Value Efficiency metric.
The executive leader can then make a business decision to go forward with this Data Optimality metric or wait the extra time necessary to harvest improved data to achieve a higher Data Optimality metric and corresponding VA improvement.
This demand-pull approach to Data Management aligns well with Agile development practices and combats the seemingly overwhelming challenges of exponential data repository growth and corresponding data management costs.
The Data Value EfficiencyOnce the Data Optimality metric and Data Evolution Roadmap are established, a Digital Value Efficiency (DVE) metric can be measured."
227,https://www.aitrends.com/ai-world-government/hhs-automating-hiring-with-help-of-ai-faa-planning-for-role-of-ai-in-modernizing/,HHS Automating Hiring with Help of AI; Role of AI in Modernizing Aviation,2020-11-05 23:16:36+00:00,"By John P. Desmond, AI Trends Editor

Innovations incorporating AI within the Office of Human Resources within the US Health and Human Services (HHS) agency, and planning how to incorporate AI into modernization of the software systems of the Federal Aviation Administration (FAA) were described by speakers at the 2nd Annual AI World Government conference and expo held virtually last week.

The Office of Human Resources within HHS in early experience with a new system incorporating AI has shortened the hiring process by an average of 75 days compared to the previous method, the managers said.

“We wanted to give our customers a modern, digital experience,” stated Blair Duncan, Chief Human Capital Officer for HHS, an agency with 85,000 employees. “HR in the federal government has not been known to be the most modern system.” The results of the effort launched 18 months ago is what the department calls the HR Exchange system, launched early this year. (See Government Executive for a mention of HR Exchange from HHS.)

HR Exchange is a platform available to employees and hiring managers via a browser, with information individual employees can access about themselves, and hiring managers can use to help fill open positions. “In the past, the only people who could access HR data were HR people. We want to give access to the customer,” Duncan said.

Working with the CIO of HHS, the Human Resources team created a system that can automate production of a Certificate of Eligibility, which is a prequalification for an applicant. “In the past, multiple candidates would apply to an open position, HR would review the resumes and pick the one that best fits the position,” said Bahar Niakan, Deputy Chief Human Capital Officer at HHS. “Then the rest of the list would be thrown away.”

This process took months and was a duplication of effort. The team decided to automate the process to enable a sharing of all applicants in the system who had a certificate of eligibility, so that other hiring managers could access the list. The team used an RPA tool to pull the resumes and extract from them job title, federal grade level, and other information. “It was still a lot of manual work,” Niakan said.

In a second phase, RPE was combined with optical character recognition to provide an advanced search capability, which would enable a 100% match of a candidate to a job posting. “Managers could not find the needle in the haystack,” she said. The system now has some 100,000 resumes in it.

The plan for a third phase calls for machine learning and natural language processing to further automate production of the certificates for qualified candidates. The unit has begun to share its innovation with other federal agencies.

As an example of the system’s usefulness, when Covid-19 hit, the Peace Corps called 7,000 employees home. All of them were out of work. The HHS Human Capital team used the system to enter the Peace Corps workers into the eligibility pool where hiring managers could see them.

“In three to four months, we hired 70 displaced Peace Corps employees and the work is still going on,” Niakan said. That hiring was done as the result of one announcement. Seeing that the system can help them and not step on their turf, hiring managers are buying in. “We have become a more collaborative community through this,” Niakan said.

The managers expect build-out of the system will call for a number of new positions to be created, with titles such as HR data experts and chatbot writers. “We are at the forefront of what the HR practice will look like in the future,” Duncan said.

FAA Looking at AI to Help Meet Challenges of More Complex Airspace

Engineers are thinking through how AI can be incorporated into the modernization of software systems supporting the National Airspace System (NAS), said Ronald Stroup, General Engineer Expert in NAS planning and analysis for the FAA, in a talk on the effort. NAS he described as a “highly complex and distributed system” that supports many safety-critical functions.

With the FAA for over 30 years, Stroup is a member of a working group of over 500 investigating the use of AI and machine learning to address the challenges in aviation. In the pandemic era, he said flights are down 50% and are not expected to recover to 2019 numbers until the 2023 to 2024 time frame. ”Many believe this is the time to invest in smart systems,” he said.

The effort has a practical outlook. “We are focusing on use cases and not technology looking for a requirement,” Stroup said. For the 60 NAS use cases he has identified to investigate for AI relevance, he will study planning, risk management and common operational metrics. He then researches AI adoption practices around the technology employed in each case, to assess maturity and relevance in other sectors, such as healthcare and finance.

“The key to applying AI and machine learning is not just about data and technology, but also the underlying organization,” he said. “Organizational transformation is a critical element.”

Challenges include the expansion of controlled airspace by unmanned drones flying at low altitudes, and at the other extreme, vehicles flying at over 60,000 feet. These can be from business aviation, commercial space operations and the Department of Defense. “Can AI help optimize the integration of diverse users in the NAS?” Stroup posed.

Another challenge is interoperability between all players in the NAS system, spanning technical systems as well as energy, communications and finance. “Can AI help integrate the enterprise application across the aviation ecosystem?” Stroup queried.

And by the way, software development is also being transformed. “In an AI world, systems are not developed by programmers based on requirements, but they learn from the data provided,” Stroup said. “Erroneous data can yield incorrect results. So data quality management standards are needed.”

The workforce on this effort will require computer science and data engineering skills, and will require “contextual teams” with leaders that know how to align resources with objectives. “The conventional waterfall model of development may not be sufficient, since AI enables continuous machine learning and training, in which each stage builds on the preceding stage,” Stroup said.

As for the roles of humans in the AI systems, “The current government policy is that the human remain in the loop, But there is no clear direction on what that means in implementation,” said Stroup (sounding like an engineer). Variations are: “humans on the loop,” with humans overseeing the automated system; “humans monitoring the loop,” allowing the human to override the automation; and “human governing the loop,” where the human can turn off the machine.

Trust in the systems will be critical to the FAA’s work. “We need a foundation of trust. Operators and users need to trust that the AI will make the right choice and not put safety at risk.” The National Institute of Standards and Technology (NIST) has identified five properties of trust in AI: explainability, resiliency, fairness, reliability and accountability.

To show evidence of trust, one AI implementation working group is considering certification for AI aerospace systems.

In closing thoughts, Stroup said, “I challenge you to think in terms of emerging technology and not just AI. For many applications, a single technology is not sufficient. It requires an interaction of multiple technologies to collaboratively solve a problem.”

And, “For the future, we have a framework and a near-term initiative and design in emerging technology for a smart aviation system, that will provide a foundation for later technology.”

Is AI an Architecture?

In closing comments at AI World Government, Program VP Ritu Jyoti of IDC posed a question, ”Is AI a feature or an architecture?”

IDC research is predicting that by 2025, organizations making good use of AI will see productivity increases of up to 100%, from greater product innovation success and improved customer satisfaction.

“These businesses will stay relevant and have a competitive advantage versus organizations that do not follow this path,” she said.

She adds, “Is AI the next big architecture revolution? Whenever there is an architecture shift, everything needs to get built from scratch. If that is the case, it promised to lead to an explosive period of growth.”

Learn more about the speakers at AI World Government.","['hiring', 'role', 'stroup', 'data', 'managers', 'hhs', 'ai', 'help', 'system', 'systems', 'human', 'aviation', 'automating', 'technology', 'modernizing', 'hr']","HR Exchange is a platform available to employees and hiring managers via a browser, with information individual employees can access about themselves, and hiring managers can use to help fill open positions.
“In the past, the only people who could access HR data were HR people.
The HHS Human Capital team used the system to enter the Peace Corps workers into the eligibility pool where hiring managers could see them.
Seeing that the system can help them and not step on their turf, hiring managers are buying in.
To show evidence of trust, one AI implementation working group is considering certification for AI aerospace systems."
228,https://research.aimultiple.com/supply-chain-automation/,Supply Chain Automation in 2021: In-depth guide,2020-12-03 07:19:36+00:00,"Supply chain management (SCM) is administration of people, organizations, resources, activities, and technologies involved in manufacturing and distributing a product or service to cut down costs and minimize shortages. Since the 1950s, different trends and technologies have been shaping supply chain managers’ strategies. As of today, AI & RPA enable companies to improve operational efficiency, cut down costs and prevent shortages.

Automation of supply chain operations is the top priority of companies. According to a survey of supply chain professionals in retail, manufacturing, and logistics fields, 41% of companies have already acquired or plan to acquire a supply chain automation technology within 12 months. Another study by Deloitte also reveals that warehouse automation(%53) is the top technology invested by top companies followed by predictive analytics (%47), Internet of Things (41%), and Cloud Logistics (40%).

What is supply chain automation?

Supply chain automation is leveraging digital technologies such as artificial intelligence (AI), machine learning (ML), Robotic Process Automation (RPA), Optical Character Recognition (OCR), and robotics to lower the operational cost of delivering a product or service.

What is the difference between supply chain & logistics AI?

Supply chain and logistics are two terms that are often confused with each other. Logistics is a distinct part of the supply chain and is essential to good supply chain performance. Supply chain management is more focused on coordinating multiple parties of the supply chain network (numerous suppliers, the company, the end-user) to achieve the companies’ objectives. On the other hand, logistics’ focus is on the movement of goods from one place to another, storing those goods, and producing the right information and documents for efficient reporting and processing. Essentially, if goods are being transported or stored, that’s a logistics process of SCM.

What activities can be automated in supply chain management?

Automation is about identifying repetitive tasks that are time-consuming or error-prone and discovering ways to automate them. Supply chain networks are rife with repeatable, process-oriented, and error-prone tasks, ranging from manual documentation errors to picking and stocking errors, shipping and receiving errors, and much more. These processes tend to be logistics processes and we’ve explored how AI can facilitate logistics in detail before. In short, 3 types of automation are possible for the supply chain:

Back-office automation: Supply chain management processes contain various documents such as delivery order, dock receipt, bill of lading (B/L), sea waybill, etc. Employees in the supply chain department continuously store and process these documents for various reasons, yet, this is a time-consuming, manual task that inhibits businesses to reach operational excellence. Using artificial intelligence & OCR, businesses can achieve nearly end-to-end document automation (involving tasks like data capture, understanding information on the document, and sending the document to the relevant person).

Supply chain management processes contain various documents such as delivery order, dock receipt, bill of lading (B/L), sea waybill, etc. Employees in the supply chain department continuously store and process these documents for various reasons, yet, this is a time-consuming, manual task that inhibits businesses to reach operational excellence. Using artificial intelligence & OCR, businesses can achieve nearly end-to-end document automation (involving tasks like data capture, understanding information on the document, and sending the document to the relevant person). Transportation automation: Autonomous Things such as autonomous trucks and drones can also be used to transport supply in the network. Some examples of transportation automation are Otto, the self-driving truck subsidiary of Uber, shipped a truckload of Budweiser from Fort Collins, Colo. to Colorado Springs in 2016. Nuro is a startup building autonomous self-driving delivery vehicles. Nuro is designed for last-mile delivery of groceries, food, consumer products, and packages. Their custom-designed R2 vehicles don’t have passenger seats; instead, they are optimized for deliveries and safety, which means they’re small, narrow, use electric motors, and have pedestrian-protecting features.

Autonomous Things such as autonomous trucks and drones can also be used to transport supply in the network. Some examples of transportation automation are

Warehouse automation: There are various aspects of warehouse operations that can be automated via robotics. Some warehouse robotics examples are Cobots to work collaboratively with humans Demand/Supply planning & Inventory automation: In 2013, Motorola reports that 41% of warehouse facilities were still reliant on pen-and-paper methods for counting a small subset of inventory, in a specific location, on a scheduled day to identify out-of-stock items or stock-at-hand levels. Though a lot has changed in warehouse technologies, most companies still have not managed to automate inventory operations. New sophisticated technologies like Woodman’s badger robots and AI-powered demand forecasting solutions help companies automate their inventory operations to minimize the occasion of out-of-stock or stock-at-hand situations. Automated guided vehicles (AGVs)/ Autonomous mobile robots (AMR) : AGVs have been around since the 1950s. However, AMRs add intelligence, guidance, and sensory awareness to conventional AGVs. This allows them to operate autonomously, but collaboratively, around humans. AMRs address the limitations of traditional AGVs and making them better suited to complex warehouses and collaborative activities. Check out the video below if you want to see how autonomous mobile robots look like in an automated warehouse.

There are various aspects of warehouse operations that can be automated via robotics. Some warehouse robotics examples are

Featured Image Source

Automation is inevitable in every industry. Feel free to check our automation-related articles:

And if you still have questions on supply chain automation, don’t hesitate to contact us:

Let us find the right vendor for your business","['automation', 'autonomous', 'various', 'indepth', 'warehouse', 'document', 'companies', 'supply', 'chain', 'technologies', 'logistics', 'guide', '2021']","According to a survey of supply chain professionals in retail, manufacturing, and logistics fields, 41% of companies have already acquired or plan to acquire a supply chain automation technology within 12 months.
What is supply chain automation?
Logistics is a distinct part of the supply chain and is essential to good supply chain performance.
Supply chain management is more focused on coordinating multiple parties of the supply chain network (numerous suppliers, the company, the end-user) to achieve the companies’ objectives.
In short, 3 types of automation are possible for the supply chain:Back-office automation: Supply chain management processes contain various documents such as delivery order, dock receipt, bill of lading (B/L), sea waybill, etc."
229,https://research.aimultiple.com/ap-ai/,5 AI Applications in Accounts Payable (AP) Processes [2021],2020-11-18 07:21:17+00:00,"We’ve written about accounts payable automation and invoice automation before, where we highlighted that AP processes can be mostly automated and shared criteria to select the right vendor. Automation is necessary for accounts payable because manual processing of accounts payable is

expensive since manual processes require labor which is costly

since manual processes require labor which is costly prone to excessive payments: A company can lose up to 4% of the amounts of money paid through invoices because of various errors such as duplicate invoices, fraud, missing early payment discounts and not noticing price hikes.

A company can lose up to 4% of the amounts of money paid through invoices because of various errors such as duplicate invoices, fraud, missing early payment discounts and not noticing price hikes. leads to dissatisfied employees and suppliers: No one likes a slow manual process

This article explains tasks within AP where businesses can leverage AI and the benefits of implementing AI into account payable processes:

Automation

Repetitive tasks There are various daily manual tasks accounts payable teams handle. These tasks are repetitive and hardly require human judgment due to their rules-based structure. Some example repetitive tasks in accounts payable are: data entry

matching invoices to supplemental documents

invoice routing

invoice filling and retrieval All these processes can be automated via artificial intelligence. Machine learning algorithms with the support of business rules can identify and extract required data from documents,

input data into required documents

link data for exceptions resolution

route documents to the appropriate individuals for validation and exceptions handling Categorization of documents sent along with invoices Invoices can be sent bundled with contracts and invoice recipients can also receive documents like credit notes or payment reminders. Combination of Optical Character Recognition (OCR), Natural Language Processing (NLP), and machine learning enable businesses to extract relevant data, understand the context, and categorize these documents. Automation of this process enables businesses to transform their documents into digital systems and ease search functionality. If you want to learn more end-to-end automation examples through the combination of various technologies, feel free to check our hyperautomation and hyperautomation applications articles.

Analytics

Balance-sheet forecasts Forecasting future revenue and expenses is critical for businesses’ financial planning. Accounts payable balance sheet entries are connected to data from operations and cash cycle. AI-powered analytics enables businesses to balance their company’s cash flow based on the analysis of historical data.

Compliance

Fraud detection

Any company can be challenged via fraudulent actions. Accounts payable fraud is mostly attempted in the form of fraudulent invoices being submitted for payment. Types of accounts payable fraud are: Invoice fraud: Involves a fraudster sending an invoice as one of the companies existing suppliers or pretending to be a new supplier. The invoice includes the bank account details of the fraudster.

Involves a fraudster sending an invoice as one of the companies existing suppliers or pretending to be a new supplier. The invoice includes the bank account details of the fraudster. Billing scheme: The fraud type that involves employees generating false payments that will be paid to themselves.

The fraud type that involves employees generating false payments that will be paid to themselves. Check tempering: Check fraud involves a person attempting to make a transaction using a check that has been faked, stolen, altered, or invalid.

Check fraud involves a person attempting to make a transaction using a check that has been faked, stolen, altered, or invalid. Expense reimbursements: A fraudulent payment scheme in which an employee makes a claim for reimbursement of false or exaggerated business expenses. Expense reimbursement fraud can be occurred due to Mischaracterized expenses Overstated expenses Fictitious expenses Double claims

A fraudulent payment scheme in which an employee makes a claim for reimbursement of false or exaggerated business expenses. Expense reimbursement fraud can be occurred due to Automated clearing house (ACH) fraud: ACH fraud involves any unauthorized funds transfer that occurs in a bank account.

ACH fraud involves any unauthorized funds transfer that occurs in a bank account. Kickback Schemes: A form of negotiated bribery in which a commission is paid to the bribe-taker in exchange for services rendered. Artificial intelligence can analyze patterns in invoices to identify any non-standard behavior that may indicate a fraudulent document. Once the AI detects frauds, it flags fraudulent transactions and informs necessary decision-makers. In addition, master data management (MDM) best practices helps companies detect changes in payment details and help identify common invoice fraud attempts. Identification of errors Human error is one of the most common accounts payable problems. Though it seems simple, human-made errors may result in serious losses that could have been easily preventable. Some human errors include lost or misplaced invoices, duplicate data, and poor data entry. Sophisticated AI algorithms can process invoices to catch invoice errors and duplicate payments. Though fraud transaction detection and identification of errors are important AI applications in audit, they are not the only ones. Feel free to check our article where we examined AI applications in the audit industry. Benefits of AI in AP Common benefits of artificial intelligence in account payable process are Faster resolution cycles & increased focus on more value-added activities: Automation enables organizations to handle invoice processing much faster than an employee would do manually. Faster invoices resolution frees the accounts payable team’s time so that they can focus on more value-added tasks.

Automation enables organizations to handle invoice processing much faster than an employee would do manually. Faster invoices resolution frees the accounts payable team’s time so that they can focus on more value-added tasks. Improved financial planning: AI makes forecasting faster and more accurate than humans. Insights from historical data such as recurring invoices help businesses decide when to release cash or take early-payment discounts.

AI makes forecasting faster and more accurate than humans. Insights from historical data such as recurring invoices help businesses decide when to release cash or take early-payment discounts. Reduced errors & Improved compliance: Manual processing of invoices involves various compliance and security risks. Appointing machines to handle these processes reduces the number of people who access the document and reduces the likelihood of human errors that may lead to compliance issues.

Manual processing of invoices involves various compliance and security risks. Appointing machines to handle these processes reduces the number of people who access the document and reduces the likelihood of human errors that may lead to compliance issues. Reduced costs: Due to all reasons we listed above, along with the elimination of high paper storage and retrieval costs in account payable processes, organizations that fully automate accounts payable processes can save significant amount. Full automation can save on average of 4% of expenses when compared to organizations that manually process invoices. If you still have questions about AI in accounts payable processes, don’t hesitate to contact us: Let us find the right vendor for your business","['fraud', 'accounts', 'invoice', 'payable', 'data', 'errors', 'ai', 'processes', '2021', 'applications', 'involves', 'ap', 'invoices']","We’ve written about accounts payable automation and invoice automation before, where we highlighted that AP processes can be mostly automated and shared criteria to select the right vendor.
Accounts payable fraud is mostly attempted in the form of fraudulent invoices being submitted for payment.
Types of accounts payable fraud are: Invoice fraud: Involves a fraudster sending an invoice as one of the companies existing suppliers or pretending to be a new supplier.
Reduced costs: Due to all reasons we listed above, along with the elimination of high paper storage and retrieval costs in account payable processes, organizations that fully automate accounts payable processes can save significant amount.
If you still have questions about AI in accounts payable processes, don’t hesitate to contact us: Let us find the right vendor for your business"
230,https://research.aimultiple.com/ecm-case-studies/,20 ECM Case Studies: Explore ECM Use Cases & Applications,2020-11-14 10:11:48+00:00,"Case studies can give impactful insights about implementing a new technology. One can learn about industries in which the technology can be applied, use cases, applicable business functions and results of the application. We aggregated 20 case studies about ECM from numerous sources so you can filter/sort them by industry (e.g. financial services, energy) or business function (e.g. sales, marketing, IT) to help you identify how your company can implement ECM. To prioritize these use cases and select vendors for your business, you can also examine the reported results and the vendors in the list below:

To learn more about ECM, you can read:

If you are ready to start implementing ECM, feel free to use the most up-to-date and comprehensive list of ECM products to select the right product for your company.

For the best document extraction tools and an overview of document extraction, feel free to read our articles on the topic:

If you want to learn more about ECM, our whitepaper on document automation may also interest you:

If you have further questions please do not hesitate to contact us:","['case', 'cases', '20', 'studies', 'list', 'document', 'results', 'applications', 'business', 'vendors', 'select', 'technology', 'explore', 'ecm', 'learn']","Case studies can give impactful insights about implementing a new technology.
One can learn about industries in which the technology can be applied, use cases, applicable business functions and results of the application.
We aggregated 20 case studies about ECM from numerous sources so you can filter/sort them by industry (e.g.
financial services, energy) or business function (e.g.
sales, marketing, IT) to help you identify how your company can implement ECM."
231,https://research.aimultiple.com/rpa-insurance/,RPA in Insurance Industry: Use Cases & Case Studies [2021],2020-10-23 06:07:44+00:00,"Insurance is heavily regulated and its processes include numerous documents. Documents are harder to automate than machine readable data. However, especially auto, property, and workers’ compensation include more standardized processes where insurance automation tools like RPA can lead to high levels of automation.

What does RPA mean for insurance companies?

Robotic Process Automation technology offers a wide range of benefits to insurance companies, from shifting the workforce to more valuable tasks to reducing manual errors in claims processing/ fraud detection processes. According to Mckinsey, the insurance industry has the potential to automate 25% of the process by 2025, and most of its automation potential comes from operational processes where RPA can help. Another study from Mckinsey also reveals that auto insurance, property & casualty (P&C) insurance, and employees’ computers are areas where insurers can benefit most from automation due to the high standardization of processes.

Regardless of where your business uses RPA, the technology reduces the time spent on repetitive tasks. UiPath estimates that current RPA technology can save insurers: 19% of the time where human expertise is currently required

34% of employee time in the data processing

23% of stakeholder interaction time

What are RPA use cases in insurance?

Claims Registration & Processing & Fraud Detection

Claims processing is a labor-intensive process where insurers need to collect information from multiple sources. Some example sources where insurers spent time on gathering and checking information are:

Auto insurance: Police reports of accidents, driver’s licenses, and photos of damaged vehicles

Police reports of accidents, driver’s licenses, and photos of damaged vehicles Travel insurance: Photos of damaged luggage and boarding pass

Photos of damaged luggage and boarding pass Life insurance: Historical medical records

Processing these data sources manually makes the process dull, time-consuming and prone to errors. According to Workfusion, automated claims processing reduces manual workload by 80% and the time necessary for the process by 50% Insurance companies can leverage RPA bots to automate the following steps of claims processing:

Extraction of information

Integration of claim related sources

Inputting data into the systems

Identification & verification of fraudulent claims

Underwriting & Pricing

Insurance underwriters evaluate and analyze the risks involved in insuring people and assets. Then, they set pricing for identified risks. The underwriting process involves collecting information regarding the background of insurable people or assets. When RPA is combined with AI and analytics, bots can

collect data from external and internal sites

fill required data fields in internal systems

assess loss runs

analyze the history of customers’ claims and provide pricing options based on previous results

Policy Administration & Servicing

Policy processes include rating, quoting, binding, issuing, endorsing, and renewing. A conventional policy administration software may be expensive, require high-maintenance, and not be scalable enough to meet the growing number of customers. RPA can automate transactional and administrative areas of policy activities such as accounting, settlements, risk capture, credit control, tax, and regulatory compliances.

Regulatory Compliance

The insurance industry is regulated by strict laws that aim to standardize documentation and audit trails. Manual control of compliance contains the risks of errors and regulatory breaches. RPA automates those processes and ensures that data is accurate, and maintains a complete log of changes. Log files enable insurers to monitor regulatory compliance regularly through internal reviews. Some compliance processes RPA can automate are

Name screening

Compliance checking

Client research & validation of customer information

Data security operations

Generation of compliance reports

Responding to Queries

Most industries involve the process of responding to customer and employee queries. RPA bots can interpret incoming emails, resolve simple inquiries, and when they detect complex queries, they can pass them to humans.

Is RPA enough to automate insurance processes?

No. Most insurance processes rely on document processing which requires document understanding. However, RPA bots come pre-equipped with relatively simple OCR tools. Insurers can work with machine learning/deep learning companies to augment the capabilities of RPA bots by adding additional modules to them from RPA marketplaces. Feel free to read our article on document automation or insurance AI to learn more.

What are real-world case studies of RPA in insurance?

Here, we listed 10 case studies from insurance companies. If you want to see a more comprehensive list of RPA case studies, feel free to check our related article.

Company RPA Partner Business Function Case Study Results A health insurance company Automation Anywhere Operations Member enrollment process

Commercial claims testing audit Reduced effort

Reduced errors A life and financial services company Automation Anywhere HR Operations HR record processing

Physician statement orders $200k savings p.a. A shared service provider, part of a insurance group UiPath Operations Streamlining processes that involves involved tracking Excel spreadsheets and consistent communication to and from all the parties involved in handling processes & reconciliation 56% reduction in incoming email resolution time

38% reduction in incoming phone call volumes, despite an increase in transaction volumes in the same time frame Bajaj Allianz General Insurance UiPath Operations Streamlining 22 processes such as procurement of proposals, approvals & issuance. Mostly focuses on processes of policy issuance. Cut down redundant tasks Improved efficiency

Higher customer satisfaction American Fidelity UiPath Customer Service Customer facing processes such as managing customer emails Increased productivity Improved accuracy

Freed employees to focus on customer service. Hollard Group UiPath Customer Service Streamlining broker communications through email automation Saved 2,000 hours per month of processing time

Fully automated 98% of the process

Reduced execution time by 600%

Cut cost per transaction by 91% EXL Automation Anywhere Operations End-to-end claims processing Reduced processing time

Reduced audit

Improved accuracy Private sector insurance company AutomationEdge Customer Service Processing queries from SMEs via email, direct branch request, phone, agents and third party resellers are automated Reduction in quote generation time

Reduced policy booking time

Increased conversion rate

Reduced errors Life insurance company AutomationEdge Policy Admin System Proposal form processing Improved customer satisfaction

Error Reduction

Improved SLA compliance

Reduction in cost of service An insurance company AutomationEdge Customer Service Automatic processing of quotes and payouts 10 FTEs cost saving

Reduced quote generation time

If you still have questions on RPA applications in the insurance industry, don’t hesitate to contact us:

Let us find the right vendor for your business","['automation', 'insurance', 'case', 'cases', 'data', 'customer', 'studies', 'process', 'industry', 'rpa', 'processes', 'processing', 'service', 'insurers', '2021']","However, especially auto, property, and workers’ compensation include more standardized processes where insurance automation tools like RPA can lead to high levels of automation.
Is RPA enough to automate insurance processes?
What are real-world case studies of RPA in insurance?
If you want to see a more comprehensive list of RPA case studies, feel free to check our related article.
Cut down redundant tasks Improved efficiencyHigher customer satisfaction American Fidelity UiPath Customer Service Customer facing processes such as managing customer emails Increased productivity Improved accuracyFreed employees to focus on customer service."
232,https://research.aimultiple.com/ecm-applications/,12 practical AI use cases / applications in ECM / CSP / IIM,2020-10-10 10:41:22+00:00,"Using AI-powered Enterprise Content Management (ECM) systems, companies are able to reduce their processing times and costs by >50% and make it easier for employees to find relevant information. This article will focus on AI use cases in ECM such as document/process automation, facilitating collaboration and content management.

ECM is also called Content Service Platforms (CSP) or Intelligent Information Management (IIM). If you want to get an overview on ECM, we explained how AI is changing Enterprise Content Management (ECM) previously and prepared an exhaustive ECM vendor selection list.

What are AI applications in ECM?

Document processing

Documents are everywhere and they can be classified as semi structured data. All documents of a certain type have certain data included in them. For example, all invoices include a sender and recipient, all contract include the agreeing parties, terms, an agreement date etc.

Companies, individuals and governments use documents to communicate with each other. Even though XML/JSON based machine-to-machine (M2M) interfaces are becoming increasingly common, documents are still an important part of communication since adoption of M2M interfaces takes time.

Document splitting

Companies often collect multiple documents from their clients and store these in a single file. For example, a visa application agency may collect a copy of your ID, passport, bank details, etc. and scan all these documents to a single file dedicated to your application process. ECM split these files into individual documents so they can be processed.

Document classification

Different documents require different processing steps. For example, invoices need to be paid out to suppliers, receipts submitted by employees need to be paid out as part of wages etc. And different types of documents can be combined in submissions to the company. For example, a supplier may send the contract along with an invoice and these documents would need to be processed in different ways. Therefore, identifying the right type of document is important to automate document processing.

An AI / ML facilitated ECM system can classify documents and store them in easily retrievable environments. Machine learning algorithms learn from existing data and apply the knowledge on unseen data. Therefore, information in the company’s existing documents can be used to classify more recent documents and store them in easily retrievable places for later usage. Based on the type of document, additional document specific workflows can be triggered for automation.

Document data capturing

Document data capturing technology can extract data from documents with high accuracy thanks to machine learning models.

Captured/extracted data processing

Once data is captured, it can be analyzed, validated or enriched, ensuring end-to-end process automation. For example, semantic analysis and natural language processing techniques can be used to analyze text data captured from documents.

If you want to learn more about document processing, feel free to check our articles about document automation and contract automation.

Unstructured data processing

Emails, videos, audio files are classified as unstructured data since a certain type of unstructured data (e.g. emails) do not include any common data beyond some metadata such as sender, recipient and email delivery date. These data also need to be analyzed and processed.

Emails

For example, an AI-powered ECM system can analyze the language used in emails and other unstructured documents to reveal information about the documents’ context and intent and decipher relationships to employees, projects, and customers.

Video

Technologies such as object detection and action recognition are widely used in the sports industry to analyze matches or players. Clubs often build video recording systems to their training facilities to extract training data, which helps coaches to build their strategies and tactics for real games in a data-driven fashion. Information regarding the fitness status of players or attributes of players can be identified by analyzing training/match videos and personalized training or diet programs can be provided to players.

Videos are widely used also with commercial and communicational purposes. However, many industries suffer from a lack of data extraction from videos and analysis of these videos. Companies can use AI-integrated ECM to understand their customers, employees or their business processes better by analyzing video content.

Other

AI-powered technologies such as image recognition and voice recognition can extract information from image and audio files. For example, images uploaded to the ECM system can be tagged automatically by image recognition systems, and the content of the image can be noted as a text without human intervention. Similarly, audio files can be transcribed into text files.

Content management","['information', 'content', 'iim', 'cases', 'data', 'used', 'type', 'example', 'videos', 'csp', 'ai', 'document', 'practical', 'applications', '12', 'documents', 'ecm']","This article will focus on AI use cases in ECM such as document/process automation, facilitating collaboration and content management.
ECM is also called Content Service Platforms (CSP) or Intelligent Information Management (IIM).
If you want to get an overview on ECM, we explained how AI is changing Enterprise Content Management (ECM) previously and prepared an exhaustive ECM vendor selection list.
Document data capturingDocument data capturing technology can extract data from documents with high accuracy thanks to machine learning models.
Unstructured data processingEmails, videos, audio files are classified as unstructured data since a certain type of unstructured data (e.g."
233,https://www.sciencedaily.com/releases/2020/12/201215112009.htm,"'Chaotic' way to create insectlike gaits for robots: By using small networks of Rössler systems, a locomotion controller enables a brain-machine interface for a six-legged antlike robot.",2020-12-20 00:00:00,"Researchers in Japan and Italy are embracing chaos and nonlinear physics to create insectlike gaits for tiny robots -- complete with a locomotion controller to provide a brain-machine interface.

Biology and physics are permeated by universal phenomena fundamentally grounded in nonlinear physics, and it inspired the researchers' work.

In the journal Chaos, from AIP Publishing, the group describes using the Rössler system, a system of three nonlinear differential equations, as a building block for central pattern generators (CPGs) to control the gait of a robotic insect.

""The universal nature of underlying phenomena allowed us to demonstrate that locomotion can be achieved via elementary combinations of Rössler systems, which represent a cornerstone in the history of chaotic systems,"" said Ludovico Minati, of Tokyo Institute of Technology and the University of Trento.

Phenomena related to synchronization allow the group to create very simple networks that generate complex rhythmic patterns.

""These networks, CPGs, are the basis of legged locomotion everywhere within nature,"" he said.

advertisement

The researchers started with a minimalistic network in which each instance is associated with one leg. Changing the gait or creating a new one can be accomplished by simply making small changes to the coupling and associated delays.

In other words, irregularity can be added by making individual systems or the entire network more chaotic. For nonlinear systems, a change of output is not proportional to a change of input.

This work shows that the Rössler system, beyond its many interesting and intricate properties, ""can also be successfully used as a substrate to construct a bioinspired locomotion controller for an insect robot,"" Minati said.

Their controller is built with an electroencephalogram to enable a brain-computer interface.

""Neuroelectrical activity from a person is recorded and nonlinear concepts of phase synchronization are used to extract a pattern,"" said Minati. ""This pattern is then used as a basis to influence the dynamics of the Rössler systems, which generate the walking pattern for the insect robot.""

The researchers tap into the fundamental ideas of nonlinear dynamics twice.

""First, we use them to decode biological activity, then in the opposite direction to generate bioinspired activity,"" he said.

The key implication of this work is that it ""demonstrates the generality of nonlinear dynamic concepts such as the ability of the Rössler system, which is often studied in an abstract scenario,"" Minati said, ""but is used here as a basis to generate biologically plausible patterns.""","['using', 'rössler', 'nonlinear', 'locomotion', 'used', 'robot', 'researchers', 'generate', 'pattern', 'system', 'networks', 'way', 'robots', 'systems', 'sixlegged', 'small', 'minati']","Researchers in Japan and Italy are embracing chaos and nonlinear physics to create insectlike gaits for tiny robots -- complete with a locomotion controller to provide a brain-machine interface.
Biology and physics are permeated by universal phenomena fundamentally grounded in nonlinear physics, and it inspired the researchers' work.
For nonlinear systems, a change of output is not proportional to a change of input.
""Neuroelectrical activity from a person is recorded and nonlinear concepts of phase synchronization are used to extract a pattern,"" said Minati.
""This pattern is then used as a basis to influence the dynamics of the Rössler systems, which generate the walking pattern for the insect robot."""
234,https://www.sciencedaily.com/releases/2020/12/201215142218.htm,"AI model shows promise to generate faster, more accurate weather forecasts",2020-12-20 00:00:00,"Today's weather forecasts come from some of the most powerful computers on Earth. The huge machines churn through millions of calculations to solve equations to predict temperature, wind, rainfall and other weather events. A forecast's combined need for speed and accuracy taxes even the most modern computers.

The future could take a radically different approach. A collaboration between the University of Washington and Microsoft Research shows how artificial intelligence can analyze past weather patterns to predict future events, much more efficiently and potentially someday more accurately than today's technology.

The newly developed global weather model bases its predictions on the past 40 years of weather data, rather than on detailed physics calculations. The simple, data-based A.I. model can simulate a year's weather around the globe much more quickly and almost as well as traditional weather models, by taking similar repeated steps from one forecast to the next, according to a paper published this summer in the Journal of Advances in Modeling Earth Systems.

""Machine learning is essentially doing a glorified version of pattern recognition,"" said lead author Jonathan Weyn, who did the research as part of his UW doctorate in atmospheric sciences. ""It sees a typical pattern, recognizes how it usually evolves and decides what to do based on the examples it has seen in the past 40 years of data.""

Although the new model is, unsurprisingly, less accurate than today's top traditional forecasting models, the current A.I. design uses about 7,000 times less computing power to create forecasts for the same number of points on the globe. Less computational work means faster results.

That speedup would allow the forecasting centers to quickly run many models with slightly different starting conditions, a technique called ""ensemble forecasting"" that lets weather predictions cover the range of possible expected outcomes for a weather event -- for instance, where a hurricane might strike.

""There's so much more efficiency in this approach; that's what's so important about it,"" said author Dale Durran, a UW professor of atmospheric sciences. ""The promise is that it could allow us to deal with predictability issues by having a model that's fast enough to run very large ensembles.""

Co-author Rich Caruana at Microsoft Research had initially approached the UW group to propose a project using artificial intelligence to make weather predictions based on historical data without relying on physical laws. Weyn was taking a UW computer science course in machine learning and decided to tackle the project.

""After training on past weather data, the A.I. algorithm is capable of coming up with relationships between different variables that physics equations just can't do,"" Weyn said. ""We can afford to use a lot fewer variables and therefore make a model that's much faster.""

To merge successful A.I. techniques with weather forecasting, the team mapped six faces of a cube onto planet Earth, then flattened out the cube's six faces, like in an architectural paper model. The authors treated the polar faces differently because of their unique role in the weather as one way to improve the forecast's accuracy.

The authors then tested their model by predicting the global height of the 500 hectopascal pressure, a standard variable in weather forecasting, every 12 hours for a full year. A recent paper, which included Weyn as a co-author, introduced WeatherBench as a benchmark test for data-driven weather forecasts. On that forecasting test, developed for three-day forecasts, this new model is one of the top performers.

The data-driven model would need more detail before it could begin to compete with existing operational forecasts, the authors say, but the idea shows promise as an alternative approach to generating weather forecasts, especially with a growing amount of previous forecasts and weather observations.","['thats', 'uw', 'weyn', 'model', 'generate', 'forecasts', 'ai', 'past', 'faster', 'accurate', 'promise', 'shows', 'forecasting', 'todays', 'weather']","Today's weather forecasts come from some of the most powerful computers on Earth.
The newly developed global weather model bases its predictions on the past 40 years of weather data, rather than on detailed physics calculations.
""We can afford to use a lot fewer variables and therefore make a model that's much faster.""
A recent paper, which included Weyn as a co-author, introduced WeatherBench as a benchmark test for data-driven weather forecasts.
The data-driven model would need more detail before it could begin to compete with existing operational forecasts, the authors say, but the idea shows promise as an alternative approach to generating weather forecasts, especially with a growing amount of previous forecasts and weather observations."
235,https://www.sciencedaily.com/releases/2020/12/201215131236.htm,"To the brain, reading computer code is not the same as reading language: Neuroscientists find that interpreting code activates a general-purpose brain network, but not language-processing centers",2020-12-20 00:00:00,"In some ways, learning to program a computer is similar to learning a new language. It requires learning new symbols and terms, which must be organized correctly to instruct the computer what to do. The computer code must also be clear enough that other programmers can read and understand it.

In spite of those similarities, MIT neuroscientists have found that reading computer code does not activate the regions of the brain that are involved in language processing. Instead, it activates a distributed network called the multiple demand network, which is also recruited for complex cognitive tasks such as solving math problems or crossword puzzles.

However, although reading computer code activates the multiple demand network, it appears to rely more on different parts of the network than math or logic problems do, suggesting that coding does not precisely replicate the cognitive demands of mathematics either.

""Understanding computer code seems to be its own thing. It's not the same as language, and it's not the same as math and logic,"" says Anna Ivanova, an MIT graduate student and the lead author of the study.

Evelina Fedorenko, the Frederick A. and Carole J. Middleton Career Development Associate Professor of Neuroscience and a member of the McGovern Institute for Brain Research, is the senior author of the paper, which appears today in eLife. Researchers from MIT's Computer Science and Artificial Intelligence Laboratory and Tufts University were also involved in the study.

Language and cognition

A major focus of Fedorenko's research is the relationship between language and other cognitive functions. In particular, she has been studying the question of whether other functions rely on the brain's language network, which includes Broca's area and other regions in the left hemisphere of the brain. In previous work, her lab has shown that music and math do not appear to activate this language network.

advertisement

""Here, we were interested in exploring the relationship between language and computer programming, partially because computer programming is such a new invention that we know that there couldn't be any hardwired mechanisms that make us good programmers,"" Ivanova says.

There are two schools of thought regarding how the brain learns to code, she says. One holds that in order to be good at programming, you must be good at math. The other suggests that because of the parallels between coding and language, language skills might be more relevant. To shed light on this issue, the researchers set out to study whether brain activity patterns while reading computer code would overlap with language-related brain activity.

The two programming languages that the researchers focused on in this study are known for their readability -- Python and ScratchJr, a visual programming language designed for children age 5 and older. The subjects in the study were all young adults proficient in the language they were being tested on. While the programmers lay in a functional magnetic resonance (fMRI) scanner, the researchers showed them snippets of code and asked them to predict what action the code would produce.

The researchers saw little to no response to code in the language regions of the brain. Instead, they found that the coding task mainly activated the so-called multiple demand network. This network, whose activity is spread throughout the frontal and parietal lobes of the brain, is typically recruited for tasks that require holding many pieces of information in mind at once, and is responsible for our ability to perform a wide variety of mental tasks.

""It does pretty much anything that's cognitively challenging, that makes you think hard,"" Ivanova says.

advertisement

Previous studies have shown that math and logic problems seem to rely mainly on the multiple demand regions in the left hemisphere, while tasks that involve spatial navigation activate the right hemisphere more than the left. The MIT team found that reading computer code appears to activate both the left and right sides of the multiple demand network, and ScratchJr activated the right side slightly more than the left. This finding goes against the hypothesis that math and coding rely on the same brain mechanisms.

Effects of experience

The researchers say that while they didn't identify any regions that appear to be exclusively devoted to programming, such specialized brain activity might develop in people who have much more coding experience.

""It's possible that if you take people who are professional programmers, who have spent 30 or 40 years coding in a particular language, you may start seeing some specialization, or some crystallization of parts of the multiple demand system,"" Fedorenko says. ""In people who are familiar with coding and can efficiently do these tasks, but have had relatively limited experience, it just doesn't seem like you see any specialization yet.""

In a companion paper appearing in the same issue of eLife, a team of researchers from Johns Hopkins University also reported that solving code problems activates the multiple demand network rather than the language regions.

The findings suggest there isn't a definitive answer to whether coding should be taught as a math-based skill or a language-based skill. In part, that's because learning to program may draw on both language and multiple demand systems, even if -- once learned -- programming doesn't rely on the language regions, the researchers say.

""There have been claims from both camps -- it has to be together with math, it has to be together with language,"" Ivanova says. ""But it looks like computer science educators will have to develop their own approaches for teaching code most effectively.""

The research was funded by the National Science Foundation, the Department of the Brain and Cognitive Sciences at MIT, and the McGovern Institute for Brain Research.","['reading', 'network', 'demand', 'math', 'brain', 'researchers', 'code', 'language', 'languageprocessing', 'computer', 'coding', 'interpreting', 'generalpurpose', 'multiple', 'neuroscientists']","The computer code must also be clear enough that other programmers can read and understand it.
In spite of those similarities, MIT neuroscientists have found that reading computer code does not activate the regions of the brain that are involved in language processing.
Instead, it activates a distributed network called the multiple demand network, which is also recruited for complex cognitive tasks such as solving math problems or crossword puzzles.
""Understanding computer code seems to be its own thing.
The other suggests that because of the parallels between coding and language, language skills might be more relevant."
236,https://www.sciencedaily.com/releases/2020/12/201211115457.htm,Challenges of fusing robotics and neuroscience,2020-12-20 00:00:00,"Combining neuroscience and robotic research has gained impressive results in the rehabilitation of paraplegic patients. A research team led by Prof. Gordon Cheng from the Technical University of Munich (TUM) was able to show that exoskeleton training not only helped patients to walk, but also stimulated their healing process. With these findings in mind, Prof. Cheng wants to take the fusion of robotics and neuroscience to the next level.

Prof. Cheng, by training a paraplegic patient with the exoskeleton within your sensational study under the ""Walk Again"" project, you found that patients regained a certain degree of control over the movement of their legs. Back then, this came as a complete surprise to you ...

... and it somehow still is. Even though we had this breakthrough four years ago, this was only the beginning. To my regret, none of these patients is walking around freely and unaided yet. We have only touched the tip of the iceberg. To develop better medical devices, we need to dig deeper in understanding how the brain works and how to translate this into robotics.

In your paper published in Science Robotics this month, you and your colleague Prof. Nicolelis, a leading expert in neuroscience and in particular in the area of the human-machine interface, argue that some key challenges in the fusion of neuroscience and robotics need to be overcome in order to take the next steps. One of them is to ""close the loop between the brain and the machine"" -- what do you mean by that?

The idea behind this is that the coupling between the brain and the machine should work in a way where the brain thinks of the machine as an extension of the body. Let's take driving as an example. While driving a car, you don't think about your moves, do you? But we still don't know how this really works. My theory is that the brain somehow adapts to the car as if it is a part of the body. With this general idea in mind, it would be great to have an exoskeleton that would be embraced by the brain in the same way.

How could this be achieved in practice?

The exoskeleton that we were using for our research so far is actually just a big chunk of metal and thus rather cumbersome for the wearer. I want to develop a ""soft"" exoskeleton -- something that you can just wear like a piece of clothing that can both sense the user's movement intentions and provide instantaneous feedback. Integrating this with recent advances in brain-machine interfaces that allow real-time measurement of brain responses enables the seamless adaptation of such exoskeletons to the needs of individual users. Given the recent technological advances and better understanding of how to decode the user's momentary brain activity, the time is ripe for their integration into more human-centered or, better ? brain-centered ? solutions.

What other pieces are still missing? You talked about providing a ""more realistic functional model"" for both disciplines.

We have to facilitate the transfer through new developments, for example robots that are closer to human behaviour and the construction of the human body and thus lower the threshold for the use of robots in neuroscience. This is why we need more realistic functional models, which means that robots should be able to mimic human characteristics. Let's take the example of a humanoid robot actuated with artificial muscles. This natural construction mimicking muscles instead of the traditional motorized actuation would provide neuroscientists with a more realistic model for their studies. We think of this as a win-win situation to facilitate better cooperation between neuroscience and robotics in the future.

You are not alone in the mission of overcoming these challenges. In your Elite Graduate Program in Neuroengineering, the first and only one of its kind in Germany combining experimental and theoretical neuroscience with in-depth training in engineering, you are bringing together the best students in the field.

As described above, combining the two disciplines of robotics and neuroscience is a tough exercise, and therefore one of the main reasons why I created this master's program in Munich. To me, it is important to teach the students to think more broadly and across disciplines, to find previously unimagined solutions. This is why lecturers from various fields, for example hospitals or the sports department, are teaching our students. We need to create a new community and a new culture in the field of engineering. From my standpoint, education is the key factor.","['training', 'neuroscience', 'users', 'brain', 'need', 'patients', 'example', 'challenges', 'exoskeleton', 'robotics', 'fusing', 'better']","With these findings in mind, Prof. Cheng wants to take the fusion of robotics and neuroscience to the next level.
To develop better medical devices, we need to dig deeper in understanding how the brain works and how to translate this into robotics.
In your paper published in Science Robotics this month, you and your colleague Prof. Nicolelis, a leading expert in neuroscience and in particular in the area of the human-machine interface, argue that some key challenges in the fusion of neuroscience and robotics need to be overcome in order to take the next steps.
We think of this as a win-win situation to facilitate better cooperation between neuroscience and robotics in the future.
As described above, combining the two disciplines of robotics and neuroscience is a tough exercise, and therefore one of the main reasons why I created this master's program in Munich."
237,https://www.sciencedaily.com/releases/2020/12/201211100627.htm,Artificial intelligence helps scientists develop new general models in ecology,2020-12-20 00:00:00,"In ecology, millions of species interact in billions of different ways between them and with their environment. Ecosystems often seem chaotic, or at least overwhelming for someone trying to understand them and make predictions for the future.

Artificial intelligence and machine learning are able to detect patterns and predict outcomes in ways that often resemble human reasoning. They pave the way to increasingly powerful cooperation between humans and computers.

Within AI, evolutionary computation methods replicate in some sense the processes of evolution of species in the natural world. A particular method called symbolic regression allows the evolution of human-interpretable formulas that explain natural laws.

""We used symbolic regression to demonstrate that computers are able to derive formulas that represent the way ecosystems or species behave in space and time. These formulas are also easy to understand. They pave the way for general rules in ecology, something that most methods in AI cannot do,"" says Pedro Cardoso, curator at the Finnish Museum of Natural History, University of Helsinki.

With the help of the symbolic regression method, an interdisciplinary team from Finland, Portugal, and France was able to explain why some species exist in some regions and not in others, and why some regions have more species than others.

The researchers were able, for example, to find a new general model that explains why some islands have more species than others. Oceanic islands have a natural life-cycle, emerging from volcanoes and eventually submerging with erosion after millions of years. With no human input, the algorithm was able to find that the number of species of an island increases with the island age and peaks with intermediate ages, when erosion is still low.

""The explanation was known, a couple of formulas already existed, but we were able to find new ones that outperform the existing ones under certain circumstances,"" says Vasco Branco, PhD student working on the automation of extinction risk assessments at the University of Helsinki.

The research proposes that explainable artificial intelligence is a field to explore and promotes the cooperation between humans and machines in ways that are only now starting to scratch the surface.

""Evolving free-form equations purely from data, often without prior human inference or hypotheses, may represent a very powerful tool in the arsenal of a discipline as complex as ecology,"" says Luis Correia, computer science professor at the University of Lisbon.","['models', 'helps', 'artificial', 'develop', 'able', 'university', 'formulas', 'natural', 'scientists', 'way', 'symbolic', 'ecology', 'ways', 'general', 'regression', 'human', 'species', 'intelligence']","In ecology, millions of species interact in billions of different ways between them and with their environment.
Artificial intelligence and machine learning are able to detect patterns and predict outcomes in ways that often resemble human reasoning.
A particular method called symbolic regression allows the evolution of human-interpretable formulas that explain natural laws.
""We used symbolic regression to demonstrate that computers are able to derive formulas that represent the way ecosystems or species behave in space and time.
The researchers were able, for example, to find a new general model that explains why some islands have more species than others."
238,https://www.sciencedaily.com/releases/2020/12/201211083041.htm,Artificial Chemist 2.0: quantum dot R&D in less than an hour,2020-12-20 00:00:00,"A new technology, called Artificial Chemist 2.0, allows users to go from requesting a custom quantum dot to completing the relevant R&D and beginning manufacturing in less than an hour. The tech is completely autonomous, and uses artificial intelligence (AI) and automated robotic systems to perform multi-step chemical synthesis and analysis.

Quantum dots are colloidal semiconductor nanocrystals, which are used in applications such as LED displays and solar cells.

""When we rolled out the first version of Artificial Chemist, it was a proof of concept,"" says Milad Abolhasani, corresponding author of a paper on the work and an assistant professor of chemical and biomolecular engineering at North Carolina State University. ""Artificial Chemist 2.0 is industrially relevant for both R&D and manufacturing.""

From a user standpoint, the whole process essentially consists of three steps. First, a user tells Artificial Chemist 2.0 the parameters for the desired quantum dots. For example, what color light do you want to produce? The second step is effectively the R&D stage, where Artificial Chemist 2.0 autonomously conducts a series of rapid experiments, allowing it to identify the optimum material and the most efficient means of producing that material. Third, the system switches over to manufacturing the desired amount of the material.

""Quantum dots can be divided up into different classes,"" Abolhasani says. ""For example, well-studied II-VI, IV-VI, and III-V materials, or the recently emerging metal halide perovskites, and so on. Basically, each class consists of a range of materials that have similar chemistries.

""And the first time you set up Artificial Chemist 2.0 to produce quantum dots in any given class, the robot autonomously runs a set of active learning experiments. This is how the brain of the robotic system learns the materials chemistry,"" Abolhasani says. ""Depending on the class of material, this learning stage can take between one and 10 hours. After that one-time active learning period, Artificial Chemist 2.0 can identify the best possible formulation for producing the desired quantum dots from 20 million possible combinations with multiple manufacturing steps in 40 minutes or less.""

The researchers note that the R&D process will almost certainly become faster every time people use it, since the AI algorithm that runs the system will learn more -- and become more efficient -- with every material that it is asked to identify.

Artificial Chemist 2.0 incorporates two chemical reactors, which operate in a series. The system is designed to be entirely autonomous, and allows users to switch from one material to another without having to shut down the system. Video of how the system works can be found at https://youtu.be/e_DyV-hohLw.

""In order to do this successfully, we had to engineer a system that leaves no chemical residues in the reactors and allows the AI-guided robotic system to add the right ingredients, at the right time, at any point in the multi-step material production process,"" Abolhasani says. ""So that's what we did.

""We're excited about what this means for the specialty chemicals industry. It really accelerates R&D to warp speed, but it is also capable of making kilograms per day of high-value, precisely engineered quantum dots. Those are industrially relevant volumes of material.""","['artificial', 'hour', 'chemical', 'system', 'rd', 'dot', 'chemist', 'abolhasani', 'dots', 'material', '20', 'quantum']","A new technology, called Artificial Chemist 2.0, allows users to go from requesting a custom quantum dot to completing the relevant R&D and beginning manufacturing in less than an hour.
""Artificial Chemist 2.0 is industrially relevant for both R&D and manufacturing.""
First, a user tells Artificial Chemist 2.0 the parameters for the desired quantum dots.
""And the first time you set up Artificial Chemist 2.0 to produce quantum dots in any given class, the robot autonomously runs a set of active learning experiments.
After that one-time active learning period, Artificial Chemist 2.0 can identify the best possible formulation for producing the desired quantum dots from 20 million possible combinations with multiple manufacturing steps in 40 minutes or less."""
239,https://www.sciencedaily.com/releases/2020/12/201211100646.htm,'The robot made me do it': Robots encourage risk-taking behavior in people,2020-12-20 00:00:00,"New research has shown robots can encourage people to take greater risks in a simulated gambling scenario than they would if there was nothing to influence their behaviours. Increasing our understanding of whether robots can affect risk-taking could have clear ethical, practical and policy implications, which this study set out to explore.

Dr Yaniv Hanoch, Associate Professor in Risk Management at the University of Southampton who led the study explained, ""We know that peer pressure can lead to higher risk-taking behaviour. With the ever-increasing scale of interaction between humans and technology, both online and physically, it is crucial that we understand more about whether machines can have a similar impact.""

This new research, published in the journal Cyberpsychology, Behavior, and Social Networking, involved 180 undergraduate students taking the Balloon Analogue Risk Task (BART), a computer assessment that asks participants to press the spacebar on a keyboard to inflate a balloon displayed on the screen. With each press of the spacebar, the balloon inflates slightly, and 1 penny is added to the player's ""temporary money bank."" The balloons can explode randomly, meaning the player loses any money they have won for that balloon and they have the option to ""cash-in"" before this happens and move on to the next balloon.

One-third of the participants took the test in a room on their own (the control group), one third took the test alongside a robot that only provided them with the instructions but was silent the rest of the time and the final, the experimental group, took the test with the robot providing instruction as well as speaking encouraging statements such as ""why did you stop pumping?""

The results showed that the group who were encouraged by the robot took more risks, blowing up their balloons significantly more frequently than those in the other groups did. They also earned more money overall. There was no significant difference in the behaviours of the students accompanied by the silent robot and those with no robot.

Dr Hanoch said: ""We saw participants in the control condition scale back their risk-taking behaviour following a balloon explosion, whereas those in the experimental condition continued to take as much risk as before. So, receiving direct encouragement from a risk-promoting robot seemed to override participants' direct experiences and instincts.""

The researcher now believe that further studies are needed to see whether similar results would emerge from human interaction with other artificial intelligence (AI) systems, such as digital assistants or on-screen avatars.

Dr Hanoch concluded, ""With the wide spread of AI technology and its interactions with humans, this is an area that needs urgent attention from the research community.""

""On the one hand, our results might raise alarms about the prospect of robots causing harm by increasing risky behavior. On the other hand, our data points to the possibility of using robots and AI in preventive programs, such as anti-smoking campaigns in schools, and with hard to reach populations, such as addicts.""","['participants', 'test', 'research', 'encourage', 'robot', 'risktaking', 'results', 'robots', 'balloon', 'behavior', 'took', 'risk']","New research has shown robots can encourage people to take greater risks in a simulated gambling scenario than they would if there was nothing to influence their behaviours.
Increasing our understanding of whether robots can affect risk-taking could have clear ethical, practical and policy implications, which this study set out to explore.
The results showed that the group who were encouraged by the robot took more risks, blowing up their balloons significantly more frequently than those in the other groups did.
There was no significant difference in the behaviours of the students accompanied by the silent robot and those with no robot.
""On the one hand, our results might raise alarms about the prospect of robots causing harm by increasing risky behavior."
240,https://www.sciencedaily.com/releases/2020/11/201118080758.htm,"New electronic chip delivers smarter, light-powered AI: Prototype tech shrinks AI to deliver brain-like functionality in one powerful device",2020-11-20 00:00:00,"Researchers have developed artificial intelligence technology that brings together imaging, processing, machine learning and memory in one electronic chip, powered by light.

The prototype shrinks artificial intelligence technology by imitating the way that the human brain processes visual information.

The nanoscale advance combines the core software needed to drive artificial intelligence with image-capturing hardware in a single electronic device.

With further development, the light-driven prototype could enable smarter and smaller autonomous technologies like drones and robotics, plus smart wearables and bionic implants like artificial retinas.

The study, from an international team of Australian, American and Chinese researchers led by RMIT University, is published in the journal Advanced Materials.

Lead researcher Associate Professor Sumeet Walia, from RMIT, said the prototype delivered brain-like functionality in one powerful device.

advertisement

""Our new technology radically boosts efficiency and accuracy by bringing multiple components and functionalities into a single platform,"" Walia who also co-leads the Functional Materials and Microsystems Research Group said.

""It's getting us closer to an all-in-one AI device inspired by nature's greatest computing innovation -- the human brain.

""Our aim is to replicate a core feature of how the brain learns, through imprinting vision as memory.

""The prototype we've developed is a major leap forward towards neurorobotics, better technologies for human-machine interaction and scalable bionic systems.""

Total package: advancing AI

Typically artificial intelligence relies heavily on software and off-site data processing.

advertisement

The new prototype aims to integrate electronic hardware and intelligence together, for fast on-site decisions.

""Imagine a dash cam in a car that's integrated with such neuro-inspired hardware -- it can recognise lights, signs, objects and make instant decisions, without having to connect to the internet,"" Walia said.

""By bringing it all together into one chip, we can deliver unprecedented levels of efficiency and speed in autonomous and AI-driven decision-making.""

The technology builds on an earlier prototype chip from the RMIT team, which used light to create and modify memories.

New built-in features mean the chip can now capture and automatically enhance images, classify numbers, and be trained to recognise patterns and images with an accuracy rate of over 90%.

The device is also readily compatible with existing electronics and silicon technologies, for effortless future integration.

Seeing the light: how the tech works

The prototype is inspired by optogenetics, an emerging tool in biotechnology that allows scientists to delve into the body's electrical system with great precision and use light to manipulate neurons.

The AI chip is based on an ultra-thin material -- black phosphorous -- that changes electrical resistance in response to different wavelengths of light.

The different functionalities such as imaging or memory storage are achieved by shining different colours of light on the chip.

Study lead author Dr Taimur Ahmed, from RMIT, said light-based computing was faster, more accurate and required far less energy than existing technologies.

""By packing so much core functionality into one compact nanoscale device, we can broaden the horizons for machine learning and AI to be integrated into smaller applications,"" Ahmed said.

""Using our chip with artificial retinas, for example, would enable scientists to miniaturise that emerging technology and improve accuracy of the bionic eye.

""Our prototype is a significant advance towards the ultimate in electronics: a brain-on-a-chip that can learn from its environment just like we do.""","['walia', 'rmit', 'electronic', 'light', 'shrinks', 'chip', 'ai', 'powerful', 'lightpowered', 'tech', 'device', 'prototype', 'artificial', 'hardware', 'technology', 'intelligence', 'smarter', 'functionality', 'technologies']","Researchers have developed artificial intelligence technology that brings together imaging, processing, machine learning and memory in one electronic chip, powered by light.
The prototype shrinks artificial intelligence technology by imitating the way that the human brain processes visual information.
The nanoscale advance combines the core software needed to drive artificial intelligence with image-capturing hardware in a single electronic device.
Lead researcher Associate Professor Sumeet Walia, from RMIT, said the prototype delivered brain-like functionality in one powerful device.
The technology builds on an earlier prototype chip from the RMIT team, which used light to create and modify memories."
241,https://www.sciencedaily.com/releases/2020/11/201117144539.htm,AI tool may predict movies' future ratings,2020-11-20 00:00:00,"Movie ratings can determine a movie's appeal to consumers and the size of its potential audience. Thus, they have an impact on a film's bottom line. Typically, humans do the tedious task of manually rating a movie based on viewing the movie and making decisions on the presence of violence, drug abuse and sexual content.

Now, researchers at the USC Viterbi School of Engineering, armed with artificial intelligence tools, can rate a movie's content in a matter of seconds, based on the movie script and before a single scene is shot. Such an approach could allow movie executives the ability to design a movie rating in advance and as desired, by making the appropriate edits on a script and before the shooting of a single scene. Beyond the potential financial impact, such instantaneous feedback would allow storytellers and decision-makers to reflect on the content they are creating for the public and the impact such content might have on viewers.

Using artificial intelligence applied to scripts, Shrikanth Narayanan, University Professor and Niki & C. L. Max Nikias Chair in Engineering, and a team of researchers from the Signal Analysis and Interpretation Lab (SAIL) at USC Viterbi, have demonstrated that linguistic cues can effectively signal behaviors on violent acts, drug abuse and sexual content (actions that are often the basis for a film's ratings) about to be taken by a film's characters.

Method:

Using 992 movie scripts that included violent, substance-abuse and sexual content, as determined by Common Sense Media, a non-profit organization that rates and makes recommendations for families and schools, the SAIL research team trained artificial intelligence to recognize corresponding risk behaviors, patterns and language.

The AI tool created receives as input all the script, processes it through a neural network and scans it for semantics and sentiment expressed. In the process, it classifies sentences and phrases as positive, negative, aggressive and other descriptors. The AI tool automatically classifies words and phrases into three categories: violence, drug abuse and sexual content.

advertisement

Victor Martinez, a doctoral candidate in computer science at USC Viterbi and the lead researcher on the study, which will appear in The Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing said, ""Our model looks at the movie script, rather than the actual scenes, including e.g. sounds like a gunshot or explosion that occur later in the production pipeline. This has the benefit of providing a rating long before production to help filmmakers decide e.g. the degree of violence and whether it needs to be toned down.""

The research team also includes Narayanan, a professor of electrical and computer engineering, computer science and linguistics, Krishna Somandepalli, a Ph.D. candidate in Electrical and Computing Engineering at USC Viterbi, and Professor Yalda T. Uhls of UCLA's Department of Psychology. They discovered many interesting connections between the portrayals of risky behaviors.

""There seems to be a correlation in the amount of content in a typical film focused on substance abuse and the amount of sexual content. Whether intentionally or not, filmmakers seem to match the level of substance abuse-related content with sexually explicit content,"" said Martinez.

Another interesting pattern also emerged. ""We found that filmmakers compensate for low levels of violence with joint portrayals of substance abuse and sexual content,"" Martinez said.

Moreover, while many movies contain depictions of rampant drug-abuse and sexual content, the researchers found it highly unlikely for a film to have high levels of all three risky behaviors, perhaps because of Motion Picture Association (MPA) standards.

advertisement

They also found an interesting connection between risk behaviors and MPA ratings. As sexual content increases, the MPA appears to put less emphasis on violence/substance-abuse content. Thus, regardless of violent and substance abuse content, a movie with a lot of sexual content will likely receive an R rating.

Narayanan whose SAIL lab has pioneered the field of media informatics and applied natural language processing in order to bring awareness in the creative community about the nuances of storytelling, calls media ""a rich avenue for studying human communication, interaction and behavior, since it provides a window into society.""

""At SAIL, we are designing technologies and tools, based on AI, for all stakeholders in this creative business -- the writers, film-makers and producers -- to raise awareness about the varied important details associated in telling their story on film,"" Narayanan said.

""Not only are we interested in the perspective of the storytellers of the narratives they weave,"" Narayanan said, ""but also in understanding the impact on the audience and the 'take-away' from the whole experience. Tools like these will help raise societally-meaningful awareness, for example, through identifying negative stereotypes.""

Added Martinez: ""In the future, I'm interested in studying minorities and how they are represented, particularly in cases of violence, sex and drugs.""","['future', 'viterbi', 'script', 'abuse', 'movies', 'predict', 'ai', 'violence', 'movie', 'sexual', 'sail', 'substance', 'usc', 'content', 'tool', 'ratings']","The AI tool automatically classifies words and phrases into three categories: violence, drug abuse and sexual content.
""There seems to be a correlation in the amount of content in a typical film focused on substance abuse and the amount of sexual content.
""We found that filmmakers compensate for low levels of violence with joint portrayals of substance abuse and sexual content,"" Martinez said.
As sexual content increases, the MPA appears to put less emphasis on violence/substance-abuse content.
Thus, regardless of violent and substance abuse content, a movie with a lot of sexual content will likely receive an R rating."
242,https://www.sciencedaily.com/releases/2020/11/201103104723.htm,Students develop tool to predict the carbon footprint of algorithms,2020-11-20 00:00:00,"On a daily basis, and perhaps without realizing it, most of us are in close contact with advanced AI methods known as deep learning. Deep learning algorithms churn whenever we use Siri or Alexa, when Netflix suggests movies and tv shows based upon our viewing histories, or when we communicate with a website's customer service chatbot.

However, the rapidly evolving technology, one that has otherwise been expected to serve as an effective weapon against climate change, has a downside that many people are unaware of -- sky high energy consumption. Artificial intelligence, and particularly the subfield of deep learning, appears likely to become a significant climate culprit should industry trends continue. In only six years -- from 2012 to 2018 -- the compute needed for deep learning has grown 300,000%. However, the energy consumption and carbon footprint associated with developing algorithms is rarely measured, despite numerous studies that clearly demonstrate the growing problem.

In response to the problem, two students at the University of Copenhagen's Department of Computer Science, Lasse F. Wolff Anthony and Benjamin Kanding, together with Assistant Professor Raghavendra Selvan, have developed a software programme they call Carbontracker. The programme can calculate and predict the energy consumption and CO 2 emissions of training deep learning models.

""Developments in this field are going insanely fast and deep learning models are constantly becoming larger in scale and more advanced. Right now, there is exponential growth. And that means an increasing energy consumption that most people seem not to think about,"" according to Lasse F. Wolff Anthony.

One training session = the annual energy consumption of 126 Danish homes

Deep learning training is the process during which the mathematical model learns to recognize patterns in large datasets. It's an energy-intensive process that takes place on specialized, power-intensive hardware running 24 hours a day.

advertisement

""As datasets grow larger by the day, the problems that algorithms need to solve become more and more complex,"" states Benjamin Kanding.

One of the biggest deep learning models developed thus far is the advanced language model known as GPT-3. In a single training session, it is estimated to use the equivalent of a year's energy consumption of 126 Danish homes, and emit the same amount of CO2 as 700,000 kilometres of driving.

""Within a few years, there will probably be several models that are many times larger,"" says Lasse F. Wolff Anthony.

Room for improvement

""Should the trend continue, artificial intelligence could end up being a significant contributor to climate change. Jamming the brakes on technological development is not the point. These developments offer fantastic opportunities for helping our climate. Instead, it is about becoming aware of the problem and thinking: How might we improve?"" explains Benjamin Kanding.

The idea of Carbontracker, which is a free programme, is to provide the field with a foundation for reducing the climate impact of models. Among other things, the programme gathers information on how much CO 2 is used to produce energy in whichever region the deep learning training is taking place. Doing so makes it possible to convert energy consumption into CO 2 emission predictions.

Among their recommendations, the two computer science students suggest that deep learning practitioners look at when their model trainings take place, as power is not equally green over a 24-hour period, as well as what type of hardware and algorithms they deploy.

""It is possible to reduce the climate impact significantly. For example, it is relevant if one opts to train their model in Estonia or Sweden, where the carbon footprint of a model training can be reduced by more than 60 times thanks to greener energy supplies. Algorithms also vary greatly in their energy efficiency. Some require less compute, and thereby less energy, to achieve similar results. If one can tune these types of parameters, things can change considerably,"" concludes Lasse F. Wolff Anthony.","['students', 'carbon', 'develop', 'model', 'algorithms', 'predict', 'learning', 'deep', 'climate', 'footprint', 'wolff', 'energy', 'consumption', 'programme', 'training', 'tool']","On a daily basis, and perhaps without realizing it, most of us are in close contact with advanced AI methods known as deep learning.
In only six years -- from 2012 to 2018 -- the compute needed for deep learning has grown 300,000%.
However, the energy consumption and carbon footprint associated with developing algorithms is rarely measured, despite numerous studies that clearly demonstrate the growing problem.
The programme can calculate and predict the energy consumption and CO 2 emissions of training deep learning models.
""Developments in this field are going insanely fast and deep learning models are constantly becoming larger in scale and more advanced."
243,https://www.sciencedaily.com/releases/2020/10/201012152055.htm,Using robotic assistance to make colonoscopy kinder and easier,2020-10-20 00:00:00,"Scientists have made a breakthrough in their work to develop semi-autonomous colonoscopy, using a robot to guide a medical device into the body.

The milestone brings closer the prospect of an intelligent robotic system being able to guide instruments to precise locations in the body to take biopsies or allow internal tissues to be examined.

A doctor or nurse would still be on hand to make clinical decisions but the demanding task of manipulating the device is offloaded to a robotic system.

The latest findings -- 'Enabling the future of colonoscopy with intelligent and autonomous magnetic manipulation' -- is the culmination of 12 years of research by an international team of scientists led by the University of Leeds.

The research is published today (Monday, 12 October) in the scientific journal Nature Machine Intelligence.

Patient trials using the system could begin next year or in early 2022.

advertisement

Pietro Valdastri, Professor of Robotics and Autonomous Systems at Leeds, is supervising the research. He said: ""Colonoscopy gives doctors a window into the world hidden deep inside the human body and it provides a vital role in the screening of diseases such as colorectal cancer. But the technology has remained relatively unchanged for decades.

""What we have developed is a system that is easier for doctors or nurses to operate and is less painful for patients. It marks an important a step in the move to make colonoscopy much more widely available -- essential if colorectal cancer is to be identified early.""

Because the system is easier to use, the scientists hope this can increase the number of providers who can perform the procedure and allow for greater patient access to colonoscopy.

A colonoscopy is a procedure to examine the rectum and colon. Conventional colonoscopy is carried out using a semi-flexible tube which is inserted into the anus, a process some patients find so painful they require an anaesthetic.

Magnetic flexible colonoscope

The research team has developed a smaller, capsule-shaped device which is tethered to a narrow cable and is inserted into the anus and then guided into place -- not by the doctor or nurse pushing the colonoscope but by a magnet on a robotic arm positioned over the patient.

advertisement

The robotic arm moves around the patient as it manoeuvres the capsule. The system is based on the principle that magnetic forces attract and repel.

The magnet on the outside of the patient interacts with tiny magnets in the capsule inside the body, navigating it through the colon. The researchers say it will be less painful than having a conventional colonoscopy.

Guiding the robotic arm can be done manually but it is a technique that is difficult to master. In response, the researchers have developed different levels of robotic assistance. This latest research evaluated how effective the different levels of robotic assistance were in aiding non-specialist staff to carry out the procedure.

Levels of robotic assistance

Direct robot control. This is where the operator has direct control of the robot via a joystick. In this case, there is no assistance.

Intelligent endoscope teleoperation. The operator focuses on where they want the capsule to be located in the colon, leaving the robotic system to calculate the movements of the robotic arm necessary to get the capsule into place.

Semi-autonomous navigation. The robotic system autonomously navigates the capsule through the colon, using computer vision -- although this can be overridden by the operator.

During a laboratory simulation, 10 non-expert staff were asked to get the capsule to a point within the colon within 20 minutes. They did that five times, using the three different levels of assistance.

Using direct robot control, the participants had a 58% success rate. That increased to 96% using intelligent endoscope teleoperation -- and 100% using semi-autonomous navigation.

In the next stage of the experiment, two participants were asked to navigate a conventional colonoscope into the colon of two anaesthetised pigs -- and then to repeat the task with the magnet-controlled robotic system using the different levels of assistance. A vet was in attendance to ensure the animals were not harmed.

The participants were scored on the NASA Task Load Index, a measure of how taxing a task was, both physically and mentally.

The NASA Task Load Index revealed that they found it easier to operate the colonoscope with robotic assistance. A sense of frustration was a major factor in operating the conventional colonoscope and where participants had direct control of the robot.

James Martin, a PhD researcher from the University of Leeds who co-led the study, said: ""Operating the robotic arm is challenging. It is not very intuitive and that has put a brake on the development of magnetic flexible colonoscopes.

""But we have demonstrated for the first time that it is possible to offload that function to the robotic system, leaving the operator to think about the clinical task they are undertaking -- and it is making a measurable difference in human performance.""

The techniques developed to conduct colonoscopy examinations could be applied to other endoscopic devices, such as those used to inspect the upper digestive tract or lungs.

Dr Bruno Scaglioni, a Postdoctoral Research Fellow at Leeds and co-leader of the study, added: ""Robot-assisted colonoscopy has the potential to revolutionize the way the procedure is carried out. It means people conducting the examination do not need to be experts in manipulating the device.

""That will hopefully make the technique more widely available, where it could be offered in clinics and health centres rather than hospitals.""","['capsule', 'research', 'task', 'using', 'arm', 'robot', 'kinder', 'colon', 'system', 'colonoscopy', 'robotic', 'easier', 'assistance']","Scientists have made a breakthrough in their work to develop semi-autonomous colonoscopy, using a robot to guide a medical device into the body.
Guiding the robotic arm can be done manually but it is a technique that is difficult to master.
The robotic system autonomously navigates the capsule through the colon, using computer vision -- although this can be overridden by the operator.
The NASA Task Load Index revealed that they found it easier to operate the colonoscope with robotic assistance.
James Martin, a PhD researcher from the University of Leeds who co-led the study, said: ""Operating the robotic arm is challenging."
244,https://www.sciencedaily.com/releases/2020/10/201006165746.htm,This 'squidbot' jets around and takes pics of coral and fish,2020-10-20 00:00:00,"Engineers at the University of California San Diego have built a squid-like robot that can swim untethered, propelling itself by generating jets of water. The robot carries its own power source inside its body. It can also carry a sensor, such as a camera, for underwater exploration.

The researchers detail their work in a recent issue of Bioinspiration and Biomimetics.

""Essentially, we recreated all the key features that squids use for high-speed swimming,"" said Michael T. Tolley, one of the paper's senior authors and a professor in the Department of Mechanical and Aerospace Engineering at UC San Diego. ""This is the first untethered robot that can generate jet pulses for rapid locomotion like the squid and can achieve these jet pulses by changing its body shape, which improves swimming efficiency.""

This squid robot is made mostly from soft materials such as acrylic polymer, with a few rigid, 3D printed and laser cut parts. Using soft robots in underwater exploration is important to protect fish and coral, which could be damaged by rigid robots. But soft robots tend to move slowly and have difficulty maneuvering.

The research team, which includes roboticists and experts in computer simulations as well as experimental fluid dynamics, turned to cephalopods as a good model to solve some of these issues. Squid, for example, can reach the fastest speeds of any aquatic invertebrates thanks to a jet propulsion mechanism.

Their robot takes a volume of water into its body while storing elastic energy in its skin and flexible ribs. It then releases this energy by compressing its body and generates a jet of water to propel itself.

At rest, the squid robot is shaped roughly like a paper lantern, and has flexible ribs, which act like springs, along its sides. The ribs are connected to two circular plates at each end of the robot. One of them is connected to a nozzle that both takes in water and ejects it when the robot's body contracts. The other plate can carry a water-proof camera or a different type of sensor.

Engineers first tested the robot in a water testbed in the lab of Professor Geno Pawlak, in the UC San Diego Department of Mechanical and Aerospace Engineering. Then they took it out for a swim in one of the tanks at the UC San Diego Birch Aquarium at the Scripps Institution of Oceanography.

They demonstrated that the robot could steer by adjusting the direction of the nozzle. As with any underwater robot, waterproofing was a key concern for electrical components such as the battery and camera.They clocked the robot's speed at about 18 to 32 centimeters per second (roughly half a mile per hour), which is faster than most other soft robots.

""After we were able to optimize the design of the robot so that it would swim in a tank in the lab, it was especially exciting to see that the robot was able to successfully swim in a large aquarium among coral and fish, demonstrating its feasibility for real-world applications,"" said Caleb Christianson, who led the study as part of his Ph.D. work in Tolley's research group. He is now a senior medical devices engineering at San Diego-based Dexcom.

Researchers conducted several experiments to find the optimal size and shape for the nozzle that would propel the robot. This in turn helped them increase the robot's efficiency and its ability to maneuver and go faster. This was done mostly by simulating this kind of jet propulsion, work that was led by Professor Qiang Zhu and his team in the Department of Structural Engineering at UC San Diego. The team also learned more about how energy can be stored in the elastic component of the robot's body and skin, which is later released to generate a jet.

Video: https://www.youtube.com/watch?v=v-UMDnSB8k0&feature=emb_logo","['jets', 'swim', 'squidbot', 'pics', 'diego', 'robot', 'fish', 'body', 'takes', 'san', 'water', 'jet', 'robots', 'squid', 'uc', 'coral']","Engineers at the University of California San Diego have built a squid-like robot that can swim untethered, propelling itself by generating jets of water.
This squid robot is made mostly from soft materials such as acrylic polymer, with a few rigid, 3D printed and laser cut parts.
Using soft robots in underwater exploration is important to protect fish and coral, which could be damaged by rigid robots.
At rest, the squid robot is shaped roughly like a paper lantern, and has flexible ribs, which act like springs, along its sides.
One of them is connected to a nozzle that both takes in water and ejects it when the robot's body contracts."
245,https://www.sciencedaily.com/releases/2020/09/200930144426.htm,"AI can detect COVID-19 in the lungs like a virtual physician, new study shows: Algorithm can accurately identify COVID-19 cases, as well as distinguish them from influenza",2020-09-20 00:00:00,"A University of Central Florida researcher is part of a new study showing that artificial intelligence can be nearly as accurate as a physician in diagnosing COVID-19 in the lungs.

The study, recently published in Nature Communications, shows the new technique can also overcome some of the challenges of current testing.

Researchers demonstrated that an AI algorithm could be trained to classify COVID-19 pneumonia in computed tomography (CT) scans with up to 90 percent accuracy, as well as correctly identify positive cases 84 percent of the time and negative cases 93 percent of the time.

CT scans offer a deeper insight into COVID-19 diagnosis and progression as compared to the often-used reverse transcription-polymerase chain reaction, or RT-PCR, tests. These tests have high false negative rates, delays in processing and other challenges.

Another benefit to CT scans is that they can detect COVID-19 in people without symptoms, in those who have early symptoms, during the height of the disease and after symptoms resolve.

However, CT is not always recommended as a diagnostic tool for COVID-19 because the disease often looks similar to influenza-associated pneumonias on the scans.

advertisement

The new UCF co-developed algorithm can overcome this problem by accurately identifying COVID-19 cases, as well as distinguishing them from influenza, thus serving as a great potential aid for physicians, says Ulas Bagci, an assistant professor in UCF's Department of Computer Science.

Bagci was a co-author of the study and helped lead the research.

""We demonstrated that a deep learning-based AI approach can serve as a standardized and objective tool to assist healthcare systems as well as patients,"" Bagci says. ""It can be used as a complementary test tool in very specific limited populations, and it can be used rapidly and at large scale in the unfortunate event of a recurrent outbreak.""

Bagci is an expert in developing AI to assist physicians, including using it to detect pancreatic and lung cancers in CT scans.

He also has two large, National Institutes of Health grants exploring these topics, including $2.5 million for using deep learning to examine pancreatic cystic tumors and more than $2 million to study the use of artificial intelligence for lung cancer screening and diagnosis.

advertisement

To perform the study, the researchers trained a computer algorithm to recognize COVID-19 in lung CT scans of 1,280 multinational patients from China, Japan and Italy.

Then they tested the algorithm on CT scans of 1,337 patients with lung diseases ranging from COVID-19 to cancer and non-COVID pneumonia.

When they compared the computer's diagnoses with ones confirmed by physicians, they found that the algorithm was extremely proficient in accurately diagnosing COVID-19 pneumonia in the lungs and distinguishing it from other diseases, especially when examining CT scans in the early stages of disease progression.

""We showed that robust AI models can achieve up to 90 percent accuracy in independent test populations, maintain high specificity in non-COVID-19 related pneumonias, and demonstrate sufficient generalizability to unseen patient populations and centers,"" Bagci says.

The UCF researcher is a longtime collaborator with study co-authors Baris Turkbey and Bradford J. Wood. Turkbey is an associate research physician at the NIH's National Cancer Institute Molecular Imaging Branch, and Wood is the director of NIH's Center for Interventional Oncology and chief of interventional radiology with NIH's Clinical Center.

This research was supported with funds from the NIH Center for Interventional Oncology and the Intramural Research Program of the National Institutes of Health, intramural NIH grants, the NIH Intramural Targeted Anti-COVID-19 program, the National Cancer Institute and NIH.

Bagci received his doctorate in computer science from the University of Nottingham in England and joined UCF's Department of Computer Science, part of the College of Engineering and Computer Science, in 2015. He is the Science Applications International Corp (SAIC) chair in UCF's Department of Computer Science and a faculty member of UCF's Center for Research in Computer Vision. SAIC is a Virginia-based government support and services company.","['research', 'virtual', 'ucfs', 'study', 'scans', 'physician', 'national', 'algorithm', 'lungs', 'covid19', 'science', 'ct', 'computer', 'influenza', 'shows', 'distinguish', 'identify', 'detect']","CT scans offer a deeper insight into COVID-19 diagnosis and progression as compared to the often-used reverse transcription-polymerase chain reaction, or RT-PCR, tests.
Bagci is an expert in developing AI to assist physicians, including using it to detect pancreatic and lung cancers in CT scans.
advertisementTo perform the study, the researchers trained a computer algorithm to recognize COVID-19 in lung CT scans of 1,280 multinational patients from China, Japan and Italy.
Then they tested the algorithm on CT scans of 1,337 patients with lung diseases ranging from COVID-19 to cancer and non-COVID pneumonia.
Bagci received his doctorate in computer science from the University of Nottingham in England and joined UCF's Department of Computer Science, part of the College of Engineering and Computer Science, in 2015."
246,https://www.sciencedaily.com/releases/2020/09/200916113601.htm,Security software for autonomous vehicles,2020-09-20 00:00:00,"Before autonomous vehicles participate in road traffic, they must demonstrate conclusively that they do not pose a danger to others. New software developed at the Technical University of Munich (TUM) prevents accidents by predicting different variants of a traffic situation every millisecond.

A car approaches an intersection. Another vehicle jets out of the cross street, but it is not yet clear whether it will turn right or left. At the same time, a pedestrian steps into the lane directly in front of the car, and there is a cyclist on the other side of the street. People with road traffic experience will in general assess the movements of other traffic participants correctly.

""These kinds of situations present an enormous challenge for autonomous vehicles controlled by computer programs,"" explains Matthias Althoff, Professor of Cyber-Physical Systems at TUM. ""But autonomous driving will only gain acceptance of the general public if you can ensure that the vehicles will not endanger other road users -- no matter how confusing the traffic situation.""

Algorithms that peer into the future

The ultimate goal when developing software for autonomous vehicles is to ensure that they will not cause accidents. Althoff, who is a member of the Munich School of Robotics and Machine Intelligence at TUM, and his team have now developed a software module that permanently analyzes and predicts events while driving. Vehicle sensor data are recorded and evaluated every millisecond. The software can calculate all possible movements for every traffic participant -- provided they adhere to the road traffic regulations -- allowing the system to look three to six seconds into the future.

Based on these future scenarios, the system determines a variety of movement options for the vehicle. At the same time, the program calculates potential emergency maneuvers in which the vehicle can be moved out of harm's way by accelerating or braking without endangering others. The autonomous vehicle may only follow routes that are free of foreseeable collisions and for which an emergency maneuver option has been identified.

Streamlined models for swift calculations

This kind of detailed traffic situation forecasting was previously considered too time-consuming and thus impractical. But now, the Munich research team has shown not only the theoretical viability of real-time data analysis with simultaneous simulation of future traffic events: They have also demonstrated that it delivers reliable results.

The quick calculations are made possible by simplified dynamic models. So-called reachability analysis is used to calculate potential future positions a car or a pedestrian might assume. When all characteristics of the road users are taken into account, the calculations become prohibitively time-consuming. That is why Althoff and his team work with simplified models. These are superior to the real ones in terms of their range of motion -- yet, mathematically easier to handle. This enhanced freedom of movement allows the models to depict a larger number of possible positions but includes the subset of positions expected for actual road users.

Real traffic data for a virtual test environment

For their evaluation, the computer scientists created a virtual model based on real data they had collected during test drives with an autonomous vehicle in Munich. This allowed them to craft a test environment that closely reflects everyday traffic scenarios. ""Using the simulations, we were able to establish that the safety module does not lead to any loss of performance in terms of driving behavior, the predictive calculations are correct, accidents are prevented, and in emergency situations the vehicle is demonstrably brought to a safe stop,"" Althoff sums up.

The computer scientist emphasizes that the new security software could simplify the development of autonomous vehicles because it can be combined with all standard motion control programs.","['vehicles', 'autonomous', 'models', 'data', 'vehicle', 'road', 'security', 'software', 'althoff', 'traffic', 'munich']","Before autonomous vehicles participate in road traffic, they must demonstrate conclusively that they do not pose a danger to others.
""These kinds of situations present an enormous challenge for autonomous vehicles controlled by computer programs,"" explains Matthias Althoff, Professor of Cyber-Physical Systems at TUM.
Algorithms that peer into the futureThe ultimate goal when developing software for autonomous vehicles is to ensure that they will not cause accidents.
The autonomous vehicle may only follow routes that are free of foreseeable collisions and for which an emergency maneuver option has been identified.
The computer scientist emphasizes that the new security software could simplify the development of autonomous vehicles because it can be combined with all standard motion control programs."
247,https://www.sciencedaily.com/releases/2020/08/200812144008.htm,Soldiers could teach future robots how to outperform humans,2020-08-20 00:00:00,"In the future, a Soldier and a game controller may be all that's needed to teach robots how to outdrive humans.

At the U.S. Army Combat Capabilities Development Command's Army Research Laboratory and the University of Texas at Austin, researchers designed an algorithm that allows an autonomous ground vehicle to improve its existing navigation systems by watching a human drive. The team tested its approach -- called adaptive planner parameter learning from demonstration, or APPLD -- on one of the Army's experimental autonomous ground vehicles.

""Using approaches like APPLD, current Soldiers in existing training facilities will be able to contribute to improvements in autonomous systems simply by operating their vehicles as normal,"" said Army researcher Dr. Garrett Warnell. ""Techniques like these will be an important contribution to the Army's plans to design and field next-generation combat vehicles that are equipped to navigate autonomously in off-road deployment environments.""

The researchers fused machine learning from demonstration algorithms and more classical autonomous navigation systems. Rather than replacing a classical system altogether, APPLD learns how to tune the existing system to behave more like the human demonstration. This paradigm allows for the deployed system to retain all the benefits of classical navigation systems -- such as optimality, explainability and safety -- while also allowing the system to be flexible and adaptable to new environments, Warnell said.

""A single demonstration of human driving, provided using an everyday Xbox wireless controller, allowed APPLD to learn how to tune the vehicle's existing autonomous navigation system differently depending on the particular local environment,"" Warnell said. ""For example, when in a tight corridor, the human driver slowed down and drove carefully. After observing this behavior, the autonomous system learned to also reduce its maximum speed and increase its computation budget in similar environments. This ultimately allowed the vehicle to successfully navigate autonomously in other tight corridors where it had previously failed.""

This research is part of the Army's Open Campus initiative, through which Army scientists in Texas collaborate with academic partners at UT Austin.

advertisement

""APPLD is yet another example of a growing stream of research results that has been facilitated by the unique collaboration arrangement between UT Austin and the Army Research Lab,"" said Dr. Peter Stone, professor and chair of the Robotics Consortium at UT Austin. ""By having Dr. Warnell embedded at UT Austin full-time, we are able to quickly identify and tackle research problems that are both cutting-edge scientific advances and also immediately relevant to the Army.""

The team's experiments showed that, after training, the APPLD system was able to navigate the test environments more quickly and with fewer failures than with the classical system. Additionally, the trained APPLD system often navigated the environment faster than the human who trained it. The peer-reviewed journal, IEEE Robotics and Automation Letters, published the team's work: APPLD: Adaptive Planner Parameter Learning From Demonstration .

""From a machine learning perspective, APPLD contrasts with so called end-to-end learning systems that attempt to learn the entire navigation system from scratch,"" Stone said. ""These approaches tend to require a lot of data and may lead to behaviors that are neither safe nor robust. APPLD leverages the parts of the control system that have been carefully engineered, while focusing its machine learning effort on the parameter tuning process, which is often done based on a single person's intuition.""

APPLD represents a new paradigm in which people without expert-level knowledge in robotics can help train and improve autonomous vehicle navigation in a variety of environments. Rather than small teams of engineers trying to manually tune navigation systems in a small number of test environments, a virtually unlimited number of users would be able to provide the system the data it needs to tune itself to an unlimited number of environments.

""Current autonomous navigation systems typically must be re-tuned by hand for each new deployment environment,"" said Army researcher Dr. Jonathan Fink. ""This process is extremely difficult -- it must be done by someone with extensive training in robotics, and it requires a lot of trial and error until the right systems settings can be found. In contrast, APPLD tunes the system automatically by watching a human drive the system -- something that anyone can do if they have experience with a video game controller. During deployment, APPLD also allows the system to re-tune itself in real-time as the environment changes.""

The Army's focus on modernizing the Next Generation Combat Vehicle includes designing both optionally manned fighting vehicles and robotic combat vehicles that can navigate autonomously in off-road deployment environments. While Soldiers can navigate these environments driving current combat vehicles, the environments remain too challenging for state-of-the-art autonomous navigation systems. APPLD and similar approaches provide a new potential way for the Army to improve existing autonomous navigation capabilities.

""In addition to the immediate relevance to the Army, APPLD also creates the opportunity to bridge the gap between traditional engineering approaches and emerging machine learning techniques, to create robust, adaptive, and versatile mobile robots in the real-world,"" said Dr. Xuesu Xiao, a postdoctoral researcher at UT Austin and lead author of the paper.

To continue this research, the team will test the APPLD system in a variety of outdoor environments, employ Soldier drivers, and experiment with a wider variety of existing autonomous navigation approaches. Additionally, the researchers will investigate whether including additional sensor information such as camera images can lead to learning more complex behaviors such as tuning the navigation system to operate under varying conditions, such as on different terrain or with other objects present.","['autonomous', 'vehicles', 'future', 'research', 'soldiers', 'learning', 'system', 'navigation', 'systems', 'humans', 'army', 'robots', 'outperform', 'appld', 'teach', 'environments']","In the future, a Soldier and a game controller may be all that's needed to teach robots how to outdrive humans.
The researchers fused machine learning from demonstration algorithms and more classical autonomous navigation systems.
Additionally, the trained APPLD system often navigated the environment faster than the human who trained it.
""Current autonomous navigation systems typically must be re-tuned by hand for each new deployment environment,"" said Army researcher Dr. Jonathan Fink.
While Soldiers can navigate these environments driving current combat vehicles, the environments remain too challenging for state-of-the-art autonomous navigation systems."
248,https://www.sciencedaily.com/releases/2020/08/200811120120.htm,Classifying galaxies with artificial intelligence,2020-08-20 00:00:00,"Astronomers have applied artificial intelligence (AI) to ultra-wide field-of-view images of the distant Universe captured by the Subaru Telescope, and have achieved a very high accuracy for finding and classifying spiral galaxies in those images. This technique, in combination with citizen science, is expected to yield further discoveries in the future.

A research group, consisting of astronomers mainly from the National Astronomical Observatory of Japan (NAOJ), applied a deep-learning technique, a type of AI, to classify galaxies in a large dataset of images obtained with the Subaru Telescope. Thanks to its high sensitivity, as many as 560,000 galaxies have been detected in the images. It would be extremely difficult to visually process this large number of galaxies one by one with human eyes for morphological classification. The AI enabled the team to perform the processing without human intervention.

Automated processing techniques for extraction and judgment of features with deep-learning algorithms have been rapidly developed since 2012. Now they usually surpass humans in terms of accuracy and are used for autonomous vehicles, security cameras, and many other applications. Dr. Ken-ichi Tadaki, a Project Assistant Professor at NAOJ, came up with the idea that if AI can classify images of cats and dogs, it should be able to distinguish ""galaxies with spiral patterns"" from ""galaxies without spiral patterns."" Indeed, using training data prepared by humans, the AI successfully classified the galaxy morphologies with an accuracy of 97.5%. Then applying the trained AI to the full data set, it identified spirals in about 80,000 galaxies.

Now that this technique has been proven effective, it can be extended to classify galaxies into more detailed classes, by training the AI on the basis of a substantial number of galaxies classified by humans. NAOJ is now running a citizen-science project ""GALAXY CRUISE,"" where citizens examine galaxy images taken with the Subaru Telescope to search for features suggesting that the galaxy is colliding or merging with another galaxy. The advisor of ""GALAXY CRUISE,"" Associate Professor Masayuki Tanaka has high hopes for the study of galaxies using artificial intelligence and says, ""The Subaru Strategic Program is serious Big Data containing an almost countless number of galaxies. Scientifically, it is very interesting to tackle such big data with a collaboration of citizen astronomers and machines. By employing deep-learning on top of the classifications made by citizen scientists in GALAXY CRUISE, chances are, we can find a great number of colliding and merging galaxies.""","['artificial', 'data', 'spiral', 'number', 'galaxy', 'images', 'subaru', 'technique', 'ai', 'galaxies', 'classifying', 'telescope', 'intelligence']","Astronomers have applied artificial intelligence (AI) to ultra-wide field-of-view images of the distant Universe captured by the Subaru Telescope, and have achieved a very high accuracy for finding and classifying spiral galaxies in those images.
Thanks to its high sensitivity, as many as 560,000 galaxies have been detected in the images.
It would be extremely difficult to visually process this large number of galaxies one by one with human eyes for morphological classification.
Then applying the trained AI to the full data set, it identified spirals in about 80,000 galaxies.
The advisor of ""GALAXY CRUISE,"" Associate Professor Masayuki Tanaka has high hopes for the study of galaxies using artificial intelligence and says, ""The Subaru Strategic Program is serious Big Data containing an almost countless number of galaxies."
249,https://www.sciencedaily.com/releases/2020/07/200727194721.htm,Randomness theory could hold key to internet security,2020-07-20 00:00:00,"Is there an unbreakable code?

The question has been central to cryptography for thousands of years, and lies at the heart of efforts to secure private information on the internet. In a new paper, Cornell Tech researchers identified a problem that holds the key to whether all encryption can be broken -- as well as a surprising connection to a mathematical concept that aims to define and measure randomness.

""Our result not only shows that cryptography has a natural 'mother' problem, it also shows a deep connection between two quite separate areas of mathematics and computer science -- cryptography and algorithmic information theory,"" said Rafael Pass, professor of computer science at Cornell Tech.

Pass is co-author of ""On One-Way Functions and Kolmogorov Complexity,"" which will be presented at the IEEE Symposium on Foundations of Computer Science, to be held Nov. 16-19 in Durham, North Carolina.

""The result,"" he said, ""is that a natural computational problem introduced in the 1960s in the Soviet Union characterizes the feasibility of basic cryptography -- private-key encryption, digital signatures and authentication, for example.""

For millennia, cryptography was considered a cycle: Someone invented a code, the code was effective until someone eventually broke it, and the code became ineffective. In the 1970s, researchers seeking a better theory of cryptography introduced the concept of the one-way function -- an easy task or problem in one direction that is impossible in the other.

advertisement

For example, it's easy to light a match, but impossible to return a burning match to its unlit state without rearranging its atoms -- an immensely difficult task.

""The idea was, if we have such a one-way function, maybe that's a very good starting point for understanding cryptography,"" Pass said. ""Encrypting the message is very easy. And if you have the key, you can also decrypt it. But someone who doesn't know the key should have to do the same thing as restoring a lit match.""

But researchers have not been able to prove the existence of a one-way function. The most well-known candidate -- which is also the basis of the most commonly used encryption schemes on the internet -- relies on integer factorization. It's easy to multiply two random prime numbers -- for instance, 23 and 47 -- but significantly harder to find those two factors if only given their product, 1,081.

It is believed that no efficient factoring algorithm exists for large numbers, Pass said, though researchers may not have found the right algorithms yet.

""The central question we're addressing is: Does it exist? Is there some natural problem that characterizes the existence of one-way functions?"" he said. ""If it does, that's the mother of all problems, and if you have a way to solve that problem, you can break all purported one-way functions. And if you don't know how to solve that problem, you can actually get secure cryptography.""

Meanwhile, mathematicians in the 1960s identified what's known as Kolmogorov Complexity, which refers to quantifying the amount of randomness or pattern of a string of numbers. The Kolmogorov Complexity of a string of numbers is defined as the length of the shortest computer program that can generate the string; for some strings, such as 121212121212121212121212121212, there is a short program that generates it -- alternate 1s and 2s. But for more complicated and apparently random strings of numbers, such as 37539017332840393452954329, there may not exist a program that is shorter than the length of the string itself.

advertisement

The problem has long interested mathematicians and computer scientists, including Juris Hartmanis, professor emeritus of computer science and engineering. Because the computer program attempting to generate the number could take millions or even billions of years, researchers in the Soviet Union in the 1960s, as well as Hartmanis and others in the 1980s, developed the time-bounded Kolmogorov Complexity -- the length of the shortest program that can output a string of numbers in a certain amount of time.

In the paper, Pass and doctoral student Yanyi Liu showed that if computing time-bounded Kolmogorov Complexity is hard, then one-way functions exist.

Although their finding is theoretical, it has potential implications across cryptography, including internet security.

""If you can come up with an algorithm to solve the time-bounded Kolmogorov complexity problem, then you can break all crypto, all encryption schemes, all digital signatures,"" Pass said. ""However, if no efficient algorithm exists to solve this problem, you can get a one-way function, and therefore you can get secure encryption and digital signatures and so forth.""

The research was funded in part by the National Science Foundation and the Air Force Office of Scientific Research, and was based on research funded by the Intelligence Advanced Research Projects Activity in the Office of the Director of National Intelligence.","['theory', 'researchers', 'complexity', 'key', 'cryptography', 'problem', 'science', 'hold', 'oneway', 'security', 'computer', 'internet', 'numbers', 'string', 'randomness', 'kolmogorov']","""Our result not only shows that cryptography has a natural 'mother' problem, it also shows a deep connection between two quite separate areas of mathematics and computer science -- cryptography and algorithmic information theory,"" said Rafael Pass, professor of computer science at Cornell Tech.
Meanwhile, mathematicians in the 1960s identified what's known as Kolmogorov Complexity, which refers to quantifying the amount of randomness or pattern of a string of numbers.
advertisementThe problem has long interested mathematicians and computer scientists, including Juris Hartmanis, professor emeritus of computer science and engineering.
In the paper, Pass and doctoral student Yanyi Liu showed that if computing time-bounded Kolmogorov Complexity is hard, then one-way functions exist.
""If you can come up with an algorithm to solve the time-bounded Kolmogorov complexity problem, then you can break all crypto, all encryption schemes, all digital signatures,"" Pass said."
250,https://www.sciencedaily.com/releases/2020/06/200624120434.htm,Towards an AI diagnosis like the doctor's: How can we make 'lazy' artificial intelligence more transparent and relevant to the clinic?,2020-06-20 00:00:00,"Artificial intelligence (AI) is an important innovation in diagnostics, because it can quickly learn to recognize abnormalities that a doctor would also label as a disease. But the way that these systems work is often opaque, and doctors do have a better ""overall picture"" when they make the diagnosis. In a new publication, researchers from Radboudumc show how they can make the AI show how it's working, as well as let it diagnose more like a doctor, thus making AI-systems more relevant to clinical practice.

Doctor vs AI

In recent years, artificial intelligence has been on the rise in the diagnosis of medical imaging. A doctor can look at an X-ray or biopsy to identify abnormalities, but this can increasingly also be done by an AI system by means of ""deep learning"" (see 'Background: what is deep learning' below). Such a system learns to arrive at a diagnosis on its own, and in some cases it does this just as well or better than experienced doctors.

The two major differences compared to a human doctor are, first, that AI is often not transparent in how it's analyzing the images, and, second, that these systems are quite ""lazy."" AI looks at what is needed for a particular diagnosis, and then stops. This means that a scan does not always identify all abnormalities, even if the diagnosis is correct. A doctor, especially when considering the treatment plan, looks at the big picture: what do I see? Which anomalies should be removed or treated during surgery?

AI more like the doctor

To make AI systems more attractive for the clinical practice, Cristina González Gonzalo, PhD candidate at the A-eye Research and Diagnostic Image Analysis Group of Radboudumc, developed a two-sided innovation for diagnostic AI. She did this based on eye scans, in which abnormalities of the retina occurred -- specifically diabetic retinopathy and age-related macular degeneration. These abnormalities can be easily recognized by both a doctor and AI. But they are also abnormalities that often occur in groups. A classic AI would diagnose one or a few spots and stop the analysis. In the process developed by González Gonzalo however, the AI goes through the picture over and over again, learning to ignore the places it has already passed, thus discovering new ones. Moreover, the AI also shows which areas of the eye scan it deemed suspicious, therefore making the diagnostic process transparent.

An iterative process

A basic AI could come up with a diagnosis based on one assessment of the eye scan, and thanks to the first contribution by González Gonzalo, it can show how it arrived at that diagnosis. This visual explanation shows that the system is indeed lazy -- stopping the analysis after it as obtained just enough information to make a diagnosis. That's why she also made the process iterative in an innovative way, forcing the AI to look harder and create more of a 'complete picture' that radiologists would have.

How did the system learn to look at the same eye scan with 'fresh eyes'? The system ignored the familiar parts by digitally filling in the abnormalities already found using healthy tissue from around the abnormality. The results of all the assessment rounds are then added together and that produces the final diagnosis. In the study, this approach improved the sensitivity of the detection of diabetic retinopathy and age-related macular degeneration by 11.2+/-2.0% per image. What this project proves is that it's possible to have an AI system assess images more like a doctor, as well as make transparent how it's doing it. This might help these systems become easier to trust and thus to be adopted by radiologists.

Background: what is 'deep learning'?

Deep learning is a term used for systems that learn in a way that is similar to how our brain works. It consists of networks of electronic 'neurons', each of which learns to recognize one aspect of the desired image. It then follows the principles of 'learning by doing', and 'practice makes perfect'. The system is fed more and more images that include relevant information saying -- in this case -- whether there is an anomaly in the retina, and if so, which disease it is. The system then learns to recognize which characteristics belong to those diseases, and the more pictures it sees, the better it can recognize those characteristics in undiagnosed images. We do something similar with small children: we repeatedly hold up an object, say an apple, in front of them and say that it is an apple. After some time, you don't have to say it anymore -- even though each apple is slightly different. Another major advantage of these systems is that they complete their training much faster than humans and can work 24 hours a day.","['transparent', 'artificial', 'relevant', 'recognize', 'clinic', 'scan', 'picture', 'learning', 'ai', 'doctors', 'lazy', 'system', 'systems', 'abnormalities', 'doctor', 'diagnosis', 'intelligence']","Artificial intelligence (AI) is an important innovation in diagnostics, because it can quickly learn to recognize abnormalities that a doctor would also label as a disease.
But the way that these systems work is often opaque, and doctors do have a better ""overall picture"" when they make the diagnosis.
Doctor vs AIIn recent years, artificial intelligence has been on the rise in the diagnosis of medical imaging.
AI more like the doctorTo make AI systems more attractive for the clinical practice, Cristina González Gonzalo, PhD candidate at the A-eye Research and Diagnostic Image Analysis Group of Radboudumc, developed a two-sided innovation for diagnostic AI.
What this project proves is that it's possible to have an AI system assess images more like a doctor, as well as make transparent how it's doing it."
251,https://www.sciencedaily.com/releases/2020/06/200617091024.htm,"Making more data available for training self-driving cars: Additional data boosts accuracy of tracking other cars, pedestrians",2020-06-20 00:00:00,"For safety's sake, a self-driving car must accurately track the movement of pedestrians, bicycles and other vehicles around it. Training those tracking systems may now be more effective thanks to a new method developed at Carnegie Mellon University.

Generally speaking, the more road and traffic data available for training tracking systems, the better the results. And the CMU researchers have found a way to unlock a mountain of autonomous driving data for this purpose.

""Our method is much more robust than previous methods because we can train on much larger datasets,"" said Himangi Mittal, a research intern working with David Held, assistant professor in CMU's Robotics Institute.

Most autonomous vehicles navigate primarily based on a sensor called a lidar, a laser device that generates 3D information about the world surrounding the car. This 3D information isn't images, but a cloud of points. One way the vehicle makes sense of this data is by using a technique known as scene flow. This involves calculating the speed and trajectory of each 3D point. Groups of points moving together are interpreted via scene flow as vehicles, pedestrians or other moving objects.

In the past, state-of-the-art methods for training such a system have required the use of labeled datasets -- sensor data that has been annotated to track each 3D point over time. Manually labeling these datasets is laborious and expensive, so, not surprisingly, little labeled data exists. As a result, scene flow training is instead often performed with simulated data, which is less effective, and then fine-tuned with the small amount of labeled real-world data that exists.

Mittal, Held and robotics Ph.D. student Brian Okorn took a different approach, using unlabeled data to perform scene flow training. Because unlabeled data is relatively easy to generate by mounting a lidar on a car and driving around, there's no shortage of it.

The key to their approach was to develop a way for the system to detect its own errors in scene flow. At each instant, the system tries to predict where each 3D point is going and how fast it's moving. In the next instant, it measures the distance between the point's predicted location and the actual location of the point nearest that predicted location. This distance forms one type of error to be minimized.

The system then reverses the process, starting with the predicted point location and working backward to map back to where the point originated. At this point, it measures the distance between the predicted position and the actual origination point, and the resulting distance forms the second type of error.

The system then works to correct those errors.

""It turns out that to eliminate both of those errors, the system actually needs to learn to do the right thing, without ever being told what the right thing is,"" Held said.

As convoluted as that might sound, Okorn found that it worked well. The researchers calculated that scene flow accuracy using a training set of synthetic data was only 25%. When the synthetic data was fine-tuned with a small amount of real-world labeled data, the accuracy increased to 31%. When they added a large amount of unlabeled data to train the system using their approach, scene flow accuracy jumped to 46%.

The research team presented their method at the Computer Vision and Pattern Recognition (CVPR) conference, which was held virtually June 14-19. The CMU Argo AI Center for Autonomous Vehicle Research supported this research, with additional support from a NASA Space Technology Research Fellowship.","['research', 'point', 'using', 'data', 'making', 'flow', 'predicted', 'system', 'selfdriving', 'cars', 'available', '3d', 'pedestrians', 'scene', 'additional', 'training', 'boosts', 'tracking']","One way the vehicle makes sense of this data is by using a technique known as scene flow.
Groups of points moving together are interpreted via scene flow as vehicles, pedestrians or other moving objects.
Mittal, Held and robotics Ph.D. student Brian Okorn took a different approach, using unlabeled data to perform scene flow training.
The researchers calculated that scene flow accuracy using a training set of synthetic data was only 25%.
When they added a large amount of unlabeled data to train the system using their approach, scene flow accuracy jumped to 46%."
252,https://www.sciencedaily.com/releases/2020/06/200611183906.htm,Self-driving cars that recognize free space can better detect objects: What a perception system doesn't see can help it understand what it sees,2020-06-20 00:00:00,"It's important that self-driving cars quickly detect other cars or pedestrians sharing the road. Researchers at Carnegie Mellon University have shown that they can significantly improve detection accuracy by helping the vehicle also recognize what it doesn't see.

Empty space, that is.

The very fact that objects in your sight may obscure your view of things that lie further ahead is blindingly obvious to people. But Peiyun Hu, a Ph.D. student in CMU's Robotics Institute, said that's not how self-driving cars typically reason about objects around them.

Rather, they use 3D data from lidar to represent objects as a point cloud and then try to match those point clouds to a library of 3D representations of objects. The problem, Hu said, is that the 3D data from the vehicle's lidar isn't really 3D -- the sensor can't see the occluded parts of an object, and current algorithms don't reason about such occlusions.

""Perception systems need to know their unknowns,"" Hu observed.

Hu's work enables a self-driving car's perception systems to consider visibility as it reasons about what its sensors are seeing. In fact, reasoning about visibility is already used when companies build digital maps.

""Map-building fundamentally reasons about what's empty space and what's occupied,"" said Deva Ramanan, an associate professor of robotics and director of the CMU Argo AI Center for Autonomous Vehicle Research. ""But that doesn't always occur for live, on-the-fly processing of obstacles moving at traffic speeds.""

In research to be presented at the Computer Vision and Pattern Recognition (CVPR) conference, which will be held virtually June 13-19, Hu and his colleagues borrow techniques from map-making to help the system reason about visibility when trying to recognize objects.

When tested against a standard benchmark, the CMU method outperformed the previous top-performing technique, improving detection by 10.7% for cars, 5.3% for pedestrians, 7.4% for trucks, 18.4% for buses and 16.7% for trailers.

One reason previous systems may not have taken visibility into account is a concern about computation time. But Hu said his team found that was not a problem: their method takes just 24 milliseconds to run. (For comparison, each sweep of the lidar is 100 milliseconds.)

In addition to Hu and Ramanan, the research team included Jason Ziglar of Argo AI and David Held, assistant professor of robotics. The Argo AI Center supported this research.","['recognize', 'research', 'reason', 'understand', 'objects', 'system', 'help', 'selfdriving', 'cars', 'systems', 'robotics', 'sees', 'perception', '3d', 'space', 'free', 'hu', 'visibility']","It's important that self-driving cars quickly detect other cars or pedestrians sharing the road.
Researchers at Carnegie Mellon University have shown that they can significantly improve detection accuracy by helping the vehicle also recognize what it doesn't see.
But Peiyun Hu, a Ph.D. student in CMU's Robotics Institute, said that's not how self-driving cars typically reason about objects around them.
Hu's work enables a self-driving car's perception systems to consider visibility as it reasons about what its sensors are seeing.
In addition to Hu and Ramanan, the research team included Jason Ziglar of Argo AI and David Held, assistant professor of robotics."
253,https://www.sciencedaily.com/releases/2020/06/200610102726.htm,World's first spherical artificial eye has 3D retina,2020-06-20 00:00:00,"An international team led by scientists at the Hong Kong University of Science and Technology (HKUST) has recently developed the world's first 3D artificial eye with capabilities better than existing bionic eyes and in some cases, even exceed those of the human eyes, bringing vision to humanoid robots and new hope to patients with visual impairment.

Scientists have spent decades trying to replicate the structure and clarity of a biological eye, but vision provided by existing prosthetic eyes -- largely in the form of spectacles attached with external cables, are still in poor resolution with 2D flat image sensors. The Electrochemical Eye (EC-Eye) developed at HKUST, however, not only replicates the structure of a natural eye for the first time, but may actually offer sharper vision than a human eye in the future, with extra functions such as the ability to detect infrared radiation in darkness.

The key feature allowing such breakthroughs is a 3D artificial retina -- made of an array of nanowire light sensors which mimic the photoreceptors in human retinas. Developed by Prof. FAN Zhiyong and Dr. GU Leilei from the Department of Electronic and Computer Engineering at HKUST, the team connected the nanowire light sensors to a bundle of liquid-metal wires serving as nerves behind the human-made hemispherical retina during the experiment, and successfully replicated the visual signal transmission to reflect what the eye sees onto the computer screen.

In the future, those nanowire light sensors could be directly connected to the nerves of the visually impaired patients. Unlike in a human eye where bundles of optic nerve fibers (for signal transmission) need to route through the retina via a pore -- from the front side of the retina to the backside (thus creating a blind spot in human vision) before reaching the brain; the light sensors that now scatters across the entire human-made retina could each feed signals through its own liquid-metal wire at the back, thereby eliminating the blind spot issue as they do not have to route through a single spot.

Apart from that, as nanowires have even higher density than photoreceptors in human retina, the artificial retina can thus receive more light signals and potentially attain a higher image resolution than human retina -- if the back contacts to individual nanowires are made in the future. With different materials used to boost the sensors' sensitivity and spectral range, the artificial eye may also achieve other functions such as night vision.

""I have always been a big fan of science fiction, and I believe many technologies featured in stories such as those of intergalactic travel, will one day become reality. However, regardless of image resolution, angle of views or user-friendliness, the current bionic eyes are still of no match to their natural human counterpart. A new technology to address these problems is in urgent need, and it gives me a strong motivation to start this unconventional project,"" said Prof. Fan, whose team has spent nine years to complete the current study from idea inception.

The team collaborated with the University of California, Berkeley on this project and their findings were recently published in the journal Nature.

""In the next step, we plan to further improve the performance, stability and biocompatibility of our device. For prosthesis application, we look forward to collaborating with medical research experts who have the relevant expertise on optometry and ocular prosthesis,"" Prof. Fan added.

The working principle of the artificial eye involves an electrochemical process which is adopted from a type of solar cell. In principle, each photo sensor on the artificial retina can serve as a nanoscale solar cell. With further modification, the EC-Eye can be a self-powered image sensor, so there is no need for external power source nor circuitry when used for ocular prosthesis, which will be much more user-friendly as compared with the current technology.","['artificial', 'worlds', 'fan', 'spherical', 'team', 'image', 'eye', 'vision', 'human', '3d', 'sensors', 'light', 'retina']","The key feature allowing such breakthroughs is a 3D artificial retina -- made of an array of nanowire light sensors which mimic the photoreceptors in human retinas.
In the future, those nanowire light sensors could be directly connected to the nerves of the visually impaired patients.
With different materials used to boost the sensors' sensitivity and spectral range, the artificial eye may also achieve other functions such as night vision.
The working principle of the artificial eye involves an electrochemical process which is adopted from a type of solar cell.
In principle, each photo sensor on the artificial retina can serve as a nanoscale solar cell."
254,https://www.sciencedaily.com/releases/2020/06/200601113315.htm,A good egg: Robot chef trained to make omelettes,2020-06-20 00:00:00,"A team of engineers have trained a robot to prepare an omelette, all the way from cracking the eggs to plating the finished dish, and refined the 'chef's' culinary skills to produce a reliable dish that actually tastes good.

The researchers, from the University of Cambridge in collaboration with domestic appliance company Beko, used machine learning to train the robot to account for highly subjective matters of taste. The results are reported in the journal IEEE Robotics and Automation Letters, and will be available online as part of the virtual IEEE International Conference on Robotics and Automation (ICRA 2020).

A robot that can cook has been an aspiration of sci-fi authors, futurists, and scientists for decades. As artificial intelligence techniques have advanced, commercial companies have built prototype robot chefs, although none of these are currently commercially available, and they lag well behind their human counterparts in terms of skill.

""Cooking is a really interesting problem for roboticists, as humans can never be totally objective when it comes to food, so how do we as scientists assess whether the robot has done a good job?"" said Dr Fumiya Iida from Cambridge's Department of Engineering, who led the research.

Teaching a robot to prepare and cook food is a challenging task, since it must deal with complex problems in robot manipulation, computer vision, sensing and human-robot interaction, and produce a consistent end product.

In addition, taste differs from person to person -- cooking is a qualitative task, while robots generally excel at quantitative tasks. Since taste is not universal, universal solutions don't exist. Unlike other optimisation problems, special tools need to be developed for robots to prepare food.

advertisement

Other research groups have trained robots to make cookies, pancakes and even pizza, but these robot chefs have not been optimised for the many subjective variables involved in cooking.

Egg dishes, omelettes in particular, have long been considered a test of culinary skill. A popular piece of French culinary mythology states that each of the one hundred pleats in a chef's hat represents a different way to cook an egg, although the exact origin of this adage is unknown.

""An omelette is one of those dishes that is easy to make, but difficult to make well,"" said Iida. ""We thought it would be an ideal test to improve the abilities of a robot chef, and optimise for taste, texture, smell and appearance.""

In partnership with Beko, Iida and his colleagues trained their robot chef to prepare an omelette, from cracking the eggs through to plating the finished dish. The work was performed in Cambridge's Department of Engineering, using a test kitchen supplied by Beko plc and Symphony Group.

The machine learning technique developed by Iida's team makes use of a statistical tool, called Bayesian Inference, to squeeze out as much information as possible from the limited amount of data samples, which was necessary to avoid over-stuffing the human tasters with omelettes.

""Another challenge we faced was the subjectivity of human sense of taste -- humans aren't very good at giving absolute measures, and usually give relative ones when it comes to taste,"" said Iida. ""So we needed to tweak the machine learning algorithm -- the so-called batch algorithm -- so that human tasters could give information based on comparative evaluations, rather than sequential ones.""

But how did the robot measure up as a chef? ""The omelettes in general tasted great -- much better than expected!"" said Iida.

The results show that machine learning can be used to obtain quantifiable improvements in food optimisation. Additionally, such an approach can be easily extended to multiple robotic chefs. Further studies have to be conducted to investigate other optimisation techniques and their viability.

""Beko is passionate about designing the kitchen of the future and believe robotics applications such as this will play a crucial part. We are very happy to be collaborating with Dr Iida on this important topic,"" said Dr Graham Anderson, the industrial project supervisor from Beko's Cambridge R&D Centre.","['omelettes', 'iida', 'test', 'egg', 'chefs', 'prepare', 'robot', 'chef', 'learning', 'taste', 'machine', 'human', 'trained', 'good']","The researchers, from the University of Cambridge in collaboration with domestic appliance company Beko, used machine learning to train the robot to account for highly subjective matters of taste.
advertisementOther research groups have trained robots to make cookies, pancakes and even pizza, but these robot chefs have not been optimised for the many subjective variables involved in cooking.
""We thought it would be an ideal test to improve the abilities of a robot chef, and optimise for taste, texture, smell and appearance.""
In partnership with Beko, Iida and his colleagues trained their robot chef to prepare an omelette, from cracking the eggs through to plating the finished dish.
The results show that machine learning can be used to obtain quantifiable improvements in food optimisation."
255,https://www.sciencedaily.com/releases/2020/04/200430091255.htm,New AI enables teachers to rapidly develop intelligent tutoring systems,2020-04-20 00:00:00,"Intelligent tutoring systems have been shown to be effective in helping to teach certain subjects, such as algebra or grammar, but creating these computerized systems is difficult and laborious. Now, researchers at Carnegie Mellon University have shown they can rapidly build them by, in effect, teaching the computer to teach.

Using a new method that employs artificial intelligence, a teacher can teach the computer by demonstrating several ways to solve problems in a topic, such as multicolumn addition, and correcting the computer if it responds incorrectly.

Notably, the computer system learns to not only solve the problems in the ways it was taught, but also to generalize to solve all other problems in the topic, and do so in ways that might differ from those of the teacher, said Daniel Weitekamp III, a Ph.D. student in CMU's Human-Computer Interaction Institute (HCII).

""A student might learn one way to do a problem and that would be sufficient,"" Weitekamp explained. ""But a tutoring system needs to learn every kind of way to solve a problem."" It needs to learn how to teach problem solving, not just how to solve problems.

That challenge has been a continuing problem for developers creating AI-based tutoring systems, said Ken Koedinger, professor of human-computer interaction and psychology. Intelligent tutoring systems are designed to continuously track student progress, provide next-step hints and pick practice problems that help students learn new skills.

When Koedinger and others began building the first intelligent tutors, they programmed production rules by hand -- a process, he said, that took about 200 hours of development for each hour of tutored instruction. Later, they would develop a shortcut, in which they would attempt to demonstrate all possible ways of solving a problem. That cut development time to 40 or 50 hours, he noted, but for many topics, it is practically impossible to demonstrate all possible solution paths for all possible problems, which reduces the shortcut's applicability.

advertisement

The new method may enable a teacher to create a 30-minute lesson in about 30 minutes, which Koedinger termed ""a grand vision"" among developers of intelligent tutors.

""The only way to get to the full intelligent tutor up to now has been to write these AI rules,"" Koedinger said. ""But now the system is writing those rules.""

A paper describing the method, authored by Weitekamp, Koedinger and HCII System Scientist Erik Harpstead, was accepted by the Conference on Human Factors in Computing Systems (CHI 2020), which was scheduled for this month but canceled due to the COVID-19 pandemic. The paper has now been published in the conference proceedings in the Association for Computing Machinery's Digital Library.

The new method makes use of a machine learning program that simulates how students learn. Weitekamp developed a teaching interface for this machine learning engine that is user friendly and employs a ""show-and-correct"" process that's much easier than programming.

For the CHI paper, the authors demonstrated their method on the topic of multicolumn addition, but the underlying machine learning engine has been shown to work for a variety of subjects, including equation solving, fraction addition, chemistry, English grammar and science experiment environments.

The method not only speeds the development of intelligent tutors, but promises to make it possible for teachers, rather than AI programmers, to build their own computerized lessons. Some teachers, for instance, have their own preferences on how addition is taught, or which form of notation to use in chemistry. The new interface could increase the adoption of intelligent tutors by enabling teachers to create the homework assignments they prefer for the AI tutor, Koedinger said.

Enabling teachers to build their own systems also could lead to deeper insights into learning, he added. The authoring process may help them recognize trouble spots for students that, as experts, they don't themselves encounter.

""The machine learning system often stumbles in the same places that students do,"" Koedinger explained. ""As you're teaching the computer, we can imagine a teacher may get new insights about what's hard to learn because the machine has trouble learning it.""

This research was supported in part by the Institute of Education Sciences and Google.","['develop', 'method', 'intelligent', 'learning', 'ai', 'problem', 'system', 'enables', 'teachers', 'solve', 'systems', 'problems', 'tutoring', 'rapidly', 'koedinger', 'learn']","Intelligent tutoring systems have been shown to be effective in helping to teach certain subjects, such as algebra or grammar, but creating these computerized systems is difficult and laborious.
It needs to learn how to teach problem solving, not just how to solve problems.
That challenge has been a continuing problem for developers creating AI-based tutoring systems, said Ken Koedinger, professor of human-computer interaction and psychology.
Intelligent tutoring systems are designed to continuously track student progress, provide next-step hints and pick practice problems that help students learn new skills.
""The machine learning system often stumbles in the same places that students do,"" Koedinger explained."
256,https://www.sciencedaily.com/releases/2020/04/200429134018.htm,Researchers help give robotic arms a steady hand for surgeries,2020-04-20 00:00:00,"Steady hands and uninterrupted, sharp vision are critical when performing surgery on delicate structures like the brain or hair-thin blood vessels. While surgical cameras have improved what surgeons see during operative procedures, the ""steady hand"" remains to be enhanced -- new surgical technologies, including sophisticated surgeon-guided robotic hands, cannot prevent accidental injuries when operating close to fragile tissue.

In a new study published in the January issue of the journal Scientific Reports, researchers at Texas A&M University show that by delivering small, yet perceptible buzzes of electrical currents to fingertips, users can be given an accurate perception of distance to contact. This insight enabled users to control their robotic fingers precisely enough to gently land on fragile surfaces.

The researchers said that this technique might be an effective way to help surgeons reduce inadvertent injuries during robot-assisted operative procedures.

""One of the challenges with robotic fingers is ensuring that they can be controlled precisely enough to softly land on biological tissue,"" said Hangue Park, assistant professor in the Department of Electrical and Computer Engineering. ""With our design, surgeons will be able to get an intuitive sense of how far their robotic fingers are from contact, information they can then use to touch fragile structures with just the right amount of force.""

Robot-assisted surgical systems, also known as telerobotic surgical systems, are physical extensions of a surgeon. By controlling robotic fingers with movements of their own fingers, surgeons can perform intricate procedures remotely, thus expanding the number of patients that they can provide medical attention. Also, the tiny size of the robotic fingers means that surgeries are possible with much smaller incisions since surgeons need not make large cuts to accommodate for their hands in the patient's body during operations.

To move their robotic fingers precisely, surgeons rely on live streaming of visual information from cameras fitted on telerobotic arms. Thus, they look into monitors to match their finger movements with those of the telerobotic fingers. In this way, they know where their robotic fingers are in space and how close these fingers are to each other.

advertisement

However, Park noted that just visual information is not enough to guide fine finger movements, which is critical when the fingers are in the close vicinity of the brain or other delicate tissue.

""Surgeons can only know how far apart their actual fingers are from each other indirectly, that is, by looking at where their robotic fingers are relative to each other on a monitor,"" Park said. ""This roundabout view diminishes their sense of how far apart their actual fingers are from each other, which then affects how they control their robotic fingers.""

To address this problem, Park and his team came up with an alternate way to deliver distance information that is independent of visual feedback. By passing different frequencies of electrical currents onto fingertips via gloves fitted with stimulation probes, the researchers were able to train users to associate the frequency of current pulses with distance, that is, increasing current frequencies indicated the closing distance from a test object. They then compared if users receiving current stimulation along with visual information about closing distance on their monitors did better at estimating proximity than those who received visual information alone.

Park and his team also tailored their technology according to the user's sensitivity to electrical current frequencies. In other words, if a user was sensitive to a wider range of current frequencies, the distance information was delivered with smaller steps of increasing currents to maximize the accuracy of proximity estimation.

The researchers found that users receiving electrical pulses were more aware of the proximity to underlying surfaces and could lower their force of contact by around 70%, performing much better than the other group. Overall, they observed that proximity information delivered through mild electric pulses was about three times more effective than the visual information alone.

Park said their novel approach has the potential to significantly increase maneuverability during surgery while minimizing risks of unintended tissue damage. He also said their technique would add little to the existing mental load of surgeons during operative procedures.

""Our goal was to come up with a solution that would improve the accuracy in proximity estimation without increasing the burden of active thinking needed for this task,"" he said. ""When our technique is ready for use in surgical settings, physicians will be able to intuitively know how far their robotic fingers are from underlying structures, which means that they can keep their active focus on optimizing the surgical outcome of their patients.""","['information', 'users', 'researchers', 'surgeries', 'distance', 'surgeons', 'help', 'proximity', 'hand', 'arms', 'robotic', 'electrical', 'surgical', 'fingers', 'steady', 'visual']","This insight enabled users to control their robotic fingers precisely enough to gently land on fragile surfaces.
By controlling robotic fingers with movements of their own fingers, surgeons can perform intricate procedures remotely, thus expanding the number of patients that they can provide medical attention.
To move their robotic fingers precisely, surgeons rely on live streaming of visual information from cameras fitted on telerobotic arms.
In this way, they know where their robotic fingers are in space and how close these fingers are to each other.
Overall, they observed that proximity information delivered through mild electric pulses was about three times more effective than the visual information alone."
257,https://www.sciencedaily.com/releases/2020/04/200423130508.htm,Researchers restore injured man's sense of touch using brain-computer interface technology,2020-04-20 00:00:00,"While we might often take our sense of touch for granted, for researchers developing technologies to restore limb function in people paralyzed due to spinal cord injury or disease, re-establishing the sense of touch is an essential part of the process. And on April 23 in the journal Cell, a team of researchers at Battelle and the Ohio State University Wexner Medical Center report that they have been able to restore sensation to the hand of a research participant with a severe spinal cord injury using a brain-computer interface (BCI) system. The technology harnesses neural signals that are so miniscule they can't be perceived and enhances them via artificial sensory feedback sent back to the participant, resulting in greatly enriched motor function.

""We're taking subperceptual touch events and boosting them into conscious perception,"" says first author Patrick Ganzer, a principal research scientist at Battelle. ""When we did this, we saw several functional improvements. It was a big eureka moment when we first restored the participant's sense of touch.""

The participant in this study is Ian Burkhart, a 28-year-old man who suffered a spinal cord injury during a diving accident in 2010. Since 2014, Burkhart has been working with investigators on a project called NeuroLife that aims to restore function to his right arm. The device they have developed works through a system of electrodes on his skin and a small computer chip implanted in his motor cortex. This setup, which uses wires to route movement signals from the brain to the muscles, bypassing his spinal cord injury, gives Burkhart enough control over his arm and hand to lift a coffee mug, swipe a credit card, and play Guitar Hero.

""Until now, at times Ian has felt like his hand was foreign due to lack of sensory feedback,"" Ganzer says. ""He also has trouble with controlling his hand unless he is watching his movements closely. This requires a lot of concentration and makes simple multitasking like drinking a soda while watching TV almost impossible.""

The investigators found that although Burkhart had almost no sensation in his hand, when they stimulated his skin, a neural signal -- so small it was his brain was unable to perceive it -- was still getting to his brain. Ganzer explains that even in people like Burkhart who have what is considered a ""clinically complete"" spinal cord injury, there are almost always a few wisps of nerve fiber that remain intact. The Cell paper explains how they were able to boost these signals to the level where the brain would respond.

The subperceptual touch signals were artificially sent back to Burkhart using haptic feedback. Common examples of haptic feedback are the vibration from a mobile phone or game controller that lets the user feel that something is working. The new system allows the subperceptual touch signals coming from Burkhart's skin to travel back to his brain through artificial haptic feedback that he can perceive.

The advances in the BCI system led to three important improvements. They enable Burkhart to reliably detect something by touch alone: in the future, this may be used to find and pick up an object without being able to see it. The system also is the first BCI that allows for restoration of movement and touch at once, and this ability to experience enhanced touch during movement gives him a greater sense of control and lets him to do things more quickly. Finally, these improvements allow the BCI system to sense how much pressure to use when handling an object or picking something up -- for example, using a light touch when picking up a fragile object like a Styrofoam cup but a firmer grip when picking up something heavy.

The investigators' long-term goal is to develop a BCI system that works as well in the home as it does in the laboratory. They are working on creating a next-generation sleeve containing the required electrodes and sensors that could be easily put on and taken off. They also aim to develop a system that can be controlled with a tablet rather than a computer, making it smaller and more portable.

""It has been amazing to see the possibilities of sensory information coming from a device that was originally created to only allow me to control my hand in a one-way direction,"" Burkhart says.","['using', 'injured', 'feedback', 'researchers', 'signals', 'system', 'braincomputer', 'touch', 'hand', 'spinal', 'technology', 'injury', 'cord', 'restore', 'mans', 'burkhart', 'interface', 'sense']","While we might often take our sense of touch for granted, for researchers developing technologies to restore limb function in people paralyzed due to spinal cord injury or disease, re-establishing the sense of touch is an essential part of the process.
It was a big eureka moment when we first restored the participant's sense of touch.""
The participant in this study is Ian Burkhart, a 28-year-old man who suffered a spinal cord injury during a diving accident in 2010.
The subperceptual touch signals were artificially sent back to Burkhart using haptic feedback.
The new system allows the subperceptual touch signals coming from Burkhart's skin to travel back to his brain through artificial haptic feedback that he can perceive."
